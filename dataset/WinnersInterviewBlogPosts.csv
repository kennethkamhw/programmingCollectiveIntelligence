title,link,publication_date,content
Computer scientist Jure Zbontar on winning the Eurovision challenge,http://blog.kaggle.com/2010/06/09/computer-scientist-jure-zbontars-method-for-winning-the-eurovision-challenge/,2010-06-09 18:22:29,"My approach was actually quite simple. The only attributes I used where the approximate betting odds and the information on past voting. I sought patterns in the voting behaviour of all countries and combined that knowledge with this year's betting odds. I used cross-validation to select my model and to avoid overfitting it.<!--more-->

<strong>Predicting the finalists </strong>

I trusted the bookmakers on this one and just took the top ten countries from each semi-final group. I got the betting odds from Betfair.

<strong>Learning the voting patterns </strong>

A simple approach worked well enough here. The idea was to calculate, for each country, the average points awarded to each other country. Coming from Slovenia which was once part of Yugoslavia, together with Croatia, Serbia, Bosnia and Herzegovina, Macedonia and Montenegro, it is perhaps not surprising that our voting patterns are rather interesting:
<pre><code>  AVG' COUNTRY
10.38  Serbia
 8.53  Croatia
 8.00  Bosnia and Herzegovina
 5.91  Macedonia
 3.21  Norway
 3.17  Russia
 3.07  Greece
...
 0.18  Portugal
 0.17  Belarus
</code></pre>
It is painfully obvious that Slovenia is not judging the quality of the artist alone and it is well known that other countries follow similar patterns. It would, therefore, seem like a good idea to use this knowledge in predicting this year's voting.<!--more-->

The estimated average points awarded are not very stable, especially for newer countries. To remedy this, instead of using:
<pre><code>avg := sum(x) / |x|
</code></pre>
I used
<pre><code>avg' := (sum(x) + 1) / (|x| + 1)
</code></pre>
The new estimate got better results on the cross-validation tests.

<strong>Betting Odds</strong>

Using just the voting patterns of countries to predict this year's results was not enough. I had to, somehow, incorporate the approximate betting odds as well. Many approaches could have worked well here. In the end I opted for the one that gave the best cross-validation results.

I had to convert the approximate betting odds into something comparable with the average points awarded. I used:
<pre><code>odds'(ctr) := 1 / log(odds(ctr)) * a + b
</code></pre>
The coefficients a and b were chosen experimentally, as the ones that gave the best cross-validation score.

A small example will elucidate how I calculated the converted betting odds.
<pre><code>odds'(Croatia) = 1 / log(odds(Croatia)) * 4.4 + 0.8 =
               = 1 / log(48) * 4.4 + 0.8
               = 1.94
</code></pre>
The converted betting odds for the top and bottom countries were:
<pre><code>ODDS' COUNTRY
5.23  Azerbaijan
3.21  Germany
2.54  Armenia
...
1.94  Croatia
...
1.45  Slovenia
1.44  Bulgaria
1.44  Macedonia
1.44  Switzerland</code></pre>
<strong>Combining the voting patterns with the betting odds</strong>

It was now time to bring everything together. This was simply a matter of summing the average points awarded with the converted betting odds.

This was how I predicted Slovenia's votes for this year:
<pre><code>COUNTRY                  AVG'  ODDS'    SUM POINTS
Serbia                 10.38 + 1.84 = 12.21     12
Croatia                 8.53 + 1.94 = 10.47     10
Bosnia and Herzegovina  8.00 + 1.49 =  9.49      8
Macedonia               5.91 + 1.44 =  7.35      7
Azerbaijan              1.80 + 5.23 =  7.03      6
Norway                  3.21 + 2.01 =  5.22      5
Greece                  3.07 + 1.96 =  5.03      4
Sweden                  2.85 + 2.18 =  5.03      3
Russia                  3.17 + 1.62 =  4.79      2
Germany                 1.50 + 3.21 =  4.71      1
Denmark                 2.42 + 2.25 =  4.66      0
...
</code></pre>
We saw earlier that Slovenia's votes have little to do with song quality, as we usually award the top points to Balkan countries, no matter how bad they sing. The added betting odds should not influence the prediction of such countries considerably. On the other hand, if we take a country that is perhaps a bit more fair, like Israel, we see that the final predictions are affected to a greater extent:
<pre><code>COUNTRY                  AVG'   ODDS'    SUM POINTS
Armenia                 7.50 +  2.54 = 10.04     12
Azerbaijan              3.75 +  5.23 =  8.98     10
Russia                  7.23 +  1.62 =  8.85      8
Ukraine                 6.30 +  1.54 =  7.84      7
Romania                 6.07 +  1.61 =  7.68      6
Greece                  4.08 +  1.96 =  6.04      5
Georgia                 4.25 +  1.77 =  6.02      4
Iceland                 3.83 +  1.72 =  5.55      3
Serbia                  3.71 +  1.84 =  5.55      2
Denmark                 3.25 +  2.25 =  5.50      1
Sweden                  3.27 +  2.18 =  5.45      0
...
</code></pre>
<h2><span style=""font-size: 13px;"">Cross validation</span></h2>
<h2><span style=""font-weight: normal; font-size: 13px;"">The most important component of my solution was cross-validation and was probably the reason why I won the competition in the first place. It enabled me to try many different models and, between them, choose the one that was most likely to give the best results.</span></h2>
The dataset was split into partitions, one for each Eurovision event. I then proceeded to build the model on all but one partition and calculated the error of that model on the partition that was left out. The procedure was repeated so that each time a different partition was left out. This gave me a fair estimate of how the model performs on unseen data.

The cross-validation procedure in pseudocode:
<pre><code>function crossValidation(dataset, buildModel):
  error = 0
  for year in eurovisionEvents:
    learnData = {example | example in dataset and example.year != year}
    testData  = {example | example in dataset and example.year == year}
    model = buildModel(learnData)
    error += testModel(model, testData)
  return error</code></pre>
<pre><span style=""font-family: monospace, Monaco, 'Courier New', Courier, monospace;""><span style=""font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px; white-space: normal; font-size: 13px;""><strong>Conclusion </strong></span></span></pre>
<pre><span style=""font-family: monospace, Monaco, 'Courier New', Courier, monospace;""><strong><span style=""font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-weight: normal; line-height: 19px; white-space: normal; font-size: 13px;"">I am well aware that certain parts of my approach are not very strong. I had to do my best with the time that was available. I have many ideas for next year, which I will, for the moment at least, keep to myself :)</span></strong></span></pre>
I really enjoy competing in events like this and hope there will be more to come in the future."
How I won the  Predict HIV Progression data mining competition,http://blog.kaggle.com/2010/08/09/how-i-won-the-hiv-progression-prediction-data-mining-competition/,2010-08-09 12:35:46,"<strong>Initial Strategy</strong>

The graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:
<ol>
	<li>To make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.</li>
	<li>To make my training set closely match the test data in order to have a population similar to the test set.<!--more--></li>
</ol>
I   identified certain areas of the dataset that didn't appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.

Originally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.

<a href=""http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions.png""><img class=""alignnone size-large wp-image-484"" style=""border: 1px solid #CCC;"" title=""Chris Raimondi Progress in Submissions"" src=""http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions-1024x741.png"" alt="""" width=""633"" /></a>

<strong>Matching Controls</strong>

The group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).

This allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.

After a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.

<!--more-->

<strong>Recursive Feature Elimination via R 'caret' package</strong>

I  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.

I  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.

The rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.

I ran this function countless times, but this is part of the actual output for my last submission:

Variables Accuracy Kappa AccuracySD KappaSD Selected

[rows omitted]

90 0.7233 0.3148 0.04884 0.1121

120 0.7383 0.3493 0.05648 0.1393 *

150 0.7276 0.3225 0.04698 0.1153

[rows omitted]

The top 5 variables (out of 120):

VL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215

The  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.

Training via R 'caret' and ‘randomForest’ packages

I  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the <a href=""http://kaggle.com/chess"">Chess contest</a>."
How I did it: Lee Baker on winning Tourism Forecasting Part One,http://blog.kaggle.com/2010/09/27/how-i-did-it-lee-baker-on-winning-tourism-forecasting-part-one/,2010-09-27 18:30:25,"<strong>About me:</strong>
I’m an embedded systems engineer, currently working for a small engineering company in Las Cruces, New Mexico. I graduated from New Mexico Tech in 2007, with degrees in Electrical Engineering and Computer Science. Like many people, I first became interested in algorithm competitions with the Netflix Prize a few years ago. I was quite excited to find the Kaggle site a few months ago, as I enjoy participating in these types of competitions.

<strong>Explanation of Technique:</strong>
Though I tried several different methods, I used a weighted combination of three predictors to come up with the final forecast.

<!--more-->

#1: After reviewing Athanasopoulos et al, it became obvious that the naive predictor was a good algorithm with which to start. It is both easy to implement and performed well when compared to the other algorithms in the paper.

After graphing a few of the time series, it became apparent that many of the series increase with time. Indeed, the second sentence of the Athanasopoulos paper states that globally tourism has grown “at a rate of 6% annually.” In order to take advantage of this knowledge, I multiplied the  (Naive algorithm’s) predicted value by a factor to take this growth into account. With some testing, I determined a 5.5% growth factor to yield the lowest MASE.

prediction1 = last_value * (1.055 ** number_of_years_in_the_future)

#2: I examined fitting a polynomial line to the data and using the line to predict future values. I tried using first through fifth order polynomials to find that the lowest MASE was obtained using a first order polynomial (simple regression line). This best fit line was used to predict future values. I also kept the r**2 value of the fit for use in blending the results of the predictor.

#3: In thinking about these two predictors, I recognized that the naive predictor, though accurate, throws away most of the provided data, and only uses a single element of the time series. The polynomial line predictor uses all of the data, weighted equally, though the most recent data is probably of more value in indicating future performance than the earlier data in the time series. I examined and eventually used an exponentially-weighted least squares regression to fit a line to the data. This algorithm gave more accurate predictions for many of the time series by itself, and also lowered the MASE when used in combination with the two above predictors. The r**2 value of this fit was also used for blending the predictors.<!--more-->

<strong>Blending stage:</strong>
I started with a basic weighted blend of predictors. I used a constant weight factor for the modified naive predictor, while blending weights for the unweighted and weighted regression lines depending on the r**2 values found in fitting the time series. I selected the logistic function as a way of gradually increasing the weight with increasing r**2 value:

weight = a * logistic( b * (r**2 - c))

Values for a, b, and c were determined by trial and error. I also examined using some numeric optimization functions included in Python to minimize a training set MASE. While this succeeded in lower the training set MASE, I discarded this method when I received a lower leaderboard MASE (possibly from overfitting).

<strong>Testing / developing algorithms:</strong>
When testing algorithms, I used the last four years of the training dataset (after removing them from the data I was using for training) to test against. While this worked well in the initial stages, I found that once my leaderboard MASE got below about 2.05, this ‘training MASE’ became a much less reliable indicator of whether the leaderboard MASE would increase or decrease with a change. So, during the last few weeks of the contest, I primarily made small tweaks, and tested their value by submitting a new prediction to Kaggle rather than comparing my MASE results. I believe that this indicates a significant difference in nature of the last 4 years of the training set compared to the 4 years in the test set. If the test set includes data from 2008-2009, I’m speculating that depressed tourism numbers as a result of the global economic recession could have caused a significant difference in the trends.

<strong>Possible improvements:</strong>
While the above method seemed to work fairly well at predicting tourism numbers, there are several steps that could have likely improved the score. I only implemented the Naive method implemented in the Athanasopoulos paper; I do think that including a couple of other algorithms’ output into the final blend could have further increased the score. If I had a few more days to work on a solution, I would have tried to implement the Theta and ARIMA methods described in the paper and looked at the effects of including them in the blend.

I think an investigation into how to come up with a blending method that doesn’t use as much manual tweaking would also be of benefit.

I enjoyed participating in part one, and look forward to part two of the contest."
How I did it: The top three from the 2010 INFORMS Data Mining Contest,http://blog.kaggle.com/2010/10/11/how-i-did-it-the-top-three-from-the-2010-informs-data-mining-contest/,2010-10-11 13:31:35,"The 2010 INFORMS Data Mining Contest has just finished. The competition attracted entries from 147 teams with participants from 27 countries. The winner was Cole Harris, followed by Christopher Hefele and Nan Zhou. Here is some background on the winners and the techniques they applied.

<strong>Cole Harris</strong>

<em>About Cole:</em>

""Since 2002 I have been VP Discovery and cofounder of <a href=""http://www.exagen.com/"">Exagen Diagnostics</a>. We mine genomic/medical data to identify genetic features that are diagnostic of disease, predictive of drug response, etc. and then develop medical tests from the results. Prior to this (2000-2002), at Quasar/Magnaflux, I developed pattern recognition algorithms for identifying defective metal parts from acoustic spectral data.  From 1990-1999 I worked for Veritas Geophysical, most of that time developing algorithms for imaging seismic data. Prior to this I was in grad school (physics): MA 1990 Johns Hopkins University.""

<!--more-->

<em>Cole's Method:</em>

""As far as techniques, my submissions were mostly based on:

1. pre-processing - I did many things, but none seemed to make a large difference in my early results on test data, so in the end, other than exclude the non-price data, I didn't filter the data. I did append the data with data advanced 5 min, 60 min, and 65 min. So for each stock there were 16 features (open,hi,lo,last)X(0min,5min,60min,65min)

2. feature selection - forward stepwise selection of stocks, reverse stepwise selection of particular features for a given stock, evaluated using logistic regression. This resulted in 5-6 features selected from 2 stocks.

models: logistic regression and neural networks (not sure which won).""

<!--more-->

<strong>Christopher Hefele</strong>

<em>About Christopher: </em>

Christopher is a Systems Engineer at AT&amp;T. He was a member of The Ensemble, the team which finished second in the $1m Netflix Prize.

<em>Christopher's Method:</em>

""I was using a simple logistic regression on Variable 74 for most of the  contest. During the last few days, when every last bit counted, I  switched to a SVM &amp; added more variables (i.e. Variables 167 &amp;  55, chosen by forward stepwise logistic regression).

In the end, to me, this contest really was a  good lesson about the power of proper variable selection &amp;  preprocessing.""

<strong>Nan Zhou</strong>

<em>About Nan:</em>

Nan is currently completing his PhD in statistics at the University of Pittsburgh. His PhD research involves the estimation and prediction of  integrated volatility and model  calibration for financial stochastic processes. Prior to this he was a graduate student at Carnege Mellon University (focusing on statistical machine learning).

<em>Nan's Method</em>:

""Among lots of other models (Support Vector Machine, Random Forest, Neutral Network, Gradient Boosting, AdaBoost, and etc.) I finally used ‘Two-Stages’ L1-penalized Logistic Regression, and tune the penalty parameter by 5-folds Cross Validation.""

You can hear more from the winners (and others) on the <a href=""http://kaggle.com/view-postlist/forum-4-informs-data-mining-2010/topic-133-and-the-winner-is/task_id-2439"">competition's forum</a>."
How I did it: Jeremy Howard on finishing second,http://blog.kaggle.com/2010/11/19/how-i-did-it-jeremy-howard-on-finishing-second/,2010-11-19 17:39:47,"Wow, this is a surprise! I looked at this competition for the first time 15 days ago, and set myself the target to break into the top 100. So coming 2nd is a much better result than I had hoped for!...  I'm slightly embarrassed too, because all I really did was to combine the clever techniques that others had already developed - I didn't really invent anything new, I'm afraid. Anyhoo, for those who are interested I'll describe here a bit about how I went about things. I suspect in many ways the process is more interesting than the result, since the lessons I learnt will perhaps be useful to others in future competitions.

<!--more-->

I realised that, by starting when there was only 2 weeks to go, I was already a long way behind.  So my best bet was to leverage existing work as much as possible - use stuff which has already been shown to work! Also, I would have to stick to stuff I'm already familiar with, as much as possible. Therefore, I decided initially to look at Microsoft's TrueSkill algorithm: there is already a C# implementation available (a language which I'm very familiar with), and it's been well tested (both in practice, on XBox live, and theoretically, in various papers).

So, step one: import the data. The excellent FileHelpers library meant that this was done in 5 minutes.

Step two: try to understand the algorithm. Jeff Moser has a brilliant exposition about how TrueSkill works, along with full source code, which he most generously provides. I spent a few hours reading and re- reading this, and can't say I ever got to a point where I fully understood it, but at least I got enough of the gist to make a start. I also watched the very interesting Turing Lecture by Chris Bishop (who's book on pattern recognition was amongst the most influential books I've read over the years), which discusses the modern Bayesian Graphical Model approach more generally, and briefly touches on the TrueSkill application.

Step three: make sure I have a way to track my progress, other than through leaderboard results (since we only get 2 submissions per day). Luckily, the competition provides a validation set, so I tried to use that where possible. I only ever did my modelling (other than final submissions) using the first 95 months of data - there's no point drawing conclusions based on months that overlap with the validation set!

I also figured I should try to submit twice everyday, just to see how things looked on the leaderboard. My day one submission was just to throw the data at Moser's class using the default settings. I noticed that if I reran the algorithm a few times, feeding in the previous scores as the starting points, I got better results. So I ran it twice, and submitted that. Result: 0.696 (1st place was about 0.640 - a long way away!) (For the predictions based on the scores, assuming the scores for [white, black] are [s1, s2], I simply used (s1+100)/(s1+s2). The 100 on the top is give white a little advantage, and was selected to get the 54% score that white gets on average).

For the next few days, I went backwards. Rather than looking at graphs of score difference vs win%, I assumed that I should switch to a logistic function, which I did, and I optimised the parameters to the using a simple hill-climb algorithm. This sent my score back to 0.724. I also tried optimising the individual player scores directly. This sent my score back to 0.701. This wasted effort reminded me that I should look at pictures before I jump into algorithms. A graph of win% against white score (with separate lines for each quartile of black score), clearly showed the a logistic function was inappropriate, and also showed that there were interactions that I needed to think about.

So, after 5 days, I still hadn't made much improvement (minor tweaks to Trueskill params had got me to 0.691, barely any improvement from day 1). So I figured I needed a whole different approach. And now I only had 10 days to go...

It concerned me that Trueskill took each individual match and updated the scores after every one - it never fed the later results back to re-score the earlier matches. It turns out that (of course!) I wasn't the first person to think about this problem, and that it had been thoroughly tackled in the "" TrueSkill Through Time"" paper from MS Research's Applied Games Group. This uses Bayesian inference to calculate a theoretically-optimal set of scores (both mean and standard deviation, by player).

Unfortunately the code was written for an old version of F#, so it no longer works with the current version. And it's been a while since I've used F# (actually, all I've done with it is some Project Euler problems, back when Don Syme was first developing it; I've never actually done any Real Work with it). It took a few hours of hacking to get it to compile. I also had to make some changes to make it more convenient to use it as a class from C# (since it was originally designed to be consumed from an F# console app). I also changed my formula for calculating predictions from scores, to use a cumulative gaussian - since that is what is suggested in the TrueSkill Through Time paper. My score now jumped to 0.669.

The paper used annual results, but it seemed to me that this was throwing away valuable information. I switched to monthly results, which meant I had to find a new set of parameters appropriate for this very different situation. Through simple trial and error I found which params were the most sensitive, and then used hill-climbing to find the optimum values. This took my score to 0.663.

Then I added something suggested in the Chessmetrics writeup on the forum - I calculated the average score of the players that each person played against. I then calculated a weighted average of each player's actual score, and the average of their opponents. I used a hill-climb algorithm to find the weighting, and also weighted it by the standard deviation of their weighting (as output by Trueskill/Time). This got me to 0.660 - 20th position, although later someone else jumped above me to push me to 21st.

The next 5 days I went backwards again! I tried an ensemble approach (weighted average of TrueSkill, TrueSkill/Time, and ELO), which didn't help - I think because TrueSkill/Time was so much better, and also because the approaches aren't different enough (ensemble approaches are best when combining approaches which are very different). I tried optimising some parameters in both the rating algorithm, and in the gaussian which turns that into probabilities for each result. I also tried directly estimating and using draw probabilities separate from win probabilities.

I realised that one problem was that my results on the validation set weren't necessarily showing me what would happen on the final leaderboard. I tried doing some resampling of the validation set, and realised that different samples gave very different results. So, the validation set did generally effectively show the impact when I made a change which was based on a solid theoretical basis, but it was also easy to get meaningless increases by thoughtless parameter optimisation.

On Nov 15 I finally made an improvement - previously in the gaussian predictor function I had made the standard deviation a linear function of the overall match level [i.e. (s1+s2)/2]. But I realised from looking at graphs that really it's that a stronger black player is better at forcing a draw - it's really driven by that, not by the combined skill. So I made the standard deviation a linear function of black's skill only. Result: 0.659.

So, it was now Nov 16 - two days to go, and not yet even in the top 20! I finally decided to actually carefully measure which things were most sensitive, so that I could carefully manage my last 4 submissions. If I had been this thorough a week ago, I wouldn't have wasted so much valuable time! So, I discovered that the following had the biggest impact on the validation set:

* - Removing the first few months from the training data; removing the first 34 months was optimal for the validation set, so I figured removing the first 40 months would be best for the full set
* - Adjusting the constant in the calculation of the gaussian's standard deviation - if too high, the predictions varied too much, if too little, the predictions were all too close to 0.5
* - And a little trick: I don't know much (anything!) about chess, but I figured that there must be some knockout comps, so people who play more perhaps are doing so because they're not getting knocked out! So, I tried using the count of a player's matches in the test set as a predictor! It didn't make a huge difference to the results, but every little bit counts...

Based on this, my next 3 submissions were:

* - Remove first 40 months: 0.658
* - Include count of matches as a prediction: 0.654
* - Increase the constant in the stdev formula by 5%: 0.653

(My final submission was a little worse - I tried removing players who hadn't played at least 2 matches, and I also increased the weight of the count of matches: back to 0.654).

For me, the main lesson from this process has been that I should more often step back and think about the fundamentals. It's easy to get lost in optimising the minor details, and focus on the solution you already have. But when I stepped away from the PC for a while, did some reading, and got back to basics with pen and paper, is when I had little breakthoughs.

I also learnt a lot about how to use validation sets and the leaderboard. In particular, I realised that when you're missing a fundamental piece of the solution, then little parameter adjustments that you think are improvements, are actually only acting as factors that happen to correlate with some other more important predictor. So when I came across small improvements in the validation set, I actually didn't include them in my next submitted answer - I only included things that made a big difference. Later in the competition, when I had already included the most important things, I re-tested the little improvements I had earlier put aside.

Please let me know if you have any questions. I would say that, overall, TrueSkill would be a great way to handle Chess leaderboards in the future. Not because it did well in this competition (which is better at finding historical ratings), but because, as shown in Chris Bishop's talk, it is amazingly fast at rating people's ""true skill"". Just 3 matches or so is enough for it to give excellent results.

<strong>Here's a little pic of my submission history</strong>

<strong>[gallery link=""file""]
</strong>

The points show the public leaderboard score, and the final score, for each submission. As you can see, they are very closely related (r^2 = 0.97). Kaggle has previously shown that a pool of other submissions from the leaderboard do not have such a close relationship - I think this is because I only had a short time, so my submissions really needed to be based on a solid theoretical basis, rather than parameter tuning. Although parameter tuning can easily overfit, you can see from the above picture that carefully targeted changes can allow you can get a good idea of how you're doing from the leaderboard. (I also found a similar level of relationship between my validation set results, and the leaderboard results).

The lines connecting the points show the progress of my scores over time. You can clearly see how after the first few days, going from standard TrueSkill over to TrueSkill Through Time, the results make a sudden jump. You can see how up until that time, I was mainly going backwards!

You can also see after I switched to TrueSkill Through Time, I had a few days of going nowhere - up until the last few days, when I forced myself to take a more thoughtful approach, due to the lack of time.

One takeaway from this chart, I think, is to note that a few days of failure is no reason to give up - improvements tend to be sudden, as new insights are made, rather than gradual. So, doing well in these competitions requires a certain amount of resilience - even when things are going badly on the leaderboard, you're skill learning, and still have the chance to make improvements as long as you're still trying.

Another takeaway is that it's better to try lots of completely different things, rather than trying to precisely tune whatever you have so far. At least in this competition, for my results, I saw big improvements from adding substantial pieces to the algorithm, and not much from fine-tuning the details."
How I did it: Martin Reichert on 3rd place in Elo comp,http://blog.kaggle.com/2010/11/20/how-i-did-it-martin-reichert-on-3rd-place-in-elo-comp/,2010-11-20 08:11:56,"<em>About Martin:</em>

Martin is a retired Senior Project Manager and IT Consulting Manager at Siemens AG with a university degree in physics.  He now likes to develop and improve rating methods, especially regarding professional boxing ratings - see <a href=""http://boxrec.com/"">http://boxrec.com</a> - maybe I will launch a competition regarding these ratings in the future ...
<!--more-->
<em>Martin’s Method:</em>

“I first evaluated established ratings like Elo and Chessmetrics - and found Chessmetrics a very promising approach, regarding the parameters: performance, opposition quality, activity, weighting and self consistant rating over a time period. By varying the parameters and algorithms and evaluating the predictions against the cross validation data, I step by step could improve my score. Finally my last submission #50 with my best public score was not the submission with my best final score. My prize-winning submission turned-out to be my submission #23 - so never give up.”

<a href=""http://kaggle.com/blog/wp-content/uploads/2010/11/elo_res.png""><img class=""alignleft size-medium wp-image-614"" title=""Development of public and final scores"" src=""http://kaggle.com/blog/wp-content/uploads/2010/11/elo_res-300x212.png"" alt="""" width=""300"" height=""212"" /></a>"
Philipp Weidmann (5th in the Elo comp) on chess ratings and numerical optimization,http://blog.kaggle.com/2010/11/27/philipp-weidmann-5th-in-the-elo-comp-on-chess-ratings-and-numerical-optimization/,2010-11-27 17:52:51,"<!-- .formula { 	text-align: center; 	font-family: serif; 	margin: 1.2em 0; } div.formula { 	padding: 0.5ex; 	margin-left: auto; 	margin-right: auto; } td.formula-l { 	text-align: left; 	padding: 0.2ex; 	border: 0ex; } td.formula-c { 	text-align: center; 	padding: 0.2ex; 	border: 0ex; } td.formula-r { 	text-align: right; 	padding: 0.2ex; 	border: 0ex; } -->

Having participated in the contest almost from the beginning and posting 162 submissions by the end, I have tried a large variety of different prediction approaches. The first of them were Elo-based, using ratings updated iteratively as the games were read in sequentially, later ones had <span class=""versalitas"">Chessmetrics</span>-style simultaneous ratings which eventually culminated in the non-rating, graph theory-based prediction system which held the top spot in the leaderboard for the past weeks yet ended up finishing somewhere in the vicinity of 15th place.

<!--more-->

In between, though, when my progress stalled and I was slowly being pushed back through the ranks, I took a few days’ time to rethink rating systems from the very core, with the idea that a purely mathematical approach might be the ingredient I needed in order to advance again. While my submissions using this approach were only moderately successful, I do think that this is the most interesting and intuitive idea that I tried in the contest, and therefore have decided to write this blog post about it.

<strong>Part I. Ideal Ratings</strong>

The basic mathematical idea behind every rating system is the following:

Let <span class=""formula"">{<em>p</em><sub><em>k</em></sub>}</span> be the set of players. Consider the set of games <span class=""formula"">{<em>g</em><sub><em>k</em></sub>}</span>; we wish to predict their individual outcomes (i.e. the white scores) <span class=""formula"">{<em>s</em><sub><em>k</em></sub>}</span> by using only two numbers for each game <span class=""formula""><em>g</em><sub><em>k</em></sub></span>: The rating <span class=""formula""><em>r</em><sub><em>w</em></sub> = <em>r</em>(<em>w</em>(<em>g</em><sub><em>k</em></sub>))</span> of the white player <span class=""formula""><em>w</em>(<em>g</em><sub><em>k</em></sub>)</span>, and the rating <span class=""formula""><em>r</em><sub><em>b</em></sub> = <em>r</em>(<em>b</em>(<em>g</em><sub><em>k</em></sub>))</span> of the white player <span class=""formula""><em>b</em>(<em>g</em><sub><em>k</em></sub>)</span>. We are therefore looking for a set of ratings <span class=""formula"">{<em>r</em><sub><em>k</em></sub>}</span> (one rating assigned to each player) with the following property:
<p class=""formula""></p>
<a class=""eqnumber"" name=""eq:Rating condition"">(1) </a><em>s</em><sub><em>k</em></sub> = <em>f</em>(<em>r</em><sub><em>w</em></sub>, <em>r</em><sub><em>b</em></sub>)

for some function <span class=""formula""><em>f</em></span>, the <em>prediction function</em>.

Of course, this condition is impossible to fulfill in most practical settings. When at least two of the games feature the same two players playing white and black and their outcomes are different, <a class=""Reference"" href=""#eq:Rating condition"">(1↑)</a> cannot hold for both of these games simultaneously.

<strong>Part II. “Optimal Ratings”</strong>

For simplicity’s sake, let the prediction function <span class=""formula""><em>f</em></span> in <a class=""Reference"" href=""#eq:Rating condition"">(1↑)</a> be defined as
<p class=""formula""></p>
<a class=""eqnumber"" name=""eq-2"">(2) </a><em>f</em>(<em>r</em><sub><em>w</em></sub>, <em>r</em><sub><em>b</em></sub>) = <em>r</em><sub><em>w</em></sub> − <em>r</em><sub><em>b</em></sub> + <span class=""bar""><em>s</em></span>

with <span class=""formula""><span class=""bar""><em>s</em></span></span> denoting the average outcome, or average white score (in the competition’s training dataset, <span class=""formula""><span class=""bar""><em>s</em></span> = 0.5456…</span>). Thus between two players of equal rating, the predicted score will precisely match the average score <span class=""formula""><span class=""bar""><em>s</em></span></span>.

This linear formulation is a very good approximation to the “ideal” prediction function, as shown by <span class=""versalitas"">Jeff Sonas</span> with his <span class=""versalitas"">Chessmetrics</span> system. It also has the advantage that it can easily be “inverted”, meaning that it is easy to find a function <span class=""formula""><em>h</em></span> which, when supplied a game score between two players, gives the <em>expected rating difference</em> that is a maximum likelihood estimator for the players’ actual rating difference. Namely,
<p class=""formula""></p>
<a class=""eqnumber"" name=""eq-3"">(3) </a><em>h</em>(<em>s</em><sub><em>k</em></sub>) = <em>s</em><sub><em>k</em></sub> − <span class=""bar""><em>s</em></span>

This function allows us to recast the condition <a class=""Reference"" href=""#eq:Rating condition"">(1↑)</a> as follows:
<p class=""formula""></p>
<a class=""eqnumber"" name=""eq-4"">(4) </a><em>s</em><sub><em>k</em></sub> − <span class=""bar""><em>s</em></span> = <em>r</em><sub><em>w</em></sub> − <em>r</em><sub><em>b</em></sub>

that is, the expected rating difference should match the actual one.

While we have only been considering the individual games <span class=""formula""><em>g</em><sub><em>k</em></sub></span> and their outcomes <span class=""formula""><em>s</em><sub><em>k</em></sub></span> so far, it is easy to formulate (weaker) conditions which look at the <em>average</em> score between two players, and thus more closely approximate their “true” relative strengths:
<p class=""formula""></p>
<a class=""eqnumber"" name=""eq:Reformulated condition"">(5) </a><em>s</em><sub><em>p</em><sub>1</sub>, <em>p</em><sub>2</sub></sub> − <span class=""bar""><em>s</em></span> = <em>r</em><sub><em>p</em><sub>1</sub></sub> − <em>r</em><sub><em>p</em><sub>2</sub></sub>

where <span class=""formula""><em>s</em><sub><em>p</em><sub>1</sub>, <em>p</em><sub>2</sub></sub></span> denotes the average score <span class=""formula""><em>p</em><sub>1</sub></span> has achieved against <span class=""formula""><em>p</em><sub>2</sub></span>.

Recognizing that many conditions of this type interlinking the players may be difficult to fulfill, we arrive at the important idea of restating <a class=""Reference"" href=""#eq:Reformulated condition"">(5↑)</a> as a minimization problem:
<p class=""formula""><a class=""eqnumber"" name=""eq:Minimization condition"">(6) </a>min<sub><em>r</em><sub><em>p</em><sub>1</sub></sub>, <em>r</em><sub><em>p</em><sub>2</sub></sub></sub><span class=""symbol"">∥</span><em>r</em><sub><em>p</em><sub>1</sub></sub> − <em>r</em><sub><em>p</em><sub>2</sub></sub> − <em>s</em><sub><em>p</em><sub>1</sub>, <em>p</em><sub>2</sub></sub> + <span class=""bar""><em>s</em></span><span class=""symbol"">∥</span></p>
Expanding the scope to look at all player connections simultaneously, this becomes
<p class=""formula""><a class=""eqnumber"" name=""eq:Global minimization condition"">(7) </a>min<sub>{<em>r</em><sub><em>k</em></sub>}</sub><span class=""bigsymbol"">∑</span><span class=""limits""><sup class=""bigsymbol""><span class=""symbol"">|</span>{<em>p</em><sub><em>k</em></sub>}<span class=""symbol"">|</span></sup><sub class=""bigsymbol""><em>i</em>, <em>j</em> = 1</sub></span><em>n</em>(<em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub>)<span class=""symbol"">∥</span><em>r</em><sub><em>p</em><sub><em>i</em></sub></sub> − <em>r</em><sub><em>p</em><sub><em>j</em></sub></sub> − <em>s</em><sub><em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub></sub> + <span class=""bar""><em>s</em></span><span class=""symbol"">∥</span></p>
with the additional idea that the higher the number of games <span class=""formula""><em>n</em>(<em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub>)</span> between two players <span class=""formula""><em>p</em><sub><em>i</em></sub></span> and <span class=""formula""><em>p</em><sub><em>j</em></sub></span> becomes, the more weight their individual condition <a class=""Reference"" href=""#eq:Minimization condition"">(6↑)</a> carries, which makes sense because their score average <span class=""formula""><em>s</em><sub><em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub></sub></span> carries more information. In a mathematical sense, ratings generated using <a class=""Reference"" href=""#eq:Global minimization condition"">(7↑)</a> can justifiably be called “optimal”. Note that for players that didn’t face each other at all (i.e. the vast majority of pairings), <span class=""formula""><em>n</em>(<em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub>)</span> will be zero, and thus the evaluation of the term inside the norm does not matter for such pairings.

<strong>Part III. Implementing “Optimal Ratings”</strong>

From a numerical standpoint, <a class=""Reference"" href=""#eq:Global minimization condition"">(7↑)</a> is an unconstrained, quadratic (if <span class=""formula""><span class=""symbol"">∥</span>⋅<span class=""symbol"">∥</span></span> is taken to be the Euclidean norm) optimization problem - with, in case of the competition, 8631 variables. This is large, especially if you don’t have a supercomputer available, but not intractible. Ordinary computer algebra systems, however, will likely not be up to the task, because the choice of optimization method is paramount for a sensible runtime.

I used a <em>truncated Newton method</em> with line search and analytically computed first derivatives, implemented in Fortran 90 (Yes!) by <span class=""versalitas"">Stephen Nash</span> (http://jblevins.org/mirror/amiller/tn.zip), which after some modifications gave convergence times of less than 3 minutes on my notebook. The ratings were then used to generate predictions via <a class=""Reference"" href=""#eq:Rating condition"">(1↑)</a>. I still remember the excitement I felt when I uploaded my first optimization-based prediction to Kaggle; naturally, I expected it to do really well, given the hyper-advanced number crunching algorithms used.

That is, until I saw the public score. It was by far my worst one at that time.

At first I couldn’t believe it. I uploaded the same prediction again, with an identical result. Then I started looking for a bug in my code, but couldn’t find one. What was going on?

<strong>Part IV. Revisiting the Optimization Problem</strong>

Where there is no software bug, there is likely a flaw within the original line of reasoning. It took a while to find it, but in the end I figured it out.

Consider the following crosstable of games played among three players, with the average score given in brackets:
<table border=""1"">
<tbody>
<tr>
<td width=""100"" align=""center"" valign=""top"">Player</td>
<td width=""100"" align=""center"" valign=""top"">1</td>
<td width=""100"" align=""center"" valign=""top"">2</td>
<td width=""100"" align=""center"" valign=""top"">3</td>
</tr>
<tr>
<td width=""100"" align=""center"" valign=""top"">1</td>
<td width=""100"" align=""center"" valign=""top"">-</td>
<td width=""100"" align=""center"" valign=""top"">3 (1.0)</td>
<td width=""100"" align=""center"" valign=""top"">0</td>
</tr>
<tr>
<td width=""100"" align=""center"" valign=""top"">2</td>
<td width=""100"" align=""center"" valign=""top"">3 (0.0)</td>
<td width=""100"" align=""center"" valign=""top"">-</td>
<td width=""100"" align=""center"" valign=""top"">2 (1.0)</td>
</tr>
<tr>
<td width=""100"" align=""center"" valign=""top"">3</td>
<td width=""100"" align=""center"" valign=""top"">0</td>
<td width=""100"" align=""center"" valign=""top"">2 (0.0)</td>
<td width=""100"" align=""center"" valign=""top"">-</td>
</tr>
</tbody>
</table>
We see that player 1 has been doing really well against player 2; his score is 100%. Similarly, player 2 has a 100% score against player 3, while player 1 and player 3 did not play against each other.

A system based on <a class=""Reference"" href=""#eq:Global minimization condition"">(7↑)</a> would make the rating difference between player 1 and player 3 unreasonably large, because there is no connection between those players at all. The optimization engine is thus free to vary the rating distance of those players, even to the point where it generates ratings that would fit those of Rybka and a monkey, respectively.

The solution is to tie the ratings of all unconnected players together, with the sensible assumption that players in the same pool are (very roughly) of the same class, and thus of similar strength. This gives rise to a new, slightly more complex optimization problem:
<p class=""formula""><a class=""eqnumber"" name=""eq:Refined minimization condition"">(8) </a>min<sub>{<em>r</em><sub><em>k</em></sub>}</sub><span class=""bigsymbol"">∑</span><span class=""limits""><sup class=""bigsymbol""><span class=""symbol"">|</span>{<em>p</em><sub><em>k</em></sub>}<span class=""symbol"">|</span></sup><sub class=""bigsymbol""><em>i</em>, <em>j</em> = 1</sub></span><em>c</em>(<em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub>)</p>
where
<p class=""formula""><a class=""eqnumber"" name=""eq:-1"">(9) </a><em>c</em>(<em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub>) =</p>

<table class=""cases"">
<tbody>
<tr>
<td class=""formula-l""><em>n</em>(<em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub>)<span class=""symbol"">∥</span><em>r</em><sub><em>p</em><sub><em>i</em></sub></sub> − <em>r</em><sub><em>p</em><sub><em>j</em></sub></sub> − <em>s</em><sub><em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub></sub> + <span class=""bar""><em>s</em></span><span class=""symbol"">∥</span></td>
<td class=""formula-l""><em>n</em>(<em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub>) &gt; 0</td>
</tr>
<tr>
<td class=""formula-l""><em>α</em><span class=""symbol"">∥</span><em>r</em><sub><em>p</em><sub><em>i</em></sub></sub> − <em>r</em><sub><em>p</em><sub><em>j</em></sub></sub><span class=""symbol"">∥</span></td>
<td class=""formula-l""><em>n</em>(<em>p</em><sub><em>i</em></sub>, <em>p</em><sub><em>j</em></sub>) = 0</td>
</tr>
</tbody>
</table>
The scaling factor <span class=""formula""><em>α</em></span> is necessary because without it, the ties <span class=""formula""><span class=""symbol"">∥</span><em>r</em><sub><em>p</em><sub><em>i</em></sub></sub> − <em>r</em><sub><em>p</em><sub><em>j</em></sub></sub><span class=""symbol"">∥</span></span> bind the ratings much too tightly together: Of <span class=""formula"">8631<sup>2</sup> ≈ 74.5</span> million parts in the sum, only about 57,000 (or less than 0.1%) are connections between players (made by games played), while the overwhelming rest are conditions basically stating that all ratings should be equal, which is what they will be if <span class=""formula""><em>α</em> = 1</span>. However, I have found a scaling factor of <span class=""formula""><em>α</em> = 0.0001</span> to work quite well in practice, and indeed, minimizing <a class=""Reference"" href=""#eq:Refined minimization condition"">(8↑)</a> with this factor gives results similar in quality to that of <span class=""versalitas"">Chessmetrics</span>, provided that some additional tricks are used (such as a precisely controlled rating pool). This is no surprise, since <span class=""versalitas"">Chessmetrics</span> is essentially an algorithm designed to accomplish this very optimization (with some slight differences). However, the advantage of this direct numerical approach is that deep insights into the mechanics of the rating system are easier to gain and implement, and doing so might give birth to some completely new ideas.

In the contest I soon found a different approach which didn’t require optimization and gave better results, so I abandoned this technique in favor of it. Nevertheless, optimization might be worth investigation further as a means for both prediction and rating generation, in chess and in other sports."
How we did it: David Slate and Peter Frey on 9th place in Elo comp,http://blog.kaggle.com/2010/11/23/how-we-did-it-david-slate-and-peter-frey-on-9th-place-in-elo-comp/,2010-11-23 19:35:02,"Our team, ""Old Dogs With New Tricks"", consists of me and Peter Frey, a former university professor.  We have worked together for many years on a variety of machine learning and other computer-related projects. Now that we are retired from full-time employment, we have endeavored to keep our skills sharp by participating in machine learning and data mining contests, of which the chess ratings contest was our fourth.
<!--more-->
Our approach to this contest has been to treat it primarily as a forecasting problem, not as an exercise in developing a chess ratings system.  We built forecasting models from the training data using a home-grown variant of ""Ensemble Recursive Binary Partitioning"", a method we have previously employed for other contests and
applications.  Like various other machine-learning/forecasting methods, this one trains a model on data consisting of records (e.g. cases, instances, objects, or in this case chess games) for which the values of a set of predictor variables and an outcome variable are known, and then applies the model to estimate (or forecast) the outcome values for a separate set of test or production data records for which the predictor values but not the outcomes are known.

Since each game in the chess dataset was supplied with a minimal amount of information (month, player id's, and result), we synthesized a variety of predictor variables based on an analysis of the dataset. We attempted to optimize parameter settings and variable selections by observing model performance both on the leaderboard set and on various holdout sets created from the last 5, 7, or 10 months of the training data.  By the end of the competition we had done over 1100 runs building models on part of the training data and testing them on the remainder, and had created and tested various combinations of approximately 65 home-grown predictors.

Of our 120 submissions, the 93rd on November 4 produced the best score on the official test data, 0.699472.  The model used for that submission employed 22 predictors:

1, 2:	White and Black player skill rankings calculated by an iterative process from the results of the training games.

3, 4:	Counts of ""quality"" games for White and Black players used in calculating vars 1 and 2.  ""quality"" games are those for which one's opponent has a skill level that is not too dissimilar from one's own.

5, 6:	Average rankings of White's and Black's opponents as calculated for vars 1 and 2.

7, 8:	Average number of games per month up to this game for White and Black.

9, 10: White and Black ratings calculated from the training data according to an ELO-like algorithm that evolves ratings chronologically from a fixed starting value.

11, 12: Maximum ratings (as calculated for vars 9 and 10) of opponents beaten by White and Black up to this game.
	
13, 14: Minimum ratings (as calculated for vars 9 and 10) of opponents lost to by White and Black up to this game.

15, 16: Mean ratings of White's and Black's previous opponents.

17, 18: Mean scores of White and Black against their common opponents.

19, 20: Rating difference between White's next and current opponents. Similarly for Black.

21, 22: Rating difference between White's current and previous opponents. Similarly for Black.

The computing resources employed for the contest consisted of a few workstations running the Linux operating system.  Our core general purpose data analysis and forecasting engine is written in ANSI C, but was driven by code specific to this contest written in the scripting language Lua. 
"
How we did it: Jeremy Howard on winning the tourism forecasting competition,http://blog.kaggle.com/2010/11/24/how-we-did-it-jeremy-howard-on-winning-the-tourism-forecasting-competitoin/,2010-11-24 19:47:13,"The following Q&amp;A is with Jeremy Howard who together with teammate Lee Baker won Kaggle's Tourism Forecasting Competition. (This was a two-part competition - Lee previously described his work on Part I of the competition in a <a href=""http://kaggle.com/blog/2010/09/27/how-i-did-it-lee-baker-on-winning-tourism-forecasting-part-one/"">separate blog post</a>).
<strong> <!--more--></strong>

<strong>- where you're from, what you have studied</strong>

I live in Melbourne, Australia, recently voted the world's 3rd most livable city... Perhaps this is inspiring a bit of data mining success around here (it's also the home of the tourism2 winner, and the Kaggle CEO)! I studied philosophy at the University of Melbourne.

<strong>- what you do</strong>

After university I worked in management consulting (McKinsey &amp; Co, AT Kearney), and then went on to found 2 businesses (FastMail.FM, an email provider, and Optimal Decisions Group, an insurance pricing optimisation specialist). Having sold both in the last couple of years, I now have the free time to follow my interests.

<strong>- core technical approach</strong>

My goal was to try to stay in line with the approach taken in the paper being submitted by the contest organisers - I wanted to find a general, automated algorithm for forecasting, which I could apply to all time series without any parameter tuning or manual involvement. I had hoped therefore to only do a single submission to the leaderboard. However, an early data problem in the posted data (later rectified by the organisers) unfortunately meant this wasn't possible. After the fixed data was posted, I only did 3 further submissions.

I realised that a fundamental issue was that the final results were calculated using a novel algorithm called ""MASE"", which is a ratio. The denominator of the ratio could in some cases be extremely small - this occured in series which had close to constant additive seasonality, no growth, and no noise. I found that the contribution of these series to the overall result was so high that in practice the algorithm should be tuned to favor these, even at the expense of other series (which had a relatively high denominator, and thus contributed much less to the overall result).

To do this, I only used linear growth (as opposed to exponential) and additive seasonality (as opposed to multiplicative) for all series, since any series which had exponential growth and/or multiplicative seasonality would have very small weights in the overall metric. Later, I experimented with allowing some series to use exponential growth and multiplicative seasonality, if the statistical evidence for those series was particularly strong, and confirmed that the impact was negative, as expected.

<strong>- methodology that proved most effective</strong>

I first created an algorithm to automatically remove outliers. Outliers can occur in, for example, a tourism time series if an area has a once-off event (positive outlier), or temporary closing of a major attraction (negative outlier), which will not impact future results. I used a customised local smoother, and used the residuals to determine seasonality. I ran this twice to create a double-smoothed time series, which I then compared to the original data, and removed data points outside 3 standard deviations of residuals.

I then fitted a weighted regression (weighted the most recent observations the most heavily) combined with weighted additive seasonality (again weighting the most recent observations the most heavily) on all but the last 2 years of each series. A simple optimiser found the optimal weighting of each in order to predict the final 2 years. This weighted model was then applied to the full data set to create predictions. The intercept of the weighted regression was adjusted such that the residual on the final observation was always zero - this was important for ensuring that the series with a low denominator in the MASE metric were forecast as accurately as possible.

I've since realised I had some bugs in my code (e.g. failing to truncate series to be positive, and some bugs with going from a validation set to the final predictions). It would be interesting to see how much better the predictions would be if these bugs were fixed.

The whole algorithm takes only 10 seconds to complete for the entire set of time series. Since the algorithm is fast, accurate, and automated, I think it is a good system for automated time series prediction. I plan to test it in the future on other data sets (e.g. the ""M3"" time series prediction competition data) to confirm that it can be effectively applied to other types of data.

<strong>- what first attracted you to the competition? </strong>

The tourism forecasting competition was my first data mining contest - I entered it in order to try to update and strengthen my analysis skills, and to learn something new (having never done time series forecasting before).

<strong>- did you do much background reading or research? </strong>

Yes, I read most of the recent papers and online tutorials by one of the conference organisers, Rob J Hyndman. I found that they were a great way for a time-series newbie like myself to get up to speed with the topic.

<strong>- what tools and programming language did you use?</strong>

I used C#, in Visual Studio 2010.

<strong>- how much time did you spend on the competition?</strong>

I spent longer than I expected because the initial data problems left me stumped and confused! Once they were fixed, I had submitted my result within a couple of hours. I estimate I spent a couple of weeks on the problem, including reading and research."
How I did it: Diogo Ferreira on 4th place in Elo chess ratings competition,http://blog.kaggle.com/2010/11/30/how-i-did-it-diogo-ferreira-on-4th-place-in-elo-chess-ratings-competition/,2010-11-30 19:26:49,"My first contact with the inner workings of the Elo rating system was in the mid-90s, when I came across an article in the Europe Echecs magazine. I remember thinking that the problem of ranking chess players was much different from chess itself, so I didn’t pay much attention to it at the time.

<!--more-->

In August 2010, when I was checking some news on slashdot.org, I was surprised to find a post on a chess ratings competition. I thought it would be interesting to give it a try, and perhaps I could spend two or three days of my holidays on it. I never thought that during the following 15 weeks(!) I would be so absorbed by the competition, with an average of one submission a day.

There were times when my laptop (a 2.4GHz dual core) would be running for days on end with both processors running two different parameter tunings. There was also a Saturday morning when I got up in the middle of the night to check how the tuning was going. At 5am I decided to do another submission; it was my 51st submission, and it was the one that made it to 4th<sup> </sup>place.

The thing that got me so enthusiastic about the competition was more than the challenge itself. It was the fact that dozens of other people were thinking about the same problem, and that every day there was some change in the leaderboard, either I was able to jump a few positions, or someone else just popped out with an amazing score! It was this continuous challenge that I found so thrilling.

Back in August I started thinking about the problem with pencil and paper, while doing a few trial submissions. From the beginning I looked at ratings as an indication of the probability of one player beating another, and then devised a probabilistic approach where the rating of each player was something between 0 and 1. Later I found that this approach was very much related to the well-known Bradley-Terry model.

The details can be found in the link below, together with an implementation (source code) in Python 2.6:

http://web.tagus.ist.utl.pt/~diogo.ferreira/chess/

I’m making this available in the hope that others will be able to improve it even further (and also share the results). One thing I learned from this experience is that the spirit of competition can push things forward, and push things with such impetus that I'm convinced all of us together did more progress in a few months than one alone could have achieved over several years.

<em>Diogo R. Ferreira is professor of information systems at the Technical University of Lisbon, where he works in the area of process mining.</em>"
How we did it: Jie and Neeral on winning the first Kaggle-in-Class competition at Stanford,http://blog.kaggle.com/2010/12/13/how-we-did-it-jie-and-neeral-on-winning-the-first-kaggle-in-class-competition-at-stanford/,2010-12-13 20:28:19,"Neeral (<em>@beladia</em>) and I (<em>@jacksheep</em>) are glad to have participated in the first Kaggle-in-Class competition for Stats-202 at Stanford and we have learnt a lot! With one full month of hard work, excitement and learning coming to an end and coming out as the winning team, it certainly feels like icing on the cake. The fact that both of us were looking for nothing else than winning the competition, contributed a lot to the motivation and zeal with which we kept going each and every day. Each of us may have spent about 100 hours on this competition, but it was totally worth it.

<!--more-->

Even though the professor and textbook already mentioned some points which ended up to be very useful, the experiences we gained from this practice were much more influential to us. We strongly recommend every data mining student to participate in such competitions, make your hands dirty, and eventually every minute you invest on it will reward back.

<strong>ANALYZE THE DATA FIRST </strong>

The most important lesson we learnt was: we should always analyze the data first.

As newbies in data mining, we had thought a cool model could give us the best result, so the main effort was to find such advanced models and keep tuning it. We used the features from the raw data, did some feature transformation to deal with missing values, and tried some supervised learning model, such as lm, randomForest and glmboost in R. It seemed working, because Random Forest could improve 18% comparing to linear regression.

However, we found the top 3 teams were more than 30% better than our result. We felt our approach was not on the right track, because we didn’t believe any model on the same features could achieve such big improvement. So we switched the focus to the data itself and began studying its characteristics. We hoped to find clues from the data: was there any trick we didn’t figure out yet?

Soon later, we found about 30% of test instances could exactly match instances in training set and got 0 error. Naturally, the next question was: for each of the rest 70% of the test instances, how to find the best training instance approximate the price for the test instance? Intuitively, a wine in the same location with most similar name and the closest year could be a good candidate. Following this idea, we thought k-NN may be a good model for this data.

The next things became relatively easy: how to define the distance between instances. We explored different distances and tried some optimizations, and finally combined the results from different k-NN modes to reduce the variance, which was inspired by the idea of ensemble methods.

In a word, we would not have thought of the k-NN model without carefully studying the data. If you only can remember one thing from this blog, we hope it is: <em>analyze the data first</em>.

<strong>MODELING</strong>

The main model we used was k-NN (k-Nearest Neighbor, with k=1) using edit distance, tf-idf cosine similarity and lucene score on wine titles as the distance metrics. We found k-NN achieved much better performance (40+% improvement) than other supervised learning methods that we tried; and it works well even when there are many missing values. One challenge to use k-NN is that the computation cost is very high. We solved this problem by clustering the wines based on a fewer indexes, such as location (the combination of country, sub-region and appellation), cleaned name (after removing the descriptive information), short name (the first 2 words of length &gt;=3 in cleaned name), brand name (the string in quotes); to ensure a cluster assignment for every test instance.

While performing look-up of neighbors for each test instance, we only selected the training instances in the same cluster (with the same index). For brand name based k-NN, for each test instance, we find the corpus of wines from the training set with the exact brand name as the test instance and find the closest match in terms of clean name(edit distance), volume and price.

Finally, we averaged all the predictions from the k-NN models. For test instances that had a missing prediction from a k-NN model, missing values were imputed as the average of non-missing predictions from other k-NN models for the test instance. We also used predictions from randomForest built on previous/elsewhere-year to replace the average of predictions from the k-NN models. If available, now price replaced the final prediction.

We found the prices of the same wines were proportional to its volume. Therefore, the price should be normalized by the volume. The prediction of price from k-NN for each test instance was then computed as the price of the closest training instance in the same cluster * (volume of test instance wine/volume of closest training instance wine).

Each k-NN model (using different clustering or distance metrics) can generate a prediction. Finally we average all the predictions to make the final one. The missing values are imputed using the average prediction from other models with non-missing predictions for the test instance.

Some improvements and tuning were made based on the ideas. For example, we applied query expansion method on the year and wine scores, we considered the difference of years and volumes when compute the distance so that the model will prefer the neighbor wines with closest year and volume, etc.

<strong>FEATURES</strong>

Even though about 80% of features in the data were continuous variables(score, alcohol content) and about 20% were nominal variables(location based, name), our final model, comprising of k-NN models utilized only the nominal variables as explanatory variables and a lot of credit goes to the fact that we were able to discover hidden useful information residing within the name/title of the wine. The following were the features derived from the name/title field:

●      volume

●      previous/elsewhere price

●      now price

●      pack (# of bottles)

●      brand name (string withing quotation marks)

●      clean name (name/title, after removing quoted text, year, punctuations, etc.)

●      short name (first 2 words (length&gt;=3) of the clean name)

●      loc (contanenation of Country, SubRegion and Appellation delimited by ‘.’)

A standard bottle of wine has a volume of 750mL and hence wines for which volume wasn’t explicitly mentioned in the name/title field, volume was defaulted to 750mL. Also, for wines where #bottles or pack size was available, volume was adjusted as the volume specified(or default) times the pack size.

<strong>HANDLING OUTLIERS</strong>

Any previous price(prevprice) in training or test data or predicted price as mapped to minprice(minimum price from training data, if predicted/previous price &lt; minprice) or maxprice(maximum price from training data, if predicted/previous price &gt; maxprice)

<strong>MODEL SELECTION</strong>

We selected final prediction based on the following order of model precedence

●      If the test instance has now price available from the title, use it as the final prediction (MAE was 0 in ⅓ cross-validation training data used as test set during learning phase)

●      If the test wine finds a neighbor in training set with edit distance &lt;= 1, directly use the price of this neighbor (MAE &lt; 0.1 in ⅓ CV training data)

●      If the test wine has previous or elsewhere price in the title, we used randomForest to predict the price using the previous/elsewhere price and year as features (MAE was about 9 for ⅓ CV training data).

●      For all the rest of the test instances, use k-NN models with k=1, then build a SVM model using the predictions from the 5 best k-NN models as the features, and finally use the average of the 5 k-NN predictions and SVM prediction.

●      Fix outliers, for e.g. if predicted price &gt; 1000, set predicted price = 999.99, which is the maximum price in the training data(based on information provided that all wines with price &gt; 1000 were removed from training and test data)

<strong>R packages used</strong>

●      randomForest  (for random forest)

●      e1071  (for svm)

●      cba  (for edit distance)

<strong>Java package used</strong>

●      Apache Lucene

<strong>FUTURE WORK</strong>

If time permitted, we would try our hands on sub-models based on locations, ratings, etc, and locally regression models based on the distance. We also thought about tuning this problem into a search problem, and using learning to rank models (using the price difference to rank items) to find the closest instance (answer) given a test instance (query).

<strong>ACKNOWLEDGEMENT</strong>

We thank Nelson Ray, the TA for Stats202(Data Mining and Analysis) class at Stanford University to organize the competition and provide great support throughout. We also thank Prof. Susan Holmes for imparting both theoretical and practical knowledge during the class that helped us exploit different methodologies and packages on the wine data.

<strong>S202 Team Eureka</strong>

<strong>Jie Yang, Neeral Beladia</strong>

<strong>jieyang, beladia AT stanford * edu</strong>"
How I did it: Yannis Sismanis on Winning the first Elo Chess Ratings Competition,http://blog.kaggle.com/2011/02/08/how-i-did-it-yannis-sismanis-on-winning-the-elo-chess-ratings-competition/,2011-02-09 04:51:28,"The attached article discusses in detail the rating system that won the Kaggle competition “Chess Ratings: Elo vs the rest of the world”. The competition provided a historical dataset of outcomes for chess games, and aimed to discover whether novel approaches can predict the outcomes of future games, more accurately than the well-known Elo rating system. The major component of the winning system is a regularization technique that avoids overfitting.

<a href=""http://www.kaggle.com/blog/wp-content/uploads/2011/02/kaggle_win.pdf"" target=""_blank"">kaggle_win.pdf</a>"
How I did it: Will Cukierski on finishing second in the IJCNN Social Network Challenge,http://blog.kaggle.com/2011/01/14/how-i-did-it-will-cukierski-on-finishing-second-in-the-ijcnn-social-network-challenge/,2011-01-14 06:02:59,"Graph theory has always been an academic side interest of mine, so I was immediately interested when Kaggle posted the IJCNN social network challenge.  Graph-theoretic problems are deceptively accessible and simple in presentation (what other dataset in a data-mining competition can be written as a two-column list?!), but often hide complex, latent relationships in the data.

<!--more-->

There’s an old adage among researchers, “a week in the library saves a year in the lab.”  Hoping not to reinvent the wheel, I combed through about 20 papers on the link prediction problem, the most helpful &amp; accessible of which was Kleinberg’s “<em>The link prediction problem for social networks.”</em> Unfortunately, at over 1 million x 1 million nodes, the graph size of the Flickr dataset precluded many of the published methods from running on a commodity desktop computer.

I programmed about 20 features (Jaccard, Adar, Katz, various neighbor-related metrics, path lengths, unseen bigrams, etc.) along with a framework whereby I would extract local “neighborhood graphs” to make the O(N^2)—or worse—computations feasible.  I had this running in parallel in Matlab on an 8-core machine with 10 gigs of RAM and an SSD drive, which really helps keep things moving when the RAM runs out and the pageouts start climbing.  I used the Boost Graph Library to speed up graph calculations.   I spent a good deal of time experimenting with different ways to build the local graphs.  Taking the first-level neighbors was possible, but adding the second-level neighbors was intractable.  I also tried, among others, methods based on enumerating the simple paths between A and B, using the nodes these paths traverse most often.  This proved to be too expensive, so I ended up using the first neighbors of A and B.

I was the first to pass the 0.9 public AUC mark when I realized that all the standard similarity features in the literature could be applied to 3 distinct problems:

(1) How similar is node A to B?

(2) How similar are the outbound neighbors of A to B?

(3) How similar are the inbound neighbors of B to A?

This was probably my most novel contribution to the contest and the idea that produced some of my best features.  I also found a method from a querying algorithm called Bayesian Sets that worked very well.   As far as I know, this was the first time it has been applied to link prediction.

Using the 60+ features I obtained from applying my features to problems (1), (2) and (3), I tested neural networks, random forests, boosting, and SVMs. I decided the forests were the way to go, since they yielded the best AUC and required no data preprocessing. It was important to do regression rather than strict classification.   ROC analysis cares only about the relative order of the points, so having “degeneracy” in prediction values is detrimental.  Neural networks were almost as good as forests, but their ROCs were just worse enough to make blending neural nets and forests worse than forests alone. With random forests, I was able to score 0.98 and above on my own validation sets, but my public score was paradoxically stuck at 0.92, no matter how many sophisticated features I added or how many training points I churned out.  As the competition continued and other teams jumped ahead, I still couldn’t pinpoint the disparity between my private and public scores.

About the same time, Bo Yang (who was on the 2nd place team in the Netflix prize and had a similar score on the IJCNN leaderboard), approached me about teaming up.  It was through my emails with Bo that I realized I had created my validation sets improperly. I was not limiting the outnodes to appear just once in the training data, which meant I was biasing the data towards outnodes with more edges.  Once I corrected this, my private AUC dropped from 0.98 to around 0.95, while the public one increased to around the same. What a relief!

The last two weeks of the competition were a frenzy to calculate features, exchange ideas with Bo, and play catch up to IND CCA’s lead.  I had the computer running 100% (x 8), 24 hours a day at this point.  When IND CCA kept improving, we decided to approach Benjamin Hamner (who had a score near 0.95 too) to make a last effort to merge and overtake.   Ben agreed to team up. We all exchanged a common set of validation files (which Ben created to precisely match the distribution of the real test set), extracted our own features, and then I combined them all by running several hundred random forests and blending the output.  Running this many forests helped to order the points in “no man’s land” around 0.5, which have the highest variance from trial to trial.

Ben had discovered a group of points he believed were true, based on a discrepancy in the way that Dirk sampled the graph (there should have been a larger number of false B edges with an in-degree of 1).   I also found a set of suspected true edges based on a probabilistic argument.   If B-&gt;A exists and A is the type of user who forms reciprocal friendships, it is highly likely that A-&gt;B exists and was removed.  Reciprocal edges were fairly rare in the data, and having a path length B-&gt;A of 1 was also rare.  Thus, having both together in the test set had an extremely low probability of occurring at random.

Ben and Bo each contributed some very strong features to my own (I’ll leave it to them to explain the details).  Ben’s Edgerank feature by itself would have finished in the top 5 of the competition!  Bo developed and tuned some very accurate kNN models, several of which scored above 0.9 by themselves.

Our best submission was the result of 94 features, some using the entire graph, some calculated on a graph consisting of local neighbors of each node.   Below I attached a table of some of our features and the AUCs they give by themselves on our validation sets.  We had 8 validation sets of 4480 points each, half fake and half real.  Random forests combined our features to a private AUC of about 0.972, which resulted in a 0.969 public final score.
<table border=""0"" cellspacing=""0"" cellpadding=""0"" width=""401"">
<tbody>
<tr>
<td width=""40"" valign=""bottom"">AUC</td>
<td width=""361"" valign=""bottom"">Brief Description</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.936</td>
<td width=""361"" valign=""bottom"">Edgerank</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.931</td>
<td width=""361"" valign=""bottom"">kNN3</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.923</td>
<td width=""361"" valign=""bottom"">kNN4</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.907</td>
<td width=""361"" valign=""bottom"">Jaccard applied to (2)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.902</td>
<td width=""361"" valign=""bottom"">kNN1</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.893</td>
<td width=""361"" valign=""bottom"">AdarB All</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.892</td>
<td width=""361"" valign=""bottom"">Common Neighbors applied to (2)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.889</td>
<td width=""361"" valign=""bottom"">Bayesian sets</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.889</td>
<td width=""361"" valign=""bottom"">Cosine applied to (2)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.888</td>
<td width=""361"" valign=""bottom"">Cosine applied to (1)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.88</td>
<td width=""361"" valign=""bottom"">Jaccard applied to (1)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.857</td>
<td width=""361"" valign=""bottom"">Adar   applied to (2)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.854</td>
<td width=""361"" valign=""bottom"">Simrank applied to (2)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.854</td>
<td width=""361"" valign=""bottom"">Percent of non-existing edges &lt; SVD(A,B)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.852</td>
<td width=""361"" valign=""bottom"">Commute Time in the local graph</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.852</td>
<td width=""361"" valign=""bottom"">Global link distance 2</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.846</td>
<td width=""361"" valign=""bottom"">Time to 1<sup>st</sup> similar neighbor in   Bayesian sets</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.843</td>
<td width=""361"" valign=""bottom"">kNN2</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.84</td>
<td width=""361"" valign=""bottom"">SVD rank 80</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.831</td>
<td width=""361"" valign=""bottom"">Common Neighbors applied to (1)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.831</td>
<td width=""361"" valign=""bottom"">AdarA All</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.826</td>
<td width=""361"" valign=""bottom"">Simrank</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.812</td>
<td width=""361"" valign=""bottom"">Dot product of columns in SVD rank 80 approx</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.812</td>
<td width=""361"" valign=""bottom"">SVD values of neighbors</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.805</td>
<td width=""361"" valign=""bottom"">Simrank applied to (1)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.794</td>
<td width=""361"" valign=""bottom"">Unseen   bigrams on Simrank</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.793</td>
<td width=""361"" valign=""bottom"">Katz   similarity of A and B</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.781</td>
<td width=""361"" valign=""bottom"">Katz   applied to (1)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.776</td>
<td width=""361"" valign=""bottom"">Bounded   Walks in the local graph</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.769</td>
<td width=""361"" valign=""bottom"">Maximum   flow in the local graph</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.764</td>
<td width=""361"" valign=""bottom"">Shortest   Paths Histogram applied to (1)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.739</td>
<td width=""361"" valign=""bottom"">Common   neighbors of in2</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.73</td>
<td width=""361"" valign=""bottom"">Common   neighbors</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.73</td>
<td width=""361"" valign=""bottom"">Cosine   similarity of A and B</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.73</td>
<td width=""361"" valign=""bottom"">Jaccard   similarity of A and B</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.73</td>
<td width=""361"" valign=""bottom"">Adar   similarity of A and B</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.727</td>
<td width=""361"" valign=""bottom"">In   degree of node B</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.714</td>
<td width=""361"" valign=""bottom"">Bayesian   Sets applied to (1)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.706</td>
<td width=""361"" valign=""bottom"">Adar   applied to (1)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.701</td>
<td width=""361"" valign=""bottom"">Pagerank   of node A</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.689</td>
<td width=""361"" valign=""bottom"">Number   of paths between A and B of length 2</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.686</td>
<td width=""361"" valign=""bottom"">Power   law exponent of the local graph</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.675</td>
<td width=""361"" valign=""bottom"">Clustering   Coeff. Node A</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.663</td>
<td width=""361"" valign=""bottom"">Unseen   bigrams on Katz</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.661</td>
<td width=""361"" valign=""bottom"">Preferential   Attachment</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.657</td>
<td width=""361"" valign=""bottom"">Betweenness   Centrality in the local graph</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.654</td>
<td width=""361"" valign=""bottom"">Clustering   Coeff. Node B</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.593</td>
<td width=""361"" valign=""bottom"">Common   neighbors of in1</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.569</td>
<td width=""361"" valign=""bottom"">Common   neighbors of out1</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.56</td>
<td width=""361"" valign=""bottom"">Pagerank   of node B</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.541</td>
<td width=""361"" valign=""bottom"">Katz   applied to (2)</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.538</td>
<td width=""361"" valign=""bottom"">B-&gt;A   exists and node A forms mutual edges?</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.533</td>
<td width=""361"" valign=""bottom"">Common   neighbors of out2</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.532</td>
<td width=""361"" valign=""bottom"">Out   degree of node B</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.521</td>
<td width=""361"" valign=""bottom"">In   degree of node A</td>
</tr>
<tr>
<td width=""40"" valign=""bottom"">0.521</td>
<td width=""361"" valign=""bottom"">Out   degree of node A</td>
</tr>
</tbody>
</table>
<strong> </strong>

Special thanks to Kaggle and the organizers for sponsoring this competition, as well as my teammates Bo Yang and Benjamin Hamner for their valuable contributions.

William Cukierski is a PhD candidate in the Biomedical Engineering Department at Rutgers University. He has a bachelor’s degree in physics from Cornell University.

<a href=""http://pleiad.umdnj.edu/~will/"">http://pleiad.umdnj.edu/~will/</a>"
How we did it: the winners of the IJCNN Social Network Challenge,http://blog.kaggle.com/2011/01/15/how-we-did-it-the-winners-of-the-ijcnn-social-network-challenge/,2011-01-16 05:35:11,"First things first: in case anyone is wondering about our team name,  we are all computer scientists, and most of us work in cryptography or related  fields. IND CCA refers to a <a href=""http://en.wikipedia.org/wiki/Ciphertext_indistinguishability#Indistinguishability_under_chosen_ciphertext_attack.2Fadaptive_chosen_ciphertext_attack_.28IND-CCA.2C_IND-CCA2.29"">property</a> of an encryption algorithm. Other than that, no  particular significance.

I myself work in computer security and privacy, and my  specialty is de-anonymization. That explains why the other team members (Elaine Shi,  Ben Rubinstein, and Yong J Kil) invited me to join them with the goal of  de-anonymizing the contest graph and combining that with machine learning.

<!--more-->

To clarify: our goal was to map the nodes in the training dataset to the real identities in the  social network that was used to create the data. That would allow us to simply look  up the pairs of nodes in the test set in the real graph to see whether or not the  edge exists. There would be a small error rate because some edges may have changed  after the Kaggle crawl was conducted, but we assumed this would be negligible.

Knowing that the social network in question is Flickr, we crawled a few million users'  contacts from the site. The crawler was written in python, using the curl library,  and was run on a small cluster of 2-4 machines.

While our crawl covered only a  fraction of Flickr users, it was biased towards high degree nodes (we explicitly  coded such a bias, but even a random walk is biased towards high degree nodes), so we were all set. By the time we crawled 1 million nodes we were hitting a 60-70%  coverage of the 38k nodes in the test set. But more on that later.

Our basic approach  to deanonymization is described in <a href=""http://33bits.org/2009/03/19/de-anonymizing-social-networks/"">my paper with Vitaly Shmatikov</a>. Broadly, there are two steps: “seed finding” and “propagation.” In the former step we somehow  deanonymize a small number of nodes; in the latter step we use these as “anchors” to propagate the deanonymization to more and more nodes. In this step the algorithm  feeds on its own output.

Let me first describe propagation because it is simpler. As  the algorithm progresses, it maintains a (partial) mapping between the nodes in the  true Flickr graph and the Kaggle graph. We iteratively try to extend the mapping as  follows: pick an arbitrary as-yet-unmapped node in the Kaggle graph, find the “most similar” node in the Flickr graph, and if they are “sufficiently similar,” they get mapped to each other.

Similarity between a Kaggle node and a Flickr node is  defined as cosine similarity between the already-mapped neighbors of the Kaggle node  and the already-mapped neighbors of the Flickr node (nodes mapped to each other are  treated as identical for the purpose of cosine comparison).

<a href=""http://www.kaggle.com/blog/wp-content/uploads/2011/01/propagation.png""><img class=""alignleft size-medium wp-image-736"" title=""propagation"" src=""http://www.kaggle.com/blog/wp-content/uploads/2011/01/propagation-300x184.png"" alt="""" width=""300"" height=""184"" /></a>

In the diagram, the blue  nodes have already been mapped. The similarity between A and B is 2 / (√3·√3) =  ⅔. Whether or not edges exist between A and A’ or B and B’ is irrelevant.

There  are many heuristics that go into the “sufficiently similar” criterion, which will be described in our upcoming paper. There are two reasons why the similarity between  a node and its image may not be 100%: because the contest graph is slightly different from our newer crawled graph, and because the mapping itself might have  inaccuracies. The /latter is minimal, and in fact the algorithm occasionally revisits  already-mapped nodes to correct errors in the light of more data.

I have elided over  many details — edge directionality makes the algorithm significantly more complex,  and there are some gotchas due to the fact that the Kaggle graph is only partially  available. Overall, however, this was a relatively straightforward adaptation of the  algorithm in the abovementioned paper with Shmatikov.

Finding seeds was much harder.  Here the fact that the Kaggle graph is partial presented a serious roadblock, and  rules out the techniques we used in the paper. The idea of looking at the  highest-degree nodes was obvious enough, but the key observation was that if you look at the nodes by highest in-degree, then the top nodes in the two graphs roughly  correspond to each other (whereas ordered by out-degree, only about 1/10 of the  Flickr nodes are in the Kaggle dataset). This is because of the way the Kaggle graph  is constructed: all the contacts of each of the 38K nodes are reported, and so the  top in-degree nodes will show up in the dataset whether or not they’re part of the  38K.

Still, it’s far from a straightforward mapping if you look at the top 20 in the two graphs. During the contest I found the mapping by hand after staring at two  matrices of numbers for a couple of hours, but later I was able to automate it using  simulated annealing. We will describe this in detail in the paper. Once we get 10-20  seeds, the propagation kicks off fairly easily.

On to the results. We were able to  deanonymize about 80% of the nodes, including the vast majority of the high-degree  nodes (both in- and out-degree.) We’re not sure what the overall error rate is, but for the high-degree nodes it is essentially zero.

Unfortunately this translated to  only about 60% of the edges. This is because the edges in the test set aren’t  sampled uniformly from the training graph; it is biased towards low-degree nodes, and deanonymization succeeds less often on low-degree nodes.

Thus, deanonymization alone  would have been far from sufficient. Fortunately, Elaine, Ben and Yong did some  pretty cool machine learning, which, while it would not have won the contest on its  own, would have given the other machine-learning solutions a run for their money.

Elaine sends me the following description:<em>
</em>

<em> </em>
<blockquote><em>Our ML algorithm is quite similar to  what <a href=""http://www.kaggle.com/view-postlist/forum-26-social-network-challenge/topic-254-sharing-techniques-used/task_id-2464"">vsh described earlier</a></em><em>. However, we implemented fewer features, and spent less  time fine tuning parameters. That's why our ML performance is a bit lower, with an  *estimated* AUC of 93.5-94%.</em>

<em>(Note that the AUC for ML is not corroborated with  Kaggle due to the submission quota, but rather, is computed over the ground truth  from the deanonymization. The estimate is biased, since the deanonymized subset is  not sampled randomly from the test set. [The 93.5-94% number is after applying  Elaine’s debiasing heuristic. -- Arvind])</em>

<em>We also used Random Forest over features a bunch of features, including the following (with acknowledgements to the ""social  network link prediction"" paper):
1) whether reverse edge is present
2) Adamic
3)  |intersection of neighbors| / |union of neighbors|
4) Neighborhood clustering  coefficient
5) Localized random walk 2 - 4 steps
6) Degree of n1/n2.
7) ... and a few  other features, most are 1-3 hop neighborhood characteristics.</em>

<em>For some of the  above-mentioned features, we computed values for the out-graph, in-graph, and the  bi-directional graph (i.e., union of in-graph and out-graph). We wanted to implement  more features, but did not due to lack of time</em>

<em>The best feature by itself is the  localized random walk 3-4 steps. This feature on its own has an *estimated* AUC of  92%. The Random Forest implementation used the Python Milk library.   http://pypi.python.org/pypi/milk/</em>

<em>Combining DA and ML:

1) Naive algorithm:
For  each test case: output DA prediction if DA prediction exists, else ML  score

</em><em>2) Improved version:
For edge (n1, n2), if n1 and/or n2 has multiple DA  candidates, and all candidates unanimously vote yes or no, output the corresponding  prediction.</em>

<em>Finally, like every one else, there were some trial-and-error tuning and  adjustments.</em></blockquote>
Once we hit No. 1, we emailed the organizers to ask if what we did  was OK. Fortunately, they said it was cool. Nevertheless, it is interesting to wonder how a future contest might prevent deanonymization-based solutions. There are two  basic approaches: construct the graph so that it is hard to deanonymize, or require  the winner to submit the code for human inspection. Neither is foolproof; I’m going to do some thinking about this but I’d like to hear other ideas.

Last but not least, a big thanks to Kaggle and IJCNN!"
How I did it: Benjamin Hamner's take on finishing second,http://blog.kaggle.com/2011/01/18/how-i-did-it-benjamin-hamners-take-on-finishing-second/,2011-01-18 18:38:47,"I chose to participate in this contest to learn something about graph theory, a field with a huge variety of high-impact applications that I'd not had the opportunity to work with before.  However, I was a late-comer to the competition, downloading the data and submitting my first result right before New Years.  From other's posts on this contest, it also seems like I'm one of the few who didn't read Kleinberg's link prediction paper during it.

<!--more-->

There were a few reasons I did not do a literature review while participating in the contest.  One was time - I had a late start, and I knew I had little time to devote to the contest for its remainder.  Short on time, I preferred to dive right into the data and see what I could make of it, as opposed to doing an extensive review of the prior literature.  I did this at the risk of missing something obvious that would help the models or ""reinventing the wheel,"" but with the benefit of having a fresh start.

To begin, I wrote a quick script to generate my own validation sets from the data and a framework to extract features from the data.  Features I extracted fell into three categories: those pertaining only to the ""A"" node (the source of the potential edge), those pertaining only to the ""B"" node (the destination of the potential edge), and those pertaining to the relation between A and B.

Next, I looked at the nodes known to follow B.  Let  <a href=""http://latex.codecogs.com/gif.latex?fr(N)""><img class=""alignnone"" src=""http://latex.codecogs.com/gif.latex?fr(N)"" alt="""" width=""47"" height=""19"" /></a> denote the set of nodes N follows, and <a href=""http://latex.codecogs.com/gif.latex?to(N)""><img class=""alignnone"" src=""http://latex.codecogs.com/gif.latex?to(N)"" alt="""" width=""43"" height=""19"" /></a> denote the set of nodes that follow N.  For each pair A and B, I took the following sum as a feature:

<a href=""http://latex.codecogs.com/gif.latex?%5Csum_%7Bn%20%5Cin%20to(B)%7D%20%5Cfrac%7B%7Cfr(A)%20%5Ccap%20fr(n)%7C%7D%7B%7Cfr(n)%7C%7D""><img class=""aligncenter"" src=""http://latex.codecogs.com/gif.latex?%5Csum_%7Bn%20%5Cin%20to(B)%7D%20%5Cfrac%7B%7Cfr(A)%20%5Ccap%20fr(n)%7C%7D%7B%7Cfr(n)%7C%7D"" alt="""" width=""174"" height=""51"" /></a>

This feature by itself worked decently well, and almost got me in the top 10.  I continued to add features that I thought would be relevant to the problem.  These included the following:

- Mean of values in the above summand- Max of values in the above summand- In-degree of A- Out-degree of A- In-degree of B- Out-degree of B- SVD- BFS-based features on the subgraph limited to the 37,689 nodes with outward links.

A Random Forest trained on this combination succeeded in getting a leaderboard AUC above 0.91.

Next, I thought about how some edges in the graph may be more important than others.  If a node follows only a few other nodes, then it is more likely to pay attention to or follow the nodes that those nodes follow.  I constructed a feature to value each potential edge based on random walks on the graph, starting at the A node.  Allowing the random walks to traverse the edges in either direction substantially improved this feature, giving it an AUC above 0.90 alone.

However, the random walk feature was computationally expensive to evaluate and optimize.  As a result, I used linear algebra to iteratively approximate the solution, and termed it edgeRank after I realized this was similar to Google's PageRank.  I optimized this feature independently over two parameters: the probability of restarting at the A node during each step in the random walk, and the weights outbound edges were assigned relative to inbound edges.  By itself, this feature achieved around 0.93 on the leaderboard.  (After reading Kleinberg's paper, a very similar feature is in there, termed a rooted PageRank).

One issue I ran into was that my predicted AUC scores from my private validation sets were much higher than my leaderboard validation scores.  As a result, I analyzed any differences between my validation sets and Dirk's test set.  The primary difference was as follows:

#A nodes, fan-out=1, test: 96  , valid: ~374#B nodes,   fan-in=1, test: 399, valid: ~2569

It ends up Dirk was not considering A nodes with a fan-out of 1 and B nodes with a fan-in of 1 for false edges, substantially altering the test distribution.  When I corrected for this, the test dataset distribution fell well within the conference intervals predicted by my private validation datasets on all the features I looked at.  Also, this likely meant that all 493 unique edges in the test set fulfilling this criteria (fan-out A == 1 or fan-in B &lt;= 1) were true edges.  This information plus the edgeRank feature alone was enough to put my leaderboard AUC above 0.95, and a Random Forest trained on all the features put me in 2nd at the time, behind IND CCA.  (When the answers were released, I saw that 488 out of 493 of those edges were true edges, so my guess was only 99% right).

A couple days before the end of the contest, I received a message from Will (who had just passed me on the leaderboard), seeing if I wanted to join him and Bo on a team to try beating IND CCA.  Given the magnitude of IND CCA's lead, I decided to.  As Will has described, we pooled our features, bounced ideas off one another, shared validation sets, and then trained a number of Random Forests for our final submission.

I'd also like to comment on the applicability of this contest to real-world problems.  In one sense, the manner the problem was presented made link-prediction almost trivially easy: it was a binary classification problem with balanced classes.  The true edges almost always belonged to nodes close together on the graph, while the false edges almost always belonged to nodes very far apart on the graph.  Thus, the AUC scores for this competition were very high.  In reality of course, there is a huge imbalance between the two classes.

A different evaluation procedure could have been providing participants with a list of originating nodes from which one or more edges may have been removed, and then having participants submit a short, ordered list of potential destination nodes for each originating node.  Participants could be evaluated on the Mean Average Precision of their submitted lists, or a similar metric. This is more applicable to social networks recommending new friends, and it encourages methodologies that could analyze and rank large numbers of potential edges for each originating node in a computationally efficient manner (such as edgeRank).

In another sense, this link prediction problem was far more difficult than it would be from the standpoint of a social network.  Networks like Facebook have vast quantities of additional information on the types and timing of interactions between nodes (each of which forms an edge), in addition to the friendship graph itself.  It probably is easier to predict new edges based on features like ""just appeared in a picture together"" in combination with the feature types used in this contest.

Congratulations to IND CCA, who won the contest with a very clever de-anonymization scheme coupled with a more standard machine-learning approach.  I considered trying to do something similar, but didn't have enough time left in the contest to dive into the Flickr API.  Additionally, I thought that de-anonymizing the data would be, in many ways, a more difficult challenge (as they said, their de-anonymization procedure was not sufficient to win).

In one regard, it's a relief to have been beaten by a team in this manner: they won by tackling a different problem, and my team still had the best machine learning methodology applied to the dataset in the competition (disregarding outside information).

Many thanks to my teammates, Will and Bo - y'all were great to work with and we made a strong run at the end!  And many thanks to Anthony and Dirk for running a well-organized contest.  I know I learned a lot, and I believe many of my competitors did the same!

<span class=""photo-credit"">Image credit: <a href=""http://commons.wikimedia.org/wiki/File:L19_January_no_malk_png.png#filelinks"">Dusk Eagle</a></span>"
How I did it: finishing second from Bo Yang's perspective,http://blog.kaggle.com/2011/01/19/how-i-did-it-finishing-second-from-bo-yangs-perspective/,2011-01-19 20:08:49,"I first saw kaggle.com in Nov 2010. I looked at the ongoing contests and found the IJCNN Social Network Challenge most interesting and decided to join, mostly because of its possible real-world application due to popularity of online social networks.

<!--more-->

I cut my teeth on collaborative filtering, prediction engine, etc on the Netflix Prize. At first I was motivated by the one million dollar prize (yay, lottery for geeks !), but found the whole technical aspect very interesting. In addition to the prize, the fact that it was a competition also served as a ""motivation engine"", of course this applies to kaggle.com too.

Anyway, I tried some quick-and-dirty methods with various levels of success, and slowly got my score up to .830 or so. At this point I decided to create my own validation data sets. At first I just picked nodes at random and I got a validation set that was very easy to predict, with my validation scores having no bearing on my test scores.

Previously I have asked a question on the discussion forum about the data sets, and in the subsequent discussion, the contest organizer gave some info on the creation of the test data set. Based on that info, I wrote a second validation data set builder, and was able to build validation data sets whose scores were reasonably close to the test scores. So my question has rewarded me greatly, in a totally unexpected way, and I also learned everything I could about the test data set, or so I thought (more on this later).

Next I tried some algorithms in earnest, adapting some of my Netflix prize code to this task. Due to limited amount of time available, I read a grand total of 3 research papers, and missed a number of well known algorithms in related fields, but as it turned out, it didn't matter.

By late 2010/early 2011, I got my score to about .930, and making progress was becoming difficult. So I contacted William Cukierski about collaboration and eventual teaming up. William agreed and once we started talking, I was astounded by the breadth of his arsenal. It looked like he had implemented almost all implementable algorithms, given his hardware constraints. In fact, the only thing holding him back was that he had inadvertently created a bad validation data set. Once that was fixed his score shot up to .950 and kept improving.

William broke the node similarity problem down into 3 sub-problems:

(1) How similar is node A to B?

(2) How similar are the outbound neighbors of A to B?

(3) How similar are the inbound neighbors of B to A?

I improved by score to about .935 based on his input. William also had a random forest overlord that was vastly superior to my own blender.

In the last week of the contest, we contacted Benjamin Hamner about teaming up and Ben agreed. Not surprisingly, Ben had a number of strong predictors, and he shared with us a startling insight about the test data set. It alone boosted my score from .935 to .952 ! But in testimony to the strength of William's random forest, in hardly made any difference there. In the end we fed almost everything into the forest and the final score was about .970.

<strong>Technical Details of My Methods</strong>

Unsurprisingly, some of the methods I tried didn't work, they include a neural network, a SVD/clustering-inspired method, and some node distance-based methods. Their scores were as high as .860, and by ""didn't work"" I mean they didn't contribute meaningfully to my blends. And fortunately, most of the methods that didn't work run much slower than the ones that did.

In the end, I had 3 major parts in my ""solution"":

1. A genetic algorithm blender. At first I used my linear regression blender from the Netflix prize. It was designed for RMSE scoring so of course it didn't work well for AUC scoring. I could easily out-blend it by picking the weights manually. Then I decided to try my Netflix GA blender, which was useless for that task, but it worked well enough here. I was still going to do a logistic blender, until I ran into William's random forest, and decided there was no point.

2. KNN. There're two keys to this:

2A. The graph is very sparse, so to predict link between nodes A and B, you should go beyond 1st-level neighbors and use 2nd-level neighbors. I tried using 3rd-level neighbors too, but at that distance the neighborhood was getting too big.

2B. Links should weight less as the link count goes up. Intuitively, imagine one million people following a celebrity, chances are most of them never met each other or the celebrity; on the other hand, if you have 30 contacts in your social network, chances are they're your family and friends and many of them know each other.

I tried a number of variations for KNN, for example following only inbound link and different link weighting functions. The best one scored 0.915 by itself, and was created by going to 2nd-level neighbors, but applying link weighting (1/sqrt(LinkCount+1)) only to 1st-level neighbors, and checking how outbound neighbors of A are similar to B (William's 2nd point above).

3. Global Link Distance (my own term here). To predict link from A to B, calculate the link distances from A to all other nodes, and the link distances from B to all other nodes. And the prediction is simply:

1/SumOverAllNodesN( 2^( LinkDistance(A,N)+LinkDistance(B,N) ) )

By itself, GLD's score was only about .850, but it blended well with KNN. I suspect it's because they complement each other: KNN works on the local neighborhood while GLD works on the entire network.

For my best submission, I blended 4 variations of KNN and 2 variations of GLD. These methods are relatively simple, fast, and effective. They all took about 1 minute to run (2.66GHz CPU, single-threaded), and the blender usually came up with a good set of weights in 15 seconds. All together they could generate a .935 test submission in about 7 minutes.

Which brings me to Ben's insight that boosted by score to .952: as an artifact of the test data set creation process, the link from A to B is almost guaranteed to be true if A has only 1 bound neighbor, or B has 0 or 1 inbound neighbor. Apparently a few other contestants on the leaderboard discovered it too. It's extremely simple, fast, and effective."
How I did it: Ming-Hen Tsai on finishing third in the R competition,http://blog.kaggle.com/2011/02/11/how-i-did-it-ming-hen-tsai-on-finishing-third-in-the-r-competition/,2011-02-12 07:48:06,"<strong>Background</strong>

I recently got my Bachelor degree from National Taiwan University (NTU). In NTU, I worked with Prof. Chih-Jen Lin's on large-scale optimization and meta-learning algorithms. Due to my background, I believe that good optimization techniques to solve convex model fast is an important key to achieve high accuracy in many application because we can don't have to worry too much about the models' performance and focusing on data itself.

<!--more-->

Noticing that the machine learning society lacks software on various kinds of algorithms that may be beneficial to our daily life, I am now developing pieces of tools that compile various state-of-the-art algorithms performing well in many data or contests. One of these pieces is lib-GUNDAM.

<strong>Methods I tried</strong><strong> </strong>

I entered the competition on Jan 30, 2011. That time, I had a submission just trying to make sure whether my lib-GUNDAM performs well on data sets other than my experimental sets. It ended up that I got 10th in my first submission. I only expanded categorical features,
trained it with Linear-SVM and submit its decision values on the data set.

Then, I used lib-GUNDAM to do data scaling and perform parameter search on linear-SVM using the training data set. Shortly, I rose to about 7th place.

I somehow got stuck here and I decided to investigate other classifiers. I tried the tree bagger in Matlab, but I found it ran too low on the data. I thought I did not have time to wait for it. Somehow, I noticed that L2 logistic regression(LR) performs better than L1-SVM in most case on the data set. Thus, I switched to using it afterwards. Fortunately, my friend Hsiang-Fu Yu made a study months ago about training L2 LR in short time and incorporated it into LIBLINEAR. By using the software, I conducted parameter search very fast. (Search for positive instance penalty weight and negative instance penalty weight.)

Also I found that explicit 2-degree polynomial expansion (bi-gram expansion), worked well on the data. I began to used it before training on all data after wards,

I rose to 4th place by using LR on best cross-validation parameters and using 2-degree polynomial expansion.

Then, I had about two days left. I thought I did not have time to train other classifiers. Two ideas came to my mind:
1. to boost the L2 LR or
2. try to add more features.

Thus, I began to try random subspace methods and adaboost to boost the performance of L2 LR. Random subspace methods usually needs diverse learners, but I did not have fast learners, so used only liblinear, which might not be diverse enough. In my expectation, it did not work.

Adaboost boosted my learners very trivially.

In time, I began to visit the forums to check whatever had been discussed. I noticed two things :
1. training and testing data have overlap instances and
2. the contest holder seemed to suggest contestants to investigate user-oriented features.

Therefore, I hard-coded labels to those testing instances which occurred in the training data. Also, I decided to add features to users. Since we have five features

* DependencyCount

* SuggestionCount

* ImportCount

* ViewsIncluding

* PackagesMaintaining for packages

I added for each user five feature by averaging the individual features his packages own. For example, for DependencyCount, for user i, we add a feature userDependencyCount to the user by \sum_{[i installed j]==1} DependencyCount_j / \sum_{[i installed j]==1}. This yielded very good performance and rose me to the third place.

8 hours before the contest ended I came up with the idea that recommendation system problems could be viewed as link prediction problems. Thus, I tried to add some link prediction features. Due to time constraint, I added only preferential attachment. Although it worked better on my cross validation, it did not work better than the previous model. I thought, I might be utilizing too much graph information. (I want to prediction if there is a link between node a and b, but I used the information regrading links between a and b. I think is a kind of
overfitting.)

Then, my submission quota was full, I did not do further experiments although I was to boost L2 LR again. I ended up in the 3rd place.

<strong>Tools I used</strong>

1. lib-GUNDAM (http://www.csie.ntu.edu.tw/~b95028/software/lib-gundam/index.php) for feature preprocessing

2. LIBLINEAR (www.csie.ntu.edu.tw/~cjlin/liblinear/) for training fast L2 LR

3. LIBLINEAR with instance weight (http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances)

4. for training adaboost because adaboost needs base learners able to take instance weight into account when learning

5. hi-ensemble (http://www.csie.ntu.edu.tw/~b95028/software/hi-ensemble/index.php) for random subspace method

6. adaboost

For algorithm details and experimental statistics, please refer to
<a href=""Background  I recently got my Bachelor degree from National Taiwan University (NTU). In NTU, I worked with Prof. Chih-Jen Lin's on large-scale optimizaion and meta-learning algorithms. Due to my backgroup, I believe that good optimization techniques to solve convex model fast is an important key to achieve high accuracy in many application because we can don't have to worry too much about the models' performance and forcusing on data itself.  Noticing that the machine learning society lacks software on various kinds of algorithms that may be beneficial to our daily life, I am now developing pieces of tools that compile various state-of-the-art algorithms performing well in many data or contests. One of these pieces is lib-GUNDAM.  Methods I tried I entered the competition on Jan 30, 2011. That time, I had a submission just trying to make sure whether my lib-GUNDAM performs well on data sets other than my experimental sets. It ended up that I got 10th in my first submission. I only expanded categorical features, trained it with Linear-SVM and submit its decision values on the data set.  Then, I used lib-GUNDAM to do data scaling and perform parameter search on linear-SVM using the training data set. Shortly, I rose to about 7th place.  I somehow got stuck here and I decided to investigate other classifiers. I tried the tree bagger in matlab, but I found it ran too low on the data. I thought I did not have time to wait for it. Somehow, I noticed that L2 logistic regression(LR) performs better than L1-SVM in most case on the data set. Thus, I switched to using it afterwards. Fortunately, my friend Hsiang-Fu Yu made a study monthes ago about training L2 LR in short time and incorporated it into LIBLINEAR. By using the software, I conducted parameter search very fast. (Search for positive instance penalty weight and negative instance panelty weight.)  Also I found that explicit 2-degree polynomial expansion (bi-gram expansion), worked well on the data. I began to used it before training on all data afterwards,  I rose to 4-th place by using LR on best cross-validation parameters and using 2-degree polynomial expansion.  Then, I had about two days left. I thought I did not have time to train other classifiers. Two ideas came to my mind : 1. to boost the L2 LR or 2. try to add more features.  Thus, I bagan to try random subspace methods and adaboost to boost the performance of L2 LR. Random subspace methods usually needs diverse learners, but I did not have fast leaners, so used only liblinear, which might not be diverse enough. In my expectation, it did not work.  Adaboost boosted my learners very trivially.  In time, I began to vist the forums to check whatever had been discussed. I noticed two things : 1. training and testing data have overlap instances and 2. the contest holder seemed to suggest contestants to investigate user-oriented features.  Therefore, I hardcoded labels to those testing instances which occured in the training data. Also, I decided to add features to users. Since we have five features * DependencyCount * SuggestionCount * ImportCount * ViewsIncluding * PackagesMaintaining for packages.  I added for each user five feature by averaging the individual features his packages own. For example, for DependencyCount, for user i, we add a feature userDependencyCount to the user by \sum_{[i installed j]==1} DependencyCount_j / \sum_{[i installed j]==1}. This yielded very good performance and rose me to the third place.  8 hours before the contest, ended I came up with the idea that recommendation system problems could be viewed as link prediction problems. Thus, I tried to add some link prediction features. Due to time constraint, I added only preferential attachment. Although it worked better on my cross validation, it did not work better than the previous model. I thought, I might be utilizing too much graph informations. (I want to prediction if there is a link between node a and b, but I used the information regrading links between a and b. I think is a kind of overfitting.) Then, my submission quota was full, I did not do further experiments although I was to boost L2 LR again. I ended up in the 3rd place.  Tools I used lib-GUNDAM (http://www.csie.ntu.edu.tw/~b95028/software/lib-gundam/index.php) for feature preprocessing LIBLINEAR (www.csie.ntu.edu.tw/~cjlin/liblinear/) for training fast L2 LR LIBLINEAR with instance weight (http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances) for training adaboost because adaboost needs base learners able to take instance weight into account when learning. hi-ensemble (http://www.csie.ntu.edu.tw/~b95028/software/hi-ensemble/index.php) for random subspace method and adaboost  For algorithm details and experimental statistics, please refer to http://www.csie.ntu.edu.tw/~b95028/papers/2011kaggleR.pdf ."">http://www.csie.ntu.edu.tw/~b95028/papers/2011kaggleR.pdf </a>"
Dave Slate on Winning the R Challenge,http://blog.kaggle.com/2011/02/15/dave-slate-on-winning-the-r-challenge/,2011-02-16 00:58:35,"I (David Slate) am a computer scientist with over 48 years of programming experience and more than 25 years doing machine learning and predictive analytics.  Now that I am retired from full-time employment, I have endeavored to keep my skills sharp by participating in machine learning and data mining contests, usually with Peter Frey as team ""Old Dogs With New Tricks"".  Peter decided to sit this one out, so I went into it alone as ""One Old Dog"".

<!--more-->

For this contest I used essentially the same core forecasting technology that I've employed in other contests: a home-grown variant of ""Ensemble Recursive Binary Partitioning"".  This is a robust algorithm that can handle large numbers of records and large numbers
of feature (predictor) variables.  Both outcome and feature variables can be boolean (2 classes), categoric (multiple classes), or numeric (real numbers plus a missing value).  For the R contest the outcome was boolean, and the features provided in the training set were a mix of all three variable types.

To help tune modeling parameters and select feature variables, I relied both on a cross-validation procedure and also on feedback from the leaderboard.  For most of the cross-validation runs I partitioned the training data into 5 subsets of roughly equal population, trained a model on the data in 4 of the 5 subsets, tested it on the 5th to produce an AUC score, and then repeated this process 4 more times, rotating the subsets so that each subset got to play the role of test set once.  I then repeated this 5-fold procedure one more time, after scrambling the data to ensure a different partitioning, so as to produce 10 AUC scores altogether.  These were averaged together into a composite score for the run.  I also computed a standard deviation and standard error of the mean for the 10 scores to get some idea of their statistical variability.  In the course of the competition I performed a total of 628 of these cross-validation runs.  By the time of my first submission on Dec 11, I had already done 115 of them.

Although my tests involved a large number of feature variable selections and parameter settings, testing was not systematic enough to conclude that the winning model was in any way optimal.  There were too many moving parts for that.

To produce my first submission I used only the feature variables provided in the training set, but I enhanced the results in two ways. One was to exploit the fact that some records occurred in both the training and test sets, so that their forecasts could simply be copied from the training labels.  The other was to use the package dependency information in the depends.csv file from the supplementary archive johnmyleswhite-r_recommendation_system-36f8569.tar.gz, which, as suggested on the contest ""Data"" page, I downloaded from <a href=""http://github.com/johnmyleswhite/r_recommendation_system"">http://github.com/johnmyleswhite/r_recommendation_system</a>.  For each record whose Package depended on a Package known to be not Installed by this User, I produced the forecast 0, and for each record whose Package was depended on by a Package known to be Installed by this User, I produced the forecast 1.

Although this first submission received the lowest final score (0.983419) of all my 55 submissions, it turned out that unbeknownst to me this would have been just sufficient to win the contest.

In the course of the contest I produced and tested a variety of additional variables, many of them based on other files in the github archive, such as imports.csv, suggests.csv, and views.csv.  I also made use of the one-line package descriptions on the ""Available Packages"" list at cran.r-project.org.  Finally, I created variables from the text in the package index pages acquired by downloading all the pages <a href=""http://cran.r-project.org/web/packages/PKGNAME/index.html"">http://cran.r project.org/web/packages/PKGNAME/index.html</a>, where PKGNAME stands for each package name.

I failed to include in my final 5 selections the submission that received the highest final score, 0.988189.  But I did include my 2nd best (0.988157), and I'll describe that submission in some detail. Note that both of these submissions were made the day before the contest ended.

The winning submission model utilized 43 features.  These included the 15 provided in the training file plus 28 synthesized feature
variables.  Although my model-building algorithm will naturally give greater weight to highly-predictive variables, it is also possible to assign an ""a priori"" weight to each variable, and I tried various values of these.  Here is a table of feature variable names, together with their types (B = boolean/binary, C = categoric/class, N = numeric), their assigned or default relative weights, and, in the case of each B or N variable, a crude indication of its utility in the form of its correlation coefficient with the outcome (Installed).  The
final column contains a brief description of the variable.

Several of the synthesized features involve some crude text analysis. In the description of those features, a ""word"" refers to a contiguous sequence of alphanumeric characters, and a ""name"" is an upper case letter followed by a contiguous sequence of alphanumeric characters.

<table border=""1"" cellspacing=""1"" cellpadding=""1"">
<tr><td>Variable name&nbsp;</td>	<td>Type&nbsp;</td><td>Weight&nbsp;</td>	<td>Corr&nbsp;</td>		<td>Description or source&nbsp;</td></tr>
<tr><td>Package             &nbsp;</td>	<td>C&nbsp;</td>	<td>0.50&nbsp;</td><td>&nbsp;</td>		   	<td>Training file&nbsp;</td></tr>
<tr><td>User                &nbsp;</td>	<td>C&nbsp;</td>	<td>1.00&nbsp;</td>	<td>&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>DependencyCount     &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.0722&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>SuggestionCount     &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.3856&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>ImportCount         &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.2849&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>ViewsIncluding      &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.1603&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>CorePackage         &nbsp;</td>	<td>B&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.0538&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>RecommendedPackage  &nbsp;</td>	<td>B&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.2858&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>Maintainer          &nbsp;</td>	<td>C&nbsp;</td>	<td>0.80&nbsp;</td>	<td>&nbsp;</td>		<td>Training file, but mapped to lower case and with non-alphnumerics mapped to '_'&nbsp;</td></tr>
<tr><td>PackagesMaintaining &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.1379&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>LogDependencyCount  &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.4112&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>LogSuggestionCount  &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.4526&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>LogImportCount      &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.3464&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>LogViewsIncluding   &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.1386&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>LogPackagesMaintaining&nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.1333&nbsp;</td>		<td>Training file&nbsp;</td></tr>
<tr><td>MaintainerName      &nbsp;</td>	<td>C&nbsp;</td>	<td>0.25&nbsp;</td>	<td>&nbsp;</td>		<td>Name extracted from Maintainer field&nbsp;</td></tr>
<tr><td>MaintainerEmail     &nbsp;</td>	<td>C&nbsp;</td>	<td>0.25&nbsp;</td>	<td>&nbsp;</td>		<td>Email address extracted from Maintainer field&nbsp;</td></tr>
<tr><td>CountSuggest2       &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.4184&nbsp;</td>		<td>Count of packages installed by User that suggest Package&nbsp;</td></tr>
<tr><td>CountImport2        &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.2291&nbsp;</td>		<td>Count of packages installed by User that import Package&nbsp;</td></tr>
<tr><td>ComPkgDescWordCnt   &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.3443&nbsp;</td>		<td>Sum, over all distinct ""words"" of length &gt;= 5 chars in 1-line description of Package, the count of packages installed by User whose 1-line descriptions also contain this ""word""&nbsp;</td></tr>
<tr><td>ComPkgDescWordCntRat3&nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.1779&nbsp;</td>		<td>Related to ComPkgDescWordCnt, but takes the ratio of sum of counts per installed package to sum per not installed&nbsp;</td></tr>
<tr><td>ComPkgNamSubMatCnt  &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.1926&nbsp;</td>		<td>Count of packages installed by User whose names, mapped to lower case, have at least 1 substring of length &gt;= 5 in common with Package name&nbsp;</td></tr>
<tr><td>ComPkgNamSubMatFracRat2&nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.0369&nbsp;</td>		<td>Related to ComPkgNamSubMatCnt, but takes the ratio of count of matching substrings per installed package to count per not installed&nbsp;</td></tr>
<tr><td>ComViewsCnt         &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.3384&nbsp;</td>		<td>Count of packages installed by User with a view in common with Package&nbsp;</td></tr>
<tr><td>ComViewsFracRat     &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.1647&nbsp;</td>		<td>Related to ComViewsCnt, but takes the ratio of count of installed packages to count of not installed&nbsp;</td></tr>
<tr><td>ComViewsCntAll      &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.3384&nbsp;</td>		<td>Same as ComViewsCnt due to a bug, but was supposed to be somewhat different&nbsp;</td></tr>
<tr><td>ComViewsFracRatAll  &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.1647&nbsp;</td>		<td>Same as ComViewsFracRat due to a bug, but was supposed to be somewhat different&nbsp;</td></tr>
<tr><td>DependsOnCnt        &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.1024&nbsp;</td>		<td>Count of packages installed by User that Package depends on&nbsp;</td></tr>
<tr><td>SuggestsOnCnt       &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.2266&nbsp;</td>		<td>Count of packages installed by User that Package suggests&nbsp;</td></tr>
<tr><td>PubYear             &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.0612&nbsp;</td>		<td>Published year, including fraction, derived from ""Published:"" field in package index page&nbsp;</td></tr>
<tr><td>ComMaintCnt         &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.3406&nbsp;</td>		<td>Count of packages installed by User that have same MaintainerEmail address as Package&nbsp;</td></tr>
<tr><td>PkgIdxTxtLen        &nbsp;</td>	<td>N&nbsp;</td>	<td>0.60&nbsp;</td>	<td> 0.2338&nbsp;</td>		<td>Length in chars of package index page text&nbsp;</td></tr>
<tr><td>PkgIdxTxtWordCnt    &nbsp;</td>	<td>N&nbsp;</td>	<td>0.40&nbsp;</td>	<td> 0.2506&nbsp;</td>		<td>Count of distinct ""words"" of length &gt;= 6 chars in Package index page text&nbsp;</td></tr>
<tr><td>ComPkgTxtWordCnt    &nbsp;</td>	<td>N&nbsp;</td>	<td>0.40&nbsp;</td>	<td> 0.5188&nbsp;</td>		<td>Sum, over all distinct ""words"" of length &gt;= 6 chars in index page text of Package, the count of packages installed by User whose index page texts also contain this ""word""&nbsp;</td></tr>
<tr><td>ComPkgTxtWordCntRat &nbsp;</td>	<td>N&nbsp;</td>	<td>0.40&nbsp;</td>	<td> 0.0832&nbsp;</td>		<td>Related to ComPkgTxtWordCnt, but takes the ratio of sum per installed packages to sum per not installed&nbsp;</td></tr>
<tr><td>ComPkgTxtWordCntFrac&nbsp;</td>	<td>N&nbsp;</td>	<td>0.40&nbsp;</td>	<td> 0.4597&nbsp;</td>		<td>Ratio of ComPkgTxtWordCnt to PkgIdxTxtWordCnt&nbsp;</td></tr>
<tr><td>PkgIdxTxtNameCnt    &nbsp;</td>	<td>N&nbsp;</td>	<td>0.40&nbsp;</td>	<td> 0.3219&nbsp;</td>		<td>Count of distinct ""names"" of length &gt;= 5 chars in Package index page text&nbsp;</td></tr>
<tr><td>ComPkgTxtNameCnt    &nbsp;</td>	<td>N&nbsp;</td>	<td>0.40&nbsp;</td>	<td> 0.5147&nbsp;</td>		<td>Sum, over all distinct ""names"" of length &gt;= 5 chars in index page text of Package, the count of packages installed by User whose index page texts also contain this ""name""&nbsp;</td></tr>
<tr><td>ComPkgTxtNameCntRat &nbsp;</td>	<td>N&nbsp;</td>	<td>0.40&nbsp;</td>	<td> 0.0691&nbsp;</td>		<td>Related to ComPkgTxtNameCnt, but takes the ratio of sum per installed packages to sum per not installed&nbsp;</td></tr>
<tr><td>ComPkgTxtNameCntFrac&nbsp;</td>	<td>N&nbsp;</td>	<td>0.40&nbsp;</td>	<td> 0.4571&nbsp;</td>		<td>Ratio of ComPkgTxtNameCnt to PkgIdxTxtNameCnt&nbsp;</td></tr>
<tr><td>DependsOnRecMis     &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td>-0.1335&nbsp;</td>		<td>Count of packages not installed by User that Package depends on&nbsp;</td></tr>
<tr><td>DependsByRecMis     &nbsp;</td>	<td>N&nbsp;</td>	<td>1.00&nbsp;</td>	<td> 0.1743&nbsp;</td>		<td>Count of packages installed by User that depend on Package&nbsp;</td></tr>
<tr><td>MaintainerNameOrEmail&nbsp;</td>	<td>C&nbsp;</td>	<td>0.25&nbsp;</td>   <td>&nbsp;</td>			<td>MaintainerName unless missing, in which case MaintainerEmail&nbsp;</td></tr>


</table>

Various other feature variables were tried, but for whatever reasons did not make the final cut.

My computing platform consisted of two workstations powered by multi-core Intel Xeon processors and running the Linux OS.  The core forecasting engine was written in C, but was controlled by a front-end program written in the scripting language Lua using LuaJIT (just-in- time Lua compiler version 2 Beta 5) for efficiency."
Marcin Pionnier on finishing 5th in the RTA competition,http://blog.kaggle.com/2011/02/17/marcin-pionnier-on-finishing-5th-in-the-rta-competition/,2011-02-17 12:08:16,"<strong>My background</strong>

I graduated on Warsaw University of Technology with master thesis about text mining topic (intelligent web crawling methods). I work for Polish IT consulting company (Sollers Consulting), where I develop and design various insurance industry related stuff, (one of them is insurance fraud detection platform). From time to time I try to compete in data mining contests (Netflix, competitions on Kaggle and tunedit.org) - from my perspective it is a very good way to get real data mining experience.

<!--more--><!--more--><!--more-->

<strong>What I tried</strong>

As far as I remember, the basis of the solution I defined at the very beginning: to create separate predictors for each individual loop and time interval. So my solution required me to build 61x10=610 regression models. I was playing with various regression algorithms, but quickly chose linear regression - because the results were good and the computation time was short. I think the key to get quite good result (especially on public RMSE :) ) was the set of attributes used. I used the following attributes for the linear regression for each individual loop&amp;time interval:
- number of minutes from 0:00 hours up to current moment (""now"")
- average drive time for given loop&amp;interval
- loop times for current moment and some number of historical moments
before (the number of time points and the loop varied between the
methods)
- differences between ""neighboring"" time moments for the above data:
just differences or differences transformed with logistic function
(1/1+e^-difference). Use of logistic function  gave a jump from public
RMSE at about 198 to 189. The idea to use of sigmoid function here was
just my intuition inspired by differences distribution.
- ""saturations"" for for each loop (except the 2 first loops at both
directions ).

I introduced the simple (and very naive) model of traffic growth:
If the speed at given loop is up to 40 km/h - the saturation is 1;

If the difference between the previous loop and the given loop is more than 5 km/h: it is assumed that this road part is partially saturated: there is segment that is moving at 30 km/h and second segment with the same speed as in the loop that is before given loop. The saturation is derived as the proportion of first segment to the whole road part. Each loop detector has its minimal value in RTAData file - after the regression this minimal value was used if predicted value was less than minimum.

I did not use historical data at all - I found them useless during the initial tests (maybe too hastily). The only source of data for learning and testing was RTAData and lengths files (also no weekends, holidays, weather conditions).

<strong>What ended up working</strong>

For each of 610 regression models the following 3 models were competing. Models were being trained with all data availabe in RTAData
file:
Model 1: For all (61) loops: current + 5 times moments before and 5 simple differences - 675 attributes,
Model 2: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 simple differences, saturations (for current time moment only) - 204 to 404 atrributes,
Model 3: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 sigmoided differences,
saturations (for current time moment only) 204 to 404 atrributes,
Model with least RMSE computed on the train file was selected for particular loop.  It is not a very good strategy, however I thought
that generally linear regression was resistant to overfitting (it is not true - as the number of variable grows, the more variance can be explained - this is what I have learnt).

This strategy gave me public RMSE 189.3

I added also 4th model, that I just used for 15, 30 minutes predictions arbitrarily:
Model 4: For all (61) loops: current + 5 times moments before and 5 sigmoided differences, saturations (for current time moment only) - 614 attributes. This turn gave mi 188.6 public result.

What is interesting, the best private solution (however not selected by me since I relied to much on public results) was 190.819 (public 197.979) , it was just the model 3 described above combined with model 5 (model 5 was used for 15,30,45,60,90 minutes predictions arbitrarily, rest model 3):
Model 5: like model 3 but also loop times are ""sigmoided"" not only differences.

<strong>What tools I used</strong>

My solution is written as Java application with Weka linked as library (as always when I try to compete in data mining contests).  Since linear regression requires to solve matrix equation (in this case quite huge), the memory allocated by the program was becoming more and more important issue (3,5GB for one thread) - at the of the competition i was using computer with 4 processors and 12 GB of RAM - with 3 separate threads building and testing the models. The whole computation for my last attempts took about 48 hours of computations."
Jeremy Howard on winning the Predict Grant Applications Competition,http://blog.kaggle.com/2011/02/21/jeremy-howard-on-winning-the-predict-grant-applications-competition/,2011-02-21 10:44:56,"Because I have recently started employment with Kaggle, I am not eligible to win any prizes. Which means the prize-winner for this comp is Quan Sun (team 'student1')! Congratulations!

My approach to this competition was to first analyze the data in Excel pivottables. I looked for groups which had high or low application success rates. In this way, I found a large number of strong predictors - including by date (new years day is a strong predictor, as are applications processed on a Sunday), and for many fields a null value was highly predictive.

<!--more-->

I then used C# to normalize the data into Grants and Persons objects, and constructed a dataset for modeling including these features: CatCode, NumPerPerson, PersonId, NumOnDate, AnyHasPhd, Country, Dept, DayOfWeek, HasPhd, IsNY, Month, NoClass, NoSpons, RFCD, Role, SEO, Sponsor, ValueBand, HasID, AnyHasID, AnyHasSucc, HasSucc, People.Count, AStarPapers, APapers, BPapers, CPapers, Papers, MaxAStarPapers, MaxCPapers, MaxPapers, NumSucc, NumUnsucc, MinNumSucc, MinNumUnsucc, PctRFCD, PctSEO, MaxYearBirth, MinYearUni, YearBirth, YearUni .

Most of these are fairly obvious as to what they mean. Field names starting with 'Any' are true if any person attached to the grant has that feature (e.g. 'AnyHasPhd'). For most fields I had one predictor that just looks at person 1 (e.g. 'APapers' is number of A papers from person 1), and one for the maximum of all people in the application (e.g. 'MaxAPapers').

Once I had created these features, I used a generalization of the random forest algorithm to build a model. I'll try to write some detail about how this algorithm works when I have more time, but really, the difference between it and a regular random forest is not that great.

I pre-processed the data before running it through the model by grouping up small groups in categorical variables, and replacing continuous columns with null values with 2 columns (one containing a binary predictor that is true only where the continuous column is null, the other containing the original column, with nulls replaced by the median). Other than the Excel pivottables at the start, all the pre-processing and modelling was done in C#, using libraries I developed during this competition. I hope to document and release these libraries at some point - perhaps after tuning them in future comps."
Max Lin on finishing second in the R Challenge,http://blog.kaggle.com/2011/02/21/max-lin-on-finishing-second-in-the-r-challenge/,2011-02-22 03:27:01,"I participated in the <a href=""http://www.kaggle.com/R"">R package recommendation engine competition</a> on Kaggle for two reasons.  First, I use R a lot.  I cannot learn statistics without R.  This competition is my chance to give back to the community a R package recommendation engine.  Second, during my day job as an engineer behind a machine learning service in the cloud, product recommendation is one of the most popular applications our early adopters want to use the web service for.  This competition is my opportunity to stand in users' shoes and identify their pain points associated with building a recommendation system.

<!--more-->

I treat R package recommendation as a binary classification task: a classifier $latex f(u, p)$ takes as input a user $latex u$ and a package $latex p$, and predicts whether the user will install the package or not.  My final submission (team name: <a href=""http://www.kaggle.com/team?team_id=2973"">Record Me Men</a>) combines four classifiers, labeled as large dots in Figure 1. The four classifiers share the same form of objective function that minimizes loss $latex L$ plus regularizers $latex R$:
<p style=""text-align: center;"">$latex J(\theta) = \sum_i L(y_i, f(u_i, p_i; \theta)) + \lambda R(\theta)$</p>
, where $latex f(u, v)$ is a classification model, $latex \theta$ is the parameters of the classification model, $latex y_i$, $latex u_i$, $latex p_i$ are the class label (package installed or not), user and package of the <em>i</em>-th training example, respectively. $latex \lambda$ controls the penalty from the regularizing function, and is chosen using cross validation.

<strong>Model 1: Baseline</strong>.  Model 1 is <a href=""https://github.com/johnmyleswhite/r_recommendation_system/blob/master/example_model_2.R"">example_model_2.R</a> that the competition organizer provides as a baseline.  In this model, a user is encoded as dummy variables $latex \mathbf{u}$, and a package is represented by seven features $latex \mathbf{p} = \{p_1, \dots, p_7\}$ (e.g, the logarithmic count of dependency.  See the R script for the complete list).  The classification model is a linear combination of user dummy variables and package features with weight parameters, $latex f(u, p) = \mathbf{\theta_u}^T \mathbf{u} + \mathbf{\theta_p}^T \mathbf{p}$.  The parameters are $latex \theta_u$ and $latex \theta_p$ (and intercept).  The loss function $latex L$ is negative logistic log likelihood.  There is no regularizer in the model.

The first model establishes a strong baseline, achieving AUC of ~0.94, labeled as M1b2 in Figure 1.  A variant of this model that omits the user feature (see <a href=""https://github.com/johnmyleswhite/r_recommendation_system/blob/master/example_model.R"">example_model_1.R</a>, also provided by the contest organizers) achieves noticeable lower AUC of 0.81 (not shown in Figure 1).  This suggests that some users are more likely to install R packages than others.  In the next model, I will explore a classification model that incorporates not only user variations but also package variations.

<strong>Model 2: Latent factor Models</strong>. In contrast to Model 1 with features derived from metadata, I focus on the user-package rating matrix.  Model 2 consists of two components: <em>baseline estimates</em> and <em>latent factors</em>.  Baseline estimates are linear combinations of three parts: global (one parameter $latex \mu$), user (one parameter per user $latex \mu_u$), and package (one parameter per package $latex \mu_p$).  For latent factors, I assumes that there are K latent factors for each user, $latex \mathbf{\beta}_u$ and each package $latex \mathbf{\beta}_p$, and the inner product of these two factors captures the interaction between a user and a package.  The classifier model in Model 2 is $latex f(u, p) = \mu + \mu_u + \mu_p + \mathbf{\beta}_u^T \mathbf{\beta}_p$.  This kind of latent factor model, also known as ""singular value decomposition"",  has been reported with great success in previous collaborative filtering studies and at the Netlifx Prize.   I choose an exponential loss function for Model 2, $latex L(y, f(u, p)) = \exp(- y f(u, p))$,

where $latex y_i \in \{1, -1\}$.  I choose exponential loss over squared loss because 1) exponential loss matches 0-1 loss better than squared loss 2) exponential loss is differentiable.  I apply L2 regularizers, $latex R(\cdot) = ||\mu_u||^2 + ||\mu_p||^2 + ||\beta_u||^2 + ||\beta_p||^2$.  I minimize the objective function using stochastic gradient descent.  The number of latent factors, K, is chosen by cross validation.

The latent factor model works very well on the R package recommendation data, achieving AUC of ~0.97 (See the M2 family in Figure 1).  I plot the performance of five latent factor models with different Ks, ranging from 10 to 50, labeled as M2k10, M2k20, ..., M2K50 in Figure 1.  As K increases, the latent factor model becomes more expressive and fit the data better, resulting in higher AUC.

<strong>Model 3: Package LDA topic</strong>.  In Model 3,  I explore new features not used in Model 1 and 2.  The new feature is a package's topic based LDA (Latent Dirichlet Allocation).  The LDA topic of a package is inferred from the word counts of its man pages.  I use the <a href=""https://github.com/johnmyleswhite/r_recommendation_system/blob/master/data/topics.csv"">topics.csv</a>, kindly prepared by the contest organizers, to map a R package to one of the 25 LDA topics (See <a href=""https://github.com/johnmyleswhite/r_recommendation_system/blob/master/topic_models.R"">topic_models.R</a> for details on running LDA, provided by the contest organizer).

The classification model of Model 3 is similar to Model 2.  Model 3 replaces user factors in Model 2 with T LDA factors weights $latex \mathbf{t}_u$ (T=25 here), and replaces package factors in Model 3 with T dummy variables $latex \mathbf{p}$.  The classification model is $latex f(u, v) = \mu + \mu_u + \mu_p + \mathbf{t}_u^T \mathbf{p}$.  The loss function is the same as Model 2, and the regularizer is L2, $latex R(\cdot) = ||\mu_u||^2 + ||\mu_p||^2 + ||\mathbf{t}_u||^2 $.

Model 3 achieves better AUC than Model 1, resulting in AUC of ~0.97 (labeled as M3u in Figure 1).  Prior to M3u, I explore a simpler model that users share the same weights $latex \mathbf{t}$.  The parameter space becomes smaller, but Model 3 with shared parameters (labeled as M3b in Figure 1) performs slightly worse than Model 3 with user-specific parameters.  This makes sense because it is unlikely every R user shares the same interest in the same set of LDA topics.

<strong>Model 4: Package task view</strong>.   R packages are organized into task views (e.g., high-performance computing,  survival analysis, and time series.  See the <a href=""http://cran.r-project.org/web/views/"">page</a> for the complete list of views).  It is possible that a user interested in a particular task would install not just one but many, even all, packages in the same task view (e.g., using install.views() R function).  We could improve recommendation if we know how much a user is interested in a particular task view.  I use the <a href=""https://github.com/johnmyleswhite/r_recommendation_system/blob/master/data/views.csv"">views.csv</a>, provided by the contest organizers, to map a package to its task view.

The classification model of Model 4 is similar to Model 3, except that LDA topics in Model 3  are replaced with task views (T=29, 28 task views plus 1 unknown view).  The performance of Model 4 is labeled as M4u in Figure 1.  Model 4 performs well and better than Model 1.  Similarly, I experiment with a variant of Model 4 that all users share the same task view parameters $latex \mathbf{t}$, labeled as M4b i n Figure 1, and it performs worse than Model 4 with per-user parameters.  The finding is consistent with Model 3, and R users, at least on the R recommendation data set, seem to have different preferences in task views.

[caption id=""attachment_79"" align=""aligncenter"" width=""442"" caption=""Figure 1.  The performance of individual models on the training set.  The x-axis is selected ten models in four model families, and the y-axis is the pooled AUC over 10-fold cross validation on the training set (the higher, the better).  The larger dots are those chosen for ensemble learning.""]<a href=""http://www.kaggle.com/blog/wp-content/uploads/2011/02/individual_models1.png""><img class=""size-large wp-image-884  "" title=""individual_models"" src=""http://www.kaggle.com/blog/wp-content/uploads/2011/02/individual_models1-1024x1024.png"" alt="""" width=""442"" height=""442"" /></a>
[/caption]

<strong>Ensemble learning</strong>.  I combine four classifiers using logistic regression.  To collect out-of-sample training data for the ensemble learner, I divide the training data into three sets: 80%, 10%, and 10%.  I train individual classifier on the 80%.  I then apply the trained classifiers on the 20%, and combine the scores from individual classifiers as training data for the ensemble learner. Both individual classifiers and the combiner are evaluated on the last 10% data set (as those shown in Figure 1 and Figure 2).  After evaluating one fold, I shift data and conduct another folds until all data are used, resulting in a total of 10 ensemble learners.  The output of these 10 ensemble learners are averaged to produce final predictions.

The ensemble learning works very well, as shown in Figure 2.  Combining M1 and M2 achieves AUC of 0.9721, which is higher than either M1 0.94 or M2 0.9702 alone.  When I combine more models and include M3 and M4, the performance gets even better.  The submission of combining four models achieves best performance on the training set among all models.  On the test set the final model achieves AUC of 0.983289 (after post-processing, see below), the highest among my submissions.  The success of ensemble training is possibly because the individual models are strong performers, and models are diverse with different classification models and features such that they complement each other.
<div class=""mceTemp mceIEcenter""><dl id=""attachment_106"" class=""wp-caption aligncenter"" style=""width: 501px;""> <dt class=""wp-caption-dt""><span style=""font-size: 11px; line-height: 17px;""><a href=""http://www.kaggle.com/blog/wp-content/uploads/2011/02/ensemble1.png""><img class=""size-large wp-image-890 "" title=""ensemble"" src=""http://www.kaggle.com/blog/wp-content/uploads/2011/02/ensemble1-1024x819.png"" alt="""" width=""491"" height=""393"" /></a>
Figure 2. The performance of ensemble learning models. The x-axis is three ensembles, ranging from two to four individual models. The y-axis is the pooled AUC of 10-fold cross validation on the training set.  The dash lines are the performance of four individual models.</span></dt> </dl></div>
<strong>Post-processing. </strong> I apply two post-processing steps before submitting each entry.  First, the label and (user, package) association on the training set is memorized.  For any (user, package) pairs in the test set that are already seen in the training set,  their labels on the training set are recalled and used instead.  Second, I assume when a user install a package P, the user also installs the packages that P depends on.  I record all packages a user installs on the training set as well as their dependent packages.  Given a pair (U, P) on the test set, if the package P is found in the set of the packages on which the user U' installed packages depend, I ignore the prediction and output 1.0 instead.  Although the dependency assumption does not completely hold true (there are known contradictory examples on the training set),  the two filters combined increases absolute AUC ~0.004, which matters in a competition with razor-thin margins between leading submissions.

I develop the classification training programs for Model 2, 3, and 4 in Python.  I use R to explore data, run logistic regression (glm() in the stats library), calculate AUC (performance() in the ROCR library), and plot results (ggplot() in the ggplot2 library).  All programs are developed and run on a commodity PC with dual-core 2GHz CPU and 4G RAM.  I make the source codes available at <a href=""https://github.com/m4xl1n/r_recommendation_system"">github</a>.

Although my submissions are based on only the data the contest organizers provide and simple linear models, by no means you should stop here.  There are many directions worth exploring, for examples, crawling CRAN to analyze the rich graphs of depends, imports, suggests, and enhances between R packages.   In an open-end contest like this, the sky is the limit.

<em>Max Lin is a software engineer with Google Research in New York City office, and the tech lead of the Google Prediction API.  Prior to Google, he published research work in video content analysis, sentiment analysis, machine learning, and cross-lingual information retrieval.  He has a PhD in Computer Science from Carnegie Mellon University.</em>

<em>He would like to thank the organizers of the R package recommendation engine competition for their hard work and efforts in putting this competition together.  He participates the competition as individuals, and writes all codes in his spare time.  Nothing expressed here may or should be attached to his employer.</em>"
Quan Sun on finishing in second place in Predict Grant Applications,http://blog.kaggle.com/2011/02/21/quan-sun-on-finishing-in-second-place-in-predict-grant-applications/,2011-02-22 04:03:17,"<strong>My background</strong>

I'm a PhD student of the Machine Learning Group in the University of Waikato, Hamilton, New Zealand. I’m also a part-time software developer for 11ants analytics. My PhD research focuses on meta-learning and the full model selection problem. In 2009 and 2010, I participated the UCSD/FICO data mining contests.

<!--more-->

<strong>What I tried and <strong>What ended up working</strong></strong>

I tried many different algorithms (mainly weka and matlab implementations) and feature sets in nearly 80 submissions. This report will briefly introduce two approaches that worked for this competition. Each of them will be discussed sequentially in the order of submissions.

After the first 10 testing submissions, I realised that there was a concept drift happening between 2007 and 2008. The success rates decline gradually from 2007. Also, on the information page of the contest, it states that “In Australia, success rates have fallen to 20-25 per cent…”. To me, this probably means, the decision rules for grant applications were somehow changed during 2007 and 2008. Here are some consequences that I could think of, including but not limited to:
<ul>
	<li>The overall success rates will continue to drop</li>
	<li>Successful applications in 2005/2006 would be declined in 2007/2008, so for 2009/2010</li>
	<li>Success patterns becoming to be “more” random</li>
	<li>Decision rules for year 2009/2010 will be close to that for 2007/2008, compared with rules for year 2006 and prior.</li>
</ul>
Based on the information and assumptions above, I decided to mainly use data points from 2007 and 2008 for training my classifiers, which turns out to be a reasonable choice.

<strong>Approach A: Ensemble Selection with transformed feature set (used in the first 20 submissions)</strong>

<strong>Data engineering/transformation part</strong>
<table border=""0"" cellspacing=""0"" cellpadding=""0"">
<tbody>
<tr>
<td width=""312"" valign=""top""><strong>Original attribute</strong></td>
<td width=""312"" valign=""top""><strong>Transformation method</strong></td>
</tr>
<tr>
<td width=""312"" valign=""top"">Start.date</td>
<td width=""312"" valign=""top"">to numeric, year, month,   day in numbers</td>
</tr>
<tr>
<td width=""312"" valign=""top"">RFCD.Code.X (X=1 to 5)</td>
<td width=""312"" valign=""top"">to nominal</td>
</tr>
<tr>
<td width=""312"" valign=""top"">Person.ID.X (X=1 to 15)</td>
<td width=""312"" valign=""top"">to nominal</td>
</tr>
<tr>
<td width=""312"" valign=""top"">Number.of.Grant.X (X=1 to   15)</td>
<td width=""312"" valign=""top"">Total number of successful/unsuccessful   grants per application</td>
</tr>
<tr>
<td width=""312"" valign=""top"">Publications AA, A, B, C</td>
<td width=""312"" valign=""top"">Total number of AA, A, B, C   publications per application</td>
</tr>
<tr>
<td width=""312"" valign=""top"">Role.X</td>
<td width=""312"" valign=""top"">Total number  of CHIEF_INVESTIGATORs,   PRINCIPAL_SUPERVISORs, DELEGATED_RESEARCHER, EXT_CHIEF_INVESTIGATORs per   application</td>
</tr>
<tr>
<td width=""312"" valign=""top"">Country.of.Birth.X</td>
<td width=""312"" valign=""top"">Total number of   Asia_Pacific born, Australia, Great_Britain, Western_Europe, Eastern_Europe,   North_America, New_Zealand, Middle_East_and_Africa per application</td>
</tr>
<tr>
<td width=""312"" valign=""top"">With.PHD</td>
<td width=""312"" valign=""top"">Total number of PhDs per application</td>
</tr>
<tr>
<td width=""312"" valign=""top"">Years.IN.UNI</td>
<td width=""312"" valign=""top"">Total number of people who   has been in the University for more than 5 years</td>
</tr>
</tbody>
</table>
After all those transformations are done, I also had a java program to transform all nominal attributes to its corresponding frequency. The frequency counting is based on all the available data points. So, the final feature set consists of the original features, transformed features and frequency.

<strong>Modeling part</strong>

My main method is called Ensemble Selection, originally proposed by Rich Caruana and co-authors of Cornell University (http://portal.acm.org/citation.cfm?id=1015432). The following pseudocode demonstrates the basic idea of Ensemble Selection:

0. Split the data into two parts: The build set and the hillclimb set

1. Start with the empty ensemble.

2. Add to the ensemble the model (trained on “build” set) in the library that maximizes the ensemble’s performance to the error metric (AUC for this contest) on a “hillclimb” (validation) set.

3. Repeat Step 2 for a ﬁxed number of iterations or until all the models have been used.

4. Return the ensemble from the nested set of ensembles that has maximum performance on the hillclimb (validation) set.

<strong>Model library used for my Ensemble Selection system:</strong>

AdaBoost, LogitBoost, RealAdaBoost, DecisionTable, RotationForest, BayesNet, NaiveBayes, 7 algorithms with different parameters, in total 28 base classifiers.

<strong>Building set and hillclimb set for Ensemble Selection:</strong>

Data points from year 2007 are used as the “build set”

Data points from year 2008 are used as the “hillclimb set”

Or

Data points from year 2007/01/01 to 2008/04/30 are used as the “build set”

Data points after year 2008/04/30 are used as the “hillclimb set”

Both setups worked well for the Ensemble Selection approach.

In summary, the final system for Approach A consists of three main components:

Data points from 2007 for training and 2008 for hillclimbing.

Ensemble Selection, num of bags: 10, hillclimb iterations = size of the model library.

In total 352 features.

Learderboard AUC: 0.956X, Best final test set AUC: 0.961X

From submission 20 to the end of the competition, the following features are added to Approach A feature set:

Number of missing values

Number of non-missing values

Missing value rate

Transform “Contract.Value.Band” to numeric values

Average contract value

RFCD.CODE mean, sum, max, min, standard deviation per application based

RFCD.PCT mean, sum, max, min, std per application based

SEO.CODE mean, sum, max, min, std per application based

SEO.PCT mean, sum, max, min, std per application based

Successful.grant mean, sum, max, min, std per application based

Unsuccessful.grant mean, sum, max, min, std per application based

Successful.grant mean average per application based

Successful.grant sum average per application based

All the above features for the first three applicants

All the above features for Unsuccessful.grant

Success rate of applicant 1, applicant 2, and applicant 3 per application based

Success rate of all applicants per application based

Mean, max, std success rates of all applicants per application based

Number of publications mean, sum, max, min, std per application based

Except the frequency counting described in Approach A, only “row-based (per-application-based)” statistical features were gradually introduced to my system during the competition, because I thought that, compared with “time based/column based features”, “row-based” statistical features would reduce the chance of overfitting.

Also, the following algorithms (with different/diverse parameter settings) were gradually added to the model library while the competition:

RandomForest

RacedIncrementalLogitBoost

Bagging with trees

ADTree

Linear Regression

RandomCommittee with Random Trees

Dagging

J48

<strong>Approach B: Rotation Forest with the feature set from Approach A</strong>

<strong> </strong>

I tried using only Rotation forest (http://www.computer.org/portal/web/csdl/doi/10.1109/TPAMI.2006.211) with the following setup:

Base classifier: M5P model tree (weka default is J48)

Rotation method: Random Projection with Gaussian distribution (weka default is PCA)

The Rotation forest classifier was trained on data points from 2007 and 2008 with the feature set from Approach A. Here are the results:

Leaderboard AUC: 0.947X, Final test set AUC: 0.962X

Averaging the two approaches could improve the final test set AUC to 0.963X.

<strong> </strong>

<strong> </strong>

<strong>What tools I used</strong>

<strong> </strong>

Software/Tools used for modelling and data analysis:

Weka 3.7.1 is used for modelling (with my own improved version of the Ensemble Selection algorithm)

Matlab and SAS are used for data visualization and statistical analysis

Java is used as the main programming language for this project

Most experiments were done on my home PC: AMD 6-core, 16G ram on Windows system."
Yuanchen He on finishing third in the Melbourne University competition,http://blog.kaggle.com/2011/03/01/yuanchen-he-on-finishing-third-in-the-melbourne-university-competition/,2011-03-01 09:40:09,"<strong>Background </strong>

I am Yuanchen He, a senior engineer in McAfee lab. I have been working on large data analysis and classification modeling for network security problems.

<strong>Method</strong>

Many thanks to Kaggle for setting up this competition. And congratulations to the winners! I enjoyed it and learned a lot from working on this challenging data and reading the winners' posts.  I am sorry I didn't find free time last week to write this report.

<!--more-->

The data came with a lot of categorical features with a high number of values. At the very beginning, I removed useless features (by weka.filters.unsupervised.attribute.RemoveUseless -M 99.0) and removed the features with almost 100% missing values. After that, I tried to transform the categorical features into a group of binary features with each is a yes or no on a specific value. I also generated 4 quarter features and 12 month features from startdate and generated binary indicator features for missing values. The binary features, date-based features, indicator features, as well as other numerical features, after simply filling missing values with mean, were fed into R randomForest classifier for RFE. With that I got 94.9x on the leaderboard. I kept tuning along this way but the accuracy cannot be improved further. Then I started to suspect there were some information loss during the process of feature transformation and feature selection.

So I tried to build classifiers directly on the categorical features without transforming them into binary features. A simple frequency based pre-filtering was applied. For a raw categorical feature, all values presented less than 10 instances in the data were combined into a specific common value ""-1"". However, R randomForest cannot accept a categorical feature with more than 32 values. So I had to split each categorical feature again into ""sub features"", with each has no more than 32 values. The way I split the values into different sub features was sorting the values with information gain first, and then top 31 values were assigned into sub feature 1, the next 31 values were assigned into sub feature 2, and so on. With this feature transformation strategy I got 94.6x on the leaderboard.

The next one I tried was simply combining the top features from the above two methods. The randomForest classifiers on the combined feature sets can improve the leaderboard ROC to 95.1x-95.3x, depending on the instances used for training. The best classifiers were generated from training only on instances after 0606, only on instances after 0612, and only on instances after 0706. Finally, I observed the prediction results from these classifiers were different enough and hence it was worth to make a major voting from them, and I got my best leaderboard AUC 95.555, which generalized to the other 75% test instances with the final AUC 96.1051"
Junpei Komiyama on finishing 4th in the Ford competition,http://blog.kaggle.com/2011/03/16/junpei-komiyama-on-finishing-4th-in-the-ford-competition/,2011-03-17 02:25:05,"<strong>My background</strong>

My name is Junpei Komiyama. I obtain a Master's degree in computational and statistical physics at The University of Tokyo, Japan. I have been working in a team developing a live-streaming website (<a href=""http://live.nicovideo.jp/"" target=""_blank"">http://live.nicovideo.jp</a>) for two years, contributing mainly to designing and implementation of DB tables, cache structures, and front-end programs of the site.

<!--more-->

<strong>What I tried</strong>

Each team received three sets of data: Training data, testing data and submission example. The training and the test data are the records of sequential observations of car drivers. The observation data consisted of eight physiological data (P1-P8), eleven environmental data (E1-E11) and eleven vehicular data (V1-V11), recorded in every 100ms. Information about meaning of each data was not given. The training data consisted of 500 car drivers' records of observation data associated with judgments as to whether each driver was alert or not. The test data set consisted of another 100 drivers' records of observation and had no overlap with the training data. We were requested to estimate whether each driver in the test data set was alert or not. Each team was evaluated by the value of“area under the receiver-operating-characteristics curve (AUC)"". All data in the training set were used for machine training. To solve this problem, I constructed a Support Vector Machine (SVM), which is one of the best tools for classification and regression analysis, using the libSVM package.

In my first attempt, I simply put the training data into libSVM with default settings (C-SVC and RBF kernel) and evaluated the test data with the trained model. This approach took more than 3 hours to complete the training step and yielded a modest AUC score (0.745). Meanwhile, observations were visualized for each person using python/gnuplot. I found some data (P3-P6) were characterized by strong noise, fluctuating more than 50 percent for every 100ms. Also, many environmental and vehicular data showed discrete values continuously increased and decreased. These suggested the necessity of pre-processing the observation data before SVM analysis for better performance.

To improve the AUC score, I took the following approaches:

1.To smoothen the observation data by removing extremely small temporal values. Sometimes observation values dropped to nearly zero and then immediately resumed to the former level, presumably due to failure of observation. Therefore, I attempted to remove such instantaneous near-zero values. However, this attempt rather caused slight reduction in the AUC score and so was abandoned.

2.To integrate the present and old time points in the observation data. The observation records were taken at every 100 ms (milliseconds), which is fairly more frequent than many events of human activities. I assumed that changes in the alert state might have a certain length of delay after an observation was made. I tested this possibility by averaging each observation datum point with an old datum point collected 100-500 ms before. However, such an attempt of data integration did not improve the AUC score.

3.To average several datum points. I found that this simple process not only improved the AUC score and also significantly reduced computing time. Therefore, this approach was taken for further optimization. Since there was the two-submissions-per-day limit for evaluation, it was necessary to examine changes in performance in a local setting through cross-validation of the SVM training data set.

<strong>What ended up working</strong>

Pre-processing before SVM: I attempted to determine the optimal number of datum points to be averaged. Averaging 100 rows into a single row resulted in too coarse data whereas averaging only 3 rows had little effect. I empirically determined that averaging 7 consecutive datum points provided the optimal result, reducing the SVM training time by 86% and increasing the AUC by approximately 0.01.

SVM: Among the available options offered by libSVM (i.e., C-SVC, nu-SVC, one-class SVM, epsilon-SVR and nu-SVR), the epsilon-SVR performed best for this problem, enhancing the AUC than C-SVC/option -b by about 0.05. Linear, polynomial, RBF and sigmoid kernels were then tested. I found that kernel choice had relatively minor effects although RBF kernel performed best.

With these optimized SVM type and kernel function, I optimized the SVM parameters namely, cost parameter c, kernel parameter g, and loss function parameter p.

In the final setting, epsilon-SVR, RBF kernel with optimized parameters (c=2, g=1/30 and p=0.1) yielded a greatly improved AUC (0.839).

<strong>Tools I used</strong>

Python for processing the CSV data.
Gnuplot for visualizing the observation data.
LibSVM for constructing a SVM model, including processes of training and prediction."
Phil Brierley on winning tourism forecasting part two,http://blog.kaggle.com/2011/03/22/phil-brierley-on-winning-tourism-forecasting-part-two/,2011-03-22 09:22:45,"I was Team “Sali Mali” which won the seasonal part of the online tourism forecasting competition. The aim was to produce the smallest MASE for the 427 quarterly time series and 366 monthly timeseries. In this article, I brieﬂy describe the methods used.

<!--more-->

<strong>Basic algorithms</strong>

No new time series forecasting algorithms were developed speciﬁcally for this contest. We basically took algorithms that already existed as ‘building blocks’ and combined their forecasts in speciﬁc ways. Based on Athanasopoulos et al. (2011), and using the R package ‘forecast’ (Hyndman, 2011), four base algorithms were used:

1.  Seasonal Naïve;

2. Damped Holt-Winters

3. ARIMA;

4. ETS.The benchmark to beat was the ETS algorithm.

The benchmark forecasts were replicated and visually inspected and it became clear that the forecasts for some of the series (only one or two) were clearly ‘unlikely’, in the sense that they basically went oﬀ the scale. The other algorithm sseemed to give much more realistic forecasts for these particular series.

The approach then taken was to concentrate on protecting against these ‘disastrous’ forecasts. The mindset was not thinking how to improve the overall accuracy, but how to prevent the worst case events. One way of achieving this is by not putting all your eggs in one basket (i.e. only relying ona single algorithm). This technique is commonly known as ‘ensembling’

<strong>An ensemble approach</strong>

Two methods were used in the ensembling process:

1.  The last 12 months of the training data was set aside as a holdout set, and the MASE acrossall the series was calculated for each algorithm on this set. Based on these MASE values, aweighting was assigned to each algorithm, with the total of the weightings summing to 1. Thus four predictions were made for each series, with the ﬁnal prediction being a weighted av-erage. The weights for each algorithm where consistent across each series within the monthlyand quarterly series types. This global weighted average method gave an improvement overthe baseline method.

2. Forecasts for three algorithms (Damped, ARIMA, ETS) were generated using four diﬀerentsized training windows. The Seasonal Naïve forecast was then added, to give 13 forecasts foreach point in each series. The ﬁnal forecast for each point was then the median value of these 13 individual forecasts. This local selection method also gave an improvement over the baseline method.

The ﬁnal solution was then a weighted average of methods 1) &amp; 2).

<strong>The cheat factor</strong>

If the ﬁrst 12 months of the benchmark forecast is simply replicated for the second 12 months, rather than relying on the algorithms forecasts for months 13-24, then the overall benchmark accuracy was improved. This is in line with the organisers’ ﬁndings that as the time horizon increases, the gain in accuracy of certain algorithms over the seasonal naïve method was diminished. The leaderboard was used to determine the ‘year two growth factor’ that should be applied. It was found that just multiplying the ﬁrst year’s predictions by approximately 1.04 gave the best second year predictions, as determined by the leaderboard score. In other words, the forecasts for months 13–24 obtained from the algorithm were completely disregarded and replaced by a simple multiple of the ﬁrst year’s forecasts.

<strong>Comments on the competition</strong>

The prediction dates for the time series are more than likely to be the last two years available, meaning that the physical dates are the same for many series (this hypothesis is based on our ﬁnding that a growth rate of 1.04 seemed to work across all series). Because tourism ﬁgures are likely to be aﬀected by global factors (GFC, exchange rates, etc., that are largely unpredictable) it is likely the values will all trend similarly. Thus the conclusion that one algorithm is better for tourism data than another algorithm must be treated with caution as it might just be the algorithm that fortunately got the trend correct for that particular moment in time.  It is suggested that more generalizable results may have been obtained if the series were deliberately staggered in time.

It seems common practice to report time series on a calendar month basis.  In reality people operate on a weekly cycle, not a monthly cycle. This causes problems in series that have signiﬁcant daily ﬂuctuations — for example car hire volumes can be very diﬀerent during the week than the weekend. The implication for forecasting is that a big diﬀerence between one month and the next, or the same month in the previous year, can be due to the diﬀerence in having four weekends in a month and ﬁve weekends in a month. Reporting tourism ﬁgures in a four weekly cycle rather than a monthly cycle would lead to improvements in forecast accuracy, and this may give better reward for eﬀort than additional algorithm development."
Inference on winning the Ford Stay Alert competition,http://blog.kaggle.com/2011/03/25/inference-on-winning-the-ford-stay-alert-competition/,2011-03-25 09:32:48,"The “Stay Alert!” competition from Ford  challenged competitors to predict whether a car driver was not alert based on various measured features.

The  training  data  was  broken  into  500  trials,  each  trial  consisted  of a  sequence  of  approximately  1200  measurements  spaced  by  0.1  seconds. Each measurement consisted of 30 features;  these features were presentedin three sets:  physiological (P1...P8), environmental (E1...E11) and vehic-ular (V1...V11).   Each feature was presented as a real number.   For each measurement we were also told whether the driver was alert or not at thattime (a boolean label called IsAlert).  No more information on the features was available.

<!--more-->

The test data consisted of 100 similar trials but with the IsAlert label hidden. 30% of this set was used for the leaderboard during the competition and 70% was reserved for the ﬁnal leaderboard.  Competitors were invited to submit a real number prediction for each hidden IsAlert label.  This realprediction should be convertible to a boolean decision by comparison with a threshold.

The accuracy assessment criteria used was “area under the curve” (AUC).   The “curve” is the receiver-operating characteristic (ROC) curve where  the  true-positive  rate  is  plotted  against  false-positive  rate  as  this threshold is varied.  An AUC value will typically vary between 0.5 (random guessing) and 1 (perfect prediction).

See the full explanation of Inference's method in the <a href=""http://www.kaggle.com/blog/wp-content/uploads/2011/03/ford.pdf"">attached PDF</a>."
José P. González-Brenes and Matías Cortés on winning the RTA challenge,http://blog.kaggle.com/2011/03/25/jose-p-gonzalez-brenes-and-matias-cortes-on-winning-the-rta-challenge/,2011-03-26 01:26:30,"<div>

We are Team Irazú, <a href=""http://www.josepablogonzalez.com/"">José P. González-Brenes</a> and <a href=""http://grad.econ.ubc.ca/gmcortes/"">Matías Cortés</a>. We finished 1<sup>st</sup> in the RTA  Challenge.

<strong>What didn’t  work</strong>

We started off exploring the data by calculating means for  different combinations of time, day of week and month. We plotted these means to  identify patterns in the data. We also explored forecasting based on <em>linear regressions</em>.

<!--more-->

We realized that including historical data led to less  accurate predictions, so we attempted using <em>weighted linear regressions</em> where  proportionally higher weights were assigned to more recent datapoints.

We also tried forecasting through <em>autoregressions</em>, where the  preceding <em>n</em> observations (<em>t-1, t-2, …, t-n) </em>are used to make a prediction.

In the end, we decided to work only with the more recent  dataset, and ignore the historical data provided by Kaggle.

<strong>What ended up  working</strong>

We used a statistical technique called Ensemble of Decision  Trees (often called Random Forest™).  One  of the nice properties of this approach is that it doesn’t assume a linear  relationship between explanatory variables and the predicted outcome.

We used the following explanatory variables:<strong> </strong>
<ul>
	<li>What time is it? (Hour + (Minute/60))</li>
	<li>What is the date? (Month + (Day/31))</li>
	<li>What day of the week is it? (Monday, Tuesday…)</li>
</ul>
<em>“All roads are equal,  but some roads are more equal than others!”<strong> </strong></em>

Our final algorithm combines two methods. Both methods  include time, date, and day of week as explanatory variables.

<strong><em> </em></strong>

<strong><em>Method A:</em></strong>

Additionally encodes most recent observation for <span style=""text-decoration: underline;"">2</span> neighboring routes

<strong><em>Method B:</em></strong>

Additionally encodes most recent observation for <span style=""text-decoration: underline;"">4</span> neighboring routes and recent trend in travel time
<table border=""0"" cellspacing=""0"" cellpadding=""0"">
<tbody>
<tr>
<td width=""154"" valign=""top"">&nbsp;
<p style=""text-align: center;""><a href=""http://www.kaggle.com/blog/wp-content/uploads/2011/03/image001.png""><img class=""aligncenter size-full wp-image-956"" title=""secret recipe"" src=""http://www.kaggle.com/blog/wp-content/uploads/2011/03/image001.png"" alt=""secret recipe"" width=""134"" height=""121"" /></a></p>
</td>
<td width=""570"" valign=""top"">We tried Method A and Method B, and discovered that for some  route segments, Method A performed better, while for others, Method B performed  better!&nbsp;

&nbsp;

Our final algorithm uses:

Method A for route segments 40105-41160

Method B for route segments 40010-40100<strong> </strong></td>
</tr>
</tbody>
</table>
&nbsp;

We prepared a PDF providing more details about Random Forests  and our solution. <a href='http://blog.kaggle.com/wp-content/uploads/2011/03/team_irazu_screencast.pdf'>Click here</a> to read  it!

</div>"
Mick Wagner on finishing second in the Ford Challenge,http://blog.kaggle.com/2011/04/20/mick-wagner-on-finishing-second-in-the-ford-challenge/,2011-04-20 17:04:46,"<strong>Background</strong>

My name is Mick Wagner and I worked by myself on this challenge in my free time.  I am fairly new to data mining but have been working in Business Intelligence the last 5 years.  I am a senior consultant in the Data Management and Business Intelligence practice at Logic20/20 in Seattle, WA.  My undergrad degree is in Industrial Engineering with an emphasis on Operations Research and Management Science out of Montana State University.   This is my second Kaggle competition I have entered.

<!--more-->

The <a href=""http://www.kaggle.com/c/stayalert"">Stay Alert challenge</a> was sponsored by Ford to help prevent distracted drivers.  The objective of this challenge is to design a detector/classifier that will detect whether the driver is alert or not alert, employing any combination of vehicular, environmental and driver physiological data that are acquired while driving.

The data for this challenge shows the results of a number of ""trials"", each one representing about 2 minutes of sequential data that are recorded every 100 ms during a driving session on the road or in a driving simulator.  The trials are samples from some 100 drivers of both genders, and of different ages and ethnic backgrounds.  The actual names and measurement units of the physiological, environmental and vehicular data were not disclosed in this challenge. Models which use fewer physiological variables are of particular interest; therefore competitors are encouraged to consider models which require fewer of these variables.

<strong>Tools Used</strong>

Microsoft Excel

Microsoft SQL Server Stack (SQL Server Engine, SQL Server Integration Services, SQL Server Analysis Services)

<strong>Data Analysis</strong>

I spent the majority of my time analyzing the data.  I inputted the data into Excel and started examining the data taking note of discrete and continuous values, category based parameters, and simple statistics (mean, median, variance, coefficient of variance).  I also looked for extreme outliers.

I read through the Kaggle discussion board to see if anything about the challenge had been changed or if Ford provided additional insight into the data.

The most important step I made was to take a step back and holistically examine the problem and the dataset.  The data was composed of various trials with different human experimenters.  My background in statistical quality control immediately told me that this would have a large impact on the statistical analysis and needed to be factored into the design of the system.  To do this, I created my own sets of test and training data.  Normally, Microsoft SQL Server randomly derives these sets based off of a HoldOutMaxPercent value that dictates the Test data size.  I made the first 150 trials (~30%) be my test data and the remainder be my training dataset (~70%).  This single factor had the largest impact on the accuracy of my final model.

My next breakthrough decision was limiting the copious amount of data that went into my algorithm.  I was concerned that using the entire data set would create too much noise and lead to inaccuracies in the model.  The final goal of the system is to detect the change in the driver from alert to not alert so that the car can self-correct or alert the driver.  So I decided to just focus on the data at the initial moment when the driver lost alertness.  This reduced my dataset significantly and I repeated my initial Excel analysis of the data.  I highlighted which factors consistently had a relatively large delta between status changes.

<strong>Failed Attempts</strong>

My first few attempts were running different types of models with the all the variables and Microsoft’s recommended variables.  I used a lift chart to compare their accuracy to each other.  My goal was to narrow down the possible algorithms given by Microsoft SQL Server Analysis Services from seven down to 2.  Several models (Association Rules, Linear Regression, and Logistic Regression) did not make sense to use because of the data types, structure of the data, and desired binary output.  The Decision Tree and Neural Network scored the highest on my lift chart.

<strong>What Ended Up Working</strong>

After testing the Decision Tree and Neural Network algorithms against each other and submitting models to Kaggle, I found the Neural Network model to be more accurate.

After initially trying the variables recommended by SSAS (SQL Server Analysis Services), I augmented the solution with the variables I found key in my initial analysis.  These variables included: E4, E5, E6, E7, E8, E9, E10, P6, V4, V6, V10, and V11.

As recommended by Ford, I tried to avoid relying on physiological variables.  I did find P6 to be helpful though.

The key strengths of my model are ease to build, ease to deploy and maintain in an industry setting with transactional data, and scalability within SQL Server.  My model uses an out of the box algorithm that is well understood and respected.  It also took me significantly less attempts (and time) to scale to the top of the standings:

1<sup>st</sup> place: 24 entries

2<sup>nd</sup> place (me): 8 entries

3<sup>rd</sup> place: 39 entries

4<sup>th</sup> place: 25 entries

&nbsp;"
Andrew Newell and Lewis Griffin on winning the ICDAR 2011 Competition,http://blog.kaggle.com/2011/05/04/andrew-newell-and-lewis-griffin-on-winning-the-icdar-2011-competition/,2011-05-04 18:36:01,"<p>At the core of our method was a system called oriented Basic Image Feature columns (oBIF columns). This system has shown good results in several character recognition tasks, but this was the first time we had tested it on author identification. As we are a computational vision group, our focus was on the visual features rather than on the machine learning, and we used a simple Nearest Neighbour classifier for our experiments and entries.</p>
<p><!--more--></p>
<p><strong>oBIF Columns</strong></p>
<p>The description of oBIFs begins with Basic Image Features (BIFs). In this system every location in an image is assigned to one of seven classes according to local symmetry type, which can be dark line on light, light line on dark, dark rotational, light rotational, slop, saddle-like or flat. The class is calculated from the output of six Derivative-of-Gaussian filters. The algorithm takes two parameters. First, the scale parameters, which determines the size of the filters and second, a threshold that determines the likelihood that a location will de classified as flat. An extension to the BIF system is to include local orientation, to produce oriented Basic Image Features (oBIFs). The local orientation that can be assigned depends on the local symmetry type. For the slope type, a directed orientation (arrow like) makes sense, whereas for the line and saddle-like types an undirected orientation (double-ended arrow) applies. The rotational and flat types have no orientation assigned. There is an extra parameter associated with oBIFs, that determines the level of orientation quantization. In previous work we have found that a set of 23 oBIFs is sufficient for most applications. Examples of an oBIF encoded image are shown in the figure, with the original section of image at the top, the oBIF encoding at a fine scale and the oBIF encoding at a coarser scale. The colours indicate symmetry type and the lines indicate orientation.</p>
<p><a href=""http://blog.kaggle.com/wp-content/uploads/2011/05/kagImage.png""><img title=""kagImage"" src=""http://blog.kaggle.com/wp-content/uploads/2011/05/kagImage.png"" alt="""" width=""350"" height=""407"" /></a></p>
<p>&nbsp;</p>
<p>In order to increase the discriminative power of the system, we then combine oBIFs at different scales to produce the oBIF column features. Typically only two scales are required to produce a sharp increase in discriminative power, whereby the set of features is increased to 232. There is an extra parameter associated with oBIF columns, which is the ratio between the scales used. Our basic approach to describing images using oBIFs, is to count the occurrences of each of the 232 types of feature and discard the locations, hence our descriptor is a 232-bin histogram. Previous experience has shown that we generally get the best results using rooted-normalized histograms( i.e. we start with histograms whose values add up to one, and square root each of the values) but for clarity we will simply refer to these as histograms in the remainder. Applying</p>
<p><strong>Applying oBIF Columns to Author Identification</strong></p>
<p>In previous work on character recognition, we have used histograms of oBIF columns directly with a Nearest Neighbour classifier. This works in general because the encoding for a particular letter ‘A’, in oBIF column space, is close to that for other ‘A’s. However, author identification is clearly a different problem as the aim is to spot differences in the style of the text, rather than recognise the content of the text. Thus, for the case of the ‘A’, the aim is to recognise a particular deviation from the average ‘A’ rather than recognise that it is an ‘A’. Therefore after calculating oBIF column histograms for all the images, we calculated the deviations from the mean histograms for training. These were then used to build a Nearest Neighbour classifier, with the Euclidean Distance as the metric.</p>
<p><strong>Parameter Tuning</strong></p>
<p>We had four parameters to tune, the scale, the ratio between the scales, the threshold and the orientation quantization. This was done using the training set and the optimal values were found to be very similar as we’ve used for other applications.</p>
<p><strong>Identifying Missing Authors</strong></p>
<p>A difficult aspect of the competition problem was the possibility that some of the images in the test set had authors that were not in the training set. In order to tackle this problem we first looked at the distribution of the test images in oBIF column histogram space to see if any were especially close (at least 3 standard deviations below the mean distance.) If none were (as was the case), the conclusion would be that no two test images were written by the same author. Then each test image was assigned to its nearest training image in oBIF column space. This process left three test images unclassified.</p>
<p><strong>Our Entries</strong></p>
<p>We made three entries.</p>
<p><strong>Entry 1</strong></p>
<p>For the first entry we were unaware that one of the columns (for author 040) had to be removed prior to submission so we got a large error.</p>
<p><strong>Entry 2</strong></p>
<p>For the second entry we assigned the three unclassified images as UNKNOWN, which produced an error on the public leaderboard equivalent to misclassifying one image.</p>
<p><strong>Entry 3</strong></p>
<p>For the third entry, we looked at the training authors that had not been assigned to any of the test images. We then looked at the next nearest neighbours for the three unclassified images to see whether any of the unassigned train authors featured there. If they did, they would be assigned to that test image. As it turned out, one of the unassigned training authors was the 2nd nearest neighbour of one of the unclassified test images, and so this single classification was changed. This gave us a public error of 0 and, as it turned out, both entries two and three got a final error of 0.</p>
"
The thrill of the chase: Tim Salimans on how he took home Deloitte/Fide chess comp,http://blog.kaggle.com/2011/05/29/the-thrill-of-the-chase-tim-salisman-on-how-he-took-home-deloittefide-chess-comp/,2011-05-29 09:52:03,"My name is Tim Salimans and I am a PhD candidate in Econometrics at Erasmus University Rotterdam. For my job I constantly work with data, models, and algorithms, and the Kaggle competitions seemed like a fun way of using these skills in a competitive and social environment. The Deloitte/FIDE Chess Rating Challenge was the first Kaggle contest I entered and I was very fortunate to end up taking first place. During the same period I also used Kaggle-in-class to host a prediction contest for an undergraduate course in Econometrics for which I was the teaching assistant. Both proved to be a lot of fun. This post is a nontechnical account of my experiences in the chess rating challenge. For more details, including my code, see <a href=""http://people.few.eur.nl/salimans/chess.html"">my web page</a>.

<!--more-->

<strong>Chess rating systems</strong>

The first thing to do when tackling a new problem is to look up what other people have done before you. Since Kaggle had already organized an earlier chess rating competition, the blog posts of the winners were a logical place to start. After reading those posts and some of the academic literature, I found that chess rating systems usually work by assuming that each player’s characteristics can be described by a single rating number. The predicted result for a match between two players is then taken to be some function of the difference between their ratings. Yannis Sismanis, the winner of the first competition, used a logistic curve for this purpose and estimated the rating numbers by minimizing a regularized version of the model fit. Jeremy Howard, the runner-up, instead used the TrueSkill model, which uses a Gaussian cumulative density function and estimates the ratings using approximate Bayesian inference.

I decided to start out with the TrueSkill model and to extend it by shrinking each player’s rating to that of their recent opponents, similar to what Yannis Sismanis had done in the first competition. In addition, I introduced weights into the algorithm which allowed me to put most emphasis on the matches that were played most recently. After some initial experimentation using the excellent <a href=""http://research.microsoft.com/en-us/um/cambridge/projects/infernet/"">Infer.NET</a> package, I programmed everything in Matlab.

<strong>Using the match schedule</strong>

The predictions of my base model scored very well on the leaderboard of the competition, but they were not yet good enough to put me in first place. It was at this time that I realized that the match schedule itself contained useful information for predicting the results, something that had already been noticed by some of the other competitors. In chess, most tournaments are organized using to the Swiss system, in which in each round players are paired with other players that have achieved a comparable performance in earlier rounds. If in a Swiss system tournament player A has encountered better opponents than player B, this most likely means that player A has won a larger percentage of his/her matches in that tournament.

In order to incorporate the information present in the match schedule, I generated out-of-sample predictions for the last 1.5 years of the data using a rolling 3-month prediction window. I then performed two post-processing steps using these predictions and the realized match outcomes. The first step used standard logistic regression and the second step used a locally weighted variant of logistic regression. The most important variables used in the post-processing procedure were:
<ul>
	<li>- the predictions of the base model</li>
	<li>- the ratings of the players</li>
	<li>- the number of matches played by each player</li>
	<li>- the ratings of the opponents encountered by each player</li>
	<li>- the variation in the quality of the opponents encountered</li>
	<li>- the average predicted win percentage over all matches in the same month for each player</li>
	<li>- the predictions of a random forest using these variables</li>
</ul>
&nbsp;

This post-processing dramatically improved my score and put me well ahead of the competition for some time. Later, other competitors made similar improvements and the final weeks of the competition were very exciting. After a long weekend away towards the end of the competition I came back to find that I had been surpassed on the leaderboard by team PlanetThanet. By tweaking my approach I was able to crawl back up during the next few days, after which I had to leave for a conference in the USA. Upon arrival I learned that I was again surpassed, now by Shang Tsung. Only by making my last submissions from my hotel room in St. Louis was I finally able to secure first place.

<strong>Conclusions</strong>

Much of the contest came down to how to use the information in the match schedule. Although interesting in its own right, this was less than ideal for the original goal of finding a good rating system. To my relief, the follow-up data set that Jeff Sonas made available showed that my model also makes good predictions without using this information. Finally, I would like to thank both the organizers and the competitors for a great competition!"
Mapping Matter with Matlab: Sergey Yurgenson on finishing second in Mapping Dark Matter,http://blog.kaggle.com/2011/09/02/mapping-matter-with-matlab/,2011-09-02 10:08:36,"<em>Sergey Yurgenson finished second the Mapping Dark Matter challenge and agreed to answer a few 'How I Did It' questions for our first post in the Mapping Dark Matter series.  Over the next few weeks we will be posting regular interviews with more of the top competitors.</em>

<strong>What was your background prior to entering Mapping Dark Matter?</strong>
I have a PhD in Physics from Leningrad (now St.Petersburg) State University, Russia.  I work at Harvard University developing software and hardware for neurobiology research and data analysis.
My first attempt at a data mining competition was the Netflix Prize.  I learned about it somewhere in the middle of the competition and spent several weeks building my model.  I managed to just barely beat the Netflix benchmark and realized that it required more time and hardware power than I was able to dedicate at the time.

Fortunately, many Kaggle competitions have a more manageable scope and can be done as a hobby rather than full time job.  Mapping Dark Matter was my third Kaggle competition; before that I came second in the RTA competition and made one submission in the Chess Rating challenge.

<strong><!--more-->What was your most important insight into the dataset?</strong>
Initially, I was trying to use a modified quadruple moments formula and fitting procedure.  However, I was not satisfied with the result.  My mind was constantly coming back to neural networks.  The only question was ‘what kind of parameters to use as inputs’?  The number of those parameters should be reasonable, and they need to describe images well.  Thus, the images’ principal components looked like the logical choice.  I calculated the positions of galaxies and stars and re-centered all the images, creating image stacks for galaxies and stars separately.  I then calculated the principle components for those stacks.

To my surprise, many of the principle components were easy to understand.  Here are components #2 and #3 and a scatter plot where x and y are amplitudes of components #2 and #3 and color corresponds to galaxy orientations:

[gallery]

Components #2 and #3 are quadruples with shifted phase and definitely reflect the orientation of elongated galaxies.  Many other components were also easy to interpret.  I used some of them to improve the center of object calculation.  The amplitudes of principle components for galaxies and stars served as inputs to the neural network.  I played with the network parameters until I found a good combination of the number of parameters and the network configuration.  In the end, I combined the results of multiple networks to calculate my final submissions.

<strong>Which tools did you use?</strong>
All calculations were done using Matlab.

<em>Thank you Sergey!</em>"
DeepZot on Dark Matter: How we won the Mapping Dark Matter challenge,http://blog.kaggle.com/2011/09/13/deepzot-on-dark-matter/,2011-09-13 10:03:54,"<em>Daniel Margala and David Kirkby (as team DeepZot) <a href=""http://www.kaggle.com/c/mdm/Leaderboard"">placed first</a> in the Mapping Dark Matter challenge.  Daniel agreed to answer a few questions for No Free Hunch as part of our <a href=""http://blog.kaggle.com/category/how-i-did-it/"">series</a> of posts on the best Mapping Dark Matter entries.  </em>

<strong>What was your background prior to entering Mapping Dark Matter?</strong>

I graduated from the University of California, Los Angeles in 2009 with a B.S. in Physics. In the course of my studies at UCLA, I learned Linux system administration and various scripting languages managing a cluster of servers and data archive for an astro-particle research group. I became interested in numerical analysis while investigating the polarity and momentum of muons produced in the atmosphere by incident cosmic rays. Currently, I am a PhD student in the Physics and Astronomy Department at the University of California, Irvine. My advisor (and DeepZot team member), Prof. David Kirkby, and I are using the <a href=""http://www.sdss3.org/surveys/boss.php"">Baryon Oscillation Spectroscopic Survey</a> (BOSS) to study the distribution of matter in our universe at the largest volumes. My work with BOSS has primarily focused on the operations software at the telescope, specifically, with the interfaces to the BOSS spectrograph, located at Apache Point Observatory in New Mexico.

<strong><!--more-->How did you come to form a team together?</strong>

I became interesting in working with David during a conversation that included an avid discussion of programming languages at a department event (where I was lured by the prospect of free food and drink, the perfect bait for graduate students). I began working on a variety of projects with David for about half a year, ranging from cosmology to electrical engineering, before we started working on the related <a href=""http://great.roe.ac.uk/great10challenge/GREAT10.html"">GREAT10 challenge</a>.

<strong>What made you decide to enter?</strong>

The GREAT10 challenge was a perfect opportunity for me to bring my freshly developed proficiency in numerical analysis to bear. As a student looking to gain experience, this was also a chance to contribute to the forefront of analysis techniques employed by the weak lensing community. The comparatively compact size and similarity between data sets made participating in the MDM challenge very attractive. The ellipticity measurement (galaxy shape) in the MDM competition was a critical step in our GREAT10 analysis, where the goal is to disentangle the shear (due to dark matter) from the intrinsic galaxy shape.

<strong>What was your most important insight into the dataset?</strong>

The most important insight was that the pixel-level residuals are a powerful tool for finding the best-fit models for galaxy and star images. We were able to assess the quality of various image models and parameters studying distributions of residuals across sets of images. This was essential to our method, which consisted of a pixel-level maximum-likelihood fit to each star and galaxy image.

The images below demonstrate our fit for a single galaxy image. From left to right, we see the original image (zoomed in on the center), our fitted model with the same resolution, and a higher resolution version of the fitted model:

[gallery]

<strong>Were you surprised by any of your insights?</strong>

We used an artificial neural network to improve the ellipticity measurements from the fitting procedure. The neural network was trained to provide corrections using a non-obvious subset of the of the likelihood output. For example, feeding the centroid position for a galaxy image to the neural network worsened the predicted correction. Without the neural network, our best entry would have ranked 8th.

<strong>Which tools did you use?</strong>

Our code consists of C++ libraries that we developed to perform the following tasks: generate images by convolving galaxy and point spread function models, access GREAT10 and MDM images, carry-out maximum-likelihood fit of images, do KSB measurement (moment inspired technique commonly used by astrophysicists) of ellipticity, and provide an ellipticity correction via machine learning algorithms.

The code utilizes a fit minimization engine (Minuit) and neural network engine (TMVA) that are both available as part of the open source (LGPL) ROOT data analysis framework (<a href=""http://root.cern.ch/"">http://root.cern.ch</a>), which is widely used by particle physicists.

<strong>What have you taken away from the competition?</strong>

I am intrigued by the tight cluster of scores near the top of the leaderboard despite the wide variety of methods applied. This suggests that those methods are making use of the maximum amount of information available in the images in the presence of pixelation and noise.

Winning the MDM competition gave us confidence in our strategy for measuring the shapes of galaxies, which we have put to use in GREAT10.

<strong>Most importantly, why 'DeepZot'?</strong>

The name DeepZot was formed by merging 'Deep Thought', the name of a fictional computer from Douglas Adams’ <em>Hitchhiker’s Guide to the Galaxy</em>, and 'Zot!', the battle cry of our school’s mascot, Peter the Anteater.

<em>Congratulations to Daniel and David!</em>"
From Soundtracks & Signatures to Stars & Galaxies: Ali Hassaïne and Eu Jin Lok on finishing third on the Mapping Dark Matter Challenge,http://blog.kaggle.com/2011/10/04/from-soundtracks-and-signatures/,2011-10-04 15:17:40,"<em>For the last of our series of interviews with the top Mapping Dark Matter competitors, Ali Hassaïne and Eu Jin Lok pulled out the stops to give some generous insight into their techniques.</em>
<h3>What was your background prior to entering Mapping Dark Matter?</h3>
<strong>Ali: </strong>I graduated from <a href=""http://cmm.ensmp.fr/index_eng.html"">Center of Mathematical morphology</a> / <a href=""http://www.mines-paristech.eu"">Mines-ParisTech</a> in 2009 with a PhD in morphological image processing. I worked within my thesis (under the supervision of Etienne Decencière and Bernard Besserer) on the restoration of optical soundtracks of old movies. I am currently a postdoctoral researcher at Qatar University working on writer identification and signature verification with Somaya Al-Ma'adeed. Earlier this year, I hosted the <a href=""http://www.kaggle.com/c/WIC2011"">ICDAR2011 Arabic Writer Identification Contest</a> on Kaggle.
<strong>Eu Jin: </strong>I have a masters degree in Econometrics and am currently an analyst at Deloitte Australia, working in data analytics. I've been with Kaggle for almost a year and have competed in many competitions, one of them being the writer identification contest hosted by Ali <em>[Kaggle note - Eu Jin came in the top 5 in that contest as well!]</em>.
<!--more-->
<h3>How did you come to form a team together?</h3>
<strong>Ali: </strong>I contacted Eu Jin because he showed very interesting ways of combining the features we provided in the writer identification contest. These same features have also been used in this contest along with many others.
<h3>What was your most important insight into the dataset?</h3>
<strong>Ali: </strong>I combined several methods, some are inspired from my PhD thesis on soundtrack restoration. Other methods are inspired from my current research on writer identification and signature verification. I also tried some methods which were specifically developed for this problem. I will briefly describe here the methods I liked the most:
<strong>Methods inspired from soundtrack restoration (the morphological approach)</strong>
For an introduction to mathematical morphology, <a href=""http://en.wikipedia.org/wiki/Mathematical_morphology"">Wikipedia's page</a> is a good starting point. <a href=""http://www.archive.org/details/SoundRec1943"">This instructional video</a> about optical soundtracks might also be of interest.
The soundtrack (the part of the film stock which contains the audio information) is sometimes badly exposed due to light diffusion during the copying process.

<dl id=""attachment_1243"" class=""wp-caption aligncenter"" style=""width: 263px;""><dt class=""wp-caption-dt""><a href=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure1.png""><img class=""size-full wp-image-1243  "" title=""Figure 1"" src=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure1.png"" alt=""Figure 1 Location of soundtrack on film stock"" width=""253"" height=""191"" /></a></dt><dd class=""wp-caption-dd"">Figure 1 Location of soundtrack on film stock</dd></dl>The effect of bad exposure is clearly visible on the peaks and valleys of optical soundtracks.

<dl id=""attachment_1228"" class=""wp-caption aligncenter"" style=""width: 238px;""><dt class=""wp-caption-dt""><a href=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure2a.png""><img class=""size-full wp-image-1228   aligncenter"" title=""Figure 2 (a)"" src=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure2a.png"" alt=""Figure 2 (a) overexposure"" width=""228"" height=""176"" /></a></dt><dd class=""wp-caption-dd"">Figure 2 (a) overexposure</dd></dl><dl id=""attachment_1229"" class=""wp-caption aligncenter"" style=""width: 238px;""><dt class=""wp-caption-dt""><a href=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure2b.png""><img class=""size-full wp-image-1229   aligncenter"" title=""Figure 2 (b)"" src=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure2b.png"" alt=""Figure 2 (b) normal exposure"" width=""228"" height=""176"" /></a></dt><dd class=""wp-caption-dd"">Figure 2 (b) normal exposure</dd></dl><dl id=""attachment_1230"" class=""wp-caption aligncenter"" style=""width: 238px;""><dt class=""wp-caption-dt""><a href=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure2c.png""><img class=""size-full wp-image-1230   aligncenter"" title=""Figure 2 (c)"" src=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure2c.png"" alt=""Figure 2 (c) underexposure."" width=""228"" height=""176"" /></a></dt><dd class=""wp-caption-dd"">Figure 2 (c) underexposure.</dd></dl>The effect of under-exposure is visually very similar to a morphological dilation with a certain structuring element. Given the physical process that causes under-exposure, it can be safely assumed that this structuring element is a disk. Therefore, the under-exposure might be restored by applying the dual morphological operator which is a morphological erosion with the same structuring element. Similarly, the over-exposure is very similar to a morphological erosion and might be corrected by applying a dilation [1] (references below).
In the same way, it can be assumed that the effect of lensing (or more generally, the application of a PSF) is very similar to a morphological operation with a certain structuring element. The structuring element is no longer a disk, however its shape can be computed from the provided star images as explained in following scheme.

[caption id=""attachment_1231"" align=""aligncenter"" width=""362"" caption=""Figure 3 Computing basic morphological operations from galaxy and star images""]<a href=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure3.png""><img class=""size-full wp-image-1231  "" title=""Figure 3"" src=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure3.png"" alt=""Figure 3 Computing basic morphological operations from galaxy and star images"" width=""362"" height=""325"" /></a>[/caption]

The <a href=""http://www.kaggle.com/c/mdm/Details/Ellipticity"">quadrupole moments</a> are then computed from all the resulting images and used as predictors.

<strong>Methods inspired from signature verification and writer identification</strong>
Several geometrical features previously used in signature verification and writer identification happen also to be very powerful predictors for computing the ellipticity. These features are also computed on the thresholded galaxy and star images. The following figure illustrates some of them.

[caption id=""attachment_1252"" align=""aligncenter"" width=""300"" caption=""Figure 4 Geometrical features""]<a href=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure-4.png""><img class=""size-medium wp-image-1252"" title=""Figure 4 Geometrical features"" src=""http://blog.kaggle.com/wp-content/uploads/2011/10/Figure-4-300x92.png"" alt=""Figure 4 Geometrical features"" width=""300"" height=""92"" /></a>[/caption]

All these predictors, along with many others, have been combined using a linear fit. The strength of this method lies in the large number of diverse predictors it combines. For those of you who might be interested in trying other models, I’ve put all the data together <a href=""http://www.kaggle.com/c/mdm/forums/t/795/piles-of-data-for-data-lovers"">here</a>.
<strong>What tools did you use?</strong>
The code is written in C++ with a strong dependency on the excellent <a href=""http://opencv.willowgarage.com"">OpenCV library</a>. I will hopefully make the code available soon.
<em>Thanks and congratulations to Ali and Eu Jin!</em>

<strong>References:</strong>
[1] J. Taquet, B. Besserer, A. Hassaïne and E. Decencière, Detection and correction of under/overexposed optical soundtracks by coupling image and audio signal processing, EURASIP Journal on Advances in Signal Processing, Vol. 2008.
[2] S. Al-Ma’adeed, E. Mohammed and D. Al Kassis, Writer identi?cation using edge-based directional probability distribution features for Arabic words, International Conference on Computer Systems and Applications (AICCSA), pp. 582-590, 2008.
[3] A. Hassaïne, V. Eglin and S. Bres, Une méthode de compression sans perte pour les images de documents basée sur la séparation en couches"
Like Popping Bubble-Wrap: Keith T. Herring on Placing Second in Wikipedia's Participation Challenge,http://blog.kaggle.com/2011/10/06/like-popping-bubble-wrap/,2011-10-07 04:36:09,"<em>Keith T. Herring placed second in the Wikipedia Participation Challenge with just three entries on the board and agreed to talk to Kaggle about his process. Read on for the first in a great series of interviews with the top competitors from the Wikipedia challenge.</em>

<strong>What was your background prior to entering the Wikipedia Participation Challenge?</strong>
I have a computer science degree from my home state, University of Illinois Urbana-Champaign (UIUC)... I then headed to Boston to get a Masters and Doctorate from MIT in Electrical Engineering and Computer Science. I now reside and work in the Queen City (official Seattle nickname from 1869 to 1982, ref: <a href=""http://en.wikipedia.org/wiki/Seattle"">Wikipedia</a>). I’ve been fortunate to have been allowed to get my hands dirty in Robotics, Wireless Communications, Remote Sensing, Machine Learning, Network Security, Financial Markets, Casino Arbitrage, and the prediction of infinitesimally small subsets of the future universe, although I don’t feel obligated to call myself a futurist or appear on the Discovery Channel as such.
<!--more-->
<strong>Why did you decide to enter the Challenge?</strong>
Two Reasons.
First, I have a lot of respect for what Wikipedia has done for the accessibility of information. Any small contribution I can make to that cause is in my opinion time well spent.
Second, a new data set is to me what a new sheet of bubble wrap is to Larry David. I can’t wait to dive in and pop all the bubbles/bits of information I can find! So this is where I give props to Kaggle: they’ve done a great job building on the success and excitement of the Netflix Prize. It's a win-win for data enthusiasts like myself and organizations like Wikipedia that have a lot of data and questions.

<strong>Which tools did you use?</strong>
My strategy was as follows:
<ol>
	<li>Compile a representative training set of Wikipedia Editing behavior. An interesting feature of this competition was that it involved a public data set. I wrote web scrapers to extract the editing history of approximately 1 million Wikipedia editors.</li>
	<li>Transform the raw edit data into a representative feature set for prediction of future editing volume. My final predictor operated on 206 features derived from editor attributes such as: edit timing, edit volume, name-space contributions, article concentration, article creation, edit automation, commenting behavior, etc.</li>
	<li>Learn a diverse set of future edit predictors. Each model/predictor I considered was a randomly constructed decision tree. I made use of an implementation (ref: Abhishek Jaiantilal and Andy Liaw) of Breiman and Cutler’s Random Forest algorithm for constructing the individual random decision trees. Further diversity was achieved by randomizing the random forest parameters for each individual tree rather than using a single optimized set of parameter values for all trees. Randomized parameters included: feature set cardinality per decision node (weak vs strong learner), in-to-out-of-bag ratio, stop- ping conditions (under vs over fitting).</li>
	<li>My final future edits predictor was formed as an optimized ensemble of the models in (3). I wrote an iterative quadratic optimizer for approximating the optimal model weighting using the out-of-bag samples, which varied across candidate models/trees. Out of the approximately 3000 models generated, 34 informative non-redundant models were retained in the final optimized ensemble.</li>
</ol>

I used the following tools/setup to implement this strategy: Ubuntu Linux, Python, MySQL, C++, and Matlab.

<strong>What was your most important insight into the dataset?</strong>
A randomly selected Wikipedia editor that has been active in the past year has approximately an 85 percent probability of being inactive (no new edits) in the next 5 months. The most informative features (wrt the features I considered) captured both the edit timing and volume of an editor. More specifically the exponentially weighted edit volume of a user (edit weight decreases exponentially with increased time between the edit and the end of the observation period) with a half-life of 80 days provided the most predictive capability among the 206 features included in the model.
Other attributes of the edit history, such as uniqueness of articles, article creation, comment behavior, etc. provided some additional useful information, although roughly an order of magnitude or less than the edit timing and volume when measured as global impact across the full non-conditioned editor universe.
An opportunity for future analysis would be to consider data relevant to any community political dynamics that may exist. Specifically edit reversion behavior and associated attributes.

<em>Thanks Keith, and congratulations on a fantastic performance!</em>"
Kernel Density at the checkout: D'yakonov Alexander on winning the dunnhumby Shopper Challenge,http://blog.kaggle.com/2011/10/16/kernel-density-at-the-checkout/,2011-10-17 06:53:59,"<em>Long-time Kaggle competitor D'yakonov Alexander won the <a href=""http://www.kaggle.com/c/dunnhumbychallenge"">dunnhumby Shopper Challenge</a> ahead of 537 other entrants who submitted a grand total of 2029 entries.  In addition to releasing his code and a description of his method, D'yakonov agreed to answer some background questions for us:</em>

<strong>What was your background prior to the dunnhumby Shopper Challenge?</strong>
I received a PhD in Mathematics from Moscow State University, Russia (my supervisor was academician Yuri Ivanovich Zhuravlev), where I now work. I like different contests and this is my fifth Kaggle competition.
<!--more-->
<strong>What approaches did you try, and what worked best?</strong>
At first I tried to use simple heuristics to understand the 'logic of the problem'. My main idea was to split the problem into two parts: the date prediction and the dollar spend prediction. For the first task I initially thought to use probability theory. But I soon found out that it was useless to predict the date if we couldn't predict the spend. Therefore I calculated not only probabilities of visits, but also the stability of users’ behavior (see Alexander's detailed <a href=""http://alexanderdyakonov.narod.ru/shopeng.pdf"">description</a> of his winning method). For that task, I used a kernel density (Parzen) estimator. But it was necessary to take account of the fact that 'fresh' data is more useful than 'old' data, so I used a weighted Parzen scheme to give greater weight to more recent data points. Then I hung parameters on my heuristics and performed my optimization.

<strong>What tools did you use?</strong>
I use MATLAB for all my Kaggle entries, just the basic M-language without any libraries.

<em>Congratulations D'yakonov Alexander on a fantastic result.  D'yakonov's winning method description and code are available to download from his website: </em>
<a href=""http://alexanderdyakonov.narod.ru/shopeng.pdf"">Click here for his method description </a>
<a href=""http://alexanderdyakonov.narod.ru/shopeng.zip"">Click here for his winning code</a>
(startsolution2.m to run)

"
The Mode is a Deceitful Beast: William Cukierski on finishing fourth in dunnhumby's Shopper Challenge,http://blog.kaggle.com/2011/10/19/deceitful-beast-william-cukierski/,2011-10-19 16:41:39,"<em>William Cukierski finished fourth in the dunnhumby Shopper Challenge, backing up his previous second-place finish in the IJCNN Social Network Challenge (you can read his write-up of that challenge <a href=""http://blog.kaggle.com/2011/01/14/how-i-did-it-will-cukierski-on-finishing-second-in-the-ijcnn-social-network-challenge/"">here</a>). At the time of writing, Will has just WON/FINISHED SECOND in the <a href=""http://www.kaggle.com/c/SemiSupervisedFeatureLearning/"">Semi-Supervised Feature Learning</a>.</em>

<strong>What was your background prior to entering the dunnhumby Shopper Challenge?</strong>

I studied physics at Cornell and am now finishing a PhD in biomedical engineering at Rutgers. During my day job, I look at ways to apply machine learning and data mining to cancer diagnosis in pathology images. During my night job (once my fiancée has gone to bed and the coast is clear) I fire up Matlab and trade hours of sleep for a chance at Kaggle glory.
<!--more-->
<strong>What made you decide to enter?</strong>

Simple data sets always catch my eye. I like to be able to get right down to the analytics, without spending hours poring over data dictionaries and mucking with strings and categorical variables and all that business. I do enough of that in my day job!

<strong>What was your most important insight into the dataset?</strong>

Like forecasting weather, this was a very “local” prediction contest. Most customers returned to the store quite soon, meaning that forecasting the date/spend too far out provided vastly diminishing returns. There was no Easter holiday to account for (which would have been a tricky detail had the competition been a year earlier). I polled friends for ideas on what might make people go shopping. Despite some good leads (one suggested that people might shop after getting a paycheck on the 1st or 15th of the month), I couldn't find any such macro trends that affected the short prediction time frame. It may seem obvious that an individual's decision to go shopping has very little to do with how much the store has taken in that day, or how many others have gone that day, but one can never ignore these possibilities in a data mining competition where fractions of a percent matter.

Due to the nature of the scoring method (namely, that you had to get the date exactly right for the spend estimate to even matter), I focused almost entirely on predicting the date. I extracted features on the historical date information alone (ignoring how much was spent and ignoring the training data after April 1st, 2011). There were strong weekday patterns, which meant many of the features worked best when computed “modulo 7”.

There are three generic classes of features one can consider in a problem like this:

1. User features: prior number of visits, mode time since visit, PCA, SVDs, etc.
2. User-Date features: prior probability of visiting on a given weekday, days since visiting, empirical probability distribution of having x days pass without a visit, etc.
3. Date features: ignored... global information not very important!

I performed logistic regression to obtain a probability of the customer visiting on each day for 2 weeks after the start of the prediction window. Each day has a different feature matrix due to the inclusion of the user-date features. The predicted date was then the one with the highest probability. For the spend, I used the “modal window” concept, which I hope other contestants will describe in more detail.

<strong>Were you surprised by any of your insights?</strong>

I was most surprised by the seemingly endless list of things which didn't work on this data! In most data mining problems, if you have method A which does well and method B which does well, you can combine them and watch your score improve. This one was tough because if A says “Tuesday” and B says “Thursday”, you can't average them and say “Wednesday.” This would have improved your score if something like RMSE was used, but it doesn't fly for the exact error metric. For all you know, that person has Yoga class and never goes shopping on Wednesday. Similarly, you can't toss the £2 gum purchases in with the £200 weekly shops and guess the person will spend £100.

To alleviate this problem, I had to be careful about the types of features used for the regression. In metaphorical terms, I tried to take modes where I normally would have taken means. This required careful attention to statistical support. The mode is a deceitful beast in that the “most common” pattern of past behavior can range from “this person comes in every 7 days without fail” all the way to “this person comes in whenever they feel like it, and it just so happens that 7 was the number that won by chance.” I experimented with a number of ways to use confidence estimates to weight, blend, or downplay features for customers who were strangers to the supermarket.

<em>Thanks William - now we're looking forward to reading about your success in the <a href=""http://www.kaggle.com/c/SemiSupervisedFeatureLearning/"">Semi-Supervised Feature Learning</a> competition!</em>"
Creatures of Habit:  Neil Schneider on placing second in the dunnhumby Shopper Challenge,http://blog.kaggle.com/2011/10/20/creatures-of-habit-neil-schneider/,2011-10-20 20:18:01,"<em>Neil Schneider placed second in the dunnhumby Shopper Challenge with a breakout performance. Read on for some insights into his methodology, and visit the links at the bottom to view his code.</em>

<strong>What was your background prior to entering the dunnhumby Shopper Challenge?</strong>
I am an Associate Actuary with Milliman's Indianapolis office. I have two degrees from Purdue University in Actuarial Science and Statistics. I graduated in December of 2007 and joined Milliman. Most of my experience is in creating actuarial models for pricing and reserving. During my time with Milliman, I have become proficient in SAS and JMP. Lately, I have completed some data analyses using R. I understand enough R to run various functions, but still rely on SAS for all my data manipulations. Our office has recently completed some work on a new method for reserve projections, based on published robust time series statistics. A lot of the research was applied from the inventory control field.
<!--more-->
<strong>What made you decide to enter?</strong>
We found Kaggle during our reserve methodology research, because of the <a href=""http://www.heritagehealthprize.com/c/hhp"">Heritage Health Prize</a>. I have competed in the <a href=""http://www.kaggle.com/c/overfitting"">""Don't Overfit!""</a> and <a href=""http://www.kaggle.com/c/mdm"">""Mapping Dark Matter""</a> competitions prior to dunnhumby's challenge. While most of the competitions sound intriguing, I only have so much free time to devote to a competition. I thought this was an excellent example of sparse time series data and was hoping to leverage knowledge from our own model to predict the outcomes. As it turned out, this was not the case.

<strong>What was your most important insight into the dataset?</strong>
One insight was that shoppers are habitual. They will tend to have their day of week to do their shopping. A customer may visit the store on different days of the week, but you can see that for different weekdays a customer will spend different amounts. Example: A customer typically spends $100-$150 on Saturday, but will visit the store just as often on Monday, but only spends $40-$60. Developing separate projections for the spend amount by weekday was important to correctly predicting the next spend.

<strong>Were you surprised by any of your findings?</strong>
I was surprised that the time series models for inventory controls performed so poorly on these projections. I was also surprised at how poorly regression models fit the spend amounts. This is probably due to the test statistic for the evaluation. Out of the box regressions will optimize predictions for the mean or quartiles of the dependant variable. This competition needed an optimization of the highest density area.

<strong>Which tools did you use?</strong>
I used SAS for most of the heavy data work. This included proc sql statements to develop the maximum density visit_spend ranges.
JMP was invaluable for visualizing the data and optimizing choices. I mainly used histograms, X vs Y plots and partition models.
Finally, I used R to run more advanced statistical models (Generalized Boosting Regression Models - Package ""gbm"").

<strong>What have you taken away from the competition?</strong>
I learned that GBMs are indeed powerful for predictions, but the interpretation of coefficients for independent variables can be meaningless. This leads me to question what methods would be the most useful for modeling, but producing quality coefficient estimates. Maybe a future competition?

<em>Congratulations Neil on a fantastic performance! Neil has <a href=""http://www.kaggle.com/c/dunnhumbychallenge/forums/t/922/2nd-place-methodology"">posted his methodology</a> in more detail, along with his SAS and R code, on the Kaggle forum.</em>"
Long live Wikipedia: Dell Zhang on placing third in Wikipedia's Participation Challenge,http://blog.kaggle.com/2011/10/26/long-live-wikipedia-dell-zhang/,2011-10-26 23:07:54,"<em>Dell Zhang placed third in Wikipedia's Participation Challenge and agreed to give us a peek into his process.</em>

<strong>What was your background prior to entering the Wikipedia Participation challenge?</strong>
I got my PhD in Computer Science from the Southeast University in China 9 years ago, and then moved to Singapore to work as a postdoc research fellow under the supervision of Prof Wee Sun Lee. It was very kind of him to send me to the first-ever Machine Learning Summer School in 2002 - that's when I discovered the fascinating world of machine learning. Since then my research has been centred around using statistical machine learning techniques to improve information retrieval and organisation. I am currently a Senior Lecturer in Computer Science at Birkbeck, University of London. I have also joined the Royal Statistical Society and learned loads of interesting stuff done by statisticians.
<!--more-->
<strong>What made you decide to enter?</strong>
Being a busy academic I can no longer spend much time on coding as I used to, but I still enjoy getting my hands dirty and playing with data now and again. So when I got some time this summer, I decided to take part in a Kaggle competition to brush up on my rusty coding skills. The Wikipedia Participation Challenge attracted me most because as a heavy user of Wikipedia I felt obliged to do something helpful for the community.

<strong>What was your most important insight into the dataset?</strong>
The most important insight was that most user's future behaviour can be largely determined by his or her recent behavioural dynamics - how the number of edits and the number of edited articles change in the last period of time.

<strong>Were you surprised by any of your insights?</strong>
I am a bit surprised that dynamics features alone can go such a long way when we choose proper temporal scales and employ a powerful machine learning method.

<strong>Which tools did you use?</strong>
I used Python to write small programs for analysing data and making predictions. I like the simplicity of Python - simple is beautiful. The machine learning methods that I have tried all come from two open-source Python modules: one is <a href=""http://github.com/scikit-learn"">scikit-learn</a>, and the other is <a href=""http://opencv.willowgarage.com/wiki/"">OpenCV</a>. Finally, Gradient Boosted Trees (implemented in OpenCV) outperformed the other methods. I must mention that the parameter tuning was carried out on a big validation dataset shared to all participants by Twan van Laarhoven. ""Such a nice guy"", indeed!

<strong>What have you taken away from the competition?</strong>
A lot of fun, and a few lessons :-) I cannot wait to see others' secret weapons for tackling this problem. Long live <a href=""http://www.wikipedia.org/"">Wikipedia</a>!

<em>Thanks very much Dell and congratulations on a great performance!</em>

<strong>Update</strong>
Dell has posted a detailed write-up at:
<a href=""http://arxiv.org/abs/1110.5051"" target=""_blank"">http://arxiv.org/abs/1110.5051</a>
And  source code at:
<a href=""http://www.dcs.bbk.ac.uk/~dell/publications/wikichallenge_zeditor.zip"" target=""_blank"">http://www.dcs.bbk.ac.uk/~<wbr>dell/publications/<wbr>wikichallenge_zeditor.zip</wbr></wbr></a>"
Small steps: Gareth Campbell on placing fourth in Wikipedia's Participation Challenge,http://blog.kaggle.com/2011/11/01/small-steps-gareth-campbell/,2011-11-02 03:57:40,"<em>A quick interview today with Dr Gareth Campbell, who placed fourth in Wikipedia's Participation Challege.</em>
<p class=""question"">What was your background prior to entering the Wikipedia Challenge?</p>
<p class=""answer"">I have a PhD in Finance.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">I wanted to test how good my econometric skills were.</p>
<p class=""question"">What was your most important insight into the dataset?</p>
<p class=""answer"">The number of edits by Wikipedia users was almost entirely dependent on the number of edits made by them in the past few months. The number of reversions, the type of article edited, comments etc. had little predictive power.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">A little surprised that the number of reversions had such little impact.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer""><a href=""http://en.wikipedia.org/wiki/Stata"">Stata</a>.</p>
<p class=""question"">What have you taken away from the competition?</p>
<p class=""answer"">Testing lots of different small steps can lead to substantial improvements in predictive power.</p>
<em>Thanks Gareth!</em>"
"Words of Wisdom From Ben Hamner, Our Newest Recruit",http://blog.kaggle.com/2011/11/17/words-of-wisdom-from-ben-hamner-our-newest-recruit/,2011-11-17 10:48:37,"<em>This week, we were thrilled to welcome to the Kaggle team <a href=""http://www.kaggle.com/users/993/ben-hamner"">Ben Hamner</a>, winner of the Semi-Supervised Learning Competition and one of our most successful competitors to date. Ben recently placed third in dunnhumby's Shopper Challenge, and had the following to say about the experience.</em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I graduated from Duke University in 2010 with a bachelors in biomedical engineering, electrical and computer engineering, and mathematics. For the past year, I applied machine learning to improve non-invasive brain-computer interfaces as a Whitaker Fellow at EPFL. On the side, I’ve participated in various predictive analytics competitions.</p>
<span class=""question"">What made you decide to enter?</span>
<p class=""answer"">The dataset was deceptively simple. It simply consisted of a list of customers’ visits and spend amounts for the past year, whereas many datasets have a much higher dimensionality and require substantially more preprocessing. This simplicity provided a good competitive testbed for more statistically oriented methodologies.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">Two patterns in the data were key: periodic weekly behavior dominated when a customer visited a store (as opposed to the time since his last visit), and a customer’s recent shopping behavior was more predictive than his past behavior. Simple models using these patterns performed very well. My best “simple” date predictor took the most commonly visited day of the week, weighted by how recent the visits were. My best “simple” spend predictor took the mode of a weighted kernel density estimate of the customer’s previous spend behavior. As with most machine learning competitions, more complex methods that incorporated these simple models were necessary for bleeding edge performance.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">Standard algorithms performed poorly on this task due to the atypical cost function. Many supervised regression methods optimize the mean squared error, but the spend prediction was evaluated with a binary metric: whether the predicted spend was with $10 of the actual spend. This meant that our task was to predict the most likely customer behavior, as opposed to the average customer behavior. All gradient based approaches I applied to this task performed relatively poorly, even when the cost function was modified to be a differentiable approximation of the binary evaluation metric. On the other hand, successful approaches to this competition performed poorly on the mean squared error metric.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">I used Matlab and Python.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">Carefully analyzing and optimizing according to the evaluation metric is crucial for competitors. The metric can dramatically affect which models perform well and which perform poorly, especially at the frontier of what is possible given the data. In my case, I optimized the model based on date prediction, and then on spend prediction given the predicted date. I optimized each of these over the individual evaluation metrics. I ran out of time to jointly optimize the date and spend prediction according to the final evaluation metric, and ultimately got beat by a model that did.</p>
<p class=""answer"">From a competition host’s perspective, the evaluation metric may not be as crucial. If the goal is to get the best models possible on a very well defined problem, then choosing the appropriate metric is absolutely vital. If the goal is to discover what is possible given an underlying set of data, or what useful patterns are hidden in the data, then the precise metric may not be as important as other considerations.</p>"
Picture Perfect: Bo Yang on winning the Photo Quality Prediction competition,http://blog.kaggle.com/2011/11/23/picture-perfect-bo-yang-on-winning-the-photo-quality-prediction-competition/,2011-11-24 01:31:13,"<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I'm a software developer and got started in machine learning in the Netflix prize.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">The locations and lists of words seem to offer many possibilities. The data is clean with no missing values. And this was a short contest so I couldn't spend too much time on it.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">My best result was a mix of random forest, GBM, and two forms of logistic regression. I put the raw data into a database and built many derived variables. I also used many raw &amp; derived variables from external, location-based data. I wrote some Javascript code to call Google Maps API and retrieved:</p>

<ul class=""answer"">
	<li>Elevation at each latitude-longitude coordinate.</li>
	<li>Country and administrative area. This could not be determined at some locations, they were all in the middle of the ocean, on small islands or boats I guess.</li>
	<li>Number of ""places"" within 50 KM and 10 KM radius of each location, and users' ratings of these places.</li>
</ul>
<p class=""answer"">I downloaded a bunch of World Development Indicators data from <a href=""http://worldbank.org/"" target=""_blank"">worldbank.org</a>. Buried among these were 10 country-tourism data which I injected into my database.</p>
<p class=""answer"">I downloaded population density data from <a href=""http://sedac.ciesin.columbia.edu/gpw/"" target=""_blank"">http://sedac.ciesin.columbia.<wbr>edu/gpw/</wbr></a> and made a rough album per capita index: albumCount/populationDensity. I figured locations that score high on this index are remote, scenic places, and those that scored low are boring urban areas.</p>
<p class=""answer"">All these external data helped, but only a little. The raw and derived variables were fed to algorithms in different combinations.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">I don't think I have any, and I'm actually very curious about Jason Tigg's insight. One day Jason suddenly gained a huge lead over everyone else, and I was convinced he found a great insight and/or external data. For the remainder of the contest, I was obsessed and went on a wild-goose chase after this insight, this ""one ring to rule them all (imagine Gollum hissing in the cave)"".</p>
<p class=""answer"">My most useful variables were simple and well known: average numbers of 'good' albums for each word, weighted with global average based on how many albums the word appear in. This was done separately for album name, album description, photo caption, and one merged word list. Then for each album and location, the average of word averages were calculated.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">Well, I was surprised I couldn't get any signal out of word pairs.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">SQL, R, C++, C#, Javascript, Google web services, and Excel.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">It's probably not worth it to spend too much time on external data, as chances are any especially useful data are already included. Time can be better spent on algorithms and included variables. For example I didn't even try to use the number of times a word appeared in each album.</p>"
Smile!  Alexander D'yakonov on placing third in the Photo Quality Prediction competition,http://blog.kaggle.com/2011/11/27/smile-alexander-dyakonov-on-placing-third-in-the-photo-quality-prediction-competition/,2011-11-28 00:57:51,"<em>Alexander D'yakonov placed third in the Photo Quality Prediction competition and agreed to give us a peek into his process.</em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I’m an Associate Professor at Moscow State University.  I like different data mining problems and have participated in many Kaggle contests.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">The problem has few features, so it did not look very complicated, and it had interesting lists of words. There were many participants, so a good result would be more challenging.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I combined random forests with a weighted k-NN. The combination used a weighted square root of the sum of squares of the predictions, with coefficients tuned by gradient descent. I did not use any external information.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">Nothing! I used Random Forests with some simple additional features.  For example, these included the “ratio” (the width of the image divided by the height of the image) and “area” (the number of pixels in the photo).  I also used a merged word list (words from album name, album description, photo caption).</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I was surprised that I couldn’t build good features from the word lists. All my engineered features were worse than I expected.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">MATLAB and R.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">It is necessary to be careful when you build the final decision. I made one mistake: instead of averaging six algorithms, I used only three algorithms and had worse performance.</p>"
Credit where credit's due: Joe Malicki on placing third in 'Give Me Some Credit',http://blog.kaggle.com/2011/12/20/credit-where-credits-due-joe-malicki-on-placing-third-in-give-me-some-credit-2/,2011-12-21 07:39:17,"<em><a href=""http://www.kaggle.com/users/2687/occupy"">Joe Malicki</a> placed third in <a href=""http://www.kaggle.com/c/GiveMeSomeCredit"">Give Me Some Credit</a> despite health problems preventing him from submitting for the majority of the competition.</em>
<p class=""question"">How does it feel to have done so well in a competition with almost 1000 teams?</p>
<p class=""answer"">Great! This was my first serious attempt at Kaggle - I've been doing data modeling for a while, and wanted to try cutting my teeth at a real competition for the first time.</p>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I have a computer science (CS) degree and have done some graduate work in CS, statistics, and applied math. I currently work for Nokia doing machine learning for local search ranking.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I tried quite a bit of preprocessing, mentioned below, and in the <a href=""http://www.kaggle.com/c/GiveMeSomeCredit/forums/t/1166/congratulations-to-the-winners/7269#post7269"">code</a> I posted to the forums.</p>
<p class=""answer"">I used random forests, gradient boosted decision trees, logistic regression, and SVMs, with various subsampling of the data for various positive/negative feature balancing. In the end, using 50/50 balanced boosting, and 10/90 balance random forests, and averaging them, won.</p>
<p class=""question"">This competition had a fairly simple data set and relatively few features – did that affect how you went about things?</p>
<p class=""answer"">Absolutely! One of the big problems in this competition was a large and imbalanced dataset - defaults are rare. I used stratified sampling for classes to produce my training set.</p>
<p class=""answer"">Transformations of the data were key. To expand the number of features, I tried to use my prior knowledge of credit scoring and personal finance to expand the feature set.</p>
<p class=""answer"">For instance, knowing that people above age 60 are likely to qualify for social security, which makes their income more stable, and that 33% (for the mortgage) and 43% (for the mortgage plus other debt) are often magic debt to income (DTI) numbers was very useful. I feel strongly that knowing what the data elements actually represent in the real world, rather than numbers, is huge for modelling.</p>
<p class=""answer"">Random forests are a great learning algorithm, but they deal poorly where transforms of features are very important. So I identified some things that looked important - combining income and DTI to estimate debt and combining number of dependents and income as a proxy for disposable income/constraints. The latter is important since struggling to barely support dependents makes default more likely.</p>
<p class=""answer"">Also, trying to notice ""interesting"" incomes divisible by 1000 was useful - I was guessing that fraud might be more likely for these, and/or they may signal new jobs where a person hasn't had percent raises. I was going to try a <a href=""http://en.wikipedia.org/wiki/Benford%27s_law"">Benford's law</a>-inspired income feature, to help detect fraud, but had to leave the competition before I got a chance.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">Forum discussion pointed out that ‘number of times past due’ in the 90's seemed like a special code, and should be treated differently. Also, noticing that DTI for those with missing income data seemed weird was critical.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I had tried keeping a small hold-out data set, and then combining all of the models I trained via a logistic regression on that data set, using the model probabilities as features, but found that only taking the best models of various classes worked better.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">R. It's a memory hog, but it has first-class algorithm implementations.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">The results I saw on the public test set differed dramatically from any results I could get on any held-out portion of the training set. In the end, the results on the private testing data set for all of the models I submitted were extremely close to my private evaluations, far closer than to the performance on the public set. This highlighted to me the importance of relying on cross-validation on large samples.</p>"
Score! Xavier Conort on coming second in 'Give Me Some Credit',http://blog.kaggle.com/2011/12/21/score-xavier-conort-on-coming-second-in-give-me-some-credit/,2011-12-22 05:55:27,"<em><a href=""http://www.kaggle.com/users/17379/gxav-xavier-conort"">Xavier Conort</a> came second in <a href=""http://www.kaggle.com/c/GiveMeSomeCredit"">Give Me Some Credit</a> and let us in on some of the tricks of the trade.</em>
<p class=""question"">How does it feel to have done so well in a contest with almost 1000 teams?</p>
<p class=""answer"">I feel great because Machine Learning is not part of my natural toolkit. I now look forward to exploiting this insight in my professional life and exploring new ideas and techniques in other competitions.</p>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I am an actuary and set up a consultancy called Gear Analytics a few months ago. It’s based in Singapore, and helps companies to build internal capabilities in predictive modeling and risk management using R. Previously, I worked in France, Brazil, China and Singapore holding different roles (actuary, CFO, risk manager) in the Life and Non-Life Insurance industry.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I didn't spend much time on preprocessing. My most important input was to create a variable which estimates the likelihood of being late by more than 90 days.</p>
<p class=""answer"">I used a mix of 15 models including Gradient Boosting Machine (GBM), weighted GBMs, Random Forest, balanced Random Forest, GAM, weighted GAM, Support Vector Machine (SVM) and bagged ensemble of SVMs. My best score, however, was an ensemble without SVMs.</p>
<p class=""question"">This competition had a fairly simple data set and relatively few features – did that affect how you went about things?</p>
<p class=""answer"">The data was simple yet messy. I found off-the-shelf techniques such as GBM could handle it. The relative simplicity of the data allowed me to allocate more time to trying different models and ensembling my individual models.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">The likelihood of being late was by far the most important predictor in my GBM and its inclusion as a predictor improved my individual fits accuracy.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I've always believed that people can benefit from diversity, but I was surprised to see how much data science can also benefit from it (through ensembling techniques). The strong performance achieved by  Alec, Eu Jin and Nathaniel (Perfect Storm) also shows that teamwork matters.</p>
<p class=""answer"">My best individual fit was a weighted GBM which scored 0.86877 in the private set. Without ensembling weaker models, my rank would have been 25.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">R, the excellent book <em><a href=""http://www-stat.stanford.edu/~tibs/ElemStatLearn/"">The Elements of Statistical Learning</a></em> and the great online Stanford <a href=""http://www.ml-class.org/course/auth/welcome"">course</a> by Andrew Ng. All are free.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">When I entered the competition, I was still unfamiliar with Machine Learning techniques as they are rarely used in the insurance industry. I was amazed by the capacity of Gradient Boosting Machine (also called Boosted Regression Trees) to learn non-linear functions (including interactions) and accommodate missing values and outliers. It is definitely something that I will include in my toolbox in the future.</p>"
The Perfect Storm: Meet the Winners of 'Give Me Some Credit',http://blog.kaggle.com/2012/01/03/the-perfect-storm-meet-the-winners-of-give-me-some-credit/,2012-01-04 03:33:37,"<div><em>The Perfect Storm, comprising <a href=""http://www.kaggle.com/users/2702/alec-stephenson"">Alec Stephenson</a>, <a href=""http://www.kaggle.com/users/3346/eu-jin-lok"">Eu Jin Lok</a> and <a href=""http://www.kaggle.com/users/2796/nathaniel-ramm"">Nathaniel Ramm</a>, brought home first prize in <a href=""http://www.kaggle.com/c/GiveMeSomeCredit"">Give Me Some Credit</a>. We caught up with Alec and Eu Jin.</em></div>
<p class=""question"">How does it feel to have done so well in a contest with almost 1000 teams?</p>
<p class=""answer"">EJ: Pretty amazing, especially when it was such an intense competition with so many good competitors. Personally, I felt a strong sense of achievement together as a team.</p>
<p class=""answer"">AS: It feels great, particularly because we won by such a well-defined margin. The gap between first and second place was the largest gap in the top 500 placings.</p>
<p class=""question"">What were your backgrounds prior to entering this challenge?</p>
<p class=""answer"">EJ: My background is in statistics and econometric modelling. More recently I've worked in data mining and machine learning for Deloitte Analytics Australia, where I am a Senior Analyst.</p>
<p class=""answer"">AS: My formal background is in mathematics and statistics. I am a largely self-taught programmer, and have written a number of R packages. I do not work in data mining, but have picked up an interest in it over the last year or so, mainly due to Kaggle! I am an academic, originally from London, and have studied or worked at universities in England, Singapore, China and Australia.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">AS: We tried many different supervised learning methods, but we decided to keep our ensemble to only those things that we knew would improve our score through cross-validation evaluations. In the end we only used five supervised learning methods: a random forest of classification trees, a random forest of regression trees, a classification tree boosting algorithm, a regression tree boosting algorithm, and a neural network.</p>
<p class=""question"">This competition had a fairly simple data set and relatively few features – did that affect how you went about things?</p>
<p class=""answer"">EJ: It meant that the barrier to entry was low, competition would be very intense and everyone would eventually arrive at similar results and methods. Before we formed a team, I knew that I would have to work extra hard and be really innovative in my approach to solving this problem. Collaboration was the last ace and as the competition started to hit the ceiling, I decided to play that card.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">EJ: I discovered 2 key features, the first being the total number of late days, and second the difference between income and expense. They turned out to be very predictive!</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">AS: I was surprised at how well neural networks performed. They certainly gave a good improvement over and above more modern approaches based on bagging and boosting. I have tried neural networks in other competitions where they did not perform as well.</p>
<p class=""question"">How did working in a team help you?</p>
<p class=""answer"">TOGETHER: As individuals, we were unlikely to win. But with Nathaniel's expertise in credit scoring, Alec's expertise in algorithms and Eu Jin's knowledge in data mining, we had something completely different to offer that was really powerful. In a literal sense, we stormed our way up to the top.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">TOGETHER: SQL, SAS, R, Viscovery and even Excel!</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">AS: That data mining is fun when you are in a team, and also how effective a team can be if the skills of its members complement each other. You can learn a lot from the people that you work with.</p>"
Jason Tigg on Coming Third on the Planet in the Claim Prediction Challenge,http://blog.kaggle.com/2012/01/05/jason-tigg-on-coming-third-on-the-planet-in-the-claim-prediction-challenge/,2012-01-06 07:32:19,"<em><a href=""http://www.kaggle.com/users/7052/jason-tigg"">Jason Tigg</a> came third in the <a href=""http://www.kaggle.com/c/ClaimPredictionChallenge"">Claim Prediction Challenge</a> and caught up with us afterwards.</em>

<em></em><span class=""question"">What was your background prior to entering the Prediction Claim challenge?</span>
<p class=""answer"">As I child I was interested in machine intelligence and when I was 14 I wrote my first ""intelligent"" program in assembler on my Dragon 32 computer to play Othello, inspired by a wonderful book ""Computer Gamemanship"" by David Levy. Through Kaggle I have made contact with David Slate of the team of ""Old Dogs with New Tricks"" who I have discovered was instrumental in pioneering the field of computer chess back in the 1970s. I studied at Oxford University where I obtained a doctorate in Elementary Particle Physics which made extensive use of an early version of Mathematica to solve some fairly complicated integral equations. Since then I have been working writing financial software for both trading and risk management. I previously entered a fascinating chess challenge on Kaggle, so this was my second competition.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">I entered the competition mostly for the fun of the challenge. The leaderboard on Kaggle is addictive and gives a real sense of competition as well as giving you a sense of how well you are understanding the algorithms and the data.</p>
<p class=""question"">What was your most important insight into the dataset?</p>
<p class=""answer"">I would say the most important insight was technical not algorithmic. The dataset was so large it required some compression to hold in RAM and some interesting iterator code to walk through one household at a time. Examining data clustered by household turned out to be particularly important.</p>
<p class=""question"">Why the name Planet Thanet?</p>
<p class=""answer"">I was born and grew up in the beautiful seaside town of Ramsgate in the Isle of Thanet in South East England.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">Not many to be honest. All the code was written in Java and no third party libraries were used.</p>
<p class=""question"">What have you taken away from the competition?</p>
<p class=""answer"">The top two teams were some distance above me so clearly I must have missed something. Hopefully I will discover what that is!</p>"
Mind Over Market: The Algo Trading Challenge 4th Place Finishers,http://blog.kaggle.com/2012/01/26/mind-over-market-the-algo-trading-challenge-4th-place-finishers/,2012-01-26 08:06:03,"<em>Anil Thomas, Chris ""Swedish Chef"" Hefele and Will Cukierski came 4th in the <a href=""http://www.kaggle.com/c/AlgorithmicTradingChallenge"">Algorithmic Trading Challenge</a>.  We</em> <em>caught up with them afterwards.</em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer""><strong>Anil</strong>: I am a Technical Leader at Cisco Systems, where I work on building multimedia server software. I was introduced to machine learning when I participated in the Netflix Prize competition. Other than Netflix Prize where I was able to eke out an improvement of 7% in recommendation accuracy, I have no significant data mining experience to speak of.</p>
<p class=""answer""><strong>Chris</strong>:  I have a MS in electrical engineering, but I have no formal background in machine learning.  My first data-mining contest was the Netflix Prize, and I learned a tremendous amount by being part of the team that came in 2nd place.  Since then, I’ve been hooked by these competitions, and have entered several Kaggle contests in my spare time.  During the day, though, I work on Voice-over-IP projects at AT&amp;T Labs, where I’m a systems engineer.</p>
<p class=""answer""><strong>Will</strong>: I studied physics at Cornell and am in the final stages of a PhD in biomedical engineering at Rutgers.  Like Chris and Anil, my first data mining contest was the Netflix prize, where I placed somewhere around 30,000th (the leaderboard doesn’t go past 1000, but what’s a few thousand places among friends).  Several years and many competitions later, I am lucky to rub elbows with the clever minds and talented folks on Kaggle.</p>
<em>Will and Chris formed a team from the start, while Anil climbed the leaderboard separately.  In the closing days of the competition, the two teams agreed to merge in order to better their respective chances at the top and only prize.  Following a long weekend of furious model blending, they ended up in 4th. All three participants wish to thank Capital Markets Cooperative Research Centre and Kaggle.com for hosting this competition.</em>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer""><strong>Anil</strong>: I had just completed the excellent online course on machine learning taught by Prof. Andrew Ng of Stanford and was looking for a challenge that goes beyond routine homework problems. This competition was a perfect fit. I have always found stock market data to be intriguing and this looked like a good opportunity to try my hand at analytics.</p>
<p class=""answer""><strong>Will</strong>: This was my first contest with financial data and a nice opportunity to peek into the world of short-term market dynamics, spreads, order books, etc.</p>
<p class=""answer""><strong>Chris</strong>:  I have always been interested in “quant” finance topics. Also, I had some success in the INFORMS 2010 contest on Kaggle, which involved predicting short-term price movements of securities.  I thought some of the lessons I learned in that contest might be helpful in this one.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer""><strong>Will</strong>: I had initial success with a kNN model and spent the majority of the competition convinced I could improve this model.  My initial feature set was picked by hand using feedback from the probe set (the last 50k trades of the train set).  Most of the features were basic transformations of the spread just before the liquidity shock.  Querying for neighbors within each security generally outperformed querying across all securities, but we did find that the combination of the two worked best.  I spent many weeks attempting to implement a more rigorous way to pick the feature space.  There are many published methods on how to learn a custom Mahalanobis distance metric using supervised labels (that is, to find S in the equation below such that trades with similar reactions would have similar distances in feature space).</p>
<p style=""text-align: center;"">$$ D_M(x) = \sqrt{ (x - \mu)^T S^{-1} (x - \mu)}$$</p>
<p class=""answer"">(Note that a diagonal S is the same thing as weighting each feature separately in Euclidean space)</p>
<p class=""answer"">However, this contest was not a traditional supervised classification problem in the sense that we had a measure of dissimilarity (the RMSE between the bid/ask responses), as opposed to neat-and-tidy class labels.  Despite numerous promising modifications and a last minute multidimensional scaling idea, I ran out of time to find a suitable Mahalanobis matrix that beat the RMSE of the initial hand-picked feature set.</p>
<p class=""answer""><strong>Chris</strong>:  I tried to keep it simple, and stuck with creating multiple variations on basic linear regression models.  I created more than 30 features derived from the original data (consisting mostly of the min/max/median/std.deviations of prices &amp; spreads &amp; the trade/quote arrival rate).  I fed those features plus the original data to a LASSO regression, which selected 18 variables.  Separate regressions were used for bids vs asks, and buys vs sells.  I also had models that predicted prices for each time period individually, as well as other models that predicted time-invariant, average prices.  Furthermore, the characteristics of the the testing &amp; training sets differed, so I tried a variety of ways to weight each row of data to correct for those differences. In the end, I just weighted each row by the ratio of how often each security appeared in the testing vs training sets.</p>
<p class=""answer""><strong>Anil</strong>: The model that worked best for me was linear regression on various indirect predictors derived from the training data. I also tried Random Forest, k-NN, k-means and SVM regression techniques. As for preprocessing, I found it advantageous to set the base predictions to match the general trajectory of the prices and then model the residuals.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer""><strong>Will</strong>: This data was very fussy, and the use of un-normalized RMSE to score the competition made for a very skewed error distribution. Chris and Anil did some great due diligence into the quirks of this dataset, so I defer to them for details.</p>
<p class=""answer""><strong>Chris</strong>:  The market open (at 8AM) was very extremely unpredictable, and contributed a disproportionally large amount of error.  For one model, I found 12% of squared error for the entire trading day occurred in the first minute of trading. To combat this, I trained some separate models for the market open, since it seemed so different (the naive benchmark model worked better than my regressions at the market open, for example).</p>
<p class=""answer"">Additionally, the farther away you got from the “liquidity shock” trade, the more unpredictable the prices were.  Looking backward in time from that “liquidity shock” trade,  my variable-selection algorithms dropped all historical bid/ask prices except those immediately before the trade, since those prices did not provide enough predictive value. As you moved forward in time from that trade, bid/ask prices got progressively harder to predict.  Using time-averages &amp; PCAs, though, you could see two common patterns in the noise:  for buys, bid/ask prices jumped up sharply  &amp; then rose slowly; for sells, bid/ask prices jumped down sharply, and then fell slowly.  Thus the “liquidity shock” trades seemed to have a permanent impact on prices,   rather than a temporary, mean-reverting one.</p>
<p class=""answer""><strong>Anil</strong>: Categorizing and plotting the data clearly showed that the bid and ask prices followed separate paths and their trajectories differed depending on who initiated the trade - buyer or seller. Performing regression separately for each category led to dramatic improvement in prediction accuracy.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer""><strong>Will</strong>: I’m unconvinced that I had noteworthy insights outside of the usual techniques to gain ground in a data mining competition.  RMSE falls by 3 methods: creating many models and blending them, better data/features, or better methods.  Chris and Anil each brought a nice blender and several models to the table.  I did what I could to make a better kNN method, but perhaps my time would have been better spent coming up with features or looking for outliers.</p>
<p class=""answer""><strong>Anil</strong>: The conventional wisdom seems to suggest that an ensemble of reasonably good models perform better than a finely tuned individual model. As a team, we had a great variety of models, but looking back, I think we would have fared better if we spent more efforts to tune the individual models. The models that used SVM and k-means with mediocre prediction accuracy ended up contributing almost nothing to the final blended result.</p>
<p class=""answer""><strong>Chris</strong>:  I knew the market open &amp; outliers would be important, but I was really surprised by how much of an impact they had on one’s RMSE.  I was also surprised to find no mean-reversion in the bid/ask prices, since that differed from the examples that the contest organizers gave.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer""><strong>Anil</strong>: I am a minimalist and typically use little more than gVim, gcc and gnuplot. For this competition, I picked up some R and was impressed by its capabilities. One could just grab an off-the-shelf package, let it loose on the data and end up with decent results. I still think lower level languages have a place in data analytics because of the flexibility that they offer. Knowing what happens under the hood can give you an edge. Sometimes small tweaks to the underlying mechanism can give you a big boost when you desperately need it. My best model was written entirely in C++ without using any 3rd party libraries. This made it possible to mold the model well enough to fit the quirks within the data.</p>
<p class=""answer""><strong>Chri</strong>s:  I used R, mostly working with the glmnet package.  I also used Python (with  the numpy package) for blending prediction sets together.</p>
<p class=""answer""><strong>Will</strong>:  Matlab</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer""><strong>Will</strong>:  Collaborating with new teammates was a nice experience.  Teammates bring different backgrounds, fresh ideas, and code in different ways.  Working alone it is easy to get stuck trying the same hammer on every nail, even if that nail happens to be a screw. That’s when a teammate can step in and tell you to stop smashing with the hammer and try a screwdriver.  Witnessing another person dissect the same data problem is a great way to pick up new tools and skills.</p>
<p class=""answer""><strong>Chris</strong>:  One lesson I learned from this competition is that one should always identify outliers as specifically as possible &amp; decide how to best deal with them.  Also, it’s really helpful to have teammates to bounce ideas off of, especially when you’re stuck or losing motivation.</p>
<p class=""answer""><strong>Anil</strong>: The discussions on the forum, especially post-contest have been illuminating. I don't think any contestant individually had quite a handle on the data. The details that contestants have shared about their findings and methods have shed light on various aspects of the data. One can actually see the pieces coming together in the giant jigsaw puzzle. I am also thankful that I got to collaborate with Chris and Will, who are first-rate data scientists and fantastic people to work with.</p>"
Meet the Winner of the Algo Trading Challenge: An Interview with Ildefons Magrans,http://blog.kaggle.com/2012/01/26/meet-the-winner-of-the-algo-trading-challenge-an-interview-with-ildefons-magrans/,2012-01-27 06:42:05,"<em>Ildefons Magrans is the winner of the <a href=""http://www.kaggle.com/c/AlgorithmicTradingChallenge"">Algorithmic Trading Challenge</a>.  He explains why he chose to measure himself against the market.</em>

<em></em><span class=""question"">What was your background prior to entering this challenge?</span>
<p class=""answer"">I hold a Masters in Computer Science, a Masters in Electrical Engineering and a PhD in Electrical Engineering.  My first machine learning experience was with fuzzy logic clustering algorithms during the final project of MsC in CS.  Recently, I have been working on two applied research projects: developing of a human-like dialog turn-taking model with a continuous-time Hidden Markov Model, and developing a classification system for a prosthetic ankle to infer the presence of stairs.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">I have been interested in algorithmic trading since I finished my PhD 3 years ago. I have been studying market micro-structure, arbitrage opportunities at different frequencies, contributing to open-source algo trading infrastructure and so on. But I never dared to use real money.  I was not sure about my skills compared to other people working in the field. This challenge was a wonderful opportunity to test myself.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I tried many techniques: (SVM, LR, GBM, RF). Finally, I chose to use a random forest.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">The training set was a nice example of how stock market conditions are extremely volatile.  Different samples of the training set could fit very different models. Lots of fun!</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I was not surprised by the difficulty level. High frequency trading is a very competitive field full of smart people trying to fish small inefficiencies.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">I did everything with R, without a database, on an i7 laptop with 16 Gbytes of RAM.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">I have had to improve my parallel programming skills in R.</p>"
Owen Zhang on Placing 2nd in the Claim Prediction Challenge,http://blog.kaggle.com/2012/01/30/owen-zhang-on-placing-2nd-in-the-claim-prediction-challenge/,2012-01-31 04:06:05,"<em>Owen Zhang, who passed the 6 CAS exams ""just for fun"", discusses placing</em> 2nd <em>in the <a href=""http://www.kaggle.com/c/ClaimPredictionChallenge"">Claim Prediction Challenge</a></em><em></em>
<p class=""question"">Why did you decide to participate in the Claim Prediction Challenge?</p>
<p class=""answer"">To continue improving and evaluating my predicative modeling knowledge and skills.</p>
<p class=""question"">Apart from monetary incentives, did anything else motivate you to participate in the competition?</p>
<p class=""answer"">To master cutting-edge analytical methodology in the context of a real world business problem, and to see where I stand in insurance modeling.</p>
<p class=""question"">How many entries did you submit?</p>
<p class=""answer"">What drove you to continue submitting new entries?   I submitted 20 entries. The reason to keep submitting is to find out if the tricks that appeared to have worked on my own validation data would work on the 4th year data as well. Another purpose is, obviously, to ""catch"" those who were in front of me. In retrospect, my 3rd serious submission would have got the same 2nd place, but I didn't know then.</p>
<p class=""question"">How would you characterize your competitors in this contest?</p>
<p class=""answer"">I kind of ""know"" some of them (such as ""old dogs with new tricks"") through other modeling/data mining competitions. I feel this is a very diverse group of modelers. Some are obviously seasoned professionals and some have apparently just started learning. I also have the impression that many competitors are not from P&amp;C insurance background.  I guess we have more machine learners here than statisticians.</p>
<p class=""question"">What did you enjoy most about the competition?</p>
<p class=""answer"">Trying to come up business stories behind partially anonymized data.</p>
<p class=""question"">What got you interested in actuarial science?</p>
<p class=""answer"">I see myself as more a predictive modeler/data miner than an actuary (although I did pass 6 CAS exams just for fun), so this question doesn't really apply to me. I am interested in predictive modeling primarily because I find it is extremely intellectually stimulating AND I appear to be reasonably good at it.</p>"
Kicking Goals: An Interview with Marcin Pionnier of the winning team in Don't Get Kicked,http://blog.kaggle.com/2012/02/06/kicking-goals-an-interview-with-marcin-pionnier-of-the-winning-team-in-dont-get-kicked/,2012-02-07 00:07:56,"<em>Marcin Pionnier, member of the <a href=""http://www.kaggle.com/c/DontGetKicked"">Don't Get Kicked</a> winning team Sollers &amp; Gxav, along with Xavier Conort, gives us some tips on how they blended different models to produce the best results.</em>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">This particular competition was classical - it was easy to start and produce first submissions. Also the domain is something that I can understand - common sense is enough to interprete variable values and to create new ones.</p>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I graduated from Warsaw University of Technology (Poland), my master thesis was related to text/web mining. Currently I work as software architect/programmer for Sollers Consulting (we operate mainly in Europe)  in the insurance area, so preparation data sets from transactional systems for risk estimation is one of my typical everyday tasks. Also I think that participating in Kaggle challenges is giving me a lot of very valuable experience. <img src=""https://lh5.googleusercontent.com/pnf4a8Dl-p8UGaSVmWPm8pl1hFcmLlj_21jOdCj_OikCjL4bkA3HJuysOUowSMpwpz8LlJpkby1hiJzsT9QDZaNR-sybNowTKaKxsgR6CFIIN6R3aAs"" alt="""" width=""1px;"" height=""1px;"" /></p>
<p class=""question"">Have you ever bought a used car?</p>
<p class=""answer"">Yes, but from authorized car dealer to mitigate the risk. It was ""Good Buy"" transaction - not to be confused with ""good bye"" :)</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I did not use any external data sources. However, I added some variables to the initial dataset. The most important among them were:</p>

<ul class=""answer"">
	<li>Various differences between the prices given</li>
	<li>Results of regression models  built on training set: prediction of OdoVeh (it was then subtracted with just OdoVeh value), WarrantyPrice, Engine (in some cases it was possible to extract engine from text fields, for the rest it was made with this regression)</li>
</ul>
<p class=""answer"">My part of our joint solution (simply blended with Xavier Conort's result) is average of seven internal models:</p>

<ul class=""answer"">
	<li>Six variants of LogitBoost algorithm that internally use Dagging Algorithm - training set is divided into stratified folds and the internal models are averaged. As weak learners for Dagging method simple DecisionStump (one node decision tree) and some up to 3-level decision tree based on DecisionStump were used.</li>
	<li>Seventh model was Alternating Decision Tree which is also similar to boosting.</li>
</ul>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">To be honest there was no data transformation/additional data generated that gave me important progress. Various data enrichment approaches were improving my score only a little. I think that in this competition the most important task was to choose algorithms that could aggregate many weak predictors in an efficient way.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I am surprised with the power of boosting weak learners - their good performance is well-known fact, however I was using this technique for the first time.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">I used Weka as library of algorithms linked with my own Java code for data pre-procesing and learning algorithms configuration. I think that such an approach gives the possibility of quick prototyping for initial solutions, and also it is possible to modify existing algorithms by copying the sources and introduce changes when needed.</p>
<p class=""question"">Do you have any advice for other Kaggle competitors?</p>
<p class=""answer"">Fusion of the R model developed by Xavier with my modeling ideas prepared on Weka-based tools gave us a big improvement on the leaderboard, so using different Machine Learning packages together with R (which seems to be most popular tool amongst Kagglers) might be good strategy.</p>"
Vladimir Nikulin on taking 2nd prize in Don't Get Kicked,http://blog.kaggle.com/2012/02/03/vladimir-nikulin-on-taking-2nd-prize-in-dont-get-kicked/,2012-02-04 01:21:25,"<em>Vladimir Nikulin, winner of 2nd prize in the <a href=""http://www.kaggle.com/c/DontGetKicked"">Don't Get Kicked</a> competition, shares some of his insights and tells us why Poland is the place-to-be for machine learning.</em>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">Both Challenges (Give Me Some Credit and Don't Get Kicked) could be regarded as classic and are very similar. That's why, I think, they were extremely popular. I have proper experience, and participated in the relevant Contests (see, for example, PAKDD07 and PAKDD10) in the past. In addition, the financial applications are directly relevant to the interests of my Department of Mathematical Methods in Economy at the Vyatka State University, Kirov, Russia.</p>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I have a PhD in mathematical statistics from the Moscow State University. By the way, I shall be visiting MSU in the middle of this February. Since 2005, I participated in many DM Challenges. In particular, some readers might be interested to consider text of my interview in Warsaw, Poland:
<a href=""http://blog.tunedit.org/2010/07/20/no-alternatives-to-data-mining/"">http://blog.tunedit.org/2010/07/20/no-alternatives-to-data-mining/</a>
This interview was given in June 2010. At that time, Kaggle was at the most early stages of development.  Also, I would like to use this opportunity to express my very high impression about support and recognition of the area of data mining in Poland.</p>
<p class=""question"">Have you ever bought a used car?</p>
<p class=""answer"">Yes, I bought three used cars while in Australia:</p>

<ul class=""answer"">
	<li>Toyota-Corona: {1978/1993/1995}</li>
	<li>Toyota-Camry: {1992/1996/2000}</li>
	<li>Toyota-Camry: {1999/2000/2011}</li>
</ul>
<span class=""answer"">where the meaning of the years is {made/bought/sold}.</span><strong></strong>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">On the pre-processing: it was necessary to transfer textual values to the numerical format. I used Perl to do that task. Also, I created secondary synthetic variables by comparing different Prices/Costs. On the supervised learning methods: Neural Nets (CLOP, Matlab) and GBM in R. No other classifiers were user in order to produce my best result.</p>
<p class=""answer"">Note that the NNs were used only for the calculation of the weighting coefficient in the blending model. Blending itself was conducted not around the different classifiers, but around the different training datasets with the same classifier. I derived this idea during last few days of the Contest, and it produced very good improvement (in both public and private).</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">Relations between the prices are much more informative compared to the prices themselves.  The next step was to rank and treat the relations in accordance to their importance.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">Yes, there was a huge jump from 0.26023 to 0.26608 in public, when I included in the model all the differences between Costs/Prices. I expected a jump, but not so big. On another occasion, I created two promising new variables, and thought it will produce some modest improvement at least. Instead, I observed deterioration.</p>
<p class=""question"">Which tools did you use?</p>
<span class=""answer"">Perl, Matlab, NNs in CLOP and GBM in R.</span><strong></strong>
<p class=""question"">Do you have any advice for other Kaggle competitors?</p>
<p class=""answer"">Be flexible and patient. Do not worry too much about the LeaderBoard. Try to concentrate on the science and fundamentals, but not on how to win.</p>
<p class=""question"">Anything else that you would like to tell us about the competition?</p>
<p class=""answer"">Currently, I am working on the detailed description of my method, and would like to share an excerpt from the Introduction:</p>
<p class=""answer"">Selection bias or overfitting represents a very important and challenging problem. As it was noticed in [1], if the improvement of a quantitativecriterion such as the error rate is the main contribution of a paper, the superiority of a new algorithms should always be demonstrated on independent validation data. In this sense,<strong> the importance of the data mining contests is unquestionable. The rapid popularity growth of the data mining challenges demonstrates with confidence that it is the best known way to evaluate different models and systems.</strong> Based on our own experience, cross-validation (CV) maybe easily overfit as a consequence of the intensive experiments. Further developments such as nested CV maybe overfitted as well. Besides, they are computationally too expensive [1], and should not be used until it is absolutely necessary, because nested CV may generate secondary serious problems as a result of 1) the dealing with an intense computations, and 2) very complex software (and, consequently, high level of probability to make some mistakes) used for the implementation of the nested CV. Moreover, we do believe that in most of the cases scientific results produced with the nested CV are not reproducible (in the sense of an absolutely fresh data, which were not used prior).</p>
<p style=""text-align: right;""><em>[1] Jelizarow, M., Guillemot, V., Tenenhaus, A., Strimmer, K. and Boulesteix, A.-L. (2010)</em><em> Over-optimism in bioinformatics: an illustration, Bioinformatics, Vol.26, No.16, pp.1990-1998.</em></p>"
Of Caffeine and Cross Validation: Tim Veitch on Don't Get Kicked!,http://blog.kaggle.com/2012/02/01/of-caffeine-and-cross-validation-tim-veitch-on-dont-get-kicked/,2012-02-01 23:55:52,"<em>Tim Veitch, the 4th prize winner of used car prediction challenge<a href=""http://www.kaggle.com/c/DontGetKicked""> Don't Get Kicked!</a>, catches up with us about finishing in the money on his second Kaggle outing.
</em>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">Curiosity, really!  Kaggle combines two of my favourite things: solving difficult problems and competition.  I had a bit of spare time over Christmas, so I thought I'd give it a go.  I'm also hoping to meet some interesting people from the Kaggle community - so feel free to get in touch!</p>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I work in my family's travel-modelling consultancy (Veitch Lister Consulting).  My work involves trying to predict the daily travel made by the millions of people living in Australia's urban areas.  This has exposed me to fairly advanced choice modelling techniques (among them logistic regression), which has proved useful on Kaggle.<img src=""https://lh4.googleusercontent.com/PITtNi70vgbpOy4CsJFjC2a66_xT93fp-yLo3ingk56jYqMsK38cv21seR_V79l2LIroRryuPNClgRSDjy_ikhksm5jx6Yo14ib2PrMoLyBIB_LlIYU"" alt="""" width=""1px;"" height=""1px;"" /></p>
<p class=""question"">Have you ever bought a used car?</p>
<p class=""answer"">I drive a used car...but I can't say that I bought it.  It was a 'hand me down' from my Mum...Love You Mum!  I do, however, feel well qualified to buy a used car thanks to this competition!</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I used logistic regression to begin with.  This meant constructing ordinal variables from each of the numeric variables (e.g. the odometer), and adding some interesting variable interactions, particularly involving the MMR variables.  I also found some interesting temporal effects, and included a dummy variable for each month in the dataset.  I then extended my simple logit model by building ""logit trees"" - ie. binary splits (to a level of 1 or 2), with a logistic regression on each leaf. Late in the process I added two data driven approaches - random forests and GBMs, which used standard packages in R.  The GBM turned out to be my highest scoring individual model, with the logit forest second.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">Probably the temporal effects.  My basic logit model suggested that the eight months from January to August 2009 were the eight months with lowest 'kick likelihood', all other things being equal.  I don't yet know the cause, but I think it would be very interesting to investigate why that period was such a good period for buying used cars.  If I'd gotten to the bottom of it, I'm sure it would have improved my model, as the effect probably varies spatially.  And it would certainly help with real life prediction.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I was continually surprised by the variables which proved important: wheel type, the month, or a lack of change in the MMR price (current - acquired).  It was surprising how relatively unimportant the make, model and vehicle type were.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">I used my own C++ library for logistic regression, and the standard Random Forest and GBM packages in R (though I did try to implement my own GBM implementation on the last night, which didn't quite work as well as the R version).  I used the Ruby scripting language to tie it all together, and Excel pivot tables / charts to analyse the data.</p>
<p class=""question"">Do you have any advice for other Kaggle competitors?</p>
<p class=""answer"">Kaggle has really reinforced to me the importance of cross validation.  I've also found getting to know the inner workings of each algorithm very rewarding - it's interesting, and it helps.  I was surprised by how well GBMs worked...that's a key learning for me.  And drink lots of coffee...but not too much!</p>
&nbsp;"
Claiming the Gold: Matthew Carle on winning the Claim Prediction Challenge,http://blog.kaggle.com/2012/02/09/claiming-the-gold-matthew-carle-on-winning-the-claim-prediction-challenge/,2012-02-09 23:53:23,"<em>At long last, we catch up with Matthew Carle, the solo winner of the <a href=""http://www.kaggle.com/c/ClaimPredictionChallenge"">Claim Prediction Challenge</a>.</em>
<p class=""question"">Why did you decide to participate in the Claim Prediction Challenge?</p>
<p class=""answer"">As an actuary, I have worked on claims models in the past, and the Claim Prediction Challenge allowed me to see how my modelling skills compare with those of other modelling experts. It also provided a way to improve modelling skills and try new techniques.</p>
<p class=""question"">Apart from monetary incentives, did anything else motivate you to participate in the competition?</p>
<p class=""answer"">Normally when building predictive models it is not easy to understand how much better your model could be. The ability to rank your models against those of your competitors provides additional insight into how good your modelling skills actually are.</p>
<p class=""question"">How many entries did you submit?  What drove you to continue submitting new entries?</p>
<p class=""answer"">I made a total of 35 entries. The improvements made by other participants provided the incentive to continue making new submissions, particularly when in competition for the top places.</p>
<p class=""question"">What did you enjoy most about the competition?</p>
<p class=""answer"">The ability to compare model performance with those of other modelling experts, and improve modelling skills through testing of different techniques.</p>
<p class=""question"">What got you interested in actuarial science?</p>
<p class=""answer"">I have always had an interest in mathematics and statistics, and in particular their application to real-world problems. The business context of actuarial science was also appealing.</p>"
Could World Chess Ratings be decided by the 'Stephenson System'?,http://blog.kaggle.com/2012/03/20/could-world-chess-ratings-be-decided-by-the-stephenson-system/,2012-03-20 11:13:38,"Congratulations to <a href=""https://www.kaggle.com/users/2702/alec-stephenson"">Alec Stephenson</a>, who was recently announced as winner of the FIDE Prize in the <a href=""https://www.kaggle.com/c/ChessRatings2"">Deloitte/FIDE Chess Rating Challenge</a>! This prize was awarded to the submission which was the most promising practical chess rating system (the criteria can be found <a href=""https://www.kaggle.com/c/ChessRatings2/details/Rules"">here</a>). The World Chess Federation (FIDE) has administered the world championship for over 60 years and manages the world chess rating system.

Here at Kaggle we're very excited about Alec's achievement. This is a major breakthrough in an area which has been extensively studied by some of the world's best minds. Alec wins a trip to the FIDE meeting to be held in Warsaw this April, where he will present his winning method. The next world chess rating system could be based on his model!

World chess ratings have always used the <a href=""http://en.wikipedia.org/wiki/Elo_rating_system"">Elo system</a>, but in the last few years there has been a movement to make the rating system more dynamic. One approach is to modify the Elo system by adjusting the so-called 'K-factors', which determine how quickly individual match results change the overall rankings. Professor Mark Glickman, chairman of the United States Chess Federation ranking committee, has proposed the <a href=""http://en.wikipedia.org/wiki/Glicko_rating_system"">Glicko system</a>, which was a key inspiration behind <a href=""http://research.microsoft.com/en-us/projects/trueskill/faq.aspx"">Microsoft's TrueSkill algorithm</a>. Jeff Sonas, with the backing of FIDE, initiated this Kaggle contest to bring in fresh ideas. He says ""of all the things learned during the contest, the one that I am most excited about is the degree to which Alec was able to <span style=""text-decoration: underline;"">improve the accuracy</span> of the well-established Glicko model <span style=""text-decoration: underline;"">without significantly increasing its complexity</span>.""

<em>We interviewed Alec after his big win...</em>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">I make a couple of submissions in most competitions and then decide from that point whether my interest is sufficient to spend the time competing seriously. What I liked about the chess competition was that, unlike more traditional data mining competitions, the data was extremely simple, containing just player identifiers and results. This meant that the competition was more theoretical than is usually the case, which benefited me as a mathematician.</p>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">My background is in mathematics and statistics. I am currently an academic, teaching courses in R, SAS and SPSS, and have worked in a number places including The National University of Singapore and Swinburne University in Australia. I will soon be taking a position at CSIRO, Australia's national science agency.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">Because of the simplicity of the data I took the view that the best approach would be to build upon methods that already exist in the literature. I took the Glicko sytem of Mark Glickman, added a couple of ideas from Yannis Sismanis and then used a data driven approach to inform further modifications. The Glicko system is based on a Bayesian statistical model; I took this and then let predictive performance, rather than statistical theory, determine my final scheme. I suspect my approach is less useful for other types of two-player games as it was essentially optimized for the chess data.</p>
<p class=""question"">What was your most important insight?</p>
<p class=""answer"">The most important and suprising thing was how competitive an iteratively updated ratings scheme could be in terms of predictive performance. It got in the top 20 overall, which was a great surprise to me, particularly given that the unrestricted schemes obtained an additional advantage from using future information that would not be applicable in practice.</p>
<p class=""question"">Do you have any advice for other Kaggle competitors?</p>
<p class=""answer"">My three tips are (1) Have a go! Start with some random numbers and progress from there. (2) Concentrate on learning new skills rather than the leaderboard. (3) Beware of anything that takes more than 10 minutes to run.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">My usual tool set is R, C, Perl and SQL, but for this competition I just used R with compiled C code incorporated via the .C interface. I'm currently working on an R package allowing users to examine different iteratively updated rating schemes for themselves. Hopefully it will also allow me to make my method a bit simpler without losing predictive performance, which may make it more palatable to the FIDE.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">An interest in methods for modelling two-player games, and a motivation to learn how to play chess! It's my second win in public Kaggle competitions, which is a nice personal achievement.</p>"
Grocking Out - 3rd place interview with Pankaj Mishra,http://blog.kaggle.com/2012/04/10/grocking-out-3rd-place-interview-with-pankaj-mishra/,2012-04-11 05:07:14,"<em>This week we catch up with the winners of the Grockit<a href=""https://www.kaggle.com/c/WhatDoYouKnow""> 'What Do You Know?'</a> Competition, which ended on Feb 29th.  The challenge was to predict which review questions a student would answer correctly when studying for the GMAT, SAT or ACT. Pankaj Mishra placed 3rd, in his first ever Kaggle competition, and offers some great tips for how to get started.</em>
<p class=""question"" dir=""ltr"">What was your background prior to entering this challenge?</p>
<p class=""answer"" dir=""ltr"">I am a Software Developer with an undergraduate degree in Aeronautics. I learned machine learning from the free Stanford Machine Learning class at <a href=""http://www.ml-class.org/course/auth/welcome"">ml-class.org</a> and the AI class at <a href=""https://www.ai-class.com/"">ai-class.com</a>. Big thanks to Andrew Ng, Sebastian Thrun, and Peter Norvig for teaching those classes so well!</p>
<p class=""question"" dir=""ltr"">What made you decide to enter?</p>
<p class=""answer"">I participated to gain experience in machine learning. I was excited to get access to high quality real-world data from Grockit and use it to solve a concrete problem.</p>
<p class=""question"" dir=""ltr"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"" dir=""ltr"">Preprocessing:</p>
<p class=""answer"" dir=""ltr"">I used Java to create a training file with one row (training example) for each user who answered five or more questions. Each row corresponds to the last question for a user. I added many columns to capture fraction-of-correct-answers-by-user, question-difficulty, etc.</p>
<p class=""answer"" dir=""ltr"">Supervised learning methods:</p>
<p class=""answer"" dir=""ltr"">My solution is a mixture of a Neural Network ensemble and a Gradient Boosting Machine (GBM) ensemble. Both of them were trained on the same training data. The training data included columns that themselves were predicted values from other models such as various Collaborative Filtering models, IRT Model, Rasch Model, and the LMER benchmark. This approach to blending is inspired by the paper <a href=""http://www.commendo.at/UserFiles/commendo/File/KDDCup2010_Toescher_Jahrer.pdf"">""Collaborative Filtering Applied to Educational Data Mining""</a>  [see Section(3): Blending].  None of my individual models scored very well on the leader board, but they did much better when combined together.</p>
<p class=""question"" dir=""ltr"">What was your most important insight into the data?</p>
<p class=""answer"" dir=""ltr"">There were some columns in the training data (e.g. number of players for a question) that I did not use in my earlier models because I had thought they would have no effect on whether a user got the question right. I was surprised to discover later that adding them to the model did improve prediction accuracy. So I think the lesson is to not listen to your intuition; let data speak for itself. In practice, though, there is not enough time to try every possible feature, so we do have to go by intuition to a degree.</p>
<p class=""question"" dir=""ltr"">Were you surprised by any of your insights?</p>
<p class=""answer"" dir=""ltr"">I was surprised by the low correlation between (1) an individual model’s performance by itself and (2) the amount of performance improvement of an ensemble when the individual model is added to it.  As an example, I had some very good individual models that when added to an ensemble barely improved the performance of the ensemble. By contrast, I also had some nearly hopeless models that when added to an ensemble significantly improved the performance of the ensemble.</p>
<p class=""answer"" dir=""ltr"">Therefore, we need lots of diverse models in the ensemble for good performance, not necessarily the best performing models. I think that is well known, but I was still surprised to observe it first hand.</p>
<p class=""question"" dir=""ltr"">Which tools did you use?</p>
<p class=""answer"" dir=""ltr"">Java and R.</p>
<p class=""answer"" dir=""ltr"">I used Java for pre-processing and building various Collaborative Filtering models and IRT models.</p>
<p class=""answer"" dir=""ltr"">I used R's nnet and gbm packages for Neural Networks and Gradient Boosting Machine respectively.</p>
<p class=""question"" dir=""ltr"">What have you taken away from this competition?</p>

<ol>
	<li>The most fun way to get better at machine learning is to work really, really hard to win a machine learning competition. When I started in December, the leaderboard had many players with much better score than mine. However, for me, the knowledge that a much better model existed was a strong motivator for finding it, and I spent virtually all my free time researching and looking for better models and techniques.</li>
	<li>Kaggle's forums for various competitions have top quality user generated content containing practical machine learning techniques that one does not find in text-books. I learned at least 2-3 techniques that helped me improve my score.</li>
	<li>One can find readily applicable techniques and models in the papers from winners of other competitions. I read almost all the papers from the Netflix challenge, Heritage Health Prize, and KDD Cup 2010. The papers from the winners of ""KDD Cup 2010 Educational Data Mining Challenge"" contain a wealth of information relevant to the Grockit competition.</li>
</ol>"
Grockit 2nd place interview with Alexander D'yakonov,http://blog.kaggle.com/2012/04/12/grockit-2nd-place-interview-with-alexander-dyakonov/,2012-04-12 22:04:39,"<em>We caught up with all time top-ranked Kaggle competitor, <span style=""color: #00ccff;""><strong><a href=""https://www.kaggle.com/users/3090/alexander-d-yakonov""><span style=""color: #00ccff;"">Alexander D'yakonov</span></a></strong></span>, on his experience with the Grockit ""<a href=""https://www.kaggle.com/c/WhatDoYouKnow"">What Do You Know</a>"" Competition.</em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I’m an Associate Professor at Moscow State University. Participating in Kaggle challenges is giving me a lot of valuable experience. I write popular scientific lectures about data mining.  In the lectures I tell about my experiences. For example,  <a href=""http://alexanderdyakonov.narod.ru/intro2datamining.pdf"">Introduction to Data Mining</a>  and <a href="" http://alexanderdyakonov.narod.ru/lpotdyakonov.pdf "">Tricks in Data Mining</a> (both in Russian).</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">In the last three competitions, I took the first, third and fourth places.  Therefore I looked for a competition to take the second place. :) And I found it!</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">My approach was to reduce this problem to a standard classification problem. I generated feature description of every pair “student – question”. I used pairs from valid_test.csv for tuning the algorithms. Here are some examples of features: an average student score, an average student score today, his time of the answering, the weighed average score (with different weighted schemes), the question difficulty, the question difficulty today, etc. There were also some features from SVD. I also added some linear combinations of the features (which increased performance). I blended GBMs (from R), GLM (from MATLAB) and neural nets (from CLOP library in MATLAB).</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">Nothing, I solved it as a standard classification problem and did not look at the data.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I was surprised that Random Forests were essentially worse than GBMs and didn't increase performance in blending.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">R and MATLAB (with CLOP library)</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">I really liked the winner’s method. And I should admit that the method is more effective than my method. But when I solved the problem, I checked a hypothesis that it could be solved as a usual classification problem. I think that my hypothesis has proved to be true.</p>"
Viva libFM - Steffen Rendle on how he won the Grockit Challenge,http://blog.kaggle.com/2012/04/20/viva-libfm-steffen-rendle-on-how-he-won-the-grockit-challenge/,2012-04-20 17:43:58,"<em><a href=""https://www.kaggle.com/c/WhatDoYouKnow"">Grockit</a> competition winner, <a href=""https://www.kaggle.com/users/25112/steffen"">Steffen Rendle</a>, shares his Factorization Machine technique.  In his own words, ""The combination of FMs and Bayesian learning was very handy as I didn't had to search for any regularization hyperparameters.""</em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I am an assistant professor in computer science at the University of Konstanz.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">I wanted to study factorization machines on a competitive setting and get some empirical evidence that they work well. The Grockit challenge raised my interest because the dataset is of reasonable size (not too small) and has interesting variables.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I used factorization machines (FM) which is a model class that I have developed over the last years. The model allows one to use feature engineering the same way as most standard machine learning tools (e.g. linear regression or SVMs) but it uses factorized variable interactions. For learning, I extended the Bayesian approach that was developed by Christoph Freudenthaler and me with classification capabilities.</p>
<p class=""answer"">I used a simple preprocessing that removes cases (student-question-combinations) that appear very often.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">The most important predictors were the student, question and the time taken for answering a question. I also derived a variable that states how often a student has tried to answer a question before. Besides this I only used the subtrack name and the game type. All other variables didn't help -- at least not in my experiments.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">Ensembles didn't work at all. I couldn't find any improvement by ensembling several models -- neither on my validation set, the public or private leaderboard. I didn't expect large improvements as I tried to have strong single models, however I was surprised to find no improvement at all.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">I used libFM which is an implementation of factorization machines. The software is written in C++ and can be obtained from my website <a href=""http://www.libfm.org/"" target=""_blank"">http://www.libfm.org/</a></p>
<p class=""answer"">For preprocessing, I used Perl.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">I think the format of machine learning challenges gives very valuable feedback to researchers. When conducting experiments for a research paper, one cannot compare to so many different approaches as you have in a Kaggle competition.</p>"
"On Diffusion Kernels, Histograms, and Arabic Writer Identification",http://blog.kaggle.com/2012/04/29/on-diffusion-kernels-histograms-and-arabic-writer-identification/,2012-04-30 04:02:20,"<em>We catch up with Yanir Seroussi, a graduate student in Computer Science, on how he took third place in the <a href=""https://www.kaggle.com/c/awic2012"">ICFHR 2012 - Arabic Writer Identifica</a><wbr><a href=""https://www.kaggle.com/c/awic2012"">tion Competitio</a><wbr><a href=""https://www.kaggle.com/c/awic2012"">n</a>.  After signing up for a Kaggle account over a year ago, he finally decided to give one of the competitions 'just a quick try'.  Famous last words...</wbr></wbr></em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I'm currently in the final phases of my PhD, which is in the areas of natural language processing and user modelling. Even though I address some predictive modelling problems in my thesis, I've never done any image processing work, though it did help to have some background knowledge in machine learning and statistics.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">I signed up to Kaggle over a year ago but never used my account. Recently, I started thinking about what I want to do once I graduate, and somehow bumped into Phil Brierley's blog. This inspired me to give one of the smaller competitions ""just a quick try"", which ended up consuming a lot of my free time...</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">My most successful submission was based on SVMs. I joined the competition quite late and didn't have time to play with blending techniques, which seem to be a key component of many winners' approaches.</p>
<p class=""answer"">As to preprocessing, I don't have any prior knowledge about image processing, so I only briefly experimented with one idea that didn't require much knowledge: converting all the images to texts with freely-available OCR software, which is based on the idea that the same OCR errors would appear for the same writers. While using this as a standalone feature yielded some interesting results, it didn't improve accuracy when used in conjunction with the provided features.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">At first I played around with SVMs and the commonly-used kernels (linear, polynomial, RBF and sigmoid). Then I remembered a recent paper that was presented at the ACL conference, about using character histograms for authorship attribution (<a href=""http://aclweb.org/anthology-new/P/P11/P11-1030.pdf"" target=""_blank"">http://aclweb.org/anthology-<wbr>new/P/P11/P11-1030.pdf</wbr></a>). Since the provided features were given in the form of histograms, I figured that the same techniques would be applicable here. And indeed, using SVMs with a diffusion kernel proved to yield the most significant performance boost.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I wouldn't call it ""surprised"", but I was a bit frustrated by the apparent lack of correlation between cross-validation results on the training data and the accuracy on the validation set. This is probably because each of the 204 writers had only two training paragraphs (all containing the same text), while the test instances were a third paragraph with different content. So any form of cross validation yielded a training subset that was very different from the full training set, and a test subset that obviously couldn't contain the ""third paragraphs"".</p>
<span class=""question"">Which tools did you use?</span>
<p class=""answer"">Mostly LibSVM and SVMLight (for some brief experiments with transductive SVMs that didn't go well). I used Python to parse the feature files, run the libraries, and produce the final results.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">A better understanding of SVMs and the conclusion that I still have a lot to learn :-)</p>"
1st place interview for Arabic Writer Identification Challenge,http://blog.kaggle.com/2012/05/03/1st-place-in-arabic-writer-indentification-challenge/,2012-05-03 15:14:57,"<em>Wayne Zhang, the winner of the <em><a href=""https://www.kaggle.com/c/awic2012"">ICFHR 2012 - Arabic Writer Identifica</a><a href=""https://www.kaggle.com/c/awic2012"">tion Competitio</a></em><a href=""https://www.kaggle.com/c/awic2012"">n</a> shares his thoughts on pushing for the frontiers in hand-writing recognition.</em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I'm pursuing my PhD in pattern recognition and machine learning. I have interests in many problems of this field, such as classification, clustering, semi-supervised learning and generative models.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">To test my knowledge on real-world problems, to compete with smart people, and to contribute in real-life prediction tasks.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I used the provided features. The writer identification problem is a multi-class classification problem, and linear discriminant analysis is suitable for this task.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">Both the training and test set are of a small size, I had to be careful about the generalization ability of the model.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">I used LDA, which was popular and successful in face recognition ten years ago. It appeared to have surprisingly good results on writer identification, possibly because the two tasks are similar. I implemented my code in Matlab, because of its superior matrix computation support.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">To work on real-world problems, you had to be careful about the overfitting problem. It is different from academic research. In real problems we need to consider many details to make a perfect system. One challenge of Kaggle competitions is that the discrepancy between the public and private scores. It makes me consider more about what the situation will be like in real world. You always have limited training data and validation data, but the test data usually are unbounded.  How to generalize your model to the unbounded data could be a problem.</p>"
Chucking everything into a Random Forest: Ben Hamner on Winning The Air Quality Prediction Hackathon,http://blog.kaggle.com/2012/05/01/chucking-everything-into-a-random-forest-ben-hamner-on-winning-the-air-quality-prediction-hackathon/,2012-05-01 18:56:00,"<em>We catch up with Ben Hamner, a data scientist at Kaggle, after he won Kaggle's <a href=""https://www.kaggle.com/c/dsg-hackathon"">Air Quality Prediction Hackathon</a>. As a Kaggle employee, he is ineligible for prizes.</em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I graduated from Duke University in 2010 with a bachelors in biomedical engineering, electrical and computer engineering, and mathematics. For the next year, I applied machine learning to improve non-invasive brain-computer interfaces as a Whitaker Fellow at EPFL. On the side, I participated in or won a number of machine learning competitions. Since November 2011, I have designed and structured a variety of competitions as a Kaggle data scientist.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">I was hanging out at Splunk (one of the SF venues hosting the hackathon). Anthony asked me some questions about extracting features from the data, which prompted me to open it up and look at it in the afternoon.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I took the lagging N components from the full time series (N=8 for the winning submission, which was selected arbitrarily) as features, then each of the 10 prediction times and 39 pollutant measures as targets. I then trained 390 Random Forests over the entire training data, one for each predicted offset time-pollutant combination. The Random Forest parameters were selected so that the models would be quick to train. The code for creating the winning model is available <a href=""https://github.com/benhamner/Air-Quality-Prediction-Hackathon-Winning-Model"">here</a>.</p>
<p class=""answer"">Some straightforward approaches to improving this model include</p>

<ul class=""answer"">
	<li>Optimizing the parameters for model performance as opposed to training time.</li>
	<li>Directly optimizing for the error metric (mean absolute error) instead of RMSE.</li>
	<li>Using a data-driven approach to select the number of previous time points to include.</li>
</ul>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">I don’t believe I had any specific insights on the data - I barely looked at it before training the model.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I was surprised that domain insight wasn’t necessary to win the hackathon. Key insights have been crucial in many of our longer-running competitions.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">Once I decided to fiddle with the data, I asked David (a fellow Kaggle data scientist) to pick a random number between one and three. He picked two, and I used MATLAB. (If he said one I would have used R, and three would have been Python).</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">Taking all the features and chucking them into a Random Forest works surprisingly well on a variety of real-world problems. This is demonstrated more empirically in <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.122.5901&amp;rep=rep1&amp;type=pdf"">this paper</a>. I'm very interested in domains such as CV and NLP where this doesn't hold true, or where the problem can't be simply formulated in the standard supervised machine learning framework.</p>
<p class=""question"">What did you think of the 24 hour hackathon format?</p>
<p class=""answer"">It was a lot of fun! I especially enjoyed seeing Kagglers in venues all over the world collaborating and competing on this problem. I'm curious to see how much better the results would be if we ran this as a standard competition over a couple months, and whether the work in the first day would comprise the majority of the improvement over the benchmark.</p>"
Petterson takes home the EMC Data Science Global Hackathon Prize,http://blog.kaggle.com/2012/05/04/petterson-takes-home-the-emc-data-science-global-hackathon-prize/,2012-05-04 15:21:17,"The<a href=""https://www.kaggle.com/c/dsg-hackathon/""> EMC Data Science Global Hackathon</a> prize was awarded to <a href=""https://www.kaggle.com/users/5018/james-petterson"">James Petterson</a>.  Check out his webpage for a more detailed description and the source code: <a href=""http://users.cecs.anu.edu.au/%7Ejpetterson/"" target=""_blank"">http://users.cecs.anu.edu.au/~<wbr>jpetterson/</wbr></a>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">I am currently finishing my PhD in machine learning at ANU. Before that I worked as a software engineer for the telecom industry for many years.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">The challenge of kaggle competitions always attracted me - I took part in two other ones in the past (<a href=""https://www.kaggle.com/c/WhatDoYouKnow"">What Do You Know</a> and<a href=""https://www.heritagehealthprize.com/c/hhp""> Heritage Health Prize</a>). I was abstaining from entering new ones as I know how time consuming this can be, but when I heard about this 24h one I couldn't resist.</p>
<p class=""answer"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I computed a set of training instances based on:</p>

<ul class=""answer"">
	<li>- mean of all variables for each prediction time</li>
	<li>- mean of all variables for each prediction time and chunkID</li>
	<li>- most recent value of all variables for each chunkID</li>
</ul>
<p class=""answer"">I did some bootstrapping to increase the size and variety of the training data, using a 24-hour moving window. I then trained 390 Generalised Boosted Regression models, one for each combination of target variable and prediction time.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">I didn't spent much time looking at the data, so I can't think of any particular insight.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I was surprised that I had a good result without spending much time trying to understand the data. I suspect that wouldn't be the case in a longer competition, though.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">Only R.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">I saw once again how powerful boosting methods are. Even though this was essentially a time series problem, a standard boosting regression method performed quite well.</p>
<p class=""question"">What did you think of the 24-hour hackathon format?</p>
<p class=""answer"">Normally competitions take 3 months or more, which tends to favour those that can spend more time on them. The 24-hour format was great in the sense that it gave a chance to those that are more time constrained. And, of course, it was a lot of fun!
I hope we will have more of these in the future.</p>
<p class=""photo-credit"">Photo by <a href=""http://www.flickr.com/photos/94693506@N00/"">ninahale</a></p>"
ASAP interview with Martin O'Leary,http://blog.kaggle.com/2012/05/13/asap-interview-with-martin-oleary/,2012-05-13 22:08:41,"<em>For the first of our interviews with top finishers in the <a href=""https://www.kaggle.com/c/asap-aes"">Hewlett Automated Essay Scoring Challenge</a>, we catch up with 6th place finisher and polymath Martin O'Leary (@mewo2).  You can also check out his blog at  <a href=""http://mewo2.github.com/"" target=""_blank"">http://mewo2.github.com/</a></em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answerr answer"">I'm a mathematician turned glaciologist, working as a research fellow at the University of Michigan. I've been involved with Kaggle for about a year now, and have had a few good finishes. I have a habit of doing well in the early part of competitions, which has got me some publicity, but doesn't translate well into final results.</p>
<p class=""answerr answer"">I've always had an interest in linguistics (at one point I considered it as a career), but this was the most serious text mining I've ever done.</p>
<!--more-->
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">Momchil Georgiev. He approached me early on about possibly collaborating, and we decided to produce individual entries first. Somehow we never got around to teaming up, and by the end he'd assembled a big enough team that I decided I'd rather try for a solo run than try to merge. I feel a little bit like Pete Best, who left the Beatles before they became famous.</p>
<p class=""answer"">More seriously, I liked the problem because it's an interesting dataset, and a problem which comes down to a lot more than just number-crunching.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">A lot of the difficulty in this problem was in finding meaningful features in the essays. I spent a lot of time on topic modelling, and looking at distributions of syntactic features. For the final prediction, I used a fairly large ensemble of different methods. Some of the essay sets worked better with boosted approaches, while others were more susceptible to neural nets.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">The choice of error metric is really important! Most algorithms are tuned to a particular notion of error, and it helps a lot to tweak things so that you're actually optimising for your target metric. In this case that meant some customisation, as the quadratic kappa used is a little unusual.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">I was quite surprised how little measures of spelling and grammar ""correctness"" mattered. Except in one case where the grading rubric explicitly mentioned it, they didn't seem to matter much at all. It warms my descriptivist heart to see that teachers are grading on more than just who can use a spellchecker and a semicolon.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">I started out using just R, but introduced Python fairly quickly because of its stronger NLP libraries. There's a good reason that NLTK is popular. I recycled a lot of old R code for various tasks, and used a mixture of custom and pre-packaged models.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">The benefits of multiple approaches. I think the winning teams did so well because they were able to combine several independently created models. Also, you can't take a month off from a competition and expect to still be winning when you get back.</p>"
Speaking in Hands:  Winner of Round 1 of the CHALEARN Kinect Gesture Challenge,http://blog.kaggle.com/2012/05/09/speaking-in-hands-winner-of-round-1-of-the-chalearn-kinect-gesture-challenge/,2012-05-09 21:14:21,"<em>We catch up with <a href=""https://www.kaggle.com/users/8668/alfnie"">Alfonso Nieto-Castanon</a>, the winner of Round 1 of the <a href=""https://www.kaggle.com/c/GestureChallenge"">CHALEARN Gesture Challenge</a>.  This fascinating series of 4 competitions revolves around gesture and sign language recognition using a Microsoft Kinect camera.</em><em>  </em> <em>A must-read for anyone planning to throw their hat in the ring for CHALEARN <a href=""https://www.kaggle.com/c/GestureChallenge2"">Round 2.</a></em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">My background is on computational neuroscience (Ph.D. Cognitive and Neural Systems, Boston University) and engineering (B.S./M.S. Telecommunication Engineering, Universidad de Valladolid). I work freelance as a research consultant and my latest projects range from development of functional connectivity MRI software and analysis methods, to brain computer interfaces for speech restoration in subjects with locked-in syndrome.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">The Chalearn dataset and goals were too interesting to pass up. I just had to give it a try.</p>
<!--more-->
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">I did not implement any learning strategy but used instead a combination of ad hoc features from the depth videos (somewhat inspired by neural processes in the visual system) with a Bayesian network model for recognition.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">Thinking of gestures as a form of communication, and realizing that the subjects in those videos were already doing what they thought would work best in order for us to interpret and recognize those gestures correctly. I imagined that a system that would mimic the specificities of the human visual system would be most likely to pick up those helpful cues from the video sequences correctly.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">I used Matlab (just the matlab base set, no specific toolboxes other than the nice set of functions provided by the contest organizers to browse the data and create a sample submission)</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">I enjoy developing problem-specific algorithms rather than using a combination of off-the-shelf procedures. This contest gave me the chance to do just that while working in one of those (few) areas where humans still outperform machines (and I am curious to see if we can further bridge that gap!)</p>"
1st place interview for Boehringer Ingelheim Biological Response,http://blog.kaggle.com/2012/07/05/1st-place-interview-for-boehringer-ingelheim-biological-response/,2012-07-05 15:39:02,"<em>3 top competitors, who met during Kaggle's first ever private competition, teamed up to win the public <a href=""https://www.kaggle.com/c/bioresponse"">Boehringer Ingelheim Predicting a Biological Response </a>competition.  Team 'Winter is Coming' ( Jeremy Achin and Tom DeGodoy, props for the name) joined forces with Sergey Yurgenson, exchanging 349 emails over 45 days, to build their winning bioresponse model.</em>
<div title=""Sergey Yurgenson, Jeremy Achin, Tom DeGodoy last submitted on 8:21 pm, Friday 15 June 2012 UTC"">
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">Tom and I met while we were both studying Math and Physics at the University of Massachusetts at Lowell and have been friends and colleagues ever since. We both have 7+ years experience doing applied predictive analytics. Most recently we were both Directors of Research and Development at Travelers Insurance (Tom on the Business Insurance side and me on the Personal Insurance side). A couple months ago, we decided to quit our jobs to start our own Data Science company. We plan to use winnings from private Kaggle competitions as our initial source of seed funding.</p>
<p class=""answer"">We met Sergey while competing against him in Kaggle’s first ever invite-only private competition. He has a background in Physics and currently works for Harvard Medical School. The Boehringer Ingelheim contest was his 8th Kaggle competition.</p>
<p class=""answer"">I think it's safe to say that all 3 of us are obsessed with Data Science to an extent that I’m not sure is healthy.</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">Tom and I were looking for our next competition and heard that Sergey was looking for teammates for this problem. We thought this contest had the potential to be big in terms of the quantity and quality of competitors (which definitely turned out to be the case). Sergey’s reason for entering the competition is analogous to the reason fish “decide” to swim--it’s required for survival. :) In Sergey’s words, he “just cannot stop competing.”</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">We used Random Forests for feature ranking &amp; selection. Methodologies explored for various roles included Random Forests, GBM, KNN, Neural Nets, Elastic Net, GLM, GAM, and SVM. Simple transformations &amp; splines were used for some models.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">I would say that the most important insight into the data was obtaining an accurate ranking of the relative importance of the variables. Eliminating variables reduced model training time (allowing us to try more things) and improved performance considerably.</p>
<p class=""answer"">Another important insight was recognizing the danger of overfitting inherent in this problem. This led us to design a testing framework that helped to ensure we didn’t overfit the public leaderboard.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">By plotting the data points using the 2nd and 3rd principal components, you can see 4 very separated clusters of points. We thought this was going to be a very important finding, but it didn’t turn out to help us.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">R &amp; Matlab. Rstudio Server is awesome if you are using multiple computers--I love having a browser open with many tabs each interfacing with a different RStudio Server.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">First and foremost, having a team with diversity and relentless tenacity is extremely important. I’ll quote Shea Parkes because I couldn’t possibly phrase it any better: “It’s quite obvious how much an ensemble of viewpoints can contribute above and beyond an ensemble of algorithms.”</p>
<p class=""answer"">The ability to collaborate effectively is critical. We exchanged 349 emails over the 45 days that we worked on the competition, and we were in sync enough to be able to share, test, and improve on each other’s work. At one point, Sergey took over 17,000 files we generated using R and combined them with his own results in Matlab allowing us to reliably test out many different blends.</p>
<p class=""answer"">Also, if you are going to run long jobs (20+ hours), make sure to put a sign up in your house to remind people not to plug an iron into the same circuit your computers are on. I went into shock when all of a sudden the monitors went black and the computers went silent. It was like someone yanked the Matrix data probe out of the back of my head.</p>

</div>"
Bond-fire of the Data Scientists - Interview with Benchmark Challenge 3rd place Finishers,http://blog.kaggle.com/2012/07/03/bond-fire-of-the-data-scientists-interview-with-benchmark-challenge-3rd-place-finishers/,2012-07-03 18:53:00,"<em>Before we dive in to the slew of interviews with the winners of the many recently finished contests, we take a sec to catch up with Vik P. and Sergey  E., the 3rd place team from the <a href=""https://www.kaggle.com/c/benchmark-bond-trade-price-challenge"">Benchmark Bond Trade Price Challenge</a> back in May.  What do you get when you combine an American diplomat with a Russian physicist?  Read more to find out.</em>
<p class=""question"">What was your background prior to entering this challenge?</p>
<p class=""answer"">Vik: I have a bit of a strange background for a Kaggle participant. I was actually a member of the U.S. Foreign Service 8 months ago, and was serving as a diplomat in South America. I have also worked in operations management, and have a degree in American History.</p>
<p class=""answer"">Sergey: I graduated from Moscow Institute of Physics and Technology in 2005 with MS degree in applied physics and mathematics. Next years I’ve been working as a senior software engineer in several different companies in Russia. I’ve always been interested in AI and it’s application for different type of real life problems.</p>
<!--more-->
<p class=""question"">What made you decide to enter?</p>
<p class=""answer"">Vik: I had previously entered the algorithmic trading challenge on Kaggle, and was eager to apply some of what I had learned to a problem with some clear similarities. After I entered, the fact that the data had a lot of angles and a lot of potential for score improvement kept me motivated.</p>
<p class=""answer"">Sergey: I’ve been studying machine learning for a while and wanted to test my skills. This is my first kaggle contest and I really enjoyed it.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer"">Vik: I actually did not do any real preprocessing. The missing values for some of the previous trades could have been dealt with using preprocessing, but I dealt with them in feature extraction by using average values over the whole 10 trade range and the most recent 5 trade range.</p>
<p class=""answer"">As far as the learning algorithms, I used 2 slightly different random forest models, which I blended using stacked generalization. Due to time constraints, which prevented me from fully exploring parameter tuning, I was unable to get good results when I blended in GBM and linear models.</p>
<p class=""answer"">Sergey: I’ve constructed a lot of features manually. The most important idea was data normalization. I tried to get rid of absolute values such as dollar prices. Then I simply used random forest implementation in R to generate predictions.</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">Vik: My most important insight was probably how irregular the distribution of the target variable is. Various normalization methods really helped in terms of producing a stronger model. The maximum size of the terminal nodes for the random forest was also an important parameter. As a lot of the rows were related due to how the time series data was constructed, setting it too low meant that extremely similar trades were split into different nodes, which actually seemed to reduce accuracy.</p>
<p class=""answer"">Sergey: It was all about how good you construct your features. I spend too much time re-implementingrandom forest in Java specifically for this problem but it didn’t give me any advantage. While some simple features build in a couple of minutes lead to significant improvement.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer"">Vik: I was very surprised by how different the prices were when dealers dealt with each other versus when dealers dealt with customers. The type of trade (dealer to dealer versus customer buy versus customer sell) was actually the single most important variable to the random forest. Although it certainly is to be expected, seeing it illustrated so starkly was interesting.</p>
<p class=""answer"">Sergey: Then I split data by trade_type and is_callable I got unexpectedly big gain in prediction performance.</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer"">Vik: I solely used R.</p>
<p class=""answer"">Sergey: R for prediction and Java for feature extraction.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer"">Vik: The key takeaway that I took from this competition was to not get bogged down with any one idea for too long. These competitions take a lot of creativity, and with that creativity can come fixation on one concept, and the need to get it to work. In the algorithmic trading competition that I participated in previously, I got fixated on the idea of using a linear model, and never really wavered from that fixation. Here, I iterated through a lot of possibilities, used what worked, and didn’t get stuck in the minutiae.</p>
<p class=""answer"">Sergey: Being in team is of crucial importance. Wish we joined our efforts a bit earlier.</p>
<p class=""photo-credit""><a href=""http://www.flickr.com/photos/slightlyeverything/5160739563/"">photo by kate hiscock</a></p>"
Hackathon How I Did It - Shanda Innovations,http://blog.kaggle.com/2012/08/06/hackathon-how-i-did-it-shanda-innovations/,2012-08-06 15:00:14,"<p class=""question""><span style=""color: #000000;"">What was your </span><wbr><span style=""color: #000000;"">background prior to entering </span><wbr><span style=""color: #000000;"">this competition?</span></wbr></wbr></p>
<p class=""answer""><span style=""color: #000000;"">We are a team focused on data </span><wbr><span style=""color: #000000;"">mining from Shanda </span><wbr><span style=""color: #000000;"">Innovations, a tech incubator </span><wbr><span style=""color: #000000;"">of Shanda Corporation from </span><wbr><span style=""color: #000000;"">China. It’s a global leading </span><wbr><span style=""color: #000000;"">interactive entertainment </span><wbr><span style=""color: #000000;"">media group . We all </span><wbr><span style=""color: #000000;"">graduated from top tier </span><wbr><span style=""color: #000000;"">universities in China, </span><wbr><span style=""color: #000000;"">majored in Computer Science, </span><wbr><span style=""color: #000000;"">and then started our career </span><wbr><span style=""color: #000000;"">in Chinese IT companies. </span><wbr><span style=""color: #000000;"">Right before the EMI </span><wbr><span style=""color: #000000;"">Competition, we was awarded </span><wbr><span style=""color: #000000;"">second place in ACM KDD-Cup </span><wbr><span style=""color: #000000;"">2012.</span></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></p>

<div>
<p class=""question""><span style=""color: #000000;"">Was your strategy any different for competing in a 24-hour hackathon vs. the longer running KDD cup?  Any advice for future hackathon participants on how to win the 'sprint' rather than the 'marathon'?</span></p>

</div>
<p class=""answer""><span style=""color: #000000;"">Our strategy for the longer running KDD-Cup and the 24-hour hackathon was very different. The 24 hour hackathon is a very intensive competition, so that what the participants need to do is to find the key features in a much faster way. This means the participants need to take simpler and more effective methods for preprocessing and post-processing. The hackathon is more demanding with getting quick reaction and making the right priorities. Given a chance, we will give the above advice for future participants.</span></p>

<div class=""answer"">

<span style=""color: #000000;"">The KDD-Cup lasting for several months is more demanding with the continuous concentration as well as better endurance of participants. Better skills in time management skill as well as project management are also necessary in this longer competition. Last but not least, participants also need to take care of each other’s motivation for people tend to lose their eagerness to participate when the time goes by and it is hard to always keep a strong motivation.</span>

</div>
<p class=""question""><span style=""color: #000000;"">What made you </span><wbr><span style=""color: #000000;"">decide to enter?</span></wbr></p>
<p class=""answer""><span style=""color: #000000;"">We would love to put </span><wbr><span style=""color: #000000;"">ourselves on the </span><wbr><span style=""color: #000000;"">international stage and to </span><wbr><span style=""color: #000000;"">compete as well as to share </span><wbr><span style=""color: #000000;"">our knowledge of data mining </span><wbr><span style=""color: #000000;"">with peers from all over the </span><wbr><span style=""color: #000000;"">world. We believe </span><wbr><span style=""color: #000000;"">participating in this </span><wbr><span style=""color: #000000;"">competition would be a </span><wbr><span style=""color: #000000;"">precious opportunity to “meet”</span><wbr><span style=""color: #000000;""> all the talents in this </span><wbr><span style=""color: #000000;"">field. In addition, the </span><wbr><span style=""color: #000000;"">competition was very exciting </span><wbr><span style=""color: #000000;"">and challenging. Given a </span><wbr><span style=""color: #000000;"">tight time constraint of the </span><wbr><span style=""color: #000000;"">competition, we viewed it as </span><wbr><span style=""color: #000000;"">a chance for overnight team </span><wbr><span style=""color: #000000;"">building.</span></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></p>
<p class=""question""><span style=""color: #000000;"">What preprocessing </span><wbr><span style=""color: #000000;"">and machine learning methods </span><wbr><span style=""color: #000000;"">did you use?         </span></wbr></wbr></p>
<p class=""answer""><span style=""color: #000000;"">Some preprocessing was given </span><wbr><span style=""color: #000000;"">to Words.txt. We mapped the </span><wbr><span style=""color: #000000;"">words that users chose to </span><wbr><span style=""color: #000000;"">describe artists to some </span><wbr><span style=""color: #000000;"">keyword IDs and used these </span><wbr><span style=""color: #000000;"">IDs in the logistic </span><wbr><span style=""color: #000000;"">regression model, which </span><wbr><span style=""color: #000000;"">greatly improved the </span><wbr><span style=""color: #000000;"">performance.</span></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></p>
<p class=""answer""><span style=""color: #000000;"">The main machine learning </span><wbr><span style=""color: #000000;"">methods were SVD++ and </span><wbr><span style=""color: #000000;"">Logistic Regression. </span></wbr></wbr></p>
<p class=""question""><span style=""color: #000000;"">What was your most </span><wbr><span style=""color: #000000;"">important insight into the </span><wbr><span style=""color: #000000;"">data?</span></wbr></wbr></p>
<p class=""answer""><span style=""color: #000000;"">For most users the training data was very sparse. </span><wbr><span style=""color: #000000;"">Therefore, we should </span><wbr><span style=""color: #000000;"">integrate more features from </span><wbr><span style=""color: #000000;"">other aspects. For instance, </span><wbr><span style=""color: #000000;"">the user profiles, words they </span><wbr><span style=""color: #000000;"">chose, and survey results </span><wbr><span style=""color: #000000;"">would be very valuable.</span></wbr></wbr></wbr></wbr></wbr></wbr></p>
<p class=""answer""><span style=""color: #000000;"">Sometimes it is easy for us </span><wbr><span style=""color: #000000;"">to trap in a fixed mindset </span><wbr><span style=""color: #000000;"">and may ignore some potential </span><wbr><span style=""color: #000000;"">important indicators. Keeping </span><wbr><span style=""color: #000000;"">our mind open is easier said </span><wbr><span style=""color: #000000;"">than done.</span></wbr></wbr></wbr></wbr></wbr></p>
<p class=""question""><span style=""color: #000000;"">Were you surprised </span><wbr><span style=""color: #000000;"">by any of your insights?</span></wbr></p>
<p class=""answer""><span style=""color: #000000;"">We were very surprised to </span><wbr><span style=""color: #000000;"">find that the variation of </span><wbr><span style=""color: #000000;"">the track scores given by </span><wbr><span style=""color: #000000;"">different people was a lot </span><wbr><span style=""color: #000000;"">more than we expected. For </span><wbr><span style=""color: #000000;"">instance, User ID 41072 </span><wbr><span style=""color: #000000;"">scored 100 to track 156 </span><wbr><span style=""color: #000000;"">whereas User ID 41286 gave </span><wbr><span style=""color: #000000;"">merely 4 to the same track! </span><wbr><span style=""color: #000000;"">It was very interesting to </span><wbr><span style=""color: #000000;"">find that people were so </span><wbr><span style=""color: #000000;"">different in music preference </span><wbr><span style=""color: #000000;"">and we believed that was why </span><wbr><span style=""color: #000000;"">so many different types of </span><wbr><span style=""color: #000000;"">music existed. By making a </span><wbr><span style=""color: #000000;"">further in-depth data </span><wbr><span style=""color: #000000;"">analysis we may discover more </span><wbr><span style=""color: #000000;"">on the music interests of </span><wbr><span style=""color: #000000;"">people. </span></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></p>
<p class=""question""><span style=""color: #000000;"">Which tools and </span><wbr><span style=""color: #000000;"">programming language did you </span><wbr><span style=""color: #000000;"">use?</span></wbr></wbr></p>
<p class=""answer""><span style=""color: #000000;"">The programming languages </span><wbr><span style=""color: #000000;"">include C++ and Python. We </span><wbr><span style=""color: #000000;"">would like to express our </span><wbr><span style=""color: #000000;"">gratitude to APEX team, the </span><wbr><span style=""color: #000000;"">author of an open source </span><wbr><span style=""color: #000000;"">toolkit called SVDFeature </span><wbr><span style=""color: #000000;"">used in our solution.</span></wbr></wbr></wbr></wbr></wbr></wbr></p>
<p class=""question""><span style=""color: #000000;"">What have you </span><wbr><span style=""color: #000000;"">taken away from this </span><wbr><span style=""color: #000000;"">competition?</span></wbr></wbr></p>
<p class=""answer""><span style=""color: #000000;"">We had a lot of fun and our </span><wbr><span style=""color: #000000;"">team became more united . </span><wbr><span style=""color: #000000;"">What is more important is </span><wbr><span style=""color: #000000;"">that we came to think about a </span><wbr><span style=""color: #000000;"">broader area of data mining </span><wbr><span style=""color: #000000;"">application. It is </span><wbr><span style=""color: #000000;"">interesting to see our </span><wbr><span style=""color: #000000;"">technology being used in </span><wbr><span style=""color: #000000;"">other industries, for example </span><wbr><span style=""color: #000000;"">the music and entertainment </span><wbr><span style=""color: #000000;"">industry this time. This is </span><wbr><span style=""color: #000000;"">an eye-opening experience </span><wbr><span style=""color: #000000;"">that brought a lot of </span><wbr><span style=""color: #000000;"">sparkles to our routine work. </span></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></p>"
Practice Fusion Diabetes Classification - Interviews with Winners,http://blog.kaggle.com/2012/10/03/practice-fusion-diabetes-classification-interviews-with-winners/,2012-10-03 16:11:39,"<em>We check in with the 1st, 2nd, and 3rd place teams in the <a href=""https://www.kaggle.com/c/pf2012-diabetes"">Practice Fusion Diabetes Classification Challenge</a> ( based on <a href=""https://www.kaggle.com/c/pf2012/prospector"">Shea Parkes' top voted</a> submission in the Prospect round).  As an experiment, we've decided to group all the winners interviews together in one post to really highlight the diversity of backgrounds among successful data scientists.</em>

<em><strong>What are your backgrounds prior to entering this competition?</strong></em>

<strong>1st place: <a href=""https://www.kaggle.com/users/5642/blind-ape"">Jose Antonio Guerrero</a> aka 'blind ape', Sevilla, Spain</strong>:<strong> </strong>My degrees are in mathematics, statistics and operations research. I’m worked in the public health sector for 25 years as researcher, IT technician and senior manager.  A year ago, when I turned 50, decided it was a good age for return at my professional origin, so I went to <a href=""http://huvr.es/"">Virgen del Rocio Universitary Hospital</a>, the flagship hospital in the region. I'm working with large size (8 figures) clinic records databases, grouping clinical cases and with quality and research issues

<strong>2nd: <a href=""https://www.kaggle.com/users/26767/mtb"">Matt Berseth</a> aka 'mtb', Jacksonville, FL, USA: </strong>I have a Bachelor's degree in computer science and a Master's degree in software engineering - both from North Dakota State University. I started my career as an intern with Microsoft and worked there full time for three years. Since leaving Microsoft, I have been working as a full stack developer with primarily Microsoft technologies for the last ten years. I have been fortunate to work in a variety of interesting areas including: automotive marketing, transportation management / logistics and healthcare IT.

<strong>3rd: <a href=""https://www.kaggle.com/users/4960/shashi-godbole"">Shashi Godbole</a> aka 'An apple a day',</strong> <strong>Mumbai, India:</strong> Data mining has been the focus of my work for the past six odd years. Earlier this year, I started a consulting firm with a friend from my alma mater. I had worked on a few other Kaggle competitions in the past (including the <a href=""https://www.heritagehealthprize.com/c/hhp"">Heritage Health Prize</a> which is still going on) but had done it mostly for fun. These earlier competitions allowed me to brush up my skills and learn the latest advances in machine learning. I also turned to R as my primary tool for analysis.

<em><strong>What made you decide to enter?</strong></em>

<strong>Jose: </strong>My experience in health sector. I’m used to clinical databases.

<strong>Matt: </strong>I studied machine learning as an undergraduate, but that was over ten years ago. So last fall when Coursera launched their machine learning course I decided to take the opportunity to get back up to speed. I enjoyed the course and took Daphane Koller's graphical models course this spring as a follow-up. With all of that theory under my belt, I decided I should apply it to something tangible like one of the Kaggle competitions.

<strong>Shashi: </strong>Healthcare is one of the core focus areas of my team at the consulting firm we are building. The problem statement of this particular competition resonated strongly with the kind of problems we are looking to solve for healthcare providers, payers and other stakeholders.

<em><strong>What preprocessing and supervised learning methods did you use?</strong></em>

<strong>Jose: </strong>Main work was data cleaning and feature creation. Grouping diagnostics and actives principles was crucial. As an advance of my solution, I based it in a hard preprocessing and feature creation work:
<ul>
	<li>Translating each medication to its active principles, route of administration.</li>
	<li>Grouping principles active by chemical families / clinical indication. In some cases, as statins, adjusting dose equivalences. Choosing for each group between number of prescriptions, dose or binary flag for feature creation.</li>
	<li>Grouping the diagnoses in base CCS and my personal experience.</li>
	<li>And much more...</li>
</ul>
After, the methods used were the well known gbm and randomforest and later stacking in a generalized additive model.

<strong>Matt: </strong>I spent a fair amount of time generating features from the ICD9, NDC and lab data. I used wikipedia heavily to learn more about diabetes and create features from the diagnoses and treatments that are related to diabetes.

All of the models I selected for my final submissions were boosted trees. I used anywhere from 5 to 13 different models and blended/combined their predictions to create my submissions.

<strong>Shashi: </strong>The preprocessing was limited to missing value imputation for a few fields in the data tables. I used random forests, gradient boosting and neural networks to build several models which were finally stacked together to generate a final solution.

<em><strong>What was your most important insight into the data?</strong></em>

<strong>Jose: </strong>The great numbers of comorbidities and symptoms associated with diabetes.

<strong>Matt: </strong>The ICD9 data is rich. There is information in the hierarchy of the codes (i.e. what level a specific code belongs to), information regarding the health of the individuals family (i.e. any of the 'family history of' codes) and information regarding the individuals behavior (i.e. any of the 'history of non-compliance' codes). And of course the conditions that are associated with each of the codes.

For the first month or so of the competition I focused almost solely on feature generation. And a majority of these features were derived from the ICD9 codes.

<strong>Shashi: </strong>One big insight was that the formats for some key fields were different in train and test datasets. This was causing a much larger error on test data than that on training data. I could not think of any explanation for this for quite a while. Fixing the formatting discrepancy resolved this issue and made the train and test errors consistent.

<em><strong>Where you surprised by any of your insights or any key features?</strong></em>

<strong>Jose: </strong>I’m surprised by gender overall impact. In the data, diabetes was much more prevalent in male (15%) than female (11%) but when fitting the model, the gender influence fell to 0.17%. Probably other comorbidities associated to gender would explain this reduction.

<strong>Matt: </strong>I was most surprised by the area's that I did not find interesting features. I did not find the lab data very useful and I thought the drug information would be more useful than it was. That surprised me. I would be interested in seeing what features the other competitors found in this area.

<strong>Shashi: </strong>I created several new features by taking ratios of different pairs of features. I was surprised by the extent to which these features improved my model. I created these features towards the very end of the competition. It helped me jump up a couple of places on the leaderboard.

<em><strong>Which tools did you use?</strong></em>

<strong>Jose: </strong>R

<strong>Matt: </strong>
<ol>
	<li>.Net / C# for writing the logic that generates the features</li>
	<li>SQL Server for storing the data as well as basic analysis (just sql queries from management studio)</li>
	<li>Python and scikit-learn for training, testing and evaluating the models</li>
	<li>R for graphical analysis (ggplot2)</li>
</ol>
<strong>Shashi:  </strong>I used only R to do all the data processing and the modeling. Excel was used just a little bit to do some quick plots on the data.

<em><strong>What have you taken away from this competition?</strong></em>

<strong>Jose: </strong>I learned a lot about active principles and their interactions with diabetes.

<strong>Matt: </strong>Trust your cross validation scores and use the public leaderboard as a measurement of competitiveness. When I selected my final models for submission, I picked the models with the lowest cross-validation scores, not the ones with the lowest public leaderboard scores. This worked well for me in this competition, using this formula, I ended up picking my five best models.

<strong>Shashi: </strong>The most important learning was that feature engineering is of utmost importance. Perhaps even more than any fine-tuning of modeling algorithms. Allocating a lot of time to just review the data and create useful features can result in significant performance improvement."
Word Tornado - Practice Fusion Open Challenge 3rd Place Interview,http://blog.kaggle.com/2012/09/25/word-tornado-practice-fusion-open-challenge-3rd-place-interview/,2012-09-25 16:03:14,"<em>We catch up with Indy Actuary <a href=""https://www.kaggle.com/users/10694/shea-parkes"">Shea Parkes</a> on his prize-winning Word Tornado entry to the <a href=""https://www.kaggle.com/c/pf2012-at/prospector"">Practice Fusion Open Challenge</a>.  Shea also had the winning entry to the prospect phase of the predictive challenge, which was the source of the <a href=""https://www.kaggle.com/c/pf2012-diabetes"">Practice Fusion Diabetes Classification</a> contest (in which he placed 5th with <a href=""https://www.kaggle.com/users/7657/nschneider"">NSchneider</a>).  These dudes know their healthcare data.</em>

<strong>What was your background prior to entering this competition?</strong>

I'm a health actuary with Milliman, Inc. I do some traditional services like pricing and reserving, but I also focus on applied statistics and statistical graphing. I have worked with EHR data for some client projects before. Neil Schneider and I have teamed up on many of the Predictive Modeling contests on Kaggle over the last couple years. You'll find us consistently just outside the money and loitering in the forums.

<strong>What made you decide to enter?</strong>

I enjoy making visualizations. I don't have a lot of experience with data-heavy dashboards, but I constantly make graphs to tell stories. As a part of any Predictive Modeling contest I enter, I create mountains of visualizations. I don't really believe a result until I can see the result. Given this, I enjoyed the novelty of a contest just about visualization.

<strong>What preprocessing methods did you use to study the data?</strong>

Mostly I dealt with flattening the data to one observation per patient. I focused on Age/Gender, Diagnosis, Medications, Smoking Status and some of the Biometrics. For the Diagnosis and Medication prevalence I did some Bayesian shrinkage since some patients were seen so rarely I did not believe their complete health status had been captured. I used non-linear expansions of the continuous variables since linear assumptions are tenuous at best with real data. My entry goes into more details about the particular form of dimensionality reduction I applied next. Lastly, I did a few supervised learning steps to make the Data.gov mash-up.

<strong>How did you decide what aspects of the data to use?</strong>

I focused on the portions that could tell a good story of redundancy. I also avoided the portions that would require heavy imputation. I initially chased down physician specialty, but there was too little variation to be interesting in an unsupervised environment. For the Data.gov portion I focused mainly on state-level information since that was the most likely shared key. EHR data is commonly sparse in socioeconomic information, so I chose income data.

<strong>Were you surprised by any of your insights or any key features?</strong>

I was surprised at how strong the Smoking Status was in the principal component analysis. I expected Age/Gender to drive the major data divisions, but Smoking Status was as important if not more so. Smokers just utilize a very different set of services than non-smokers.

<strong>Which tools did you use?</strong>

I did all of my analysis in R (<a href=""http://cran.r-project.org/"" target=""_blank"">http://cran.r-project.org/</a>). I cite all of the great packages I used in my report. It was the first time I had tried flattening a relational database via R, and it went much smoother than I expected. It was also the first time using the Markdown language, which was also surprisingly easy.

<strong>What have you taken away from this analysis</strong>

A much better understanding of the Markdown language and the RStudio /Knitr implementation in particular. I will definitely be using that in my consulting work going forward."
Overkill Analytics: Wordpress Winner Describes His Method,http://blog.kaggle.com/2012/09/21/overkill-analytics-wordpress-winner-describes-his-method/,2012-09-21 18:53:14,"<em>Crossposted from <a href=""http://www.overkillanalytics.net/"">Overkill Analytics</a>, the newly launched extra-curricular data science blog by <a href=""https://www.kaggle.com/c/predict-wordpress-likes"">Gigaom-Wordpress Challenge</a> winner <a href=""https://www.kaggle.com/users/47730/carter-s"">Carter S.</a>  You can also read more about his 'overkill' philosophy on <a href=""http://gigaom.com/data/forget-your-fancy-data-science-try-overkill-analytics/"">Gigaom</a>.</em>

I’d like to start this blog by discussing my first <a title=""Kaggle"" href=""http://www.kaggle.com/"">Kaggle</a> data science competition – specifically, the “<a href=""http://http//www.kaggle.com/c/predict-wordpress-likes"">GigaOM WordPress Challenge</a>”.   This was a competition to design a recommendation engine for WordPress blog users; i.e. predict which blog posts a WordPress user would ‘like’ based on prior user activity and blog content.   This  post will focus on how my engine used the WordPress social graph to find candidate blogs that were not in the user’s direct ‘like history’ but were central in their ‘like network.’
<h3><strong>My Approach</strong></h3>
My general approach – consistent with my <a title=""About Me"" href=""http://www.overkillanalytics.net/about-me/"">overkill analytics</a> philosophy – was to abandon any notions of elegance and instead blindly throw multiple tactics at the problem.   In practical terms, this means I hastily wrote ugly Python scripts to create data features, and I used oversized RAM and CPU from an Amazon EC2 spot instance to avoid any memory or performance issues from inefficient code.   I then tossed all of the resulting features into a glm and a random forest, averaged the results, and hoped for the best.   It wasn’t subtle, but it was effective. (Full code can be found <a href=""http://www.kaggle.com/c/predict-wordpress-likes/forums/t/2631/preliminary-winners-code-modified-timeline/14185#post14185"">here</a> if interested.)
<h3><strong>The WordPress Social Graph</strong></h3>
From my brief review of other winning entries, I believe one unique quality of my submission was its limited use of the WordPress social graph.   (Fair warning:  I may abuse the term ‘social graph,’ as it is not something I have worked with previously.)   Specifically, a user ‘liking’ a blog post creates a link (or edge) between user nodes and blog nodes, and these links construct a graph connecting users to blogs outside their current reading list:

<a href=""http://www.overkillanalytics.net/wp-content/uploads/2012/09/blog-wordpress-likegraph.png""><img title=""blog-wordpress-likegraph"" src=""http://www.overkillanalytics.net/wp-content/uploads/2012/09/blog-wordpress-likegraph-580x213.png"" alt="""" width=""580"" height=""213"" /></a>
Defining user-blog relationships by this ‘like graph’ opens up a host of available tools and metrics used for social networking and other graph-related problems.
<h3><strong>Node Distance, a.k.a. Two Degrees of Separation</strong></h3>
The simplest of these graph metrics is the concept of <a href=""http://en.wikipedia.org/wiki/Distance_(graph_theory)"">node distance</a> within graphs.  In this case, node distance is the smallest number of likes required to traverse between a particular user node and a particular blog node.   In the diagram above, for example, User A and Blog 4 have a node distance of 3, while User C and Blog 5 have a distance of 5.

The chart below breaks down likes from the last week of the final competition training data (week 5) by the node distance between the user and the liked blog within their prior ‘like graph’ (training data weeks 1-4):

<a href=""http://www.overkillanalytics.net/wp-content/uploads/2012/09/blog-wordpress-likesbydistance.png""><img title=""blog-wordpress-likesbydistance"" src=""http://www.overkillanalytics.net/wp-content/uploads/2012/09/blog-wordpress-likesbydistance-580x348.png"" alt="""" width=""580"" height=""348"" /></a>

As you can see, nearly 50% of all new likes are from blogs one ‘edge’ from the user – i.e., blogs the user had already liked in the prior four weeks.    These ‘like history’ blogs are a small, easily manageable population for a recommendation engine, and there are many relevant features that can be extracted based on the user’s history with the blog.   Therefore, the like history set was the primary focus of most contest submissions (including mine).

However, expanding the search for candidates one more level – to a distance of 3 edges/likes traversed – encompasses 90% of all new likes.   A ‘distance 3’ blog wold be a blog that is not in the subject’s immediate like history but that is in the history of another user who had liked at least one blog in common with the subject.   This is admittedly a large space (see below), but I think it significant that &gt;90% of a user’s liked blogs in a given week can be found by traversing through just one common reader in the WordPress graph.   Finding the key common readers and common blogs, therefore, is a promising avenue for finding recommendation candidates that are new to the subject user.
<h3><strong>Node Centrality, a.k.a. Finding The Common Thread</strong></h3>
As referenced above, the main problem with using the distance 3 blogs as recommendation candidates is that the search space is far too large – most users have tens of thousands of blogs in their distance 3 sets:

<a href=""http://www.overkillanalytics.net/wp-content/uploads/2012/09/blog-wordpress-distance3blogcopuntcdf.png""><img title=""blog-wordpress-distance3blogcopuntcdf"" src=""http://www.overkillanalytics.net/wp-content/uploads/2012/09/blog-wordpress-distance3blogcopuntcdf-580x377.png"" alt="""" width=""580"" height=""377"" /></a>

As seen from the above chart, while a significant portion of users (~20%) have a manageable distance 3 blog set (1,000 to 2,000 blogs), the vast majority have tens of thousands of blogs within that distance.   (Some post-competition inspection shows that many of these larger networks are caused by a few ‘hyper-active’ users in the distance 3 paths.  Eliminating these outliers could be a reasonable way to create a more compact distance 3 search set.)

One could just ignore the volume issues and run thousands of distance 3 blog candidates per user through the recommendation engine.  However, calculating the features and training the models for this many candidate blogs would be computationally intractable (even given my inclination for overkill).  To get a manageable search space, one needs to find a basic, easily calculable feature that identifies the most probable liked blogs in the set.

The metric I used was one designed to represent <a href=""http://toreopsahl.com/tnet/weighted-networks/node-centrality/"">node centrality</a>, a measure of how important a node is within a social graph.  There are many <a href=""http://en.wikipedia.org/wiki/Centrality"">sophisticated, theoretically sound ways to measure node centrality</a>, but implementing them would have required minutes of exhaustive wikipedia reading.   Instead, I applied a highly simplified calculation designed to measure a blog node’s three-step centrality within a specific user’s social graph:
<ul>
	<li>Step 1(a):  Calculate all three-step paths from the subject user in the graph (counting multiple likes between a user and blog and multiple possible paths);</li>
	<li>Step 1(b): Aggregate the paths by the end-point blog; and</li>
	<li>Step 1(c): Divide the aggregated paths by the total paths in step 1(a).</li>
	<li><a href=""http://knowyourmeme.com/memes/profit"">Step 2: ???</a></li>
	<li><a href=""http://knowyourmeme.com/memes/profit"">Step 3: Profit.</a></li>
</ul>
The metric is equivalent to the probability of reaching a blog in three steps from the subject user, assuming that at each outbound like/edge has an equal probability of being followed.   It is akin to Google’s original <a href=""http://en.wikipedia.org/wiki/PageRank"">PageRank</a>, except only the starting user node receives an initial ‘score’ and only three steps are allowed when traversing the graph.

I don’t know if this is correct or theoretically sound, but it worked reasonably well for me – substantially lifting the number of likes found when selecting candidates from the distance 3 graph:

<a href=""http://www.overkillanalytics.net/wp-content/uploads/2012/09/blog-wordpress-centralitylift.png""><img title=""blog-wordpress-centralitylift"" src=""http://www.overkillanalytics.net/wp-content/uploads/2012/09/blog-wordpress-centralitylift-580x295.png"" alt="""" width=""580"" height=""295"" /></a>

As shown above, if you examine the first 500 distance 3 blogs by this node centrality metric, you can find over 20% of all the likes in the distance 3 blog set.   If you selected 500 candidates by random sample, however, only 3% of the likes from this population would be found.   While I am certain this metric could be improved greatly by using more sophisticated centrality calculations, the algorithm above serves as a very useful first cut.
<h3><strong>Some Very Simple Code</strong></h3>
I’d feel remiss not putting any code in this post.   Unfortunately, there was a lot of bulky data handling code I used in this competition to get to the point where I could run the analysis above, so posting the code that produced this data would require a lot of extra files.  I’d happily send it all to anyone interested, of course, just e-mail me.

However, in the interest of providing something, below is a quick node distance algorithm in Python that I used after the fact to calculate node distances in the like graph.  This is just a basic breadth-first search implemented in Python, with the input graph represented as a dictionary with node names as keys and sets of connected node names as values:

[sourcecode language=""python"" wraplines=""false""]

def distance(graph, start, end):

  # return codes for cases where either the start point
  # or end point are not in the graph at all
   if start not in graph: return -2
   if end not in graph: return -3

# set up a marked dictionary to identify nodes already searched
   marked = dict((k, False) for k in graph)
   marked[start] = True

# set a FIFO queue (just a python list) of (node, distance) tuples
   queue = [(start, 0)]

# as long as the queue is full...
  while len(queue):
     node = queue.pop(0)

    # if the next candidate is a match, return the candidate's distance
     if node[0] == end:
        return node[1]

    # otherwise, add all the nodes connected to the candidate if not already searched
    # mark them as searched (added to queue) and associate them with candidate distance + 1
     else:
        nextnodes = [nn for nn in graph.get(node[0], set()) if marked[nn] == False]
        queue.extend((nn, node[1]+1) for nn in nextnodes)
        marked.update(dict((nn, True) for nn in nextnodes))

  # if you fall through, return a code to show no connection
  return -1

[/sourcecode]

<strong>Conclusion</strong>

In the end, this node centrality calculation served as a feature in my recommendation engine’s ranking and – more importantly – as a method of identifying the candidates to be fed into the engine.   I have not done the work to see how much this feature and selection method added to my score, but I know as a feature it added 2%-3% to my validation score, a larger jump than many other features.   Moreover, my brief review of the other winner’s code leads me to think this may have been a unique aspect of my entry – many of the other features I used were closely correlated elements in the other’s code.

More crucially, for actual implementation by Automattic the ‘like graph’ is a more practical avenue for a recommendation engine, and is probably what the company uses to recommend new posts.   Most of the work we did in the competition differentiated between posts from blogs the user was already reading – useful, but not a huge value-add to the WordPress user.   Finding <em>unseen</em> blog posts in which a user may have interest would be a more relevant and valuable tool, and finding new blogs for the user with high centrality in their social graph is a reasonable way to find them.   From my work on the competition, I believe these methods would be more promising avenues than NLP and topic-oriented methods.

The above posts covers all of my thinking in applying network concepts to the WordPress challenge problem, but I am certain  I only scratched the surface.   There are a host of other metrics that make sense to apply (such as <a href=""http://en.wikipedia.org/wiki/Eccentricity_(mathematics)"">eccentricity</a>, <a href=""http://www.analytictech.com/networks/centrali.htm"">closeness</a> and <a href=""http://www.analytictech.com/networks/centrali.htm"">betweenness</a>).   If you work for WordPress/Automattic (and that is the only conceivable reason you made it this far), I’d be happy to discuss additional ideas, either on this blog or in person.

<span class=""photo-credit"">Photo Credit: <a href=""http://www.flickr.com/photos/nirak/"">karindalziel</a></span>"
EHR with R - Practice Fusion Open Challenge 2nd place Visualizations,http://blog.kaggle.com/2012/09/25/ehr-with-r-practice-fusion-open-challenge-2nd-place-visualizations/,2012-09-25 16:53:11,"<em>First time Kaggler Yasmin Lucero aka <a href=""https://www.kaggle.com/users/52400/yolio"">Yolio</a> took home 2nd place in the <a href=""https://www.kaggle.com/c/pf2012-at/prospector"">Practice Fusion Open Challenge</a> by combining Electronic Health Records with general population data.  Also, lots of good tips on using R for visualizations ( Go ggplot2! )</em>

<strong>What was your background prior to entering this competition?</strong>

I earned my PhD doing mathematical biology and statistics in the field of marine fisheries science. I have done analytical work on a variety of problems in environmental science, mostly working for NOAA (National Oceanographic and Atmospheric Administration).
<strong></strong>

<strong>What made you decide to enter?</strong>

It was a chance for me to build my portfolio. I recently decided that I want to move into doing data analytics work for a tech company in the health and wellness sphere. I need opportunities to demonstrate how my skills transfer to this new area and how I add value. This project was perfect for that.
<strong></strong>

<strong>What preprocessing methods did you use to study the data?</strong>

I work primarily in R. I used a package called RSQLite to access the SQL database. I did this partly because I thought I would be doing a lot of joins, but I found that practice fusion had already provided tables with almost everything I wanted (the data was in great shape). I did eventually write a few SQLite queries of my own: I wanted the number of doctor visits, medications and diagnoses for each patient. You can sort of do this in R with the aggregate function, but the database was large enough for it to be quite slow. SQL is really good at doing that sort of thing fast.

Once the data was in R, I did exploratory analysis using the functions str and table. Then, I made many histograms and other plots. I spent lots of time studying the metadata/schema that was provided in a pdf file. I did a bit of recoding. I recoded dmIndicator to a logical variate (TRUE/FALSE). I recoded the NIST smoking codes. That was tricky; I had to dig around the internet quite a bit to make sense of the metadata that was provided. I eventually was able to recode the NIST codes into a binary logical variate called Smoker. I also came across some obvious measurement error: there were several weight measurements for greater than a 1000 pounds and height measurements for people greater than 9 feet tall. I ended up cutting out lots of unrealistic weight/height measures. I converted Year of Birth to age. I ended up removing all of the data for Puerto Rico, since I couldn't get comparative general population data.

<strong>How did you decide what aspects of the data to use?</strong>

I wanted to find out how representative this medical population was of the general U.S. population. So, I was interested in any data that I could get both for the EHR and the general population, via the census or other surveys. I searched the internet for general population data for anything that was also in this data. This boiled down: obesity rates, smoking status, diabetes status, state of residence, age and gender.
<strong></strong>

<strong>Were you surprised by any of your insights or any key features?</strong>

Yes, I thought that the spatial distribution of the patient population would drive the overall population characteristics, but this didn't happen. It seemed that the EHR population represented a specific demographic slice that was the same regardless of which state they were in.
<strong></strong>

<strong>Which tools did you use?</strong>

The graphics are all made with the ggplot2 package in R. I made a lot of use of the reshape2 package as well, to prepare the data for plotting. And I used RSQLite to get the data into R, and to implement a few queries. And I used the knitr package to generate the markdown report.
<strong></strong>

<strong>What have you taken away from this analysis?</strong>

I think that my favorite result was the age/bmi distribution for diabetics vs non-diabetics. Most diabetics are older, but young diabetics are much more prone to be very overweight. Older diabetics have a weight distribution that isn't that different from older non-diabetics. I also thought it was interesting that while there is a strong relationship between diabetes and BMI, most overweight people are not diabetic. Even at extremely high BMI, 2/3 of people are not diabetic."
Datalanche: PF Open Challenge 1st place,http://blog.kaggle.com/2012/09/25/datalanche-pf-open-challenge-1st-place/,2012-09-25 18:19:07,"<em>For the final entry in our How I Did It series on the  <a href=""https://www.kaggle.com/c/pf2012-at/prospector"">Practice Fusion Open Challenge</a>, we spoke with the winner, <a href=""https://www.kaggle.com/users/46082/ryan-pedela"">Ryan Pedela</a>, the CEO and co-founder of medical info search engine <a href=""http://www.datalanche.com"">Datalanche</a> ( currently in Private Beta, but you can check it out with the login info provided in his contest submission)</em>

<strong>What was your background prior to entering this competition?</strong>

Our team at Datalanche has experience and expertise in computer science, computer graphics, gaming, and data science.
<div>

<strong>What made you decide to enter?</strong>

</div>
We had already started work on a medical information search engine powered by de-identified patient records. The competition and Practice Fusion’s data set aligned perfectly with our goals. The competition gave us an opportunity to build and test our search engine using a high-quality, real-world data set of 10,000 de-identified patient records.
<div>

<strong>What preprocessing methods did you use to study the data?</strong>

</div>
One of our goals is to give users a dynamic, near real-time experience based on their input. This allows them to quickly see how a patient’s demographics, medical history, etc affect any given medical statistic. Practice Fusion’s data set was re-organized so that we could quickly compute statistics based on the user’s input.

We use MedlinePlus Connect, an API from the National Library of Medicine, to provide our users encyclopedia information for every medical topic in our database. The API returns encyclopedia information formatted as HTML, but we needed a different HTML formatting than the one provided. We wrote Python scripts to reformat the encyclopedia information.
<div>

<strong>How did you decide what aspects of the data to use?</strong>

</div>
According to a 2010 study by the Pew Research Center [1], the most commonly searched medical topics are symptoms, medical conditions, and treatments. We decided to focus on medical conditions and medications since they are commonly searched and a significant percentage of Practice Fusion’s data set was devoted to those medical topics.

references:
1. <a href=""http://pewinternet.org/Reports/2011/HealthTopics.aspx"" target=""_blank"">http://pewinternet.org/<wbr>Reports/2011/HealthTopics.aspx</wbr></a>
<div>

<strong>Were you surprised by any of your insights or any key features?</strong>

</div>
We were surprised by the breadth of medical conditions and medications represented in Practice Fusion’s data set given its relatively small size. With only 10,000 patients in the data set, 21 patients had been diagnosed with multiple sclerosis, a rare disease, and some had been treated with interferon, a medication prescribed for treatment of multiple sclerosis. Several other relatively rare medical conditions and medications are also represented in the data set. To us, that shows Practice Fusion’s data set has great coverage for medical conditions and medications.
<div>

<strong>Which tools did you use?</strong>

</div>
The website is built with Javascript, HTML5, CSS3 on the client. On the server, we use Node.js for the web server, our relational database is MySQL, and Apache Solr for search. Data preprocessing scripts are written in Python.
<div>

<strong>What have you taken away from this analysis?</strong>

</div>
We believe improving everyone's access to reliable medical information will improve health care. Our mission is to find medical correlations, trends, facts or ""insights"" which can only be found by analyzing large amounts of anonymous medical data, then intuitively showcase those insights. This competition showed us that the medical data necessary to accomplish our mission is available.

<span class=""photo-credit"">Photo Credit: <a href=""http://www.flickr.com/photos/by-sgillies/"">sgillies</a></span>"
Troll Detection with Scikit-Learn,http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/,2012-09-26 21:01:12,"<em>Cross-post from <a href=""http://peekaboo-vision.blogspot.com/"">Peekaboo</a>, <a href=""https://www.kaggle.com/users/52378/andreas-mueller"">Andreas Mueller</a>'s computer vision and machine learning blog.  This post documents his experience in the<a href=""https://www.kaggle.com/c/detecting-insults-in-social-commentary""> Impermium Detecting Insults in Social Commentary</a> competition, but rest of the blog is well worth a read, especially for those interested in computer vision and Python scikit-learn and -image.</em>

Recently I entered my first <a href=""http://www.kaggle.com/"">kaggle</a> competition - for those who don't know it, it is a site running machine learning competitions. A data set and time frame is provided and the best submission gets a money prize, often something between 5000$ and 50000$.

I found the approach quite interesting and could definitely use a new laptop, so I entered <a href=""http://www.kaggle.com/c/detecting-insults-in-social-commentary"">Detecting Insults in Social Commentary. </a>

My weapon of choice was Python with <a href=""http://scikit-learn.org/dev/"">scikit-learn</a> - for those who haven't read my blog before: I am one of the core devs of the project and never shut up about it.

During the competition I was visiting Microsoft Reseach, so this is where most of my time and energy went, in particular in the end of the competition, as it was also the end of my internship. And there was also the <a href=""http://peekaboo-vision.blogspot.de/2012/09/scikit-learn-012-released.html"">scikit-learn release</a> in between. Maybe I can spent a bit more time on the next competition.
<h3><strong>The Task</strong></h3>
The task was to classify forum posts / comments into ""insult"" and ""not insult"".
The original data set was very  small, ~3500 comments, each usually between 1 and 5 sentences.
One week before the deadline, another ~3500 data points where released (the story is a bit more complicated but doesn't matter so much). Some data points had timestamps (mostly missing in training but available in the second set and the final validation).
<h3><strong>The Result (Spoiler alert)</strong></h3>
I made 6th place.<a title=""Vivek Sharma last submitted on 7:47 pm, Wednesday 19 September 2012 UTC"" href=""http://www.kaggle.com/users/5048/vivek-sharma"" target=""_blank""> Vivek Sharma</a> won.
From some mail exchanges, comments in my blog and a <a href=""http://www.kaggle.com/c/detecting-insults-in-social-commentary/forums/t/2744/what-did-you-use"">thread I opened in the competition forum</a>, I know that at least places 1, 2, 4, 5 and 6 (me) used <a href=""http://scikit-learn.org/dev/"">scikit-learn</a> for classification and / or feature extraction. This seems like a huge success for the project! I haven't heard from the third place, yet, btw.

Enough blabla, now to the interesting part:
First <a href=""https://github.com/amueller/kaggle_insults/"">my code on github</a>. Probably not so easy to run. Try my ""<a href=""https://github.com/amueller/scikit-learn/tree/working"">working"" branch of sklearn</a> if you are interested.
<h3><strong>Things That worked</strong></h3>
My two best performing models are actually quite simple, so I'll just paste them here.
The first uses character n-grams, some handcrafted features (in BadWordCounter), chi squared and logistic regression (output had to be probabilities):

[sourcecode language=""python"" wraplines=""false""]
select = SelectPercentile(score_func=chi2, percentile=18)
clf = LogisticRegression(tol=1e-8, penalty='l2', C=7)
countvect_char = TfidfVectorizer(ngram_range=(1, 5), analyzer=&quot;char&quot;, binary=False)
badwords = BadWordCounter()
ft = FeatureStacker([(&quot;badwords&quot;, badwords), (&quot;chars&quot;, countvect_char), ])
char_model = Pipeline([('vect', ft), ('select', select), ('logr', clf)])
[/sourcecode]

The the second is very similar, but also used word-ngrams and actually preformed a little better on the final evaluation:

[sourcecode language=""python"" wraplines=""false""]
select = SelectPercentile(score_func=chi2, percentile=16)

clf = LogisticRegression(tol=1e-8, penalty='l2', C=4)
countvect_char = TfidfVectorizer(ngram_range=(1, 5), analyzer=&quot;char&quot;, binary=False)
countvect_word = TfidfVectorizer(ngram_range=(1, 3), analyzer=&quot;word&quot;, binary=False, min_df=3)
badwords = BadWordCounter()

ft = FeatureStacker([(&quot;badwords&quot;, badwords), (&quot;chars&quot;, countvect_char),(&quot;words&quot;, countvect_word)])
char_word_model = Pipeline([('vect', ft), ('select', select), ('logr', clf)])
[/sourcecode]

My final submission contained two more models and also the combination of all four. As expected, the combination performed better than any single model, but the improvement over char_model_word was not large (0.82590 AUC vs 0.82988 AUC, the winner had 0.84249).

Basically all parameters here are crazily cross-validated, but many are quite robust (C= 12 and percentile=4 will give about the same results).

Some of the magic happens obviously in BadWordCounter. You can see the implementation <a href=""https://github.com/amueller/kaggle_insults/blob/master/features.py#L35"">here</a>, but I think the most significant features are ""number of words in a badlist"", ""ratio of words that is in badlist"", ""ratio of words in ALL CAPS"".

Here is a visualization of the largest coefficients of three of my model. Blue means positive sign (insult), red negative (not insult):
<div><a href=""http://2.bp.blogspot.com/-PMVd1aUeky4/UGNp0ALNAiI/AAAAAAAAAIw/HvK9q7HIDeo/s1600/all_plots_small.png""><img src=""http://2.bp.blogspot.com/-PMVd1aUeky4/UGNp0ALNAiI/AAAAAAAAAIw/HvK9q7HIDeo/s1600/all_plots_small.png"" alt="""" border=""0"" /></a></div>
<div><a href=""http://blog.kaggle.com/wp-content/uploads/2012/09/all_plots.png"">Full sized image</a></div>
Most of the used features are quite intuitive, which I guess is a nice result (bad_ratio is the fraction of ""bad"" words, n_bad is the number).

But in particular the character plot looks pretty redundant, with most of the high positives detecting whether someone is a moron or idiot or maybe retarded...
Still it performs quite well (and of course these are only 100 of over 10,000 used features).

For the list of bad words, I used one that allegedly is also used by google.
As this will include ""motherfucker"" but not ""idiot"" or ""moron"" (two VERY important words in the training / leaderboard set), I extended the list with these and whatever the thesaurus said was ""stupid"".
Interestingly in some models, the word ""fuck"" had a very large negative weight.
I speculate this is caused by n_bad (the number of bad words) having a high weight and ""fuck"" not actually indicating insults.

As a side note: for the parameter selection, I used the ShuffleSplit (as <a href=""https://github.com/ogrisel"">Olivier</a> suggested), as StratifiedKFold didn't seem to be very stable. I have no idea why.
I discovered very close to the end that there were some duplicates in the training set (I think one comment was present 5 times), which might have been messing with the cross-validation.
<h3><strong>Things that didn't work</strong></h3>
<h4><strong>Feature selection:</strong></h4>
I tried L1 features selection with logistic regression followed by L2 penalized Logistic regression, though it was worse than univariate selection in all cases.
I also tried RFE, but didn't really get it to work. I am not so familiar with it and didn't know how to adjust the step-size to work in reasonable time with so many features.
I also gave the randomized logistic regression feature selection a shot (only briefly though), also without much success.
<h4><strong>Classifiers:</strong></h4>
One of my submissions used elastic net penalized SGD, but that also turned out to be a bit worse than Logistic Regression.
I also tried Bernoulli naive Bayes, KNN, and random forests (after L1 feature selection) to no avail.
What surprised me most was that I couldn't get SVC (LibSVM) to work.
The logistic regression I used (from LibLinear) was a lot better than the LibSVM with Platt-scaling. Therefore I didn't really try any fancy kernels.
<h4><strong>Features:</strong></h4>
I tried to use features from PCA and K-Means (distance to centers).
I also tried to use the chi squared kernel approximation in RandomizedChi2,
as this often worked very well for bag of visual words, but didn't see any improvement.
I also played with <a href=""http://pypi.python.org/pypi/jellyfish/0.1.2"">jellyfish</a>, which does some word stemming and standardization, but couldn't see an improvement.

<strong>A long complicated pipeline:</strong>

I also tried to put more effort into handcrafting the features and parsing the text.
I used sentence and word tokenizers from <a href=""http://nltk.org/"">nltk</a>, used collocations, extracted features <a href=""https://github.com/amueller/kaggle_insults/blob/master/features.py#L169"">using regex</a>, even tried to count and correct spelling mistakes.
I briefly used part-of-speech tag histograms, but gave up on POS-tagging as it was very slow.
You can look up the details of what I tried <a href=""https://github.com/amueller/kaggle_insults/blob/master/features.py#L123"">here</a>.
The model using these features was by far the worst. I didn't use any character features, but many many handcrafted ones. And it didn't really overfit.
It was also pretty bad on the cross-validation on which I designed the features.
Apparently I didn't really find the features I was missing.
I also used a database of positive and negative connotated words.

I should probably have tried to combine each of these features with the other classifiers, though I wanted to avoid building to similar models (as I wanted to average them). Also I didn't really invest enough time to do that (my internship was more important to me).
<h3><strong>Things I implemented</strong></h3>
I made several additions to scikit-learn particularly for this competition.
They basically focused on text feature extraction, parameter selection with grid search and feature selection.

These are:
<h4><strong>Merged</strong></h4>
<ul>
	<li>Enable grid searches using Recursive Feature Elimination. (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1128"">PR</a>)</li>
	<li>Add minimum document frequency option to CountVectorizer (n-gram based text feature extraction) (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1128"">PR</a>)</li>
	<li>Sparse Matrix support in Recursive Feature Elimination. (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1029"">PR</a>)</li>
	<li>Sparse Matrix support in Univariate Feature Selection. (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1025"">PR</a>)</li>
	<li>Enhanced grid search for n-gram extraction. (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1024"">PR</a>)</li>
	<li>Add AUC scoring function. (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1013"">PR</a>)</li>
	<li>MinMaxScaler: Scale data feature-wise between given values (i.e. 0-1). (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1131"">PR</a>)</li>
</ul>
<h4><strong>Not merged (yet)</strong></h4>
<ul>
	<li>FeatureUnion: use several feature extraction methods and concatenate features. (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1173"">PR</a>)</li>
	<li>Sparse matrix support in randomized logistic regression (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1133"">PR</a>).</li>
	<li>Enhanced visualization and analysis of grid searches. (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1128"">PR</a>)</li>
	<li>Allow grid search using AUC scores. (<a href=""https://github.com/scikit-learn/scikit-learn/pull/1014"">PR</a>)</li>
</ul>
<h3><strong>Things I learned</strong></h3>
I learned a lot about how to process text. I never worked with any text data before and I think now I have a pretty good grip on the general idea. The data was quite small for this kind of application but still I think I got a little feel.
Also, it seems to me that the simplest model worked best, feature selection and feature extraction are very important, though hand-crafting features is very non-trivial.
To recap: my best single model was the ""char_word_model"",  which can be constructed in 7 lines of sklearn stuff,  together with 30 lines for custom feature extraction. I think if I had added also the date, I might have had a good chance.
<h3><strong>Things that worked for others</strong></h3>
Most contestants used similar models as I did, i.e. linear classifiers,
word and character n-grams and some form of counting swearwords.
Vivek, who won, found that SVMs worked better for him than logistic regression. Chris Brew, who came in fourth, only used character n-grams
and a customized SGD classifier. So even with very simple features, you can
get very far.
It seems most people didn't use feature selection, which I tried a lot.

The most commonly used software was scikit-learn, as I said above, R, and <a href=""http://nlp.stanford.edu/software/classifier.shtml"">software from the Stanford NLP</a> group.

For details on what others used, see the discussion in the <a href=""http://www.kaggle.com/c/detecting-insults-in-social-commentary/forums/t/2744/what-did-you-use"">kaggle forum</a>.
<h3><strong>Final Comments</strong></h3>
After the first version of this blog-post (which I now shamelessly rewrote), I got a huge amount (relatively speaking) of feedback from other competitors.
Thanks to everybody who shared there methods - in the comments, at kaggle, and at the <a href=""http://sourceforge.net/mailarchive/message.php?msg_id=29871399"">scikit-learn mailing list</a> - and even <a href=""https://github.com/cbrew/Insults"">their code</a>!

I feel it is great that even though this is a competition and money is involved, we can openly discuss what we use and what works. I think this will help push the ""data science"" community and also will help us create better tools.

There where several thing that seemed a bit weird about the competition.
I know the competitions are generally still somewhat in a beta, phase, but there are some things that could be improved:

The scores from the leader board dropped significantly, from  <a href=""http://www.kaggle.com/c/detecting-insults-in-social-commentary/leaderboard/milestone"">around 91 AUC</a> to <a href=""http://www.kaggle.com/c/detecting-insults-in-social-commentary/leaderboard"">around 83 AUC</a> on the final evaluation. I'm pretty sure I did not overfit (in particular the leader board score was always close to my cross validation score and I only scored on the leader board 4 times). Some discussion about this is <a href=""http://www.kaggle.com/c/detecting-insults-in-social-commentary/forums/t/2737/ask-the-leaders-what-should-have-i-done-to-avoid-overfitting"">here</a>. Generally speaking, some sanity tests on the data sets would be great.

I was a bit disappointed during the competition as cross-validation seemed very noisy and my standard deviation captured the scores of the first 15 places.
That also made it hard to see which changes actually helped.
Also, there seemed to be a high amount of label noise.

For example most of my models had this false positive:

<em>Are you retarded faggot lol If you are blind and dont use widgets then that doesnt mean everyone else does n't use them Widgets is one of the reasons people like android and prefer it agains iOS You can have any types of widgets for weather seeing your twitter and stuff and on ios you scroll like an idiot like a minute and when you finally found the apps you still have to click a couple of times before you see what you need Android 2:0 iOS ; ] </em>

Hope you enjoyed this lengthy post :)"
How We Did It: CPROD 1st place interview,http://blog.kaggle.com/2012/10/04/how-we-did-it-cprod-1st-place-interview/,2012-10-04 17:02:18,"<em>We catch up with the team of undergrads who took 1st place in the<a href=""https://www.kaggle.com/c/cprod1""> CPROD (Consumer Products) Challenge</a>.  They'll be presenting their results this December at the ICDM-2012 conference.</em>
<h3><strong>What was your background prior to entering this competition?</strong></h3>
We are undergraduate students from Tsinghua University, China. Before entering the competition, we have some experience about developing software and applications using techniques from machine learning and nature language processing. What’s more, we attended <a href=""https://www.kddcup2012.org/"">KDD Cup 2012</a> Track 1 with the same team name “ISSSID” and ranked 8<sup>th</sup> finally.
<h3><strong>What made you decide to enter?</strong></h3>
We found that the problem was both challenging and research-oriented. In addition, the competition is a part of the ICDM 2012 conference.
<h3><strong>What preprocessing and supervise learning methods did you use?</strong></h3>
Preprocessing: (1) JSON format to plain text format (2) cleaning the data by deleting all useless characters and symbols. (3) Change all uppercase for few products to lowercase

Supervision learning: We employed “Conditional Random Field” Model. We choose this algorithm because it converges faster and is easy to implement. We used tool MALLET for this purpose.
<h3><strong>What was your most important insight into the data?</strong></h3>
The specific characters of products naming (Example: iPhone – mixture of both uppercase and lowercase) and human sematic behavior analysis (Example: my iPhone or &lt;action&gt; by &lt;company name&gt;) are the most important insight that helped us to improve the precision overall. We finally took voting approach (to find which category the product belongs to) based on our experimental results.
<h3><strong>Were you surprised by any of your insights or any key features?</strong></h3>
When we merged the Conditional Random Field Model to other two models (Standard and Rule Template) we have, the performance we achieved significantly increased. We got approximately 3% improvement in F1 score.

Notably combinations of CRF models achieved the highest score in the private leaderboard, but not in the public leaderboard.
<h3><strong>Which tools did you use?</strong></h3>
Languages: C++, Python, Perl

Tool: Mallet
<h3><strong>What have you taken away from this competition?</strong></h3>
Real Life problem challenges because of the following reason:

1)    Data is from heterogeneous dataset.

2)    Generally entity resolution is quite difficult task.

3)    Huge product name list.

4)    Semantics used in the forum environment poses challenge.

Even we entered the competition very late, we have devised several approaches and ran quiet good amount of experiments to move in the right direction. We learnt working on real life problem poses lot many challenges. Working on this problem improved our approach, creativity and knowledge. Now we look forward to work more on such real life dataset problems. Finally we are glad to win the competition in a popular conference, and of course bucks!"
Tuzzeg the Troll-hunter: Impermium 2nd place Interview,http://blog.kaggle.com/2012/10/18/tuzzeg-the-troll-hunter-impermium-2nd-place-interview/,2012-10-18 16:36:32,"<em>We check in with the 2nd place winner of the <a href=""https://www.kaggle.com/c/detecting-insults-in-social-commentary"">Impermium ""Troll-dar"" Competition</a>.  He's also published his <a title=""my code"" href=""https://github.com/tuzzeg/detect_insults"" rel=""nofollow"" target=""_blank"">code</a> and a more detailed <a title=""brief explanation"" href=""https://github.com/tuzzeg/detect_insults/blob/master/README.md"" rel=""nofollow"" target=""_blank"">explanation</a> of his approach on github.</em>

<strong>What was your background prior to entering this challenge?</strong>

I used to work in Yandex (Russian N1 search engine) on text classification
problems. I also finished great online courses: ML class by Andrew Ng and
NLP class by Manning and Jurafsky. Actually I am not a strong ML hacker, I
think my advantage was in variety in extracted features and text processing
methods.

<strong>What made you decide to enter?</strong>

I recognized this Kaggle competition as an opportunity to experiment with
text processing tasks and to learn more about machine learning techniques.

<strong>What preprocessing and supervised learning methods did you use?</strong>

I used stemming and dependency parsing in preprocessing. I also used
language model code which I wrote during studying at the NLP class. As for
learning methods - I used logistic regression for basic classifiers and
random forest for the final ensemble.

<strong>What was your most important insight into the data?</strong>

Sentence level features. After examining classification errors I realized
that many insulting posts were one sentence posts, and in bigger posts
there were one insulting sentence.

<strong>Were you surprised by any of your insights?</strong>

One of the most surprising thing for me was that the simple stem-based
features (subsequnces and ngrams) work much better in the final ensemble
than complex features based on parser results and POS tags.
Syntax features (features build around dependency parser results) alone
gave me pretty great AUC, but got very low feature importance in the final
ensemble.

<strong>Which tools did you use?</strong>

Stanford POS tagger and parser for preprocessing. scikit-learn for learning.

<strong>What have you taken away from this competition?</strong>

I learned how to build ensembles using stacking (I said - I am not a ML
hacker ;). Also got some insight how to use different NLP features.
This competition definitely gave me great opportunity to stretch my
knowledge about ML and NLP. Eager to participate in next competition, just
need make up for the lost sleep ;)

Photo Credit: <strong id=""yui_3_5_1_3_1350578668924_998""><a id=""yui_3_5_1_3_1350578668924_997"" href=""http://www.flickr.com/photos/dorkomatic/"">Howard Dickins</a></strong>"
Team DataRobot: Merck 2nd place Interview,http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/,2012-11-04 20:10:43,"<em>Team DataRobot explains how to take on the <a href=""https://www.kaggle.com/c/MerckActivity"">Merck Molecular Activity Challenge</a> using smoke alarms and airplanes.</em>

<strong>What was your background prior to entering this challenge?</strong>

<a href=""https://www.kaggle.com/users/17379/xavier-conort"">Xavier</a>: I run a consultancy Gear Analytics specialized in predictive analytics in Singapore. Previously, I worked in France, Brazil, China and Singapore holding different roles (actuary, CFO, risk manager) in the life and non-life insurance industry.

<a href=""https://www.kaggle.com/users/9441/jeremy-achin"">Jeremy</a> and <a href=""https://www.kaggle.com/users/16192/tom-degodoy"">Tom</a>: We met while we were both studying Math and Physics at the University of Massachusetts at Lowell and have been friends and colleagues ever since. We both have 7+ years experience doing applied predictive analytics. Most recently we were both Directors of Research and Modeling at Travelers Insurance (Tom on the Business Insurance side and Jeremy on the Personal Insurance side). Earlier this year, we decided to quit our jobs to start our own Data Science company (DataRobot). In our previous three kaggle competitions, we placed 3rd (private), 1st (Bio) and 4th (Diabetes).

<strong>What made you decide to enter?</strong>

Xavier: I was looking for a competition to team up with my buddies Tom and Jeremy, who I met through Kaggle. We previously teamed up with Sergey Yurgenson for the ""Practice Fusion Diabetes Classification"" competition. We got good results but failed to finish in the top 3 (4th place). We also knew that we had good chance to win the Merck competition as we did quite well for the ""Biological Response"" (1st and 5th place).

Jeremy and Tom: We were looking for a competition to team up with our buddy Xavier, who we met through Kaggle, and we thought we’d be able to leverage what we had learned during the BioResponse competition which we placed 1st in. Also, because we quit our jobs earlier this year, we were hoping to place in the top 3 and win some money to pay for a few more months of ramen noodles and canned tuna fish. If we placed 1st or 2nd we thought we might even be able to turn our internet back on--if our neighbor changes his wifi password on us again, we are screwed.

<strong>What preprocessing and supervised learning methods did you use?</strong>

Methodologies explored for various roles include Random Forests, PCA, GBM, KNN, Neural Nets, Elastic Net, GAM, and SVM.

<strong>What was your most important insight into the data?</strong>

For most problems, GBM and SVM had similar predictive power and produced similar predictions. However, for problem 5, SVM's predictions deviated significantly from GBM's predictions and scored badly on the public leaderboard. This confirmed the importance of having at least 2 very different models in one's toolkit. We also improved the accuracy slightly by capping the predictions of a few problems.

<strong>Were you surprised by any of your insights?</strong>

The most surprising thing was that almost all attempts to use subject matter knowledge or insights drawn from data visualization led to drastically worse results. We actually arranged a 2 hour whiteboard lecture from a very talented biochemist and came up with some ideas based on what we learned, but none of them worked out. Also, the visualizations that were shared as part of the visualization part of the competition were incredible (thanks to all who contributed!). We drew much insight from them which led us to try some new approaches that we were absolutely sure would work. However, most of these approaches failed to improve results, and many of them drastically decreased our public leaderboard scores. The visualizations did make us take a second look at capping which helped a bit.

We were also surprised to see how well our internal CV scores correlated with the public (and private) leaderboard scores. This was unexpected because of all the evidence suggesting the test sets were very different from the training sets for some problems. Only for problem 4 did the public leaderboard give us faulty feedback, but since problem 4 was so small, we knew to include a submission that ignored the public leaderboard feedback and included our best CV model for problem 4.

<strong>Which tools did you use?</strong>

We used R, Python, and a lot of computing power. We really didn’t start working on the problem until around 2 weeks before the deadline, so we had to cram lots of cpu cycles into a short amount of time.  We used our 9 Ubuntu servers, Amazon, plus Xaviers magic macbook which he somehow gets to perform like it is a 32 core machine with 256GB of RAM. I (Jeremy) was sure the thing would combust at any moment, so I made sure all the smoke alarms in the house had fresh batteries.

We also used airplanes--Xavier came to the US and worked with us in person for 4 days which allowed us to get a good head start on the problem and make a solid two week plan (which we stuck to for the most part).

<strong>What have you taken away from this competition?</strong>

We have some new techniques we need to learn if we want to compete for first place in future competitions.

Kaggle is a great place to meet people with the same interest and great results can come from this: friendship + powerful models!

<strong id=""internal-source-marker_0.11779254907742143"">
</strong>"
Team '.' takes 3rd in the Merck Challenge,http://blog.kaggle.com/2012/11/05/team-takes-3rd-in-the-merck-challenge/,2012-11-05 16:00:25,"<p class=""question"">So, what's with the punctuation mark for a team name?</p>
<p class=""answer""><em>Eu Jin Lok:</em> Apologies for the team name, I know it’s annoying. If you were wondering, I chose it for its functionality: (1) It’s hard for people to notice; (2) It’s hard for people to click (if they want to find out our names).</p>
<p class=""question"">What was your background prior to entering this <a href=""https://www.kaggle.com/c/MerckActivity"">challenge</a>?</p>
<p class=""answer""><em><a href=""https://www.kaggle.com/users/6696/zach"">Zach Mayer</a>:</em> I've got an undergraduate degree in biology, and a professional background in applied statistics and predictive modeling.  I currently work for management consulting firm AlixPartners.</p>
<p class=""answer""><em><a href=""https://www.kaggle.com/users/3346/eu-jin-lok"">Eu Jin Lok</a>:</em> I majored in Marketing and Econometrics in University, and like Zach, I'm currently working for Deloitte in the data analytics unit as a senior consultant.</p>
<p class=""answer""><em><a href=""https://www.kaggle.com/users/808/alexander-larko"">Alexander Larko</a>:  </em>I have a Master's degree in computer science - from South Russian State Technical University (Novocherkassk Polytechnic Institute). I started my career as an engineer with Scientific Research Institute of the city of Donetsk and worked there for three years. After that, I left to join a manufacturing firm and spent the next 25 years of my career as researcher, IT - engineer and senior manager for the firm. Now, I'm working for a small IT company as a technical director</p>
<p class=""question"">What made you decide to enter?</p>
<p class=""answer""><em>AL:</em> I liked the challenge, because it's an interesting set of data.</p>
<p class=""answer""><em>ZM:</em> Well it seemed like an easy regression problem at first, at least until we realized how much the size of the data sets varied.  Self-evaluation on this problem was especially challenging.</p>
<p class=""answer""><em>EJ:</em> Zach and I have been working together for a while on the HHP contest. On day, Zach mentioned the competition to me in passing so I thought why not give it a go. its a pretty interesting problem to solve...and also not to mention the prize was attractive.</p>
<p class=""question"">How did you guys meet each other?</p>
<p class=""answer""><em>ZM:</em> Eu Jin and I met when we started collaborating on the <a href=""https://www.heritagehealthprize.com/c/hhp"">Heritage</a> prize, and we liked working together, so we entered a couple other competitions.</p>
<p class=""question"">How did you decide to start working together on the Merck comp?</p>
<p class=""answer""><em>EJ: </em>One day, all of a sudden, there was a raft of new competitions on Kaggle and they were all really interesting, but deadline is so close together, like the MERCK and US Census. So I decided that the best strategy is to work with Zach whom I'm already working with on the HHP contest. We entered the MERCK and US Census competition together, doing a tag team, swamping and changing as we get new ideas. Half way through the contest, our work/career ate into our Kaggle time, so I invited Alex to join us as I've seen him competing for 2 years now and thought it would be nice to work together with him.</p>
<p class=""question"">What preprocessing and supervised learning methods did you use?</p>
<p class=""answer""><em>EJ:</em> I used SVD to reduce features which was used as training data. For models, I tried everything from GBMs to PLS but it came down to just SVM and Random Forest.</p>
<p class=""answer""><em>AL:</em> The key success factor was selecting the significant variables, and for this I used a gradient increase as a feature selector. I used SVMs, gradient increases and neural networks to build several models which was subsequently put together to create the final submission.</p>
<p class=""answer""><em>ZM:</em> I created a glmnet model that used sparse matrix representations of each data set. Unfortunately, my approach did not crystalise to a strong solution. So, for the rest of the time, I helped Eu Jin with SVD and PCA when his laptop ran out of RAM!</p>
<p class=""question"">What was your most important insight into the data?</p>
<p class=""answer"">ZM: Alex discovered that a GBM run on a sample of the data could be used to select features and greatly speed up the full model.</p>
<p class=""answer""><em>EJ:</em>I was surprised by Alex's approach wherein he ran a GBM on a sample of the data which he then used to select features. Did not expect that to work well but it did so that was an insight for me.</p>
<p class=""answer""><em>AL:</em> Yea me too! But also the temporal effect of the dataset, which was prevalent in the activity of the molecules.</p>
<p class=""question"">Were you surprised by any of your insights?</p>
<p class=""answer""><em>ZM:</em> Not particularly.  A big portion of this competition was the technical challenge of pre-processing and modeling on large data sets.</p>
<p class=""answer""><em>AL: </em>I was surprised by the coincidence of errors in different parts of the test suite (public error, private error).</p>
<p class=""answer""><em>EJ:</em> I didn't really have any insights, couldn't be more surprised.....</p>
<p class=""question"">Which tools did you use?</p>
<p class=""answer""><em>Together:</em> R.
<em>AL:</em> And open office too.
<em>EJ:</em> And excel too, for graphs.</p>
<p class=""question"">What have you taken away from this competition?</p>
<p class=""answer""><em>EJ:</em> Alot of what we have learnt on the MERCK contest, I will take it to the <a href=""https://www.kaggle.com/c/us-census-challenge"">US Census</a> and the HHP. On a serious note, try everything and don't give up. Every tiny effort you put in will bring you closer to the top.</p>
<p class=""answer""><em>ZM:</em> RAM is cheap and you should have a lot on your prototyping machine!  I personally couldn't afford to keep an m2.4xlarge EC2 instance running for a month or 2...</p>
<p class=""answer""><em>AL:</em> The benefits of multiple approaches.</p>"
Deep Learning How I Did It: Merck 1st place interview,http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/,2012-11-01 18:34:42,"<strong>What was your background prior to entering this <a href=""https://www.kaggle.com/c/MerckActivity"">challenge</a>?</strong>

We are a team of computer science and statistics academics. <a href=""http://www.utstat.toronto.edu/~rsalakhu/"">Ruslan Salakhutdinov</a> and <a href=""http://www.cs.toronto.edu/~hinton/"">Geoff Hinton</a> are professors at the University of Toronto. <a href=""http://www.cs.toronto.edu/~gdahl"">George Dahl</a> and <a href=""http://www.cs.toronto.edu/~ndjaitly"">Navdeep Jaitly</a> are Ph.D. students working with Professor Hinton. <a href=""http://www.math.washington.edu/~cjordan1/"">Christopher ""Gomez"" Jordan-Squire</a> is in the mathematics Ph.D. program at the University of Washington, studying (constrained) optimization applied to statistics and machine learning.

With the exception of Chris, whose research interests are somewhat different, we are highly active researchers in the burgeoning subfield of machine learning known as deep learning, a sub-field revived by Professor Hinton in 2006. George and Navdeep, along with collaborators in academia and industry, brought deep learning techniques to automatic speech recognition. Systems using these techniques are being commercialized by companies around the world, including Microsoft, IBM, and Google.

<strong>What made you decide to enter?</strong>

We wanted to show the Kaggle community the effectiveness of neural networks that use the latest techniques from the academic machine learning community, even when used on problems with relatively scarce data, such as the one from this competition. Neural nets similar to the ones we used have recently demonstrated a lot of success in computer vision, speech recognition, and other application domains.

<strong>What preprocessing and supervised learning methods did you use?</strong>

Since our goal was to demonstrate the power of our models, we did no feature engineering and only minimal preprocessing. The only preprocessing we did was occasionally, for some models, to log-transform each individual input feature/covariate. Whenever possible, we prefer to learn features rather than engineer them. This preference probably gives us a disadvantage relative to other Kaggle competitors who have more practice doing effective feature engineering. In this case, however, it worked out well. We probably should have explored more feature engineering and preprocessing possibilities since they might have given us a better solution.

As far as supervised learning goes, our solution had three essential components: single-task neural networks, multi-task neural networks, and Gaussian process regression. The neural nets typically had multiple hidden layers, used rectified linear hidden units, and used ""<a href=""http://arxiv.org/abs/1207.0580"">dropout</a>"" to prevent overfitting. No random forests were harmed (or used) in the creation of our solution. We used simple, greedy, equally-weighted averaging of these three basic model types. At the very end we began experimenting with gradient boosted decision-tree ensembles to hedge our solution against what we believed other competitors would be using and improve our averages a bit. We didn't have a lot of time to explore these models, but they seemed to make very different predictions from our other models and were thus more useful in our averages than their often weaker individual performances would suggest. For similar reasons, we suspect that averaging our models with the models from other top teams could improve performance quite a bit.

<strong>What was your most important insight into the data?</strong>

Our single most important insight was that the similarity between the fifteen tasks could be exploited well by a neural network using all inputs from all tasks and with an output layer with fifteen different output units. This architecture allows the network to reuse features it has learned in multiple tasks and share statistical strength between tasks. Since we can only assume that Merck is interested in even more than the fifteen molecular targets in the competition data, it should be possible to gain even more benefits from combining more and more targets.

<strong>Were you surprised by any of your insights?</strong>

We were somewhat surprised that using ridge regression for model averaging did not provide any detectable improvement over simple equally-weighted averaging.

<strong>Which tools did you use?</strong>

We used <a href=""http://www.gaussianprocess.org/gpml/code/matlab/doc/"">Matlab code</a> released by Carl Rassmussen and Chris Williams to accompany their Gaussian processes book. For the neural nets we used a lot of our own research code (in python) and wrote some new neural net code specifically for the competition. Our research code is designed to run on GPUs using CUDA. The GPU component uses <a href=""http://www.cs.toronto.edu/~tijmen/gnumpy.html"">Tijmen Tieleman's gnumpy library</a>. Gnumpy runs on top of <a href=""http://code.google.com/p/cudamat/"">Volodymyr Mnih's cudamat library</a>. We also used scikits.learn for a variety of utility functions, our last minute experiments with gradient boosted decision trees, and our ill-fated attempts at more sophisticated model averaging.

<strong>What have you taken away from this competition?</strong>

Our experience has confirmed our opinion that training procedures for deep neural networks have now reached a stage where they can outperform other methods on a variety of tasks, not just speech and vision. In the Netflix competition, the Toronto group publicized their novel use of restricted Boltzmann machines for collaborative filtering, and the winners used this method to create several of the models that were averaged to produce the winning solution. In this competition we decided not to share our neural network methods before the close of the competition, which may have helped us win.<strong id=""internal-source-marker_0.6058537808712572""></strong>"
t-Distributed Stochastic Neighbor Embedding Wins Merck Viz Challenge,http://blog.kaggle.com/2012/11/02/t-distributed-stochastic-neighbor-embedding-wins-merck-viz-challenge/,2012-11-02 19:09:54,"<em>We spoke with the <a href=""https://www.kaggle.com/c/MerckActivity/prospector#186"">Merck Visualization Challenge</a> winner about his technique.  All algorithms and visualizations were produced using Matlab R2011a. Implementations of t-SNE (in Matlab, Python, R, and C) are available from <a href=""http://homepage.tudelft.nl/19j49/t-SNE.html"">the t-SNE website</a>.</em>

<strong>What was your background prior to entering this challenge?</strong>

I am a post-doctoral researcher at Delft University of Technology (The Netherlands), working on various topics in machine learning and computer vision. In particular, I focus on developing new techniques for dimensionality reduction, embedding, structured prediction, regularization, face recognition, and object tracking.

<strong>What made you decide to enter?</strong>

I entered the visualization challenge to test the effectiveness of an embedding technique, called t-Distributed Stochastic Neighbor Embedding (<a href=""http://homepage.tudelft.nl/19j49/t-SNE.html"">t-SNE</a>), that Geoffrey Hinton and I developed a few years ago (building on earlier work by Geoffrey Hinton and Sam Roweis).

<strong>What preprocessing or data munging methods did you use?</strong>

<strong></strong>
The main ingredient of my visualization approach is formed by t-SNE (L.J.P. van der Maaten and G.E. Hinton. Visualizing Data using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008). T-SNE represents each object by a point in a two-dimensional scatter plot, and arranges the points in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points. When you construct such a map using t-SNE, you typically get much better results than when you construct the map using something like principal components analysis or classical multidimensional scaling, because (1) t-SNE mainly focuses on appropriately modeling small pairwise distances, i.e. local structure, in the map and (2) because t-SNE has a way to correct for the enormous difference in volume of a high-dimensional feature space and a two-dimensional map. As a result of these two characteristics, t-SNE generally produces maps that provide much clearer insight into the underlying (cluster) structure of the data than alternative techniques.

To produce the visualizations I submitted to the challenge, I ran t-SNE on the raw data and I plotted the resulting two-dimensional map as a scatter plot, coloring the points according to either their index in the data set or according to their activity value. The first coloring provides insight into how the data distributions changes over time (using index as a surrogate for time), whereas the second coloring provides insight into how well the activity values may be predicted from the raw data. I also constructed the similar plots in which I also included the test data in the t-SNE analysis and colored the test points in a neutral gray color, to obtain insight in the difference between the training and the test distributions.

<strong>What was your most important insight into the data?</strong>

<strong></strong>
One of the key insights that my visualizations give into the data distribution is that it changes enormously over time. When coloring the points according to their index, for many data sets, distinct colored clusters can be identified that suggest the data comprises batches of very different measurements. Maps that include the test data (depicted in a neutral gray color) reveal that the test distribution is very different from the training distribution for many data sets. I confirmed this finding using a simple experiment: I trained logistic regressors to discriminate the training from the test data (as is often done in importance-weighting approaches to covariate shift), and found that these logistic regressor have zero error for almost all data sets. This suggests the support of the training and test distribution are almost completely disjoint.

<strong>Were you surprised by any of your insights?</strong>

<strong></strong>
The enormous difference between the training and test distributions was quite surprising: the difference is so large, that standard importance-weighting techniques for covariate shift will completely fail (because nearly all training points obtain an infinitesimal weight). I am curious to see how the contestants in the prediction challenge have dealt with this problem. I am also interested to know what the underlying phenomenon is that leads to the enormous shift in the data distribution (perhaps such knowledge suggests a preprocessing of the data that would reduce the shift).

Another surprising result was that the individual data sets appear to have quite different structure. This suggests that different data sets may be best modeled by different prediction models.

<strong>What have you taken away from this competition?</strong>

<strong></strong>
Always visualize your data first, before you start to train predictors on the data! Oftentimes, visualizations such as the ones I made provide insight into the data distribution that may help you in determining what types of prediction models to try.

<strong>Brief Bio</strong>

I studied computer science at Maastricht University (The Netherlands), and obtained my Ph.D. from Tilburg University (The Netherlands) in 2009 for a thesis that used machine learning and computer vision techniques to analyze archaeological data. As a Ph.D. student, I became interested in using dimensionality reduction to visualize high-dimensional data and whilst visiting Geoffrey Hinton's lab at University of Toronto, Geoffrey and I developed t-SNE. After being doctored, I became a post-doctoral researcher at University of California San Diego, where I studied new algorithms for structured prediction and online learning, and where I worked on machine-learning applications in software engineering and face analysis. At present, I am a post-doctoral researcher at Delft University of Technology (The Netherlands), where I work on a range of topics including embedding, structured prediction, regularization, face recognition, and object tracking.

I decided the enter the competition to test out the effectiveness of t-SNE on the challenging Merck data sets. Because I do not have any prior knowledge of the underlying process that generated the data, I had no means of ""helping"" t-SNE to produce an appropriate map of the data: visualizing this data really was a ""blind"" test of t-SNE. I am happy to see that t-SNE was effective on the Merck data in that it was helpful in building an intuition for the underlying data distribution.

I like the Kaggle platform a lot because it provides a very fair way to compare different learning approaches. This makes the platform a very valuable addition to the experimental evaluations that are done in machine-learning papers (in such papers, experimental conditions may have been used that favor the approach developed by the authors of those papers; on Kaggle, such subtle ways of ""cheating"" are impossible). A downside of many Kaggle competitions is that the best performance is typically obtained by an ensemble of a large number of blended predictors. This makes it hard for individual machine-learning researchers to be very competitive."
A Bayesian approach to Observing Dark Worlds,http://blog.kaggle.com/2012/12/19/a-bayesian-approach-to-observing-dark-worlds/,2012-12-19 17:43:46,"<em>Cross-posted from <a href=""http://homepages.inf.ed.ac.uk/imurray2/"">Iain Murray's homepage</a>. He's also sharing his <a href="" http://homepages.inf.ed.ac.uk/imurray2/pub/12kaggle_dark/dw_murray_code-1.0.tar.gz"">code</a>.</em>

In December 2012 I entered the <a href=""http://www.kaggle.com/c/DarkWorlds"">Kaggle/Winton Observing Dark Worlds</a> competition. My predictions placed 2nd out of 357 teams. This short note describes my approach.
<h2>Modeling the data, and making predictions</h2>
I took the “obvious” Bayesian approach to this challenge, although didn’t implement it quite as carefully as possible. This style of solution involves three steps: 1) Build a probabilistic model of the data; 2) Infer underlying explanations of the test data; 3) Fit the ‘best’ estimates that minimize the expected error (measured by the metric used on the leaderboard).

<strong>1) The model:</strong> The observed data were positions of galaxies and their measured ellipticities. The ellipticities were clearly Gaussian-distributed. The dark matter halos exert a “force” on the galaxies, locally shifting the mean of the distribution over ellipticities. The example maximum likelihood code, provided by the organizers, gave me the equations to describe how the ellipticities are shifted by a force. That only left me to model the force as a function of displacement from a halo center.

Based on a quick visualization, I picked a force term of <em>m</em>/max(<em>r</em>,<em>r<sub>0</sub></em>), where <em>m</em> is proportional to the halo mass, <em>r</em> is the radial distance from the halo center, and <em>r<sub>0</sub></em> is a core radius inside which the force doesn’t increase. I doubted that this model was quite right, so I also increased the variance of the ellipticities inside the halo core, hoping to increase the robustness of the model.

(I was a little negligent: I should have considered more flexible models of the forces, with more free parameters, and learned whether and how to use them from the data. I didn’t make time to do that, or even test properly whether increasing the within-core variance was a good idea. I’d be more careful in a real collaboration.)

<strong>2) Inference:</strong> Given the model, I applied a standard method to simulate plausible explanations of the test skies. I used Slice Sampling, an easy-to-use Markov chain Monte Carlo (MCMC) method, and didn’t have to tune any tweak parameters. My results on the training skies seemed to be similar if I initialized the dark matter halos at their true locations, or randomly, so I assumed that the sampler was working well enough for the competition. The inference stage was by far the easiest: very little coding or computer time, and no tweaking required.

If you would like to try out slice sampling, you could play with my <a href=""http://homepages.inf.ed.ac.uk/imurray2/teaching/09mlss/"">Octave/Matlab-based MCMC tutorial exercise</a>. The <a href=""http://www.cs.toronto.edu/~radford/slice-aos.abstract.html"">original paper (Neal, 2003)</a> is excellent, or for a shorter introduction, see Chapter 29 of<a href=""http://www.inference.phy.cam.ac.uk/mackay/itila/book.html"">David MacKay's book</a>.

Sampling gives me a bag of samples: a set of plausible dark matter halo positions and masses. If I showed you a movie, stepping through the different plausible halo locations, you would probably understand roughly what we can and can’t know. You might say something like “I can see that there’s definitely a halo in the top right, but the other one could be nearly anywhere”. Unfortunately I’m not able to cram a bag of samples, or your verbal explanation, into the <tt>.csv</tt> submission format defined by the competition rules. There are a plethora of possible ways to make a single guess, including: picking a random sample, using the most probable sample, or using the mean of the samples. But these methods shouldn’t be used without justification.

<strong>3) Making a leaderboard submission:</strong> A sensible way to make a guess, when forced to, is to minimize the expected size of your mistakes. In this competition, the size of a mistake was measured in a complicated way: the “Dark Worlds Metric”. I’d like to say that I minimized the expected value of this metric, but it was hard to deal with. The best prediction for a sky depends on the mistakes made in other skies, and I didn’t know which skies would be in the final test set. Rather than considering possible test set splits (and possibly inferring the split!), I used a pragmatic hack. I optimized the prediction for each sky using the Dark Worlds Metric applied to a fictitious test set containing multiple copies of the same sky with different halo locations given by the MCMC samples. I hoped that this procedure was close enough to the ideal procedure that I would do well.
<h2>Discussion</h2>
Given the setup of this competition, I doubt that it is possible to do much better than the style of approach described here. Tim Salimans, who won, followed <a href=""http://timsalimans.com/observing-dark-worlds/"">a very similar approach</a>. He might have solved the optimization problem better (I should have used gradient-based optimization, but didn’t). Tim fixed his <em>r<sub>0</sub></em> parameters, based on the training data, whereas I left them uncertain. I didn’t work hard enough to notice that these parameters were always close to one of two fixed values (an artifact of the competition if true). Hierarchical learning of the distribution of such tweak parameters is important in real problems, and would be the way to go in future.

If the organizers ran MCMC on the model that they actually used to generate the data, that would find the best reasonable performance: what could be achieved without getting lucky, or fitting to the test data with multiple submissions. I have <a href=""http://www.kaggle.com/c/DarkWorlds/forums/t/3375/more-stable-evaluation"">described in a separate Forum post</a>how it would also be possible for the organizers to tell whether entrants to the competition were just lucky, by testing their expected performance over all reasonable explanations of the test set, rather than just one.

I know from the <a href=""http://www.kaggle.com/c/DarkWorlds/forums"">Forum</a> that some of the competitors near the top of the leaderboard didn't use Bayesian approaches: there are other ways to get good predictions, and my approaches to machine learning are not always Bayesian either. However, in this problem, the Bayesian approach was natural and straightforward. Unlike some methods discussed on the Forum, no careful tuning, special consideration of edge effects, or choosing of grids was required. The result of inference (given enough samples) is a complete description of what is known about the sky. The resulting samples aren’t tied to one evaluation metric, but can be used to find an estimate that will work well under any.

Thanks very much to the organizers: Tom Kitching, David Harvey, Kaggle, and Winton for a fun competition. Congratulations to Tim Salimans for coming first!

<del>I will release my code. Check back later if you want it. Email me if it isn't here by January 2013.</del>

&nbsp;

<a href=""http://homepages.inf.ed.ac.uk/imurray2/pub/12kaggle_dark/dw_murray_code-1.0.tar.gz"">Code is now available here</a>.

<span class=""photo-credit"">Header Image:  Iain's '<a href=""http://homepages.inf.ed.ac.uk/srodnes/CSE/?p=1935"">Research in a Nutshel</a>l' video</span>"
1st Place: Observing Dark Worlds,http://blog.kaggle.com/2012/12/19/1st-place-observing-dark-worlds/,2012-12-19 19:03:28,"<em>Cross-posted from <a href=""http://timsalimans.com/"">Tim Salimans on Data Analysis</a>.  He'll post the Matlab code for his solution sometime later this week</em>

Kaggle recently ran another <a href=""http://www.kaggle.com/c/DarkWorlds"">great competition</a>, which I was very fortunate to win. The goal of this competition: detect clouds of dark matter floating around the universe through their effect on the light emitted by background galaxies.

From the competition website:
<blockquote>There is more to the Universe than meets the eye. Out in the cosmos exists a form of matter that outnumbers the stuff we can see by almost 7 to 1, and we don’t know what it is. What we do know is that it does not emit or absorb light, so we call it <em><strong>Dark Matter</strong></em>. Such a vast amount of aggregated matter does not go unnoticed. In fact we observe that this stuff aggregates and forms massive structures called <strong><em>Dark Matter Halos</em></strong>. Although dark, it warps and bends spacetime such that any light from a background galaxy which passes close to the Dark Matter will have its path altered and changed. This bending causes the galaxy to appear as an ellipse in the sky.</blockquote>
The task is then to use this “bending of light” to estimate where in the sky this dark matter is located.

Although the description makes this sound like a physics problem, it is really one of statistics: given the noisy data (the elliptical galaxies) recover the model and parameters (position and mass of the dark matter) that generated them. After recovering these dark matter halos, their positions could then be uploaded to the Kaggle website where a complicated loss function was used to calculate the accuracy of our estimates.

Bayesian analysis provided the winning recipe for solving this problem:
<ol>
	<li>Construct a prior distribution for the halo positions <img title=""p(x)"" src=""http://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""p(x)"" />, i.e. formulate our expectations about the halo positions before looking at the data.</li>
	<li>Construct a probabilistic model for the data (observed ellipticities of the galaxies) given the positions of the dark matter halos: <img title=""p(e|x)"" src=""http://s0.wp.com/latex.php?latex=p%28e%7Cx%29&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""p(e|x)"" />.</li>
	<li>Use Bayes’ rule to get the posterior distribution of the halo positions: <img title=""p(x|e) = p(e|x)p(x)/p(e)"" src=""http://s0.wp.com/latex.php?latex=p%28x%7Ce%29+%3D+p%28e%7Cx%29p%28x%29%2Fp%28e%29&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""p(x|e) = p(e|x)p(x)/p(e)"" />, i.e. use to the data to guess where the dark matter halos might be.</li>
	<li>Minimize the expected loss with respect to the posterior distribution over the predictions for the halo positions: <img title=""\hat{x} = \arg\min_{\text{prediction}} \mathbb{E}_{p(x|e)} L(\text{prediction},x)"" src=""http://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D+%3D+%5Carg%5Cmin_%7B%5Ctext%7Bprediction%7D%7D+%5Cmathbb%7BE%7D_%7Bp%28x%7Ce%29%7D+L%28%5Ctext%7Bprediction%7D%2Cx%29&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""\hat{x} = \arg\min_{\text{prediction}} \mathbb{E}_{p(x|e)} L(\text{prediction},x)"" />, i.e. tune our predictions to be as good as possible for the given error metric.</li>
</ol>
For step 1. I simply assumed that the dark matter halos were distributed uniformly at random across the sky. Step 2 is more complicated. Fortunately the competition organizers provided us with a set of training skies for which the positions of the dark matter halos was known, as well as a summary of the physics behind it all. After reading through the tutorials and forum posts it became clear that the following model should be reasonable:

<img title=""p(e_{i}|x) = N(\sum_{j = \text{all halos}} d_{i,j}m_{j}f(r_{i,j}), \sigma^{2})"" src=""http://s0.wp.com/latex.php?latex=p%28e_%7Bi%7D%7Cx%29+%3D+N%28%5Csum_%7Bj+%3D+%5Ctext%7Ball+halos%7D%7D+d_%7Bi%2Cj%7Dm_%7Bj%7Df%28r_%7Bi%2Cj%7D%29%2C+%5Csigma%5E%7B2%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""p(e_{i}|x) = N(\sum_{j = \text{all halos}} d_{i,j}m_{j}f(r_{i,j}), \sigma^{2})"" />,

where <img title=""N()"" src=""http://s0.wp.com/latex.php?latex=N%28%29&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""N()"" /> denotes the normal distribution, <img title=""d_{i,j}"" src=""http://s0.wp.com/latex.php?latex=d_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""d_{i,j}"" /> is the <em>tangential direction</em>, i.e. the direction in which halo <img title=""j"" src=""http://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""j"" /> bends the light of galaxy <img title=""i"" src=""http://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""i"" />, <img title=""m_{j}"" src=""http://s0.wp.com/latex.php?latex=m_%7Bj%7D&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""m_{j}"" /> is the mass of halo <img title=""j"" src=""http://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""j"" />, and <img title=""f(r_{i,j})"" src=""http://s0.wp.com/latex.php?latex=f%28r_%7Bi%2Cj%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""f(r_{i,j})"" /> is a decreasing function in the euclidean distance <img title=""r_{i,j}"" src=""http://s0.wp.com/latex.php?latex=r_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""r_{i,j}"" /> between galaxy <img title=""i"" src=""http://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""i"" /> and halo <img title=""j"" src=""http://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""j"" />.

After looking at the data I fixed the variance of the Gaussian distribution <img title=""\sigma^{2}"" src=""http://s0.wp.com/latex.php?latex=%5Csigma%5E%7B2%7D&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""\sigma^{2}"" /> at 0.05. Like most competitors I also noticed that all skies seemed to have a single large halo, and that the other halos were much smaller. For the large halo I assigned the halo mass <img title=""m"" src=""http://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""m"" /> a log-uniform distribution between 40 and 180, and I set <img title=""f(r_{i,j}) = 1/\text{max}(r_{i,j},240)"" src=""http://s0.wp.com/latex.php?latex=f%28r_%7Bi%2Cj%7D%29+%3D+1%2F%5Ctext%7Bmax%7D%28r_%7Bi%2Cj%7D%2C240%29&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""f(r_{i,j}) = 1/\text{max}(r_{i,j},240)"" />. For the small halos I fixed the mass at 20, and I used <img title=""f(r_{i,j}) = 1/\text{max}(r_{i,j},70)"" src=""http://s0.wp.com/latex.php?latex=f%28r_%7Bi%2Cj%7D%29+%3D+1%2F%5Ctext%7Bmax%7D%28r_%7Bi%2Cj%7D%2C70%29&amp;bg=ffffff&amp;fg=000&amp;s=0"" alt=""f(r_{i,j}) = 1/\text{max}(r_{i,j},70)"" />. The resulting model is likely to be overly simplistic but it seems to capture most of the signal that is present in the data. In addition, keeping the model simple protected me against overfitting the data. Note that I assumed that the galaxy positions were independent of the halo positions, although it turns out this may not have been completely accurate.

After completing step 1 and 2, step 3 and 4 are simply a matter of implementation: I choose to use a simple random-walk Metropolis Hastings sampler to approximate the posterior distribution in step 3. The optimization in step 4 was done using standard gradient-based optimization, with random restarts to avoid local minima.

Like I remarked in the competition forums, the outcome of this competition was more noisy than is usual: final prediction accuracy was judged on a set of only 90 cases, with an evaluation metric that is very sensitive to small (angular) perturbations of the predictions. The public leaderboard standings were even more random, being based on only 30 cases. In fact, the 1.05 public score of my winning submission was only about average on the public leaderboard. All of this means I was very lucky indeed to win this competition. Nevertheless, the runner-up seems to have taken a <a href=""http://homepages.inf.ed.ac.uk/imurray2/pub/12kaggle_dark/"">very similar approach</a>, suggesting there is at least something to be said for looking at this kind of problem from a Bayesian perspective.

Finally, I would like to thank the organizers Dave and Tom, and sponsor Winton Capital for organizing a great competition. Looking at a problem different from the standard regression/classification problems was very refreshing.

<span class=""photo-credit"">Photo Credit: <a href=""http://visservices.sdsc.edu/projects/enzo/"">Dark Matter Visualization for LBC Dataset</a>, NPACI Visualization Services - <a href=""http://users.sdsc.edu/~amit/"">Amit Chourasia</a>, <a href=""mailto:cutchin@sdsc.edu"">Steve Cutchin</a>.  San Diego Supercomputer Center, ENZO Early Universe Simulation</span>
<h2></h2>"
5 Lessons Learned from the Event Recommendation Challenge,http://blog.kaggle.com/2013/02/25/5-lessons-learned-for-the-event-recommendation-challenge/,2013-02-25 17:16:32,"<em>Cross-posted from <a href=""http://www.rouli.net/2013/02/five-lessons-from-kaggles-event.html"">rouli.net</a>.</em>

Nothing could have prepared me for the pleasant surprise awaiting me last Thursday's morning. Just a few hours earlier I've submitted my final predictions on <a href=""http://www.kaggle.com/c/event-recommendation-engine-challenge"">Kaggle's Event Recommendation Engine Challenge</a> (in short - the goal is to predict which events will be interesting to a given user). The public leader board was frozen a week earlier, and at the time I was at the 11th place, after falling six places in a couple of days. Moreover, during the board-freeze session, I didn't find any feature or modeling technique that improved significantly my score on my own validation set. I was hoping to be in the top 10%, but woke up to find that I won the third place, reaching my best ranking ever.

I strive to learn from each competition I participate in, and this one is no different. However, my take-aways from this challenge don't involve a new algorithm or feature selection insights. Rather, they are lessons about handling data on Kaggle challenges and in real life :). But first, the boring stuff:
<ul>
	<li>I used pandas, which I really wanted to try out (great, but I'm missing some of R's data exploration methods), scikit-learn and iPython notebook (fantastic!).</li>
	<li>I've treated the challenge as a classification problem, and ranked the events by their predicted probability to be classified as interesting.</li>
	<li>My model is basically an average between a Random Forest and a Gradient Bosting Classifier, with a sprinkle of Naive Bayes on the events' descriptions.</li>
	<li>The top feature for me (as judged by the random forest), was the time between when the user was presented with the event and the event's date, or delta for short. Next come the ratio of invited users that decided not to attend an event (surprisingly, the higher the ratio, the more interesting is the event),  the total number of attendees, and the estimated distance between the user and the event (more on this later).</li>
</ul>
<h3></h3>
<h3><strong>Lesson One: Over Fitting is a Bitch</strong></h3>
As you can see in the graph below, the public score was a very bad predictor for the private (and final) score.
<div></div>
<div><a href=""http://2.bp.blogspot.com/-G_CZZFuHJvk/USfbi7XFatI/AAAAAAAAAeU/mqMiFyZPbss/s1600/Screen+shot+2013-02-22+at+10.55.27+PM.png""><img class=""aligncenter"" alt="""" src=""http://2.bp.blogspot.com/-G_CZZFuHJvk/USfbi7XFatI/AAAAAAAAAeU/mqMiFyZPbss/s320/Screen+shot+2013-02-22+at+10.55.27+PM.png"" width=""320"" height=""296"" border=""0"" /></a></div>
<div></div>
Luckily, I've trusted more my five-fold validation averaged score than the scores I got on the public leader board. But boy, was that frustrating. A few times during the competition I've submitted my predictions after fixing some bug or improving the reliability of some feature just to get a lower public score. Even worst, in the last couple of days before the board freeze, I was unable to improve my public score and was watching new-comers getting far better scores than mine. I was exiled from the warm comfort of the fourth place to the 11th place. It wasn't easy to ignore the public score and not to optimize against it, but luckily I did so.
<div></div>
<div>
<h3></h3>
<h3><strong>Lesson Two: Don't Believe Random Forest's Hype</strong></h3>
</div>
It was my first time successfully employing a random forest classifier as the main predictive mode (usually, logistic regression works better for me), and I believed all the hype about random forests being better at avoiding over fitting. However, the telltale this isn't the case here was observing that adding features to the model sometimes decreased my validation score.

I've combated that behavior by limiting the trees in the random forest to a certain maximum depth and a minimum number of samples at each leaf, and averaging with the GB classifier (which I haven't tweaked).

Knowing what I know now, I would have also dropped some features that I've added to the model just because they seemed relevant and I believed <a href=""http://www.overkillanalytics.net/"">Overkill Analytics</a>' approach of adding as many features as possible to a random forest, and let it sort them out.
<div>
<div>
<h3></h3>
<h3><strong>Lesson Three: There's Always Some Data Leakage</strong></h3>
</div>
</div>
Midway through the competition I've discovered a feature with a very strong predictive power. Turns out that in many cases you could guess whether a user in the train set is interested in a specific event just by looking at the timestamp when he observed that event. If a given user observed several events at timestamp X, and another one at timestamp Y&gt;X (even if those are just a few seconds apart), that other ""later"" event was probably marked as interesting.

Obviously this was some sort of a bug in the train set, and once I've proved that it was exploitable by reaching the third spot, I've (foolishly, since I didn't get any ""finders fee"" :)) alerted Kaggle about it. [<em>Kaggle note:  Much appreciated!!</em>] Fortunately, I got back to third place without such dirty tricks.

However, in the way the train and test sets were assembled, there was some leakage that could (and should) be exploited. First, for each user we had a list of about six events, from which exactly one was marked as interesting. That is, we are given a lot of information in comparison of the classical classification problem. I've exploited that by having a feature that compared the delta of each event displayed to a user against the delta of the earliest event. This proved to have more predictive power than the number of friends a user had in the event.

Moreover, we can assume that startup behind the competition already propose only relevant events to its users. For example, users from Indonesia were rarely presented with events happening in the US. They probably do this by examining IP address which they chose not to share in the train set.

This led to my first attempt and still most successful at creating a ""distance between user and event"" feature; I simply calculated the median of the locations of the events presented to the user, and for each event calculated its distance from that median, as though the median was a substitute to the user's location. This worked better than deriving the user coordinates by looking at events occurring at the same city as the one found in the user's profile, and averaging between them.
<div></div>
<h3></h3>
<h3><strong>Lesson Four: Always (Always!) Use Random Seeds</strong></h3>
Some of my features are derived from graphs, such that a pagerank score for events according to their attendance graph (where events share an edge with a weight that depends on the Jaccard similarity between the users that attended them), or, like many other did, events and users clusters. Sadly, because my hardware isn't powerful enough, I had to prune the graphs and I did so by deleting edges at random.

Big mistake. Now I cannot recreate my final submission, and after all the work I put into it, I may not see the prize money. I was smart enough to set a random seed for my random forest, but too lazy or stupid to that when creating the graph. Gah!
<h3></h3>
<h3><strong>Lesson Five: Things that didn't Work</strong></h3>
I've tried a lot of things during this competition, most of them didn't work, or at least I couldn't prove them to be working. However, they are good things to consider in future competitions:
<div>
<ul>
	<li>I've tried using <a href=""http://en.wikipedia.org/wiki/Elo_rating_system"">ELO rating</a> as an extra event-rating technique over the classification ones. It showed some (a lot!) of promise in the public test set, but failed in my 5 fold validation sets.</li>
	<li>I've tried calculating the ""distance"" between a user and an event by considering a graph with edges between users according to their friendship status, and between users and events according to their attendance. This was a complete failure.</li>
	<li>Naive Bayes (or more precisely, Multinomial Naive Bayes) on the event's descriptions was very disappointing. I then splited the users according to geographies and had a different NB classifier for each, which made this technique only slightly disappointing.</li>
	<li>Page-ranks of events (and even more so, page-ranks of the events organizers) didn't contribute much.</li>
	<li>I've tried filling up missing users and events locations by using an iterative approach; I guessed the location of each user by averaging the coordinates of the events  she attended, and the location of each event by averaging the coordinates of the users who attended it. I did this repeatedly until I failed to discover the locations of any new events or users. This actually made my performance worse.</li>
</ul>
<h3>Epilogue</h3>
</div>
Wow, did you really read this very long blog post? If so, you may be interested in following me on <a href=""http://twitter.com/rouli"">Twitter</a>. Hope you enjoyed it, and feel free to comment, especially if you notice any grammatical or spelling mistakes :)."
Q&A with Xavier Conort,http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/,2013-04-10 22:59:59,"Xavier Conort is currently the <a href=""https://www.kaggle.com/users"" target=""_blank"">number 1</a> ranked Kaggle data scientist and member of team ""Gxav &amp;*"", winners of <a href=""https://www.gequest.com/c/flight"" target=""_blank"">Flight Quest</a>.

<strong>Q: What is your background? What did you study in school, and what has your career path been like?</strong>

<a href=""https://www.kaggle.com/users/17379/xavier-conort"" target=""_blank"">Xavier Conort</a>: I am a French actuary with more than 15 years of working experience in France, Brazil, China, and Singapore. I studied actuarial science and statistics in ENSAE Paris Tech and University Paris Denis Diderot. Before becoming a data science enthusiast, I held different roles in the insurance industry (actuary, CFO, and risk manager).

I currently work in the Data Analytics department of I<sup>2</sup>R (Institute for Infocomm Research, a research institute under the A*STAR family in Singapore) and develop analytics techniques and solutions together with my teammates of the GE Flight Quest. Our department has around 40 data scientists and serves several major clients like Visa and Boeing. We are one of Singapore’s largest R&amp;D teams of data scientists.

My teammates Hong Cao, Hon Nian Chua, Clifton Phua, and Ghim Eng Yap have PhDs in various areas of data analytics. They were all trained in Singapore, except Clifton who was trained in Australia. Recently, Hon Nian completed his post-doc stints in the University of Toronto and Harvard University, and Clifton left our department and joined SAS.

<strong>Q: How long have you been competing on Kaggle?</strong>

I started to compete about 18 months ago but am already considered a veteran.

<strong>Q: What other kinds of challenges have you solved for companies through Kaggle?</strong>

The problems I solved for companies through Kaggle were very diverse. I, with Marcin Pionnier, detected if a car purchased at auction is a good buy or a lemon in “<a href=""https://www.kaggle.com/c/DontGetKicked"" target=""_blank"">Don’t Get Kicked</a>"" (1<sup>st</sup>). I predicted with my teammates from DataRobot biological activities of different molecules given numerical descriptors generated from their chemical structures in the “<a href=""https://www.kaggle.com/c/MerckActivity"" target=""_blank"">Merck Molecular Activity Challenge</a>"" (2<sup>nd</sup>). I forecasted monthly online sales in “<a href=""https://www.kaggle.com/c/online-sales"" target=""_blank"">Online Product Sales</a>"" (2nd).  I modeled the probability that somebody will experience financial distress in “<a href=""https://www.kaggle.com/c/GiveMeSomeCredit"" target=""_blank"">Give Some Credit</a>"" (2nd). I developed scoring engines to support the grading of student written essays in the 2 <a href=""https://www.kaggle.com/c/asap-aes"" target=""_blank"">challenges </a>hosted by the Hewlett Foundation (4th). I predicted customer retention for Allstate in “Will I Stay or Will I Go?"" (4th). And I identified patients diagnosed with Type 2 Diabetes in “<a href=""https://www.kaggle.com/c/pf2012-diabetes"" target=""_blank"">Practice Fusion Diabetes Classification</a>"" (4th).

My teammates for GE Flight Quest have also won academic data mining competitions (outside Kaggle) together with various colleagues from I2R. They placed 1<sup>st</sup> in PAKDD 2012 Churn Prediction, ACML 2012 Fraud Detection in Mobile Advertising, and Opportunity’s 2011 Mobile Activity Recognition Challenge. In addition, they have achieved top-5 positions in many other competitions.

<strong>Q: What do you like best about these competitions? Why do you think they’re successful at solving problems for businesses and other organizations?</strong>

I like the diversity of problems to solve and I enjoy getting live feedback from the public leaderboard. It makes the fight for the best model very concrete.

I believe that the competition framework is a win-win scenario. Competitors get access to real-world data to test their algorithms and their modeling skills. Competition hosts benefit by bringing out the best from us, obtain very strong accuracy benchmarks and get the opportunity to implement innovative solutions coming from different industries.

<strong>Q: What skills do you think are important for a successful data scientist? Did you learn these skills in school, on the job, or on your own?</strong>

I think that what makes a good data scientist is more of the right attitude than skills. Besides a strong background in statistics or computer science, a good data scientist is a person who loves to solve problems. (S)he is not afraid of putting is (possibly) unrecognized hard work because short cuts rarely produce good results from data. And (s)he is open-minded and is excited to learn new things.

I personally discovered machine learning 2 years ago, thanks to Andrew Y. Ng’s Coursera course and Hastie et al’s book titled “The Elements of Statistical Learning,” but learned to really make sense from data when I was working for the insurance industry as an actuary and CFO, and in university when I studied statistics.

My wife (also an actuary) tells me I don't think like a normal person (usually after I've given her a long complicated answer to what she thinks is a 30 second question), but she thinks that's mainly because I'm French.

<strong>Q: Why do you think your algorithm/predictive model was able to improve on aviation industry benchmarks?</strong>

It is certainly due to the fact that many industries work in isolation. Companies like Kaggle, with its large community of data scientists and I<sup>2</sup>R (my current workplace) are changing the game by bringing new solutions for those industries.

<strong>Q: What was your process in developing Flight Quest algorithm/predictive model?</strong>

The algorithms we used are very standard for Kagglers. We used Gradient Boosting Machine and Random Forest, which have proved to work very well in other competitions too.

We spent most of our efforts in feature engineering. Our final feature selection is a collection of flight statistics and attributes, weather information during the flights, traffic in airports and weather conditions at arrival. We were also very careful to discard features likely to expose us to the risk of over-fitting our model.

<strong>Q: Based on the data you were given, what challenges did you encounter when developing your model? Was there anything outside of the data you had to consider?</strong>

Unlike the usual competitions, we did not have standard structured data that we could use to produce a quick first solution. We spent a tremendous time  exploring the numerous datasets, visualizing the data, understanding which data could bring value, and elaborating a strategy to convert this insight in usable features before producing a first model.

<strong>Q: What was the most challenging part of this data quest?</strong>

The timeline of the competition was our biggest challenge. The most critical deadline of the competition was just a few days after Chinese New Year. Chinese New Year is a 4-day period during which you are supposed to spend time with your family, not with data and algorithms!

<strong>Q: What is your definition of a data scientist? What impact will data science and data scientists have on the aviation industry?</strong>

I will consider myself a fully qualified data scientist when I am able to build a one-stop solution that produces high accuracy for very large data sets.

Proliferation of the use of sensor networks and low-cost communications generate large volumes of operational data in the aviation and other industries. This opens up tremendous opportunities for data scientists to contribute in various aspects. Our department is already working with aircraft manufacturers and suppliers to apply data science to the areas of manufacturing equipment health monitoring, fuselage integrity monitoring and engine airflow optimization."
"Q&A With Guocong Song, 3rd Prize, Job Salary Prediction Competition",http://blog.kaggle.com/2013/04/23/qa-with-guocong-song-3rd-prize-job-salary-prediction-competition/,2013-04-23 20:48:02,"<strong>What was your background prior to entering this challenge?</strong>

I had been working on wireless communication and signal processing for over 10 years and was well established. I received the 2010 IEEE Stephen O. Rice Prize (best paper award for communications), and was serving as an editor for IEEE Transaction on Wireless Communications. It was my wife who told me about the Netflix prize two years ago. Since then, I'm more interested in data science. Of course, participating in Kaggle challenges gives me valuable experience.

<strong> What made you decide to enter?</strong>

Ben's benchmark code already established the pipeline that avoids a lot work on data IO. It was extremely attractive to me at that time since I was very exhausted with the GE flight quest. I started working on the problem two weeks before the deadline. Therefore, I would like to thank Ben for his initial work. Technically speaking, most text mining problems belong to classification; I wanted to gain some experience of regression with text mining.

<strong>What preprocessing and supervised learning methods did you use?</strong>

Typical text feature extraction techniques are applied to the raw data, such as text normalization, stop words, n-grams, TF-IDF. I tried ridge regression, SGD, random forests and also converted the regression problem into a classification one, for which I tried native Bayes, SVM, and logistic regression. Finally, I blended the SGD regression and logistic regression based predictor.

<strong>What was your most important insight into the data?</strong>

Since salaries are not distributed smoothly, some models that can explore local properties would outperform linear regression. My background in information theory also helped me discover that 4~5 bits good enough to quantize salary values, which benefits computational complexity reduction.

<strong> Were you surprised by any of your insights?</strong>

No surprise on the score of each submission made a surprise to me. Overfitting didn't bother me with most methodologies I tried. The results are very consistent in cross-validation and two leader boards.

<strong>Which tools did you use?</strong>

Python, scitkit-learn.

<strong>What have you taken away from this competition?</strong>

In this competition, there are no significant features at all. It is not surprising that the first and second winners all use neural networks. More interestingly, my model can be regarded as a neural network with a manually created hidden layer. It does help me understand neural networks / deep learning better.

-------------------------------------------------------------------------------

<strong><a href=""http://blog.kaggle.com/wp-content/uploads/2013/04/guocong-song.jpeg""><img class=""alignright"" alt=""guocong song"" src=""http://blog.kaggle.com/wp-content/uploads/2013/04/guocong-song.jpeg"" width=""168"" height=""168"" /></a></strong>

<em><a href=""https://www.kaggle.com/users/41275/guocong-song"">Guocong Song</a> placed third in the Adzuna Job Salary Prediction competition. He received his PhD in Electrical and Computer Engineering from Georgia Institute of Technology MS, and his BS in Electrical Engineering from Tsinghua University Aside from data science, his expertise is in: Signal processing, stochastic optimization, wireless networks and devices He has received the IEEE Stephen O. Rice Prize Paper Award, and the best paper award in IEEE Transactions on Communications in 2010. He lives in Cupertino, CA.</em>"
Q&A With Job Salary Prediction First Prize Winner Vlad Mnih,http://blog.kaggle.com/2013/05/06/qa-with-job-salary-prediction-first-prize-winner-vlad-mnih/,2013-05-07 05:23:52,"<b>What was your background prior to entering this challenge?</b>

I just completed a PhD in Machine Learning at the University of Toronto, where <a href=""http://www.cs.toronto.edu/~hinton/"">Geoffrey Hinton</a> was my advisor. Most of my work is on applying deep learning techniques to aerial image analysis, so I have a lot of experience in training neural networks with tens of millions of parameters on big datasets.

<b>Why did you enter?</b>

I had a bit more spare time after completing my thesis so I decided to do a quick project before leaving Toronto.  I chose this particular competition because it involved text data and, while that is not something I had a lot of experience with, it seemed like a problem where neural nets should do well (and indeed the 2nd place finisher also used a neural net).

<b>What preprocessing and supervised learning methods did you use?</b>

I did relatively little preprocessing and feature engineering.  I used separate bags of words for the job title, description, and the raw location.  I also found that stemming the words in the title and description using the Porter stemmer and encoding them using tf-idf slightly improved the performance. The other fields, like the category, contract, and source, were represented using a 1-of-K encoding.  The resulting input representation had between 10000 and 15000 features depending on how many of the top words I used.  I did experiment with a number of alternative features and encodings but I did not get any noticeable improvements.

For the supervised learning part, I used deep neural networks implemented on a GPU.  I trained the neural nets by optimizing mean absolute error (the evaluation metric for this contest) using minibatch stochastic (sub)-gradient descent and used dropout in order to help avoid overfitting.  My best single neural network achieved a score of about 3475 on the public leaderboard, but my final submission averaged the predictions of three neural networks to get down to about 3435.  I did not combine neural networks with any other learning methods.

This approach might sound familiar to readers of this blog because my office mates, <a href=""http://www.cs.toronto.edu/~gdahl/"">George Dahl</a> and <a href=""http://www.cs.toronto.edu/~ndjaitly/"">Navdeep Jaitly</a>, and their team mates recently used a nearly identical architecture in their winning entry for the Merck Molecular Activity Challenge, although there are some differences due to the particulars of that contest.

<b>What was your most important insight into the data?</b>

My most important insight was to simply train a powerful and flexible model by directly optimizing the loss function used to determine the winner. Some competitors used complicated ensembles of many disparate models, most of which were not optimizing the correct objective. These people needed to use leaderboard and validation error feedback much more heavily than I did since their model selection process was the only part of their pipeline that directly optimized the evaluation metric.

<b>Were you surprised by any of your insights?</b>

I was somewhat surprised by how little improvement I got from my attempts to engineer better features.  For example, I didn't get any improvement from using bigrams or from adding information derived from the normalized location or location tree.  Since other competitors have reported noticeable gains in performance from using these features on the competition forum, I suspect that the deep nets I trained were able to learn some of these features automatically.  While this is definitely a pleasing result, it is a little surprising even to neural network experts because neural nets are generally considered to be quite sensitive to the input representation.

<b>Which tools did you use?</b>

I used Python along with a number of open-source Python packages.  I used pandas for loading and exploring the data and scikit-learn for its feature extraction pipeline, although I ended up implementing my own text vectorizers for improved memory efficiency.  I also used NLTK for its implementation of the

Porter stemmer.  Finally, I used my own implementation of deep neural networks which relies on <a href=""http://www.cs.toronto.edu/~tijmen/"">Tijmen Tieleman's</a> <a href=""http://www.cs.toronto.edu/~tijmen/gnumpy.html"">gnumpy</a> library and my own <a href=""https://code.google.com/p/cudamat/"">cudamat</a> library for GPU support.

<b>What have you taken away from this competition?</b>

I learned quite a bit about how feature engineering interacts with different neural network architectures.  In particular, I thought it was really interesting that <a href=""http://www.kaggle.com/users/23631/vlado-boza"">Vlado Boza</a> placed 2nd with a completely different neural network architecture and set of features.

<em><a href=""https://www.kaggle.com/users/5983/lazylearner"">Vlad Mnih</a> is a machine learning researcher based in London, England.  He holds a PhD in Machine Learning from the University of Toronto and an MSc in Machine Learning from the University of Alberta.</em>"
"Q&A with Vlado Boza, 2nd Place Winner, Job Salary Prediction Competition",http://blog.kaggle.com/2013/04/29/qa-with-vlado-boza-2nd-place-winner-job-salary-prediction-competition/,2013-04-29 16:29:50,"<strong>What was your background prior to entering this challenge?</strong>

I am finishing my Master’s degree in computer science. I was a software engineering intern at Google working on some machine learning problems. I've also entered several Kaggle competitions during the last year. I am the founder of Black Swan Rational - a Slovak company specialized in predictive analytics.

<strong>What made you decide to enter?</strong>

I had some spare time, so I decided it to spend it on some Kaggle competition. At that time there were three competitions running: Job Salary Prediction, Blue Book for Bulldozers, and Whale Detection. Whale Detection already had quite impressive submissions and I didn't want to spent time just by tweaking a model to get a 0.001 % difference. With Blue  Book, I thought that there would be no significant difference between the random forest benchmark and the best submission and it would end up as a big ensemble fight. The Job salary data seemed to be pretty clean and easy to work with. And also there were a lots of possible approaches.

<strong>What preprocessing and supervised learning methods did you use?</strong>

I extracted simple binary text features from title and description and also used categorical features for location, company, and source. My whole model was just an old-school neural network with two small hidden layers trained by back propagation. Before that I used nearest neighbor model which was quite successful (got error around 4200).

<strong>What was your most important insight into the data?</strong>

During one point I found out that there are too many similar ads and that their salary differs on average by 2000. I used this in my nearest neighbor model. But neural network could handle this even better without any hacks.

<strong>Were you surprised by any of your insights?</strong>

Ad similarity was the only thing.

<strong>Which tools did you use?</strong>

I have coded all of my algorithms in C++ (I did small preprocessing in Python). I tried to use scikit-learn but it didn't lead to any big success.

<strong>What have you taken away from this competition?</strong>

I have to improve my coding practices. I've made many stupid bugs just because of this. And I also should start to use some versioning system better than “do backup sometimes”.

&nbsp;

------------------------------------------------------------------------------

<img class=""alignright"" alt=""e02980bdb58baa615c3c7f0d8952b9b9"" src=""http://blog.kaggle.com/wp-content/uploads/2013/04/e02980bdb58baa615c3c7f0d8952b9b9-240x198.jpeg"" width=""192"" height=""158"" />

<b></b><em><a href=""https://www.kaggle.com/users/23631/vlado-boza"">Vlado Boza</a> won Second Prize in the Adzuna Job Salary Prediction Competition. He is finishing his Master's studies of computer science at Comenius University in Bratislava. He spent two summers as Software engineering intern at Google working on machine learning problems. His interests include building fast and effective algorithms, hard optimization problems and machine learning.</em>"
Summary of the Whale Detection Competition,http://blog.kaggle.com/2013/05/06/summary-of-the-whale-detection-competition/,2013-05-07 05:16:14,"Posting a summary on behalf of Cornell researchers. From my side I would like to add, that Marinexplore has partnered with Cornell University to develop acoustics related capabilities of our spatio-temporal data platform. Improved analytics of acoustic data is relevant not only to shipping industry, but also to other businesses like offshore industry. Globally there are many public acoustic datasets yet to be integrated with <a href=""http://marinexplore.org/"" target=""_blank"" rel=""nofollow"">marinexplore.org</a> as well.

Thank you everyone for participating in our challenge and pushing the boundaries together. Feel free to contact me directly should you want to use our solutions in your organization, explore collaboration options, join our team or just learn more about Marinexplore.

Meanwhile we posted a summary of the competition in <a href=""http://marinexplore.org/blog/the-kaggle-challenge-improves-cornells-whale-detection-model-to-98/"" target=""_blank"" rel=""nofollow"">our blog</a> and launched an exploratory data challenge for finding the best use of public ocean data with a <a href=""http://marinexplore.org/blog/launching-the-earth-day-data-challenge-adding-7-new-data-sources/"" target=""_blank"" rel=""nofollow"">prize of $3000</a>.

<em>André Karpištšenko</em>
<em>Co-founder at Marinexplore, Chief Scientist</em>
<em>andre@marinexplore.com
skype:andre</em>

<hr />

The Bioacoustic research program (BRP) at Cornell University has had the honor to co-host with Marinexplore the first ever North Atlantic right whale call-classification competition. Thank you all for contributing your time and never-ending brainstorms, and for making the competition exciting, interesting, intellectually rewarding and totally successful.

We received the documents and source codes from the top two winning Kaggle participants. Many participants also kindly share their insightful thoughts and even source codes on the competition’s message board. We are currently building a new automated right whale detection-classification system, which will include the algorithms from the Kaggle competition and will apply it to a 44-month, continuous recording dataset. We expect that this system will yield a greater understanding of right whale calling behavior, such as their daily &amp; seasonal communication patterns, as well a deeper understanding of the influences of human noise on the whales’ acoustic communication and habitat. You, the participants in this competition, have been and still are the most important partners in our efforts to save right whales.

<strong>Methods</strong>

Both winners used an approach that defines a frequency-time “tight box” bounding the occurrence of the right whale call in a spectrogram, followed by extraction of a customized set of features for each tight box. The 1st place winning team used a multiple template matching approach, while the 2nd place winning team used a Viterbi algorithm to find the exact trajectories of frequency up-sweeps. The tight boxes make the features more consistent and robust and thus more frequency-invariant and/or time-invariant.

Both winning methods also designed several feature vectors from different perspectives to incorporate information from either the spectrum, the temporal dynamics of a call’s frequency-modulation, and even the temporal ordering of labeling (positive or negative). The last variable, temporal ordering, emerged from the ordering and numbering of the files and labels identifying the calls in the dataset. As a result, many positive classification events appear consecutively. This temporal clustering feature in this dataset might not be something reliable that we could use in our updated automated detection system. However, this feature could be useful to discriminate between right whale up-calls, which almost always occur as individual transients, and humpback whale frequency-modulated upsweeps, which are either notes within a song or produced as a series of calls.

Many participants applied a deep learning approach (in particular, a convolutional network) and achieved high scores (e.g. contestants ranked #3, #4, and #6). In our understanding of their deep learning approach, the spectrogram of a right whale call is treated as an image in much the same way as a handwritten digit.

Many contestants used Python as the preferred programming language, reflecting the fact that modules of Python, such as Sci-Kit-learn, Sci-py, Num-py, have become standards in the world of data analysis. Accordingly, several classifiers, for example gradient boosting and random forest, were preferred over others by the participants.

<strong>Data integrity</strong>

Several participants expressed concerns about data integrity. To some participants some of the audio clips tagged as right whale up-calls did not sound like an up-call, and vice versa. The following are two additional results we need to keep in mind for the particular dataset used in this competition:

(i) Some audio clips had very low signal-to-noise ratio (SNR).

(ii) An audio clip tagged as a right whale up-call might actually be a non-biological sound or a sound from a different species.

When both (i) and (ii) occur simultaneously, things can get tricky. The energy from a right whale call might be much lower than the energy from the other sound object in the sound sample. On the other hand, some audio clips tagged as “no-call” sounded like and could appear similar to an up-call in a spectrogram. One possible explanation for this conundrum is that humpback whales, which are renown for their vocal virtuosity, are responsible for these confounding calls. However when humpbacks produce up-call like sounds, they typically produce them in a repetitive sequence. Thus, if a longer acoustic sample had been provided, instead of just the 2-sec clip, discrimination between a single call occurrence (i.e. a right whale up-call) and a sequence (i.e., a humpback song note or call sequence) might have been more obvious, thereby improving correct classification of the sound.

<strong>Future</strong>

We are going to apply the top two winning methods, along with other methods developed in the Bioacoustic Research Program, to improve our abilities to automatically detect and classify right whale calls. The suite of new methods will also include deep learning and computer-vision-based techniques. All of these methods will be a core part of our new, automated acoustic detection-classification system for large-scale analysis for endangered species, including whales, elephants and birds. One of the first technical challenges is to have the automatic detection-classification process operate on a continuous, long-duration audio stream (e.g. months to years). We’re investigating methods from computer vision and image processing that will locate connected regions, as well as an efficient method for applying a sliding window, by which classification is repeatedly applied along a continuous audio stream. Presently a comprehensive performance evaluation is ongoing using an 8-day dataset. One goal in the next few months is to apply methods from this competition on a 44-month, continuous underwater sound recording. Another very important goal is to use the source code that you all have produced to improve automatic detection-classification systems that listen for whales in order reduce the chances of whales being killed by ships (e.g. right whales in the shipping lanes off Boston, USA, <a href=""http://www.listenforwhales.com/"" target=""_blank"" rel=""nofollow"">www.listenforwhales.com</a>).

It is very obvious from the energy and productivity of the participants in this competition that this was not just about prize money. It was about how a group of smart, motivated people, who were strangers, could work as a group of competitive altruists, to produce software that will have a real benefit for the natural world and the ocean environment, and especially for improving the chances of survival for a species that is near extinction. A huge, huge thank you to all the participants of this excellent competition.

And a huge, huge thank to Kaggle and Marinexplore for enabling this to become reality."
Q&A With Amazon Access Challenge First Prize Winner Paul Duan,http://blog.kaggle.com/2013/08/29/qa-with-amazon-access-challenge-first-prize-winner-paul-duan/,2013-08-29 16:45:04,"<p dir=""ltr"">We caught up with the winner of the immensely popular Amazon Access Challenge to see how he edged out thousands of competitors to predict which employees should have access to which resources.</p>
<p class=""question"" dir=""ltr"">What was your background prior to entering this challenge? What did you study in school, and what has your career path been like?</p>
<p dir=""ltr"">My background is a bit eclectic; I spent my time in undergrad multitasking between three universities (UC Berkeley, Sciences Po Paris, and the Sorbonne), where I studied mathematics, econom(etr)ics, and social sciences. I’ve since been self-learning machine learning and programming on the job -- the engineers I work with would tell you that I still have a long way to go regarding the latter.</p>
<p dir=""ltr"">I recently moved from France to San Francisco. I currently work as a Data Scientist at Eventbrite, where I am in charge of building the fraud and spam detection models. I also teach data science on occasion, most recently at Zipfian Academy.</p>
<p class=""question"" dir=""ltr"">Why did you enter?</p>
<p dir=""ltr"">First, I was curious to see how well I could place using the knowledge I had recently acquired. Given of the huge number of participants in the Amazon challenge, this was the ideal competition to enter.</p>
<p dir=""ltr"">Second, the fact you’re studying the same dataset as many other people makes for great dialogue opportunities. This is why I tried sharing as much as possible during the competition, and the response has been very inspiring. People were starting interesting discussions left and right, notably <a href=""http://www.kaggle.com/users/66023"" target=""_blank"">Miroslaw Horbal</a> who sparked a great exchange about feature selection.</p>
<p class=""question"" dir=""ltr"">What preprocessing and supervised learning methods did you use?</p>
<p dir=""ltr"">I used an ensemble of linear and tree-based models that were each trained on a slightly different feature set. The features themselves were extracted by cross-tabulating each categorical variable so as to get an idea of how rare each combination is -- because most requests end up being approved and because the number of different categories made up for a lot of noise, I chose to treat the problem more as an outlier detection problem and I created my features as such.</p>
<p dir=""ltr"">The models were then combined by using their output as an input for a modified linear regression. This second-stage model also incorporated the size of the support for each category as meta features in order to dynamically decide which base model to trust the most in different situations. I ultimately teamed up with <a href=""http://www.kaggle.com/users/73603"" target=""_blank"">Benjamin Solecki</a>, who used a very similar method with slightly different features, which further improved our score when incorporated into the ensemble.</p>
<p dir=""ltr"">You can find the code <a href=""https://github.com/pyduan/amazonaccess"" target=""_blank"">on Github</a> and a more detailed explanation of the methodology on <a href=""http://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/5283/winning-solution-code-and-methodology/28620#post28620"" target=""_blank"">the forums</a>.</p>
<p class=""question"" dir=""ltr"">What was your most important insight into the data?</p>
<p dir=""ltr"">Not spending too much time in feature selection vs. feature engineering.  Because there was a lot of noise in the dataset and the variance seemed to be high depending on how I would split my train/cross-validation sets, I focused mostly on improving the generalization power of my algorithms by creating classifiers with different strengths.</p>
<p class=""question"" dir=""ltr"">Were you surprised by any of your insights?</p>
<p dir=""ltr"">I noticed that fine tuning (both in terms of feature selection and hyperparameter optimization) didn’t seem as critical in the context of ensembles of different classifiers. In fact, I would sometimes notice that changes that improved the performance of each of my individual models would actually decrease the performance of the overall ensemble!</p>
<p class=""question"" dir=""ltr"">Which tools did you use?</p>
<p dir=""ltr"">I used mostly Python with the scikit-learn library, with a mix of pandas and R for data exploration.</p>
<p class=""question"" dir=""ltr"">What have you taken away from this competition?</p>
<p dir=""ltr"">I learned quite a bit about how to find a balance between feature engineering and feature selection, both in the context of single models and more complex ensembles. The fact the data only consisted of categorical variable was an added challenge as well.</p>
<p dir=""ltr"">I think what is great about Kaggle competitions is the fact they are self-contained: you have a well-defined problem, a well-defined dataset, and a clear evaluation metric. As such, they are an ideal testing ground for new ideas and algorithms. In real life, things are unfortunately not as easy. You often end up having to figure out how you want to build your model, what data you want to use (and how to get it), and the optimization objectives all at once -- sometimes for problems that don’t even exist yet.</p>"
"Q&A with José Guerrero, 1st Place Winner, See Click Predict Fix Competition",http://blog.kaggle.com/2013/12/23/qa-with-jose-guerrero-1st-place-winner-see-click-predict-fix-competition/,2013-12-23 20:10:22,"<h3>What was your background prior to entering this challenge?</h3>

I have a BSc + MSc in Mathematics, Statistic and Operations Research and postgraduate in Scientific Programming. I'm a health data analyst in a university hospital. I discovered Kaggle when Heritage Health Prize was launched and since then I've participated in several challenges.

<h3>What made you decide to enter?</h3>

I couldn't participate in the hackathon, so I think this was like a second opportunity. The problem was very attractive and I thought the idea of citizen participation for detect community issues was very interesting.

<h3>What preprocessing and supervised learning methods did you use?</h3>

After reading the hackathon forum and seeing how the better results were obtained with a short training data set with most recent observations, I decided as a priority to use as much data as possible for giving the model enough robustness.

The main problem was dealing with the time anomalies, so I forced the models to learn without using absolute time features (day the issue was sent).

The hypotheses were:
<ul>
	<li>The response to an issue depends (directly or inversely) of number of recent issues and similar issues (time dimension).</li>
	<li>The response to an issue depends (directly or inversely) of number of issues and similar issues reported close (geographic dimension).</li>
	<li>There are geographic zones more sensitive to some issues (geographic dimension).</li>
</ul>
With that in mind I defined three time windows -- short, middle and long -- and three epsilon parameters for using them in a radial basis distance-weighted average for each issue.

The selection of these values were for adjusting the decay shape in a way the weights represent city, district and neighbour ambits.

For each issue I computed a 3x3 grid of features for each tag group, using radial basis weights respects the distance in kms between issues.

For a period of last 150 days and for each issue, I computed the LOO (Leave One Out) weighted radial basis average for comments, votes and views for city, district and neighbour parameters.

For summary feature I created a binary bag of more frequents words.

I fitted several models: boosted trees, random forest and general linear models and ensembled them with a ridge regression to calibrate the estimations.

<h3>What was your most important insight into the data?</h3>

The source of the issues had a great importance in the responses, and in each city the features' range of values and variability were very different. This explains why using stratified models worked so well. The preprocessing of the data was crucial for fitting models for all the cities at a time.

<h3>Were you surprised by any of your insights?</h3>

In my models, bag of words of summary had a small influence in predictions. I think probably a binary bag of words is too simple and using high order tuples would be necessary.

The models trained with the grid of features got extract the time and geographical information without using an absolute time feature nor longitude and latitude.

<h3>Which tools did you use?</h3>

R packages gbm, randomForest and glmnet.

<h3>What have you taken away from this competition?</h3>

I'm surprised how well the big column approach (train the responses all together and stacking the dataset one time for each one of them) works in this case.

And with this competition I got the #1 spot in Kaggle rankings. I'll never forget that!

------------------------------------------------------------------------------

<a href=""http://blog.kaggle.com/wp-content/uploads/2013/12/jos-a-guerrero.png""><img class=""alignright"" alt=""José A. Guerrero"" src=""http://blog.kaggle.com/wp-content/uploads/2013/12/jos-a-guerrero.png"" width=""192"" height=""192"" /></a>

<em><span style=""font-size: medium;""><a title=""Jose Guerrero"" href=""http://www.kaggle.com/users/5642/jos-a-guerrero"">José Guerrero</a> won First Place in the <a title=""See Click Predict Fix"" href=""http://www.kaggle.com/c/see-click-predict-fix"">See Click Predict Fix</a> Competition. He has worked more than 25 years in the health sector in Spain in epidemiology, research, electronic medical records, and senior management at a university hospital.  He is currently crunching big databases at the region's main hospital.</span></em>"
"Q&A with James Petterson, 3rd Place Winner, See Click Predict Fix Competition",http://blog.kaggle.com/2014/01/07/qa-with-james-petterson-3rd-place-winner-see-click-predict-fix-competition/,2014-01-07 21:52:18,"<h3>What was your background prior to entering this challenge?</h3>
I studied Electrical Engineering during undergraduate school, and worked as a software engineer in the telecom industry for several years. Later on I moved to Australia to pursue a PhD in Machine Learning at <a href=""http://www.anu.edu.au/"">ANU</a>/<a href=""http://www.nicta.com.au"">NICTA</a>, which I finished a couple of years ago. I'm currently working as a Data Scientist at <a href=""http://www.commbank.com.au/"">Commonwealth Bank</a>.
<h3>What made you decide to enter?</h3>
I'm currently refraining from participating in long competitions, given how time consuming they can be, but since I had good results in the <span style=""color: #00ccff;""><a href=""http://www.kaggle.com/c/the-seeclickfix-311-challenge/leaderboard""><span style=""color: #00ccff;"">See Click Predict Fix - Hackathon</span></a></span> I thought it would be worth trying this one.
<h3>What preprocessing and supervised learning methods did you use?</h3>
I combined several methods in an ensemble, the main ones being boosting trees (GBM) and linear regression. I also tried random forests and neural networks, but I didn't invest much time in them.
<h3>What was your most important insight into the data?</h3>
Probably the most important insight was the fact that the distributions of the features and the labels were highly dependant on physical location and time.

The first aspect is easy to model, either by building separate models for each city, or by using an algorithm that can capture feature interactions.

The temporal aspect, however, is harder to deal with. Assuming the conditional distribution of the labels given the features is the same across all periods, we can apply covariate shift corrections. I tried a few methods, such as Kernel Mean Matching, but the performance gains were negligible.

What did work was applying a simple constant scaling to the predictions (one for each city and each target variable). It only worked well, however, because I used feedback from the leaderboard to adjust these constants. This is not ideal, as we won't be able to do that in a real life situation, but given the way the competition was designed, it is unlikely that it would be possible for anyone to win without resorting to this kind of adjustment.
<h3>Were you surprised by any of your insights?</h3>
I was surprised that I couldn't extract much information from the description texts, as I thought that would be one of the richest sources of information.
<h3>Which tools did you use?</h3>
Most of the work was done in R. For linear models I used <a href=""https://github.com/JohnLangford/vowpal_wabbit/wiki"">vowpal wabbit</a>, and to compute vector representations of the description texts I tried <a href=""https://code.google.com/p/word2vec/"">word2vec</a>.
<h3>What have you taken away from this competition?</h3>
It reminded me once again of the power of building several models together. That was the winner's <em>big column</em> approach (described <a href=""http://www.kaggle.com/c/see-click-predict-fix/forums/t/6464/congratulations-to-the-winners/35506#post35506"">here</a>), where all three target variables (comments, views and votes) where trained in a single model."
"Q&A with Bryan & Miroslaw, 2nd Place in the See Click Predict Fix Competition",http://blog.kaggle.com/2014/01/07/qa-with-bryan-miroslaw-2nd-place-in-the-see-click-predict-fix-competition/,2014-01-08 01:17:37,"<h3>What was your background prior to entering this challenge?</h3>
My professional background is in business intelligence and analytics/reporting and Miroslaw’s background is in mathematics, so neither of us has a formal background in machine learning. However, we have both taken multiple online classes in machine learning topics, including Andrew Ng’s excellent StanfordX Machine Learning course.
We also have both competed in quite a few Kaggle competitions in the past year, steadily improving our skills and knowledge with each finish. Kaggle and its community have proven to be a goldmine for anyone interested in learning real-world machine learning techniques outside of academia.
<h3>What made you decide to enter?</h3>
Miroslaw and I both competed independently in the first portion of the SeeClickFix competition, a 24-hour Hackathon, and we performed well. We really enjoyed the dataset and the objective, so competing in the second portion of the contest seemed a natural progression for us both.

In the second portion of the contest, we both competed independently until about two weeks before the contest completion. At that time, we had both been steadily climbing the leaderboard with myself hitting 1st place on Nov. 16th and Miroslaw right behind me at 4th place. However, J.A. Guerrero had just joined the contest, and he and other top competitors were making steady gains on the leaderboard as well. At that time we decided it was best to hedge our bets and improve our odds of placing in the money by combining our models into a powerful ensemble.
<h3>What preprocessing and supervised learning methods did you use?</h3>
My approach was composed of a segmentation ensemble. This consisted of a distinct base model trained independently on each city and with remote API sourced issues trained together separately as well -- for a total of five segments. Because the variables for each of the cities seemed quite distinct (a lot of variable interaction present), this data seemed ideally suited to a segmentation ensemble. One advantage of segmentation is that it greatly reduces the number of samples and the dimensionality that each base model must deal with, so I was able to utilize gradient boosted regressors (GBMs) for most of the base models.

Miroslaw’s approach was more focused on the text analysis side of the data. Using text from the issue summary and description fields, he created a tri-gram TFIDF vector from the entire training set, then combined it with other features that we had engineered. Because of the high dimensionality, only linear models were practical for this approach, and of those he found that ridge regression performed best.

After we teamed up, our first week was spent strengthening our individual models by integrating some of the stronger features from each other’s code into our own and measuring the improvement, based on a combination of cross-validation score and leaderboard feedback. Our intuition was that while we did want to keep our models distinct to reduce bias within our ensemble, if a feature was proven to improve both cross-validation and leaderboard scores significantly, than its signal was powerful enough to justify inclusion in both base models.

Then our final week was spent determining the ideal weights for averaging our models’ predictions together. A simple 50/50 blend made a surprisingly large gain on the leaderboard, but we then went more in-depth and performed segment based weighting for each of the models. For this we used a linear regression model to derive weights based on optimal cross-validation scores for each segment. Not surprisingly, the findings were that in some segments and targets Miroslaw's model performed better and needed to be weighted higher, while on others mine performed better. Lastly, to avoid overfitting the cross-validation test set we reduced weights to be less extreme if the linear model weighted either of our individual models too strongly.

The features used in our models were largely similar with other top competitors. We found that some of the best signals in the data came from description length, geographic location, issue tag type, issue source, summary/description text, and whether the issue was created on a weekend. From these, we created various binary and one hot encoded features, and we also used a reverse-geocoding service to derive zipcodes and neighborhoods from the issue’s latitude/longitude fields given in the data. This ended up paying off for us as neighborhoods in particular ended up being powerful predictors, more powerful than simply using latitude and longitude to approximate location.
<h3>What was your most important insight into the data?</h3>
Because this contest was temporal in nature, using time-series models to make future predictions, most competitors quickly realized that proper calibration of predictions was a major factor in reducing error. Even during the initial Hackathon stage of the contest, it became well known on the competition forum that one needed to apply scalars to predictions in order to optimize leaderboard scores.

But while scaling was common knowledge, our most important insight came in applying our segmentation approach to the scalars. For example, rather than apply one optimized scalar to all predicted views for the entire test set, we applied optimized scalars for each distinct segment of the test set (the remote API sourced issues and the four cities). We then optimized the scalars using a combination of leaderboard feedback and cross-validation scores. What we found was that each segment responded differently to scaling-- so trying to apply one scalar to all issues, as many of our competitors were doing, was not optimal.
<h3>Were you surprised by any of your insights?</h3>
By far our biggest surprise was the effectiveness of creating an ensemble model from our individual models. Initially, before deciding to team up, we were concerned that we may see minimal gain from combining our predictions, but we quickly realized that this was not the case. Even a simple average of the two gave a huge decrease in error.

The important lesson for us was that two distinctly developed models will yield huge gains when combined together, particularly when the two have a high degree of diversity (variance in errors) which is critical for an ensemble to perform well. We were fortunate that while our models shared many of the same features, Miroslaw’s had been designed to take more advantage of the text-based features and mine had been designed more to take advantage of the segments in the data.
<h3>Which tools did you use?</h3>
We both used the same stack of Python tools: scikit-learn, PANDAS, and NumPy. This ended up being a fortunate coincidence as it made it easy for us to share code snippets with each other.
<h3>What have you taken away from this competition?</h3>
First, it was a great experience collaborating together as a team. This was our first experience working on a team in a Kaggle contest, and we both agreed that it will not be the last. Working together allowed us to see the same problem from new angles and we learned many new techniques in just the two weeks we worked together. It was enlightening seeing the many creative solutions and insights that we each had developed. Our only regret is not teaming up earlier in the contest.

Second, we both learned the power of ensembles. Prior to this contest, neither of us had utilized higher level ensembles in previous competitions, always instead focusing on improving one strong model. No longer. Going forward, ensembles will be an important and often used tool in our toolkit. In fact, we were so motivated by our results that Miroslaw and I are developing a Python module to help facilitate the creation and use of ensembles.

Lastly, we thoroughly enjoyed participating in this contest, and we greatly appreciate <a href=""http://seeclickfix.com/"" target=""_blank"">SeeClickFix.com</a>, David Eaves, and Kaggle for graciously providing us the opportunity to work with their data. Working with this data set was a blast and we had a great time learning from it and gaining new insight.

------------------------------------------------------------------------------

<a href=""http://blog.kaggle.com/wp-content/uploads/2014/01/bryan-miroslaw.jpg""><img class=""alignright size-full wp-image-4619"" alt=""Bryan Gregory and Miroslaw Horbel"" src=""http://blog.kaggle.com/wp-content/uploads/2014/01/bryan-miroslaw.jpg"" width=""261"" height=""120"" /></a>

<em><span style=""font-size: medium;""><a title=""Bryan Gregory"" href=""http://www.kaggle.com/users/100725/bryan-gregory"">Bryan Gregory</a> holds a B.S. in Information Systems from Texas A&amp;M University and an M.B.A. with a concentration in Information Technology from Baylor University. He is a frequent Kaggler with an interest in the practical application of machine learning and business intelligence.</span></em>

<em><span style=""font-size: medium;""><a title=""Miroslaw Horbal"" href=""http://www.kaggle.com/users/66023/miroslaw-horbal"">Miroslaw Horbal</a> has a B.S. in Mathematics with a specialization in Combinatorics and Optimization from the University of Waterloo. He is a self-taught machine learning enthusiast with an addiction to data science competitions.</span></em>"
Yesterday a kaggler... Dogs vs Cats,http://blog.kaggle.com/2014/02/05/yesterday-a-kaggler-dogs-vs-cats/,2014-02-05 10:10:35,"We're seeing the <a href=""https://www.kaggle.com/c/dogs-vs-cats/forums/t/6984/thanks-and-congrats"" target=""_blank"">discussion of many various models</a> emerge on the <a href=""https://www.kaggle.com/c/dogs-vs-cats/forums"" target=""_blank"">forum</a> of the recently closed Dogs vs Cats competition from the Kaggle Playground.... but this blog post from the 8th place team <strong>fastml.com</strong> is worth a repost on No Free Hunch:
<blockquote>
<h3><a style=""text-decoration: none;"" href=""http://fastml.com/yesterday-a-kaggler-today-a-kaggle-master-a-wrap-up-of-the-cats-and-dogs-competition/""><strong>Yesterday a kaggler, today a Kaggle master: a wrap-up of the cats and dogs competition</strong></a></h3>
<span style=""color: #999999;"">Out of 215 contestants, we placed 8th in the Cats and Dogs competition at Kaggle. The top ten finish gave us the master badge. The competition was about discerning the animals in images and here’s how we did it.</span></blockquote>"
Winning the Personalized Web Search Challenge: team Dataiku Data Science Studio,http://blog.kaggle.com/2014/02/06/winning-personalized-web-search-team-dataiku/,2014-02-06 19:05:50,"<h3>What was your background prior to entering this challenge?</h3>
We're a team of four. <strong><a href=""http://www.kaggle.com/users/45978/christophe-bourguignat"" target=""_blank"">Christophe Bourguignat</a></strong> is a telecommunication engineer during the day, but he becomes a serial Kaggler at night, <strong><a href=""http://www.kaggle.com/users/111271/kenji-lef-vre"" target=""_blank"">Kenji Lefèvre</a></strong> has a PhD in Mathematics and his background shows dangerous similarities with that of Baron Münchhausen. Finally, <strong><a href=""http://www.kaggle.com/users/70112/matt-sco"" target=""_blank"">Matthieu Scordia</a></strong> and I, <strong><a href=""http://www.kaggle.com/users/137030/paul-masurel"" target=""_blank"">Paul Masurel</a></strong>, are normal, healthy, happy, model employees of Dataiku (<a href=""http://www.dataiku.com"" target=""_blank"">www.dataiku.com</a>), respectively as Data scientist and Software Engineer. We all share a great interest in data science.
<h3>What made you decide to enter?</h3>
At Dataiku, we're building the perfect platform for Data Science. Florian (our CEO) saw in this competition an opportunity to test whether our product would make it possible for four data scientists to work together efficiently on a complex project, so he asked me to lead a team to compete in this challenge. (By the way, Dataiku would like to sponsor other teams on future Kaggle challenges, and provide them with the Studio and adapted computing power. If you’re interested, please contact us.)

On the other hand, Christophe had already developed a plain addiction to Kaggle and data science before knowing us. Finally Kenji saw this challenge as an accelerated introduction to a field that was brand new to him.
<h3>What preprocessing and supervised learning methods did you use?</h3>
A lot of our features consisted of family of counters that would express how the user reacted in a past similar situation. For instance, for each occurrence where the same user had been offered the same URL in the past, we labelled the outcome as one of the five following possibilities:
<ul>
	<li>the user skipped the URL, meaning he did not click on the URL but clicked on an URL which was on a lower rank.</li>
	<li>the user missed the URL, meaning he did not click on the URL and did not click on any URL with a lower rank.</li>
	<li>the user clicked on the url with a satisfaction of 0, 1, or 2</li>
</ul>
We normalized the counter of each of the labels using additive smoothing with an arbitrary prior.

For supervised learning methods, our final solution was using Lambda Mart, an algorithm considered as the state of the art for Learning-To-Rank. Unfortunately, it relies on Gradient Boosting Trees, which do not parallelize. Our best submission could not take advantage of our 12 cores, and took around 30 hours to compute.

In order to quickly study the effect of the different feature, we preferred using scikit-learn's Random Forest and a point-wise classification approach.

Here are a couple of Grandma's Tricks:

We tuned our hyperparameter (min_samples_leaf) to directly maximize the NDCG (the formula used for scoring solutions in this contest) on our cross validation set. To do so, we reinvented without knowing it a poorer version of an algorithm called golden section search.

Also, in order to compare different models we seeded the random selection of our cross validation set.
<h3>What was your most important insight into the data?</h3>
Reading related papers on the subject, we kind of knew that even though collaborative filtering techniques came to mind for this problem, they weren't the actual meat of the data. We primarily focused on fully mining more straightforward information: has the user already visited the URL? The domain? Was it for the same query? etc.

At the end of the contest we put more effort in trying to use more collaborative information. Our regularized SVD on domains just gave a score increase of 2.10-5 which was very disappointing.

Though spending more time on collaborative information was probably the key to beat Yandex's <strong>pampampampam</strong> team, retrospectively I'm still happy we did not focus on it too early.
<h3>Were you surprised by any of your insights?</h3>
We worked a lot at the beginning of the contest to build a perfect reproduction of Yandex's test dataset to avoid any bias in our training. But the default baseline we measured was way over the one announced by Yandex. The difference measured could not be explained by the score estimator variance.

We wondered whether we could explain this discrepancy by some seasonality in Yandex's score: people search different things during the weekend, or even a weekly scheduling of some backend scoring process at Yandex could be the culprit.

In any case, we did notice a strong day-of-the-week seasonality: Yandex's initial ranking was not as good during the weekend. Unfortunately, this was working against explaining the gap we had between their baseline score and ours. As of today, we still do not understand the inconsistency.

This however helped us understand that Day 1 was a Tuesday, which was also confirmed by the seasonality of the user's requests.
<h3>Which tools did you use?</h3>
Obviously Dataiku Data Science Studio. The studio is language agnostic and allows data scientists to work in R, SQL, Hive, Pig... you name it. But I'm a Python advocate and all our code was written in Python. We also used a Java library called Ranklib for LambdaMart. Finally the random tree forest implementation was that of scikit-learn.

Most probably because of Python's design (google ""GIL"" for more information), the current version of scikit-learn parallelization is based on multi-processing. It means that taking advantage of the 12 cores of our computer would have required 12 times as much RAM.

For this reason, we used a fork of joblib from Olivier Grisel that fixes this issue by making job processes share memory. Scikit-learn is pretty popular among Kagglers so they will be happy to know that in future versions of scikit-learn, all these problems will be solved in an even more elegant fashion.
<h3>What have you taken away from this competition?</h3>
We did not expect to do so well. The top of the leaderboard is full of former winners with a far more impressive academic pedigree than ours. At the risk of sounding cheesy, we attribute our result to teamwork. None of us would have reached top-10 ranks individually. In our case, the benefits of teamwork were not about the conjunction of different expertises! Your mates spot your bugs faster than you, they point out your fallacies, and they think about the feature you would have missed. Working in team is an efficient safety net, and a great timesaver. Finally, teamwork offers sheer emulation. Teamwork and Data Science are just by nature a perfect fit, and Dataiku Science Studio did a perfect job making it possible.

------------------------------------------------------------------------------

<img class=""alignright size-full wp-image-4619"" alt=""team Dataiku"" src=""http://blog.kaggle.com/wp-content/uploads/2014/02/team-dataiku.jpg"" width=""516"" height=""120"" />

<em><span style=""font-size: medium;"">Teammates of Dataiku Data Science Studio are based in Paris, France.</span></em>"
Winning the Galaxy Challenge with convnets: Sander Dieleman,http://blog.kaggle.com/2014/04/18/winning-the-galaxy-challenge-with-convnets/,2014-04-18 20:40:59,"Sander won First Place in <a title=""The Galaxy Challenge"" href=""http://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge"" target=""_blank"">The Galaxy Challenge</a>, sponsored by GalaxyZoo and Winton Capital. Although he already published a fantastic <a title=""benanne.github.io"" href=""http://benanne.github.io/2014/04/05/galaxy-zoo.html"" target=""_blank"">write-up</a> on his own blog, Sander sat down for <em>No Free Hunch</em> to answer more questions for the Kaggle community.

[caption id="""" align=""alignnone"" width=""512""]<img src=""http://upload.wikimedia.org/wikipedia/commons/7/74/Ngc5194.jpg"" alt=""The Whirlpool Galaxy (Spiral Galaxy M51, NGC 5194)"" width=""512"" height=""356"" />Image Credit: NASA and European Space Agency[/caption]
<h3>What was your background prior to entering this challenge?</h3>
I'm a PhD student in the Reservoir Lab of Prof. Benjamin Schrauwen at Ghent University in Belgium. My main research focus is applying deep learning and feature learning techniques to music information retrieval (MIR) problems, e.g. audio-based music classification, automatic tagging and music recommendation. I'd previously participated in the <a href=""https://www.kaggle.com/c/msdchallenge"" target=""_blank"">Million Song Dataset challenge</a> and the <a href=""https://www.kaggle.com/c/whale-detection-challenge"" target=""_blank"">whale detection challenge</a> on Kaggle.
<h3>What made you decide to enter?</h3>
I thought the problem was an excellent match for a feature learning approach. There was lots of image data, and feature learning techniques are known to work particularly well in this setting. But more importantly, it was atypical image data: images of galaxies have quite different statistical properties compared to typical 'natural' images. This made a feature learning approach all the more attractive, because a lot of common knowledge about image features does not apply here.
<h3>What preprocessing and supervised learning methods did you use?</h3>
I used raw pixel data as input to my models, but I did do some preprocessing in the sense that I downsampled and cropped the images to reduce the dimensionality, and applied random perturbations to artificially increase the amount of training data (data augmentation). This was necessary to reduce overfitting. All preprocessing was done on the fly during training.

I used convolutional neural networks with up to seven layers (4 convolutional layers and 3 fully connected layers). The goal of the competition was to predict a set of weighted probabilities, which adhered to certain constraints. I incorporated these constraints into the networks.

I also modified the network architecture to increase parameter sharing, by taking advantage of the rotation invariance property of galaxy images. I cut the images into several overlapping parts and rotated them, so that the network would be able to apply its learned filters in various orientations.

I trained the networks with stochastic gradient descent and momentum, using dropout in the fully connected layers for regularisation. I used rectified linear units in all convolutional layers, and maxout nonlinearities in the fully connected hidden layers.

My best single model had 7 layers and about 42 million parameters. Of course it was overfitting significantly, but despite that it still achieved the best score on the validation set.

In the end I trained 17 different models, with different architectures that were variations of the 7-layer architecture of the best model. This helped increase variance, which lead to a nice improvement when the predictions of all these models were averaged. I also averaged predictions across various transformed (i.e. rotated, zoomed) versions of the input images. These two levels of averaging gave my score a nice boost in the last few days of the competition.

I wrote a detailed account of my approach on <a href=""http://benanne.github.io/2014/04/05/galaxy-zoo.html"" target=""_blank"">my blog</a> and the code and documentation <a href=""https://github.com/benanne/kaggle-galaxies"" target=""_blank"">here on GitHub</a>.
<h3>What was your most important insight into the data?</h3>
Exploiting invariances in the data using data augmentation and modifications to the network architecture proved to be instrumental to get a good result. This goes to show that using a feature learning approach does not excuse you from having to get to know the data. You're not doing any feature engineering, but you still have to do some engineering. It just happens at a higher level of abstraction.
<h3>Were you surprised by any of your insights?</h3>
Originally I didn't include image flipping in the data augmentation process. I figured it wouldn't make much of a difference-- after all, it only doubles the effective number of examples the network sees. But when I eventually added it, my score jumped quite significantly. In the end this makes sense: rotation and scale invariance are much easier for the network to learn than invariance to flipping, because flipping an image displaces its features to a much larger extent.
<h3>Which tools did you use?</h3>
I used Python and implemented the convolutional neural networks in Theano. Its symbolic differentiation support allowed me to experiment with a lot of different approaches, without having to recalculate the gradients every time. Theano also makes it really easy to use GPU acceleration, which proved essential to be able to train the networks in a reasonable amount of time.

I used Theano wrappers for the cuda-convnet GPU convolution implementation by Alex Krizhevsky, which are included in the pylearn2 library. This gave a nice speed boost over Theano's own implementation. I used scikit-image for data augmentation.

I also used sextractor, a tool to extract properties of objects from astronomical images, and used this data to recenter and rescale the images. This didn't improve results, but I included a few models with this recentering and rescaling in the final ensemble to increase variance.
<h3>What have you taken away from this competition?</h3>
For problems like this, I believe that it's better to make your model able to deal with invariances, rather than to try and remove the invariances by normalising the data first. For example, instead of rotating all the galaxy images so their major axes are aligned, it's better to try and make the model work for all possible rotations. You'll need a bigger model, but the end result will be more robust.

I also learned that overfitting can happen at many levels: once a network has learned to be rotation invariant in its lower layers, it can overfit much more easily in the higher layers because it has figured out that you're repeatedly showing it rotated versions of the same image. So preventing overfitting in the lower layers can actually make it worse in the higher layers.

Even though data augmentation helped a lot, nothing beats having more training data. The competition organisers mentioned that the competition data is only a subset of what's available, so I think it would be interesting to train a network on a larger set of images. I believe there is still a lot of room for improvement.

------------------------------------------------------------------------------

<img class=""alignright"" src=""http://www.gravatar.com/avatar/7adf2ba2b14fa670c7da87c87d474046.png?r=pg&amp;s=120"" alt=""sedielem"" width=""120"" height=""120"" />

<em><span style=""font-size: medium;"">Sander Dieleman is a PhD student in the Reservoir Lab of Prof. Schrauwen at Ghent University in Belgium. His main research focus is applying deep learning and feature learning techniques to music information retrieval (MIR) problems, such as audio-based music classification, automatic tagging and music recommendation.</span></em>"
"Q&A with Gregory Matthews and Michael Lopez, 1st Place in March ML Mania",http://blog.kaggle.com/2014/04/21/qa-with-gregory-and-michael-1st-place-in-march-ml-mania/,2014-04-21 21:51:26,"Gregory <span style=""color: #333333;"">Matthews </span> and Michael Lopez are the members of team <a href=""https://www.kaggle.com/c/march-machine-learning-mania/leaderboard/private"" target=""_blank"">One shining MGF</a> who climbed up to first place during a raucous ride on the leaderboard of Kaggle's March Machine Learning Mania. After all predictive models were frozen on March 19, things unfolded to real-world game results in the 2014 NCAA Tournament [see the other blog posts tagged as <a href=""http://blog.kaggle.com/tag/march-mania/"">march-mania</a>]. We asked Greg and Mike to tell us how they approached the problem, working together for the first time on Kaggle.
<h3>What was your background prior to entering this challenge?</h3>
Greg: I have a Ph.D. in statistics from the University of Connecticut, completed a post-doc at the University of Massachusetts-Amherst, and in the fall will be an Assistant Professor of statistics at Loyola University Chicago. This was my first Kaggle contest.
<p style=""text-align: right;""><span style=""color: #333399;"">Mike: I am a Ph.D. candidate in biostatistics from Brown University, and in the fall will be an Assistant Professor of statistics at Skidmore College. This was also my first Kaggle contest.</span></p>

<h3>What made you decide to enter?</h3>
Greg: I’ve always been interested in evaluating and predicting sports using statistical methods. I am particularly interested in professional football, professional baseball, and college basketball. This contest seemed right up my alley.
<p style=""text-align: right;""><span style=""color: #333399;"">Mike: Greg emailed me.</span></p>

<h3>What preprocessing and supervised learning methods did you use?</h3>
Greg &amp; Mike: Our winning submission was the combination of two models, a margin-of-victory based model (MOV) and an efficiency model using Ken Pomeroy’s data (KP). For first round games, the MOV model used the actual spread posted in Las Vegas for each game; for future games, we used previous game outcomes to predict a margin of victory. At the end, the spread (or the expected margin) was converged into a probability using logistic regression. For the KP model, we tried different regression models using different team-wide efficiency metrics, eventually settling one that minimized our loss function on the training data. At the end, we used a weighted average of the two probabilities (one from the MOV model, one from the KP model) as our final submission.
<h3>What was your most important insight into the data?</h3>
Greg: The Las Vegas line is absolutely incredible at predicting games. As they say, if you can’t beat them, use their data in a Kaggle contest. Also, when training the models, we didn’t just try to predict the tournament games, as there is a relatively small number of those types of games. Instead, we also trained or models on regular season data, too.
<p style=""text-align: right;""><span style=""color: #333399;"">Mike: Like Greg said, it seemed silly to only train our models on a sample of 63 tournament games each season, when, in fact, there are hundreds of games played each week. Not sure it helped us, but we also ignored tournament specific information (i.e. a team’s seed number).</span></p>

<h3>Were you surprised by any of your insights?</h3>
Greg: I was surprised by how well our simple models performed. Using the right data was MUCH more important to our models performing well than using more sophisticated models.
<p style=""text-align: right;""><span style=""color: #333399;"">Mike: I think we gave ourselves a chance with a good model, but there was probably a decent amount of luck involved, too. Also, identifying the specific loss function for this Kaggle contest, and where it comes from, seemed to help our model.</span></p>

<h3>Which tools did you use?</h3>
Greg &amp; Mike: R and R Studio
<h3>What have you taken away from this competition?</h3>
Greg: Don’t cheat.
<p style=""text-align: right;""><span style=""color: #333399;"">Mike: I should always say yes when Greg emails me.</span></p>
<a href=""http://blog.kaggle.com/wp-content/uploads/2014/02/mania.jpg""><img class=""alignleft size-full wp-image-4641"" src=""http://blog.kaggle.com/wp-content/uploads/2014/02/mania.jpg"" alt=""mania"" width=""627"" height=""349"" /></a>

------------------------------------------------------------------------------

<em><span style=""font-size: medium;"">Michael Lopez is a 4th-year Ph.D. student in the Department of Biostatistics at Brown University. Mike's website is <a href=""http://statsbylopez.com/"" target=""_blank"">statsbylopez.com</a></span></em>

<em><span style=""font-size: medium;"">Gregory J. Matthews is currently a lecturer and the Associate Director of the Institute for Computational Biology, Biostatistics, and Bioinformatics in the Department of Public Health at UMass-Amherst. Greg's website is <a href=""http://statsinthewild.com/"" target=""_blank"">statsinthewild.com</a></span></em>"
"Sergey Kozub, A Winner in both Flight Quest 1 and 2",http://blog.kaggle.com/2014/04/22/sergey-kozub-a-winner-in-both-flight-quest-1-and-2/,2014-04-22 17:28:05,"Cross-posted from <a title=""Why do some flights arrive early while others touch down hours late?"" href=""http://www.gereports.com/post/83432530908/why-do-some-flights-arrive-early-while-others-touch"">GEreports.com</a>:

<a href=""http://blog.kaggle.com/wp-content/uploads/2014/04/quest_gif.png""><img class=""aligncenter size-full wp-image-4678"" src=""http://blog.kaggle.com/wp-content/uploads/2014/04/quest_gif.png"" alt=""quest_gif"" width=""478"" height=""270"" /></a>
<p style=""color: #333333;"">One afternoon a year ago, Sergey Kozub, a software developer in the Russian city of Kursk, was scrolling through messages on the popular programming forum topcoder when he hit on a link to Kaggle. Kaggle, the world’s largest open community of data scientists, had just partnered with GE and Alaska Airlines and challenged the public to come up with software that would reduce flight delays and make airlines more efficient and profitable.</p>
<p style=""color: #333333;"">Kozub spent the next several months writing algorithms and crunching data after work to come up with a solution. Today, he became <a style=""color: #7db6dc;"" href=""http://www.gequest.com/c/flight2-final/details/winners"">one of the winners</a> of the second leg of the <a style=""color: #7db6dc;"" href=""https://www.gequest.com/"">Industrial Internet Flight Quest</a> contest. “It was a very challenging competition,” Kozub says. “In the end, the difference between the scores of the top competitors was about five basis points.” In other words, miniscule.</p>
<p style=""color: #333333;"">Commercial airlines spend an estimated $22 billion annually managing flight plan efficiency. “Flight plans don’t always stick to a schedule,” says John Gough, director for Fuel and Carbon Solutions at GE Aviation. “There are gate conflicts, flight delays and unexpected fuel consumption. All these factors add up quickly.”</p>
<p style=""color: #333333;"">GE estimates that if every scheduled flight worldwide was able to reduce the distance it flew by only 10 miles, airlines could potentially cut annual fuel consumption by 360 million gallons and save the industry over $3 billion each year.</p>
<p style=""color: #333333;"">Gough says that the Flight Quest challenge, which received 6,800 submissions from 58 countries, is looking for an algorithm that could provide real-time, flight plan intelligence to the pilots “so they can make smarter decisions in the cockpit.”</p>
<p style=""color: #333333;"">Kozub, as team <span style=""color: #00ccff;""><strong>charango</strong></span>, won the 2nd prize in the second phase of Flight Quest and $50,000. He built a flight optimization model using dynamic programming to find a rough estimate of a flight route between airports and then make it more efficient. “It’s a custom-made solution,” he says. “The general approaches are well known, but the actual implementation requires a lot of domain-specific knowledge and attention to detail.”</p>
<p style=""color: #333333;"">His algorithms analyzed weather data from the National Oceanic and Atmospheric Administration, airport ground conditions and also flight statistics.</p>
<p style=""color: #333333;"">The winners of the second stage of the Flight Quest challenge will share $250,000 – the same amount set aside for the first stage winners, who were <a style=""color: #7db6dc;"" href=""http://www.gereports.com/post/74545138591/touch-down-ges-quest-to-know-when-your-flight-will"">announced last year</a>. (Kozub finished 4th during the first stage. He is the only contestant to win a prize in both legs.)</p>
<p style=""color: #333333;"">Kozub says that “there was some element of luck to winning” since the final results were very tight. “The chance of spending a full month without a reward was very high,” he says.</p>
<span style=""color: #00ccff;""><strong><a href=""http://www.gereports.com/post/83432530908/why-do-some-flights-arrive-early-while-others-touch""><span style=""color: #00ccff;"">See the original story on GEreports.com »</span></a></strong></span>"
Winners in Large Scale Hierarchical Text Classification: team Anttip,http://blog.kaggle.com/2014/05/15/winners-in-large-scale-hierarchical-text-classification-team-anttip/,2014-05-15 23:24:03,"The leader of team <strong><a href=""https://www.kaggle.com/c/lshtc/leaderboard/private"" target=""_blank""><span style=""color: #00ccff;"">anttip</span></a></strong> in this year's Large Scale Hierarchical Text Classification challenge was Antti Puurula. He's a PhD student in the Machine Learning Group at the University of Waikato, supervised by Prof. Ian Witten. His current interests include text mining, information retrieval, machine learning and graphical models. We asked him about his first place performance with teammates <strong><span style=""color: #00ccff;"">jread</span> </strong>and <strong><span style=""color: #00ccff;"">Albert</span></strong>. That competition asked participants to classify Wikipedia documents into one of 325,056 categories.
<h3>What was your background prior to entering this challenge?</h3>
I have a background in natural language processing and speech recognition research. Currently I'm finishing my PhD thesis on text mining using generative models, that proposes models using sparse computation as a solution for scalability in text mining.
<h3>What made you decide to enter?</h3>
I found out about LSHTC2 early in my studies, and decided to give it a try. By LSHTC3 my team was getting results close to the top.We had an ensemble framework that we used to participate in LSHTC3, and we thought it would be easy to set it up for LSHTC4 and see how far we got. I asked my earlier teammate Albert Bifet about trying this, and he brought Jesse Read along as well.
<h3>Which tools did you use?</h3>
For the base-classifiers we used the SGMWeka toolkit I've developed mostly for personal research use. On top of this we used Weka, Meka for one of the base-classifiers, and a number of Python and Unix shell scripts for model optimization and processing files. We used a handful of quad-core CPUs with 16GB RAM. Having only 16GB machines for use actually hurt our score quite a bit, and we had to prune and sub-sample the data to use any more complex models.
<h3>What preprocessing and supervised learning methods did you use?</h3>
Overall we used quite a big selection of various methods in our base-classifiers and the ensemble, since the toolkit could be used to try out different ideas. Using a large number of ideas was good for ensemble modeling, since it diversified the base-classifiers for model combination. In terms of machine learning the base-classifiers were mostly based on extensions of Multinomial Naive Bayes, and the ensemble used a variant of Feature-Weighted Linear Stacking.
<h3>What was your most important insight into the data?</h3>
There were a couple big ones. The biggest one is that you should always understand how the competition measure works and make sure your solution optimizes it. We realized that optimizing the Macro-averaged F-score measure becomes problematic with the very large number of labels used in the competition (325K). Other people on the competition forum noticed this as well. Our earlier system optimized other measures such as Micro-averaged Fscore, and we were far behind the leading participants before we started to think about this issue. Simple corrections such as surrogate measures and post-processing seemed to help a little, but our final solution worked best: instead of predicting the labels for documents, we predicted the documents for labels. This type of ""transposed prediction"" gave us a huge improvement in terms of Macro-averaged F-score, but it could have other uses as well.
<h3>Were you surprised by any of your insights?</h3>
The problem with the competition measure was surprising, and made me reconsider how to approach the task, and how to apply classification to datasets in general.

There were many other surprises. The commonly used feature weights and similarity measures for text data needed considerable modification to work optimally for this dataset. This might be the general case when working with text data, but there is little research work on this. I was also surprised by how scalable classification with inverted indices worked out to be, after some further optimizations to use safe pruning of parameters and multi-threading in classification. We used a handful of commodity cluster machines to optimize and test the tens of base-classifiers in our ensemble, while the other participants seemed to use a single base-classifier and post-processing. Some competitors opted out of the competition altogether, since they were not familiar with methods that scale to LSHTC.
<h3>What have you taken away from this competition?</h3>
Coming from a research background, I learned that competitions are different from research in many ways, and they can be highly rewarding if you take the participation seriously. Typically in research you start from the theory, look at a number of measures and datasets, and you ignore details such as feature pre-processing and similarity measures. In a competition you need to start from the data, look at the measure that matters on that dataset, and get the details right. The theory-first mindset can keep you from making new discoveries about the data and reaching a good score. Improving the score can lead to breaking some commonly accepted ways of doing things, and this can open new perspectives on the theory as well."
First Place in Allstate Purchase Prediction Challenge,http://blog.kaggle.com/2014/07/11/first-place-in-purchase-prediction-challenge/,2014-07-11 23:19:38,"<h3>What was your background prior to entering this challenge?</h3>
<strong>Jiří Materna</strong>: We are colleagues from CGI Prague data science team, and we come both from mathematical and IT backgrounds. I have a Masters degree in Computer Science and over 15 years of experience in IT <span style=""color: #333399;"">–</span> mostly in telco and banking. Most of the time I have been involved in projects with large data processing (migration, cleansing, integration, predictive modelling, etc). So far I have participated in about 20 Kaggle competitions, and for the rest of the team this was their first Kaggle competition.

<strong>Lukáš Drápal</strong> and <strong>Jana Papoušková</strong> are math geeks of the team. Lukáš has a Masters degree in Stochastics &amp; Financial Mathematics from Vrije University Amsterdam, and Jana got her Masters degree in Mathematics from Czech Technical University. They love to apply their deep mathematical knowledge to solve practical problems.

In addition, <strong>Emil Škultéty</strong> and <strong>Nomindalai Naranbaatar</strong> are the IT experts of the team. They both received a Masters degree in IT and have a wide range of practical experience.
<h3>What made you decide to enter?</h3>
<strong>Jiří</strong>: From my experience from previous Kaggle competitions I knew that Kaggle is a great place to learn. One not only touches upon the quirks and perks of algorithms, but the community sharing (especially post-competition) provides a way to improve your skills and to learn from the very best.

During one of our meetings we decided that we would like to participate in a competition altogether. With an enormous number of participating teams, the <a href=""https://www.kaggle.com/c/allstate-purchase-prediction-challenge"">Allstate Purchase Prediction competition</a> seemed like a true challenge.
<h3>What was your most important insight into the data?</h3>
<p style=""text-align: right;""><span style=""color: #333399;""><strong>Lukáš:</strong> The greatest influence on our chosen approach was the chosen metrics. If even one of the seven predicted policy settings was wrong, it was a complete miss. With this metric the provided last quoted benchmark (LQB) was darn good. We wanted to be really sure when making any changes to LQB – sort of a belt and braces approach.</span></p>
<strong>Jiří</strong>: We built three different models that predicted the complete policy and changed a record from LQB only if these coincided. On top of this we've built a classifier that stated whether the LQB is likely to work for the given record. So, sometimes even when all three models suggested a change from the LQB, we still kept it. In the end, only about 2000 out of 55000 records were changed from the LQB. Most of these changes were done for cases with little information about customer behavior – for those with only two records.
<p style=""text-align: right;""><span style=""color: #333399;""><strong>Jana</strong>: Another challenge was to determine how to group policy options together.</span></p>
<strong>Jiří</strong>: Yes, building an individual model for each of the policy option was bad as this lacked interaction between chosen policies. On the other hand, there were not enough data to build a model with all possible policy combination as outcomes. Moreover, this would be computationally too demanding. We settled for a compromise – we have modeled jointly groups (typically pairs) of variables that had the highest correlation.
<h3>How did you preprocess the data?</h3>
<strong>Jiří</strong>: The data used for each model were different. The first model was simply just using the data provided – information about a customer and previous offers. The second model added more variables – a variable with mean cost in each state and each location. Moreover, after prediction of the first variable, a variable stating whether the model output is different from the LQB was added. The last pair was then modeled with information whether any other variable has changed. The third model was the most complex – many features regarding changes in customer’s previous offers (ratios of current and previous costs, time differences, etc) have been added.
<h3>Which tools did you use?</h3>
<strong>Jiří</strong>: We have used Oracle XE as data storage for some basic exploratory data analysis and feature transformations. Otherwise, we used R for everything. Three different models were built using multinomial gradient boosting machine (gbm library in R) algorithm, the classifier used adaboost metric which seems to be reasonable proxy to AUC metric. Along the way, we have also fixed an issue within multinomial gbm for large number of classed, which we hope will be on GitHub soon. As our major improvements came late in the competition we did not have time to try the same approach with other algorithms like random forests.
<h3>Were you surprised by any of your insights?</h3>
<strong>Jiří</strong>: We were often surprised how changes from the LQB did not work the way we have expected. This led us to the conservative approach described above. Also, given the score on the public leaderboard (11th) we were surprised to see us on the top of the private leaderboard. We believe that the main reason for this jump was the conservative approach that didn’t overfit that much (together with a portion of luck that is always needed in such a tight competition).
<h3>What have you taken away from this competition?</h3>
<strong>Emil</strong>: First of all, this competition convinced me again how important it is to properly understand provided datasets. I learned that feature engineering brings greater benefit than algorithm parameters tuning. And finally, it is always worth to try different approaches, because it supports creative ideas and because proper blending of different solutions together is almost always beneficial.
<p style=""text-align: right;""><span style=""color: #333399;""><strong>Lukáš</strong>: Absolutely. I have realized how important it is to understand the evaluation metrics and how it can greatly affect the right approach.</span></p>
<strong>Jana</strong>: Yes, I totally agree. Also as this was my first competition I have learned that the results on public and private leaderboard can be really different. There might be no reason to be afraid of rivals who seem more capable than you. :)

------------------------------------------------------------------------------

<img class=""alignright size-full wp-image-4619"" src=""http://blog.kaggle.com/wp-content/uploads/2014/07/team-prazaci.jpg"" alt=""team Prazaci"" width=""666"" height=""120"" />

<em><span style=""font-size: medium;"">The team name Prazaci comes from all members being based in the great city of Prague in the Czech Republic.</span></em>"
11th hour win of Greek Media Monitoring Challenge,http://blog.kaggle.com/2014/07/28/11th-hour-win-of-greek-media-monitoring-challenge/,2014-07-28 18:36:49,"<em><span style=""color: #00ccff;""><strong><a href=""http://www.kaggle.com/users/3090/alexander-d-yakonov"" target=""_blank""><span style=""color: #00ccff;"">Alexander D'yakonov</span></a></strong></span> won the competition <a href=""https://www.kaggle.com/c/wise-2014/details/winners"" target=""_blank"">Greek Media Monitoring Multilabel Classification</a> which is associated with the <a href=""http://delab.csd.auth.gr/wise2014/"" target=""_blank"">WISE 2014</a> conference in Thessaloniki, Greece. Alexander has <a href=""http://blog.kaggle.com/author/Alexander.Dyakonov/"" target=""_blank"">quite a few winning posts</a> on</em> No Free Hunch, <em>and we again asked him to share some insights with Kaggle:</em>
<h3>What was your background prior to entering this challenge?</h3>
I am a professor at Lomonosov Moscow State University and a Kaggle member since 2010. I try to popularize data mining in Russia. For example, last year I organized a special seminar for students and young scientists — several tasks for them were to participate in Kaggle contests. This seminar was very popular and I’ll try to do something better this autumn.
<h3>What made you decide to enter?</h3>
I wanted to compete and chose several contests, but I did not have much spare time… so got a final solution only in WISE 2014. There was a quite simple problem: input data were real vectors with unit L2-norm and labels. The only difficulty was that one vector might have several labels. I already had solved similar problems. And my previous Kaggle contest (<a href=""https://www.kaggle.com/c/lshtc"" target=""_blank"">LSHTC</a>) was related with multi-label text classification like this one. It is interesting that the two highest teams on the leaderboard in these two contests are the same.
<h3>What preprocessing and supervised learning methods did you use?</h3>
I tried to generate new features, use SVD, and transform initial data, but it only slightly increased performance. My final solution did not use all these tricks. I realized that linear methods (ridge regression and logistic regression) were more suitable for this problem than kNN and naïve bayes. In my final blending I used all these linear methods and kNN. My algorithm consisted of two parts: linear combinations of regressors for each label and a binary decision rule. Such algorithms are very popular in Russia, for example in «the algebraic approach to classification». This technique had been developing by academician Yuri Zhuravlev and his scientific school since 1978 and is unknown in Europe and USA.
<h3>What was your most important insight into the data?</h3>
The same vectors had different lists of labels. It was very strange. I didn’t use cv, instead the first texts ­– for training, and the last ones – for local tests.
<h3>Which tools did you use?</h3>
I used python and scikit-learn. In my previous contests my main tools were Matlab and R.
<h3>What have you taken away from this competition?</h3>
I was in sixth place during the last week of the contest and did not have any new ideas. Suddenly I thought up my model 4 hours before the end. I tried the model in my local tests and it sufficiently increased the performance. I ran the model on the whole training set. It took almost 4 hours to build regressors and tune parameters, so I made my final submissions several minutes before the end. I was a lucky that I didn’t make a mistake in the code. I took away that it was possible to win the contest in 4 hours."
Learning from the best,http://blog.kaggle.com/2014/08/01/learning-from-the-best/,2014-08-01 12:58:27,"<em>Guest contributor</em> <a title=""David Wind"" href=""http://www.davidwind.dk/"" target=""_blank"">David Kofoed Wind</a> <em>is a PhD student in Cognitive Systems at The Technical University of Denmark (DTU):</em> <img class=""aligncenter size-full wp-image-4704"" src=""http://blog.kaggle.com/wp-content/uploads/2014/07/wind.jpg"" alt=""Learning from the best"" width=""580"" height=""300"" /> As a part of my <a href=""http://bit.ly/1zydyng"">master's thesis on competitive machine learning</a>, I talked to a series of Kaggle Masters to try to understand how they were consistently performing well in competitions. What I learned was a mixture of rather well-known tactics, and less obvious tricks-of-the-trade. In this blog post, I have picked some of their answers to my questions in an attempt to outline some of the strategies which are useful for performing well on Kaggle. As the name of this blog suggests, there is no free hunch, and reading this blog post will not make you a Kaggle Master overnight. Yet following the steps described below will most likely help with getting respectable results on the leaderboards. I have partitioned the answers I got into a series of broad topics, together with a list of miscellaneous advice in the end.
<h3>Feature engineering is often the most important part</h3>
With the extensive amount of free tools and libraries available for data analysis, everybody has the possibility of trying out advanced statistical models in a competition. As a consequence of this, what gives you most “bang for the buck” is rarely the statistical method you apply, but rather the features you apply it to. By feature engineering, I mean using domain specific knowledge or automatic methods for generating, extracting, removing or altering features in the data set.
<blockquote><em>For most Kaggle competitions the most important part is feature engineering, which is pretty easy to learn how to do. (<a href=""https://www.kaggle.com/users/3375/tim-salimans"">Tim Salimans</a>) </em>

<em>The features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering. (<a href=""https://www.kaggle.com/users/19298/luca-massaron"">Luca Massaron</a>)</em>

<em>Feature engineering is certainly one of the most important aspects in Kaggle competitions and it is the part where one should spend the most time on. There are often some hidden features in the data which can improve your performance by a lot and if you want to get a good place on the leaderboard you have to find them. If you screw up here you mostly can’t win anymore; there is always one guy who finds all the secrets. However, there are also other important parts, like how you formulate the problem. Will you use a regression model or classification model or even combine both or is some kind of ranking needed. This, and feature engineering, are crucial to achieve a good result in those competitions. There are also some competitions where (manual) feature engineering is not needed anymore; like in image processing competitions. Current state of the art deep learning algorithms can do that for you. (<a href=""https://www.kaggle.com/users/42188/josef-feigl"">Josef Feigl</a>)</em></blockquote>
On the contrary, sometimes the winning solutions are those which go a non-intuitive way and simply use a black-box approach. An example of this is the <a href=""https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest"">Solar Energy competition</a> where the Top-3 entries <a href=""https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest/forums/t/6321/our-approach"">almost did not use any feature engineering</a> (even though this seemed like an intuitive approach for the competition) – and simply combined the entire data set into one big table and used a complex black-box model (for example an ensemble of gradient boosting regressors).
<h3>Simple models will get you far</h3>
When looking through the descriptions of different solutions after a competition has ended, there is often a surprising number of very simple solutions obtaining good results. What is also (initially) surprising, is that the simplest approaches are often described by some of the most prominent competitors.
<blockquote><em>I think beginners sometimes just start to “throw” algorithms at a problem without first getting to know the data. I also think that beginners sometimes also go too-complex-too-soon. There is a view among some people that you are smarter if you create something really complex. I “try” to follow Albert Einsteins advice when he said, “Any intelligent fool can make things bigger and more complex. It takes a touch of genius – and a lot of courage – to move in the opposite direction”. (<a href=""https://www.kaggle.com/users/9766/breakfastpirate"">Steve Donoho</a>)</em>

<em>My first few submissions are usually just “baseline” submissions of extremely simple models – like “guess the average” or “guess the average segmented by variable X”. These are simply to establish what is possible with very simple models. You’d be surprised that you can sometimes come very close to the score of someone doing something very complex by just using a simple model. (<a href=""https://www.kaggle.com/users/9766/breakfastpirate"">Steve Donoho</a>)</em>

<em>You can go very far [with simple models], if you use them well, but likely you cannot win a competition by a simple model alone. Simple models are easy to train and to understand and they can provide you with more insight than more complex black boxes. They are also easy to be modified and adapted to different situations. They also force you to work more on the data itself (feature engineering, data cleaning, missing data estimation). On the other hand, being simple, they suffer from high bias, so they likely cannot catch a complex mapping of your unknown function. (<a href=""https://www.kaggle.com/users/19298/luca-massaron"">Luca Massaron</a>) </em></blockquote>
<h3>Overfitting the leaderboard is a real issue</h3>
During a competition, you have the possibility of submitting to the leaderboard. By submitting a solution to the leaderboard you get back an evaluation of your model on the public part of the test set. It is clear that obtaining evaluations from the leaderboard gives you additional information/data, but it also introduces the possibility of overfitting to the leaderboard-scores. Two fairly recent examples of competitions with overfitting to the leaderboard, were <a href=""https://www.kaggle.com/c/battlefin-s-big-data-combine-forecasting-challenge"">Big Data Combine</a> and <a href=""https://www.kaggle.com/c/stumbleupon"">StumbleUpon Evergreen Classification Challenge</a>. In the following table the top-10 entries on the public leaderboard for the StumbleUpon Challenge are shown together with their respective rankings on the private leaderboard.
<table width=""100%"">
<tbody>
<tr>
<td><strong>Username</strong></td>
<td><strong>Public rank</strong></td>
<td><strong>Private rank</strong></td>
</tr>
<tr>
<td>Jared Huling</td>
<td>1</td>
<td>283</td>
</tr>
<tr>
<td>Yevgeniy</td>
<td>2</td>
<td>7</td>
</tr>
<tr>
<td>Attila Balogh</td>
<td>3</td>
<td>231</td>
</tr>
<tr>
<td>Abhishek</td>
<td>4</td>
<td>6</td>
</tr>
<tr>
<td>Issam Laradji</td>
<td>5</td>
<td>9</td>
</tr>
<tr>
<td>Ankush Shah</td>
<td>6</td>
<td>11</td>
</tr>
<tr>
<td>Grothendieck</td>
<td>7</td>
<td>50</td>
</tr>
<tr>
<td>Thakur Raj Anand</td>
<td>8</td>
<td>247</td>
</tr>
<tr>
<td>Manuel Días</td>
<td>9</td>
<td>316</td>
</tr>
<tr>
<td>Juventino</td>
<td>10</td>
<td>27</td>
</tr>
</tbody>
</table>
This challenge had 7,395 samples and <a href=""http://www.kaggle.com/c/stumbleupon/forums/t/6185/what-is-up-with-the-final-leaderboard"">it was generally observed that the data were fairly noisy</a>. In the Big Data Combine competition, the task was to predict the value of stocks multiple hours into the future, which is <a href=""http://en.wikipedia.org/wiki/Efficient-market_hypothesis"">generally thought to be extremely difficult</a>. The extreme jumps on the leaderboard are most likely due to the sheer difficulty of predicting stocks, combined with overfitting.
<blockquote><em>The leaderboard definitely contains information. Especially when the leaderboard has data from a different time period than the training data (such as with the heritage health prize). You can use this information to do model selection and hyperparameter tuning. (<a href=""https://www.kaggle.com/users/3375/tim-salimans"">Tim Salimans</a>) </em>

<em>The public leaderboard is some help, [...] but one needs to be careful to not overfit to it especially on small datasets. Some masters I have talked to pick their final submission based on a weighted average of their leaderboard score and their CV score (weighted by data size). Kaggle makes the dangers of overfit painfully real. There is nothing quite like moving from a good rank on the public leaderboard to a bad rank on the private leaderboard to teach a person to be extra, extra careful to not overfit. (<a href=""https://www.kaggle.com/users/9766/breakfastpirate"">Steve Donoho</a>)</em>

<em>Overfitting to the leaderboard is always a major problem. The best way to avoid it is to completely ignore the leaderboard score and trust only your cross-validation score. The main problem here is that your cross-validation has to be correct and that there is a clear correlation between your cv-score and the leaderboard score (e.g. improvement in your cv-score lead to improvement on the leaderboard). If that’s the case for a given competition, then it’s easy to avoid overfitting. This works usually well if the test set is large enough. </em><em>If the test set is only small in size and if there is no clear correlation, then it’s very difficult to only trust your cv-score. This can be the case if the test set is taken from another distribution than the train set. (<a href=""https://www.kaggle.com/users/42188/josef-feigl"">Josef Feigl</a>) </em></blockquote>
<h3>Ensembling is a winning strategy</h3>
If one looks at the winning entries in previous competitions, a general trend is that most of the prize-winning models are ensembles of multiple models. The power of ensembling can also be <a href=""http://www.tandfonline.com/doi/abs/10.1080/095400996116839#.U78BPx8cjmE"">justified mathematically</a> (links to paid article).
<blockquote><em>No matter how faithful and well tuned your individual models are, you are likely to improve the accuracy with ensembling. Ensembling works best when the individual models are less correlated. Throwing a multitude of mediocre models into a blender can be counterproductive. Combining a few well constructed models is likely to work better. Having said that, it is also possible to overtune an individual model to the detriment of the overall result. The tricky part is finding the right balance. (<a href=""https://www.kaggle.com/users/7837/anil-thomas"">Anil Thomas</a>)</em>

<em>I am a big believer in ensembles. They do improve accuracy. BUT I usually do that as a very last step. I usually try to squeeze all that I can out of creating derived variables and using individual algorithms. After I feel like I have done all that I can on that front, I try out ensembles. (<a href=""https://www.kaggle.com/users/9766/breakfastpirate"">Steve Donoho</a>) </em>

<em>Ensembling is a no-brainer. You should do it in every competition since it usually improves your score. However, for me it is usually the last thing I do in a competition and I don’t spend too much time on it. (<a href=""https://www.kaggle.com/users/42188/josef-feigl"">Josef Feigl</a>)</em></blockquote>
<h3>Predicting the right thing is important</h3>
One task that is sometimes trivial, and other times not, is that of “predicting the right thing”. It seems quite trivial to state that it is important to predict the right thing, but it is not always a simple matter in practice.
<blockquote><em>A next step is to ask, “What should I actually be predicting?”. This is an important step that is often missed by many – they just throw the raw dependent variable into their favorite algorithm and hope for the best. But sometimes you want to create a derived dependent variable. I’ll use the GE Flight Quest as an example: you don't want to predict the actual time the airplane will land; you want to predict the length of the flight; and maybe the best way to do that is to use the ratio of how long the flight actually was to how long it was originally estimated to be and then multiply that times the original estimate. (<a href=""https://www.kaggle.com/users/9766/breakfastpirate"">Steve Donoho</a>) </em></blockquote>
There are two ways to address the problem of predicting the right thing: The first way is the one addressed in the quote from Steve Donoho above about predicting the correct derived variable. The other is to train the statistical models using the appropriate loss function.
<blockquote><em>Just moving from RMSE to MAE can drastically change the coefficients of a simple model such as a linear regression. Optimizing for the correct metric can really allow you to rank higher in the LB, especially if there is variable selection involved. (<a href=""https://www.kaggle.com/users/19298/luca-massaron"">Luca Massaron</a>)</em>

<em>Usually it makes sense to optimize the correct metric (especially in your cv-score). [...] However, you don’t have to do that. For example one year ago, I’ve won the Event Recommendation Engine Challenge which metric was MAP. I never used this metric and evaluated all my models using LogLoss. It worked well there. (<a href=""https://www.kaggle.com/users/42188/josef-feigl"">Josef Feigl</a>)</em></blockquote>
As an example of why using the wrong loss function might give rise to issues, look at the following simple example: Say you want to fit the simplest possible regression model, namely just an intercept a to the data:

$$x = \left(0.1,\;\; 0.2,\;\; 0.4,\;\; 0.2,\;\; 0.2,\;\; 0.1,\;\; 0.3,\;\; 0.2,\;\; 0.3,\;\; 0.1,\;\; 100\right)$$

If we let \(a_{\text{MSE}}\) denote the \(a\) minimizing the mean squared error, and let \(a_{\text{MAE}}\) denote the \(a\) minimizing the mean absolute error, we get the following

$$a_{\text{MSE}} \approx 9.2818$$

$$a_{\text{MAE}} \approx 0.2000$$

If we now compute the MSE and MAE using both estimates of \(a\), we get the following results:

$$\frac{1}{11} \sum_i \left| x_i - a_{\text{MAE}} \right| = 9.5909$$

$$\frac{1}{11} \sum_i \left| x_i - a_{\text{MSE}} \right| = 16.4942$$

$$\frac{1}{11} \sum_i \left( x_i - a_{\text{MAE}} \right)^2 = 905.4660$$

$$\frac{1}{11} \sum_i \left( x_i - a_{\text{MSE}} \right)^2 = 822.9869$$

We see (as expected) that for each loss function (MAE and MSE), the parameter which was fitted to minimize that loss function achieves a lower error. This should come as no surprise, but when the loss functions and statistical methods become very complicated, it is not always as trivial to see if one is actually optimizing the correct thing.
<h3>Additional and personal advice</h3>
One of the most important things I have personally taken away from the Kaggle competitions I have participated in, is to get started immediately and to get something on the leaderboard as fast as possible. It is easy to underestimate the amount of work it takes to build a complete pipeline from reading in the data to outputting a submission file in the right format. Getting a simple benchmark on the leaderboard is a good way to get started, and if you are not able to replicate a benchmark score, then that should be the first step before trying out advanced approaches.
<blockquote><em>My most surprising experience was to see the consistently good results of Friedman’s gradient boosting machine. It does not turn out from the literature that this method shines in practice. (<a href=""https://www.kaggle.com/users/6516/g-bor-tak-cs"">Gábor Takács</a>)</em></blockquote>
As a fresh Kaggler, it is very tempting to try out the biggest baddest model first. Ideally, one should start by allocating a fair amount of time to looking at, and playing with the data. Trying out simple models and plotting different variables together is a very important part of getting good results on Kaggle. Starting out with a complex model will slow you down since training and testing time will be higher - and this means that you do not have time to try as many different things. Even though I have personally entered quite a few Kaggle competitions, allocating enough time to simply look at the data is still one the things I am struggling with the most.
<blockquote><em>The more tools you have in your toolbox, the better prepared you are to solve a problem. If I only have a hammer in my toolbox, and you have a toolbox full of tools, you are probably going to build a better house than I am. Having said that, some people have a lot of tools in their toolbox, but they don’t know *when* to use *which* tool. I think knowing when to use which tool is very important. Some people get a bunch of tools in their toolbox, but then they just start randomly throwing a bunch of tools at their problem without asking, “Which tool is best suited for this problem?” (<a href=""https://www.kaggle.com/users/9766/breakfastpirate"">Steve Donoho</a>)</em></blockquote>
A tip that many of the top-performers mention is to make heavy use of the Kaggle forums. During the competitions, many participants write interesting questions which highlight features and quirks in the data set, and some participants even publish well-performing benchmarks with code on the forums. After the competitions, it is common for the winners to share their winning solutions. Reading those carefully will almost surely give you a good idea to try out the next time.
<blockquote><em>The best tip for a newcomer is to read the forums. You can find a lot of good advice there and nowadays also some code to get you started. Also, one shouldn't spend too much time on optimizing the parameters of the model at the beginning of the competition. There is enough time for that at the end of a competition.</em> <em> (<a href=""https://www.kaggle.com/users/42188/josef-feigl"">Josef Feigl</a>)</em>

<em>In each competition I learn a bit more from the winners. A competition is not won by one insight, usually it is won by several careful steps towards a good modelling approach. Everything play its role, so there is no secret formula here, just several lessons learned applied together. I think new kagglers would benefit more of carefully reading the forums and the past competitions winning posts. Kaggle masters aren't cheap on advice!</em> <em> (<a href=""https://www.kaggle.com/users/24266/leustagos"">Lucas S.</a>)</em></blockquote>"
3rd Place interview from the KDD Cup 2014,http://blog.kaggle.com/2014/08/05/3rd-place-interview-from-the-kdd-cup-2014/,2014-08-06 04:21:26,"<em><span style=""font-size: medium;"">Kiran placed 3rd in the <a title=""KDD Cup 2014"" href=""https://www.kaggle.com/c/kdd-cup-2014-predicting-excitement-at-donors-choose"" target=""_blank"">KDD Cup</a> and shared this interview with No Free Hunch:</span></em>
<h3>What was your background prior to entering this challenge?</h3>
I am a computer science engineer and management post-grad, heading marketing analytics, mobile analytics and customer analytics for Flipkart.com (the 'Amazon' of India), where I use data sciences in my work. Prior to this I was at Amazon.com and Dell. I have spent several years at Dell.com in a variety of roles in digital analytics leveraging data sciences.

I am self-taught and learnt most things on my own/on-the-job in the field of machine learning starting off with SAS (closed source) and transitioning to R &amp; Python (open source) over the last 5 years. I have participated in several Kaggle competitions to try, use and learn new techniques and was at one point ranked among the top 10 WW data miners. I have freelanced with US startups via the Konnect program helping solve problems like predicting multiple sclerosis recurrence, recommendation engine for music labels, among others. In my professional life, I have leveraged data sciences to solve the multi-touch attribution problem in e-commerce, to solve the store optimization problem of ranking configurator modules (both at Dell), to build an email rules engine and a world-class segmentation engine that was scaled to the entire customer database (both at Flipkart).
<h3>What made you decide to enter?</h3>
There were basically two reasons:
<ul>
	<li>KDD is the #1 conference for data miners worldwide and I wanted to participate in the competition</li>
	<li>The nature of the problem is very interesting. It has all the nuances of a difficult data sciences problem -- namely time based cross-validation, imbalanced dataset, sparsity, huge data size and high dimensionality (as a result of the text data).</li>
</ul>
<h3>What preprocessing and supervised learning methods did you use?</h3>
Using visualization I found out early on that the dataset prior to 2010 did not have any labels and excluded that from training. I also added variables when data was missing for certain features -- to see if they could make a difference with the patterns of missing values.

The supervised learning methods I tried out for this competition were gradient boosting machines, vowpal wabbit, large scale regularized logistic regression &amp; Support vector machines (liblinear), random forest and a bayesian regularized neural network.
<h3>What was your most important insight into the data?</h3>
The following were key insights:
<ul>
	<li>Some of the recently posted projects did not have sufficient time to be interesting projects</li>
	<li>The text features - i.e. the essay content, the title, description of the project were not very useful in prediction - but were useful for ensembling models</li>
	<li>Part of speech features were useful</li>
	<li>Time of the year is an important feature</li>
	<li>Some donors are likely to donate more than other donors</li>
	<li>The location of the school requesting donation is important as there are people who like to donate in a specific region</li>
</ul>
<h3>Were you surprised by any of your insights?</h3>
I did not expect parts of speech features to be useful and I expected the text mining to yield stronger results. Time based cross-validation was very important to not overfit the leaderboard.
<h3>Which tools did you use?</h3>
I used R and Python - both open source - leveraging the rich libraries that these languages provide. Besides these there are excellent libraries of vowpal wabbit, xgboost and liblinear that can be called from the command line. All these on my Ubuntu desktop were very powerful.

Treetagger is very useful to get parts of speech features with unstructured text data.
<h3>What have you taken away from this competition?</h3>
There are several intricacies with different algorithms that enable them to function well.
Example: GBM implementation in python gives very good results when we treat factor variables as integers instead of dummy coding them; Random Forest with undersampling is very powerful.

It is important to be able to run algorithms on multiple cores (parallel processing) to get fast results.

Finally, having a good repository/your own library of code that you can leverage saves a lot of time.

------------------------------------------------------------------------------

<img class=""alignright size-thumbnail wp-image-4707"" src=""http://blog.kaggle.com/wp-content/uploads/2014/07/k_photo-150x150.jpg"" alt=""Created with Nokia Smart Cam"" width=""150"" height=""150"" />

<em><span style=""font-size: medium;"">Kiran is a data sciences and business analytics leader working with Flipkart.com. Prior to this he worked at Amazon.com, was one of the first/oldest members in the e-biz team at Dell.com where he spent many years. His research interests span the areas of imbalanced datasets, large dimensionality and text mining with a specific strong interest in the field of digital analytics and e-business/e-commerce.</span></em>"
Lessons Learned from the Hunt for Prohibited Content on Kaggle,http://blog.kaggle.com/2014/09/15/lessons-from-avito-prohibited-content-kaggle/,2014-09-15 16:57:30,"<p style=""text-align: right;""><em>(Cross-posted from <a href=""http://mlwave.com/lessons-from-avito-prohibited-content-kaggle/"" target=""_blank"">MLWave.com</a>)</em></p>
Kaggle hosted a contest together with <a href=""http://www.avito.ru/"" target=""_blank"">Avito.ru</a>. The task was to automatically <a href=""https://www.kaggle.com/c/avito-prohibited-content/"">detect illicit content in the advertisements</a> on their site.
<span id=""more-734""></span>

Many competitors were using <a href=""http://hunch.net/~vw/"">Vowpal Wabbit</a> for this challenge. Some aided by the <a href=""https://www.kaggle.com/c/avito-prohibited-content/forums/t/10120/beating-the-benchmark"">benchmark</a> from <span style=""color: #00ccff;""><strong><a style=""color: #00ccff;"" href=""https://www.kaggle.com/users/28038/foxtrot"">Foxtrot</a></strong></span>, others by starting out the challenge with it. The highest ranking model using VW for a base was <span style=""color: #00ccff;""><strong><a style=""color: #00ccff;"" href=""https://www.kaggle.com/users/102203/yr"">yr</a></strong></span>'s <a href=""http://www.kaggle.com/c/avito-prohibited-content/forums/t/10178/congrats-barisumog-giulio/52810#post52810"">implementation</a>. This #4 spot used the benchmark provided by Avito as part of the pipeline.

Our team (<span style=""color: #00ccff;""><strong><a style=""color: #00ccff;"" href=""http://julesvanligtenberg.nl/"">Jules van Ligtenberg</a></strong></span>, <span style=""color: #00ccff;""><strong><a style=""color: #00ccff;"" href=""https://www.kaggle.com/users/115173/phil-culliton"">Phil Culliton</a></strong></span> and me, <span style=""color: #00ccff;""><strong><a style=""color: #00ccff;"" href=""https://www.kaggle.com/users/114978/triskelion"">Triskelion</a></strong></span>) ended up in 8th place with an average precision of ~0.985. A team of Russian moderators had an average precision of ~0.988 when labeling the dataset. Our team did not speak Russian, just English, Dutch and <a href=""http://en.wikipedia.org/wiki/MurmurHash"">MurmurHash</a>.
<blockquote>It is truly amazing that so many international teams that have no knowledge of Russian language made it to the top. –<cite>Ivan Guz, PhD, Competition Sponsor</cite></blockquote>
<h2>Insights</h2>
<h3>What did work</h3>
<ul>
	<li><strong>Ensembling Vowpal Wabbit models</strong>. By simply averaging the ranks of different submission files one could up the score. Combining a squared, logistic and hinge loss model this way gave a score of ~0.982, while all individual models scored around ~0.977.</li>
	<li><strong>Using an illicit score.</strong> This changes the problem from classification to regression. Instead of training models on labels of [illicit, non-illicit], we used the provided “closing hours” and “is proved” variables to create an “illicit score”. The worst offenders for this model are ads that are “blocked” by a moderator, “proved” by an experienced moderator, and “closed” within minutes of being published on the site.</li>
	<li><strong>All loss functions</strong> gave good results. Initially we gravitated towards logistic loss and hinge loss. Later we added a squared loss and a quantile loss. For example averaging the ranked outputs of both a logistic and a hinge loss model, with all the parameters and data the same, gave a ~0.003 increase in score. We will study these “hybrid” loss functions better.</li>
	<li>...</li>
</ul>
<em>Read the full post from Triskelion on</em> <a href=""http://mlwave.com/lessons-from-avito-prohibited-content-kaggle/"">MLWave.com</a> ! <em>And look for the three winners' interviews coming soon.</em>"
1st Place: The Hunt for Prohibited Content,http://blog.kaggle.com/2014/11/03/1st-place-the-hunt-for-prohibited-content/,2014-11-03 19:02:53,"<h3>What was your background prior to entering this challenge?</h3>

<strong>Giulio</strong>: I hold Masters in Statistics and Biostatistics and have worked 15 years in HealthCare Insurance as a Statistician and Data Scientist. While I do pretty much everything from munging and exploration of large, complex, noisy data, to creating presentation for executives, my focus and passion remain on advanced analytics and applied machine learning. I’ve been programming in SAS for my whole career but picked up Python and R after I started competing on Kaggle.

<strong>barisumog</strong>: I have a BS in civil engineering. I've worked as an analyst for an international cement company for 10 years. My responsibilities included gaining insights from raw reports from a variety of departments (Marketing &amp; Sales, Credit &amp; Risk, and Operations), discovering actionable items for the management level, and offering strategic planning recommendations for the executive level.

<h3>What made you decide to enter?</h3>

<strong>Giulio</strong>: My industry, healthcare, offers great opportunities, through analytics, to impact and help people in very difficult times in their life. From that perspective my work is very rewarding. However this industry is just starting to catch up with many cutting edge application of data science, big data and machine learning. In that sense, competitions that provide the most diverse experience from healthcare and insurance are those I can learn the most from and thus more rewarding. Furthermore I really wanted to try to do well in a text mining competition and this one had a very sizable portion of text data.

<strong>barisumog</strong>: I've been studying machine learning on my own for over a year now. I've entered numerous competitions on Kaggle before. I find the competitive spirit very motivating during the competitions. And after the competition, there's always helpful discussion on the forums. I’ve always been interested in natural language processing. I used to code chatbots when I was younger. The fact that the data in this competition was in Russian, which I don’t speak, intrigued me enough to give it a shot.

<h3>What preprocessing and supervised learning methods did you use?</h3>

<strong>Giulio</strong>: One thing I found out soon was that Russian text did not really need any special preprocessing and I was able to easily improve the benchmark code using no preprocessing at all. For the text portion of the data I used a series of Stochastic Gradient Descent models on various parts of the text features (title,description, attributes) and fed those predictions into a Random Forest along with additional dense features (category and subcategory being the most important). Since plain accuracy was so high across the train dataset, I then used semi­-supervised learning to score the test set and retrain the algorithm on train and test combined.

<strong>barisumog</strong>: First of all, I worked on a category and subcategory basis, instead of working with the data as a whole. I concatenated the text fields (title, description, attributes), and created 3 different tf­idf matrices for each category / subcategory. One tf­idf used the raw text, one applied stemming, and one used stop­ words. The main reason behind this was to introduce some diversity, which was a key element in this competition. I only used textual features, as I couldn't provide additional value from any non­text features I tried. I trained Support Vector Classifiers on each tf­idf for every category / subcategory. I also exploited Giulio's semi-­supervised approach, which worked quite well.

<h3>What was your most important insight into the data?</h3>

<strong>Giulio</strong> &amp; <strong>barisumog</strong>: The following were key insights:
<ul>
	<li>The Real Estate category, a large portion of the data but with very few blocked posts, added no value to the models, it actually made them worse. All of our models do not even bother scoring this category.</li>
	<li>No need to do fancy preprocessing on text features.</li>
	<li>A blend of two very high scoring models did not necessarily translate into a higher overall score. Diversity was much more important.</li>
</ul>

<h3>Were you surprised by any of your insights?</h3>

<strong>Giulio</strong>: I did expect feature engineering to add lots of value by extracting pieces of text that could be flags of blocked posts. For example, counts of mixed language words which are often used by fraudsters to bypass fraud detection algorithms. But none of what I have tried really added much value.

<strong>barisumog</strong>: It was moderately easy to get above 0.975 on the public leaderboard, and I initially thought the top ranks would be close to perfect in the end. But once we reached 0.985, it became harder than I expected to improve.

<strong> Which tools did you use?</strong>

<strong>Giulio</strong>: I used Python for the whole competition. Mostly scikit learn. Google translator came in handy as I was looking into misclassified observations.

<strong>barisumog</strong>: Python and scikit learn.

<h3>What have you taken away from this competition?</h3>

<strong>Giulio</strong>: Some techniques and methods are generalizable in a much more flexible way than I had imagined. I was somewhat surprised that I could get so much out of Russian text without doing any preprocessing on it. Also, simple algorithms and creative approaches can go a long way.

<strong>barisumog</strong>: The main route in most text heavy data is tf­idf. A good majority of teams probably had some component based on that. What makes the difference is the creative little things you mix in. In our case, the two main ones were applying semi­-supervised learning, and ignoring a sizable category of posts. This was also my first competition I worked in a team, and I have learned a lot from Giulio in that respect."
2nd Place: The Hunt for Prohibited Content,http://blog.kaggle.com/2014/11/03/2nd-place-the-hunt-for-prohibited-content/,2014-11-03 19:24:24,"<h3>What was your background prior to entering this challenge?</h3>
<strong>Mikhail</strong>: I'm a student of Moscow Institute of Physics and Technology. I also do have some background in applied math and CS. Now I""m getting Master degree. My bachelor thesis was 'active learning.' Started just a year ago, began with reading machine learning course by K. Vorontsov and attending Alexandr Dyakonov""s seminars. Suppose it was quite good introduction into data science.

<strong>Dmitry</strong>: I'm graduate of MIPT (same university as Mikhail). I knew about Kaggle from the very beginning but started to compete after having Machine Learning course at School of Data Analysis (where I""m currently getting my Master Degree). This course gave me required tools, techniques and experience to compete at a good level. Lately I worked at Yandex doing feature engineering related to ad CTR prediction and Yandex web-ranking tasks.
<h3>What made you decide to enter?</h3>
<strong>Mikhail</strong>: I wanted to learn how to process raw text data. Also had a desire to try a bunch of ideas learned from my previous contests and my former team-mates. However, AVITO.ru is a Russian company so this contest was some kind of a challenge among locals, Russian data scientists.

<strong>Dmitry</strong>: An idea to compete where you can do more. My key interests are feature engineering possibilities, big data and russian language of course.
<h3>What preprocessing and supervised learning methods did you use?</h3>
<strong>Mikhail &amp; Dmitry</strong>: We used sklearn and LibFM and found LibFM quite powerful tool for such a task. We were surprized by the fact that preprocessing ( stemming and removing stopwords) gave no profit at all. Two-levels model became our solution: outputs of SVM and LibFM were ensembled by Random Forest. Such technique is widely used and it was one of the ideas Mikhail wanted to try when entering the competition.
<h3>What was your most important insight into the data?</h3>
<strong>Mikhail &amp; Dmitry</strong>:<strong> </strong>We found that human error rate of labeling was very significant. In fact, the task was not to block illegal contect, but to redict moderator's verdict. This difference in importante, especially if we want to get score 0.98+. Also we were surprised that Random Forest was pretty good for blending. We spent a lot of time tring linear ensembling models in thougs that blending method MUST be as simple as possible. But RF outperformed all linear models from first submission.
<h3>Which tools did you use?</h3>
<strong>Mikhail &amp; Dmitry</strong>:<strong> </strong>We used python, scikit-learn and LibFM. We found that LinearSVC works perfect and Random Forest in sklearn 0.15 works much faster than former 0.14 version =)
<h3>What have you taken away from this competition?</h3>
<strong><strong>Mikhail &amp; Dmitry</strong></strong>:That you should learn from the best - all ideas are public and you can find them in solutions of past contests. Blending is a real power! And not only linear combinations work well. Also, teamwork gets a lot of advantages - mainly, ideas."
Convolutional Nets and CIFAR-10: An Interview with Yann LeCun,http://blog.kaggle.com/2014/12/22/convolutional-nets-and-cifar-10-an-interview-with-yan-lecun/,2014-12-22 16:19:55,"Recently Kaggle hosted a <a href=""http://www.kaggle.com/c/cifar-10/"">competition</a> on the <a href=""http://www.cs.toronto.edu/~kriz/cifar.html"">CIFAR-10 dataset</a>. The CIFAR-10 dataset consists of 60k 32x32 colour images in 10 classes. This dataset was collected by <a href=""http://www.cs.toronto.edu/~kriz/"">Alex
Krizhevsky</a>, <a href=""http://www.cs.toronto.edu/~vnair/"">Vinod Nair</a>, and <a href=""http://www.cs.toronto.edu/~hinton/"">Geoffrey Hinton</a>.

Many contestants used convolutional nets to tackle this competition. Some resulted in scores that beat human performance on this classification task. In this blog series we will interview three contestants and also a founding father of convolutional nets: <a href=""http://yann.lecun.com/"">Yann LeCun</a>.

[caption id=""attachment_4725"" align=""aligncenter"" width=""297""]<img class=""wp-image-4725 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2014/12/cifar-10-297x300.png"" alt=""cifar-10"" width=""297"" height=""300"" /> Example of cifar-10 dataset[/caption]
<h2>Yann LeCun</h2>
Yann Lecun is currently <a href=""https://research.facebook.com/researchers/684639631606527/yann-lecun/"">Director</a> of <a href=""https://research.facebook.com/ai"">AI Research</a> at Facebook and a professor at <a href=""http://www.cs.nyu.edu/~yann/"">NYU</a>.
<h3>Which other scientists should be named and celebrated for the successes of convolutional nets?</h3>
Certainly, <a href=""http://personalpage.flsi.or.jp/fukushima/index-e.html"">Kunihiko Fukushima</a>'s work on the Neo-Cognitron was an inspiration. Although the early forms of convnets owed little to the Neo-Cognitron, the version we settled on (with pooling layers) did.

[caption id=""attachment_4726"" align=""aligncenter"" width=""437""]<a href=""http://blog.kaggle.com/wp-content/uploads/2014/12/neo-cognitron-layers.png""><img class=""wp-image-4726 size-full nofloat"" src=""http://blog.kaggle.com/wp-content/uploads/2014/12/neo-cognitron-layers.png"" alt=""neo-cognitron-layers"" width=""437"" height=""275"" /></a> A schematic diagram illustrating the interconnections between layers in the Neo-Cognitron. From <em>Fukushima K. (1980) <a href=""http://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf"">Neocognitron</a>: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position</em>.[/caption]
<h3>Can you recount an aha!-moment or theoretical breakthrough during your early research into convolutional nets?</h3>
Not really. It was the logical thing to do. I had been playing with multi-layer nets with local connections since 1982 or so (not having the right learning algorithm though. <a href=""http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"">Backprop</a> didn't exist). I started experimenting with shared weight nets while I was a postdoc in Toronto in 1988.

The reason for not trying earlier is simply that I didn't have the software nor the data. Once I arrived at <a href=""https://www.bell-labs.com/"">Bell Labs</a>, I had access to a large dataset and fast computers (for the time). So I could try full-size convnets, and it worked amazingly well (though it required 2 weeks of training).
<h3>What is your opinion on the recent popularity of convolutional nets for object recognition? Did you expect it?</h3>
Yes. I knew it had to happen. It was a matter of time until the datasets become large enough and the computers powerful enough for deep learning algorithms to become better than human engineers at designing vision systems.

There was a symposium entitled ""<a href="" http://www.frontiersincomputervision.com/"">frontiers in computer vision</a>"" at MIT in August 2011. The title of my <a href="" http://www.frontiersincomputervision.com/slides/FCV_Learn_LeCun.pdf"">talk</a> was “5 years from now, everyone will learn their features (you might as well start now)”. <a href=""http://www.cs.ubc.ca/~lowe/home.html"">David Lowe</a> (the inventor of <a href=""http://www.cs.ubc.ca/~lowe/keypoints/"">SIFT</a>) said the same thing.

[caption id=""attachment_4727"" align=""aligncenter"" width=""498""]<a href=""http://blog.kaggle.com/wp-content/uploads/2014/12/invariant-features-slide-s.png""><img class=""wp-image-4727 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2014/12/invariant-features-slide-s.png"" alt=""invariant-features-slide-s"" width=""498"" height=""372"" /></a> A slide from the talk LeCun Y. (2011) “5 years from now, everyone will learn their features (you might as well start now)”.[/caption]

Still, I was surprised by how fast the revolution happened and how much better convnets are, compared to other approaches. I would have expected the transition to be more gradual. Also, I would have expected unsupervised learning to play a greater role.
<h3>The character recognition model at AT&amp;T was more than a simple classifier, but a complete pipeline. Can you tell more about the implementation problems your team faced?</h3>
We had to implement our own program language and write our own compiler to build this. <a href=""http://leon.bottou.org/"">Leon Bottou</a> and I had written a neural net simulator called SN, back in 1987/1988. It was a Lisp interpreter with a numerical library (multidimensional arrays, neural net graphs…). We used this at Bell Labs to develop the first convnets.

Then in the early 90’s, we wanted to use our code in products. Initially, we hired a team of developers to convert our Lisp code to C/C++. But the resulting system could not be improved easily (it wasn’t a good platform for R&amp;D). So Leon, <a href=""http://nips.cc/Conferences/2014/Program/speaker-info.php?ID=720"">Patrice Simard</a> and I wrote a compiler for <a href=""http://lush.sourceforge.net/credits.html"">SN</a>, which we used to develop the next generation OCR engine.

That system integrated a segmenter, a convnet, and a graphical model on top. The whole thing was trained end to end.

The graphical model was called a “<a href=""http://leon.bottou.org/talks/gtn"">graph transformer network</a>”. It was conceptually similar to what we now call a conditional random field, or a structured perceptron (which it predates), but it allowed for non-linear scoring function (CRF and structured perceptrons can only have linear scoring functions).

The whole infrastructure was written in SN and compiled. This is the system that was deployed in ATM machines and check reading machines in 1996 and was reading 10 to 20% of all the checks in the US by the late 90’s.

[caption id=""attachment_4729"" align=""aligncenter"" width=""320""]<a href=""http://blog.kaggle.com/wp-content/uploads/2014/12/lenet-51.gif""><img class=""wp-image-4729 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2014/12/lenet-51.gif"" alt=""lenet-5"" width=""320"" height=""200"" /></a> An animation showing LeNet 5 in action. From ""<em><a href=""http://yann.lecun.com/exdb/lenet/dancing384.html"">Invariance and Multiple Characters with SDNN</a> (Multiple Characters Demo)</em>"".[/caption]
<h3>In comparison with other methods, training convnets is pretty slow. How do you deal with the trade-off between experimentation and increased model training times? What does a typical development iteration look like?</h3>
In my experience, the best large-scale learning systems always take 2 or 3 weeks to train, regardless of the task, the method, the <a href=""http://yann.lecun.com/exdb/publis/pdf/farabet-ecvw-11.pdf"">hardware</a>, or the data.

I don’t know if convnets are “pretty slow”. Compared to what? They may be slow to train, but the alternative to “slow learning” is months of engineering efforts which doesn’t work as well in the end. Also, convnets are actually pretty fast to run (after training).

In a real application, no one really cares how long it takes to train. But people care a lot about how long it takes to run.
<h3>Which recent papers on convolutional nets are you most excited about? Any papers or ideas we should look out for?</h3>
There are lots and lots of ideas surrounding convnets and deep learning that have lived in relative obscurity for the last 20 years or so. No ones cared about it, and getting papers published was always a struggle. So, lots of ideas were never properly tried, never published, or were tried and published but soundly ignored and quickly forgotten. Who remembers that the first learning-based face detector that actually worked was a convolutional net (back in <a href=""http://www.academia.edu/3073679/Original_approach_for_the_localisation_of_objects_in_images"">1993</a>, eight years before <a href=""http://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework"">Viola-Jones</a>)?

[caption id=""attachment_4730"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2014/12/face-detection.jpg""><img class=""wp-image-4730 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2014/12/face-detection-300x172.jpg"" alt=""face-detection"" width=""300"" height=""172"" /></a> A figure with predictions from Vaillant R., Monrocq C., LeCun Y. (1993) ""<em>An original approach for the localisation of objects in images</em>"".[/caption]

Today, it’s really amazing to see so many young and bright people devoting so much creative energy to the topic and coming up with new ideas and new applications. The hardware / software infrastructure is getting better, and it’s becoming possible to train large networks in a few hours or a few days. So people can explore many more ideas that in the past.

One thing I’m excited about is the idea of “<a href=""http://arxiv.org/abs/1312.6203"">spectral convolutional net</a>”. This was a paper at ICLR 2014 by folks from my NYU lab about a generalization of convolutional nets that can be applied to any graphs (regular convnets can be applied to 1D, 2D or 3D arrays that can be seen as regular grids in terms of graph). There are practical issues, but it opens the door to many more applications of convnets to unstructured data.

[caption id=""attachment_4731"" align=""aligncenter"" width=""438""]<a href=""http://blog.kaggle.com/wp-content/uploads/2014/12/mnist-sphere.png""><img class=""wp-image-4731 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2014/12/mnist-sphere.png"" alt=""mnist-sphere"" width=""438"" height=""156"" /></a> MNIST digits on a sphere. From Bruna J., Zaremba W., Szlam A., LeCun Y. (2013) ""<em>Spectral Networks and Deep Locally Connected Networks on Graphs</em>"".[/caption]

I’m very excited about the application of convnets (and recurrent nets) to natural language understanding (following the seminal work of <a href=""http://ronan.collobert.com/"">Collobert</a> and <a href=""http://www.thespermwhale.com/jaseweston/"">Weston</a>).
<h3>Since the error rate of a human is estimated to be around 6%, and Dr. Graham showed results of 4.47%, do you consider CIFAR-10 to be a solved problem?</h3>
It’s a solved problem in the same sense as <a href=""http://yann.lecun.com/exdb/mnist/"">MNIST</a> is a solved problem. But frankly, people are more interested in <a href=""http://www.image-net.org/"">ImageNet</a> than in CIFAR-10 nowadays. In that sense, CIFAR-10 is not a “real” problem. But it’s not a bad benchmark for a new algorithm.
<h3>What would it take for convnets to see a much wider adoption in the industry? Will training convnets and the software to set them up become less challenging?</h3>
What are you talking about? Convnets are absolutely everywhere now (or about to be everywhere) in industry: <a href=""https://www.facebook.com/publications/546316888800776/"">Facebook</a>, <a href=""http://research.google.com/archive/large_deep_networks_nips2012.html"">Google</a>, <a href=""http://research.microsoft.com/apps/pubs/?id=68920"">Microsoft</a>, <a href=""http://ttic.uchicago.edu/~haotang/speech/06638949.pdf"">IBM</a>, <a href=""http://cikm2013.org/slides/kai.pdf"">Baidu</a>, <a href=""http://www.jpathinformatics.org/article.asp?issn=2153-3539;year=2013;volume=4;issue=1;spage=9;epage=9;aulast=Malon;type=0"">NEC</a>, <a href=""https://engineering.twitter.com/research/publications"">Twitter</a>, <a href=""http://labs.yahoo.com/publication/learning-features-and-parts-for-fine-grained-recognition/"">Yahoo!</a>….

That said, it’s true that all of these companies have significant R&amp;D resources and that training convnets can still be challenging for smaller companies or companies that are less technically advanced.

It still requires quite of bit of experience and time investment to train a convnet if you don’t have prior training. Soon however, there will be several simple to use open source packages with efficient back-ends for that.
<h3>Are we close to the limit for convnets? Or could CIFAR-100 be ""solved"" next?</h3>
I don’t think it’s a good test. <a href=""https://www.kaggle.com/c/dogs-vs-cats/forums/t/6845/example-use-decaf-via-nolearn-for-94-accuracy/37703"">ImageNet</a> is a much better test.
<h3>Shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional architectures. <a href=""http://arxiv.org/pdf/1312.6184v5.pdf"">Do deep nets really need to be deep?</a></h3>
Yes, deep nets need to be deep. Try to train a shallow net to emulate a deep convnet trained on ImageNet. Come back when you have done that. In theory, a deep net can be approximated by a shallow one. But on complex tasks, the shallow net will have to be be ridiculously large.
<h3>Most of your academic work is highly practical in nature. Is this something you purposefully aim for, or is this an artefact of being employed by companies? Can you tell about the distinction between theory and practice?</h3>
Hey, I’ve been in academia since 2003, and I’m still a part-time professor at NYU. I do theory when it helps me understand things. Theory often help us understand what’s possible and what’s not possible. It helps suggest proper ways to do things.

But sometimes theory restricts our thinking. Some people will not work with some models because the theory about them is too difficult. But often, a technique works well before the reasons for it working well are fully understood theoretically.

By restricting yourself to work on stuff you fully understand theoretically, you are condemned to using conceptually simple methods.

Also, sometimes theory blinds us. For example, some people were dazzled by kernel methods because of the cute math that goes with it. But, as I’ve said in the past, in the end, kernel machines are shallow networks that perform “glorified template matching”. There is nothing wrong with that (SVM is a great method), but it has dire limitations that we should all be aware of.

[caption id=""attachment_4732"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2014/12/slide-invariant.png""><img class=""wp-image-4732 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2014/12/slide-invariant.png"" alt=""slide-invariant"" width=""500"" height=""368"" /></a> A slide from <em>LeCun Y. (2013) <a href=""http://www.slideshare.net/yandex/yann-le-cun"">Learning Hierarchies from Invariant Features</a></em>[/caption]
<h3>What is your opinion on a well-performing convnet without any theoretical justifications for why it should work so well? Do you generally favor performance over theory? Where do you place the balance?</h3>
I don’t think their is a choice to make between performance and theory. If there is performance, there will be theory to explain it.

Also, what kind of theory are we talking about? Is it a <a href=""http://en.wikipedia.org/wiki/Stability_%28learning_theory%29#History"">generalization bound</a>? Convnets have a finite VC dimension, hence they are consistent and admit the classical VC bounds. What more do you want? Do you want a tighter bound, like what you get for SVMs? No theoretical bound that I know of is tight enough to be useful in practice. So I really don’t understand the point. Sure, generic VC bounds are atrociously non tight, but non-generic bounds (like for SVMs) are only slightly less atrociously non tight.

If what you desire are convergence proofs (or guarantees), that’s a little more complicated. The loss function of multi-layer nets is non convex, so the easy proofs that assume convexity are out the window. But we all know that in practice, a convnet will almost always converge to the same level of performance, regardless of the starting point (if the initialization is done properly). There is theoretical evidence that there are lots and lots of equivalent local minima and a very small number of “bad” local minima. Hence convergence is rarely a problem.
<h3>What is your opinion on AI hype. Which practices do you think are detrimental to the field (of AI in general and specifically convnets)?</h3>
AI hype is extremely dangerous. It killed various approaches to AI at least 4 times in the past. I keep <a href=""https://plus.google.com/+YannLeCunPhD/posts/Qwj9EEkUJXY"">calling out hype</a> whenever I see it, whether it’s from the press, from startups looking for investors, from large companies looking for PR, or from academics looking for grants.

There is certainly quite a bit of hype around deep learning at the moment. I don’t see a particularly high level of hype around convnets specifically. There is more hype around “cortical this”, “spiking that”, and “neuromorphic blah”. Unlike many of these things, convnets actually yield good results on useful tasks and are widely deployed in industrial applications.
<h3>Any interesting projects at Facebook involving convnets that you could talk a little more about? Some basic stats about the size?</h3>
<a href=""http://valse.mmcheng.net/ftp/20141126/MingYang.pdf"">DeepFace</a>: a convnet for face recognition. There are also convnets for image tagging. They are big.

[caption id=""attachment_4733"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2014/12/deepface.jpg""><img class=""wp-image-4733 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2014/12/deepface.jpg"" alt=""deepface"" width=""600"" height=""229"" /></a> A figure describing the architecture from the presentation ""<em>Taigman Y., Yang M., Ranzato M., Wolf L. (2014) DeepFace for Unconstrained Face Recognition</em>"".[/caption]
<h3>Recently you posted about <a href=""https://www.facebook.com/yann.lecun/posts/10152348155137143"">4 types of serious researchers</a>. How would you label yourself?</h3>
I’m a 3, with a bit of 1 and 4.
<ol>
	<li><i>""People who want to explain/understand learning (and perhaps intelligence) at the fundamental/theoretical level.</i></li>
	<li><i>People who want to solve practical problems and have no interest in neuroscience.</i></li>
	<li><i>People who want to understand intelligence, build intelligent machines, and have a side interest in understanding how the brain works.</i></li>
	<li><i>People whose primary interest is to understand how the brain works, but feel they need to build computer models that actually work in order to do so.""</i></li>
</ol>
<h3>Anything you wish to say to the top contestants in the CIFAR-10 challenge? Anything you wish to say to (hobbyist) researchers studying convnets? Anything in general you wish to say about the CIFAR dataset/problem?</h3>
I’m impressed by how much creativity and engineering knack went into this. It’s nice that people have pushed the technique as far as it will go on this dataset.

But it’s going to get easier and easier for independent researchers and hobbyist to play with these things and apply them to larger datasets. I think the successor to CIFAR-10 should be ImageNet-1K-128x128. This would be a version of the <a href=""http://www.image-net.org/challenges/LSVRC/2011/"">1000 category ImageNet classification task</a> where the images have been normalized to 128x128. I see several advantages:
<ol>
	<li>the networks are small enough to be trainable in a reasonable amount of time on a high-end gamer rig;</li>
	<li>the network you get at the end can actually be used for useful application (like robot vision);</li>
	<li>the network can be run in real time on embedded platforms, like smart phones or the <a href=""https://developer.nvidia.com/jetson-tk1"">NVIDIA Jetson TK1</a>.</li>
</ol>
[caption id=""attachment_4734"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2014/12/imagenet.jpg""><img class=""wp-image-4734 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2014/12/imagenet.jpg"" alt=""imagenet"" width=""600"" height=""489"" /></a> Predictions on ImageNet. From ""<em>Krizhevsky A., Sutskever I., Hinton. G.E. (2012) <a href=""http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"">ImageNet Classification with Deep Convolutional Neural Networks</a></em>"".[/caption]
<h3>The need to have large amounts of labeled data can be a problem. What is your opinion on nets trained on unlabeled data, or the automatic labeling of data through image search engines?</h3>
There are tasks like video understanding and natural language understanding where we are going to have to use unsupervised learning. But these modalities have a temporal dimension that changes how we can approach the problem.

Clearly, we need to devise algorithms that can learn the structure of the perceptual world without being told the name of everything. Many of us have been working on this for years (if not decades), but none of us has a perfect solution.
<h3>What is your latest research focusing on?</h3>
There are two answers to this question:
<ol>
	<li>Projects I’m personally involved in (enough that I would be co-author on the papers);</li>
	<li>projects that I set the stage for, encourage other work on, and advise at the conceptual level, but in which I am not involved enough to be co-author on a paper.</li>
</ol>
A lot of (1) is at NYU and a lot of (2) is at Facebook.

The general areas are:

unsupervised learning that discovers “invariant” features, the marriage of deep learning and structured prediction, the unification of supervised and unsupervised learning, solving the problem of learning long-term dependencies, building learning systems with short-term/scratchpad memory, learning plans and sequences of actions, different ways to optimize functions than to follow the gradient, the integration of representation learning with reasoning (read Leon Bottou’s excellent position paper “<a href=""http://research.microsoft.com/pubs/192773/tr-2011-02-08.pdf"">from machine learning to machine reasoning</a>”), the use of learning to perform inference efficiently, and many other topics.

<hr />

&nbsp;

<strong><a href=""http://blog.kaggle.com/2015/01/02/cifar-10-competition-winners-interviews-with-dr-ben-graham-phil-culliton-zygmunt-zajac/"">Read an interview</a> with the <a href=""https://www.kaggle.com/c/cifar-10"">CIFAR-10 - Object Recognition in Images</a> competition's first place winner, Dr. Ben Graham. Also includes interviews with top contenders, Phil Culliton and Zygmunt Zając.</strong>

&nbsp;

&nbsp;"
"CIFAR-10 Competition Winners: Interviews with Dr. Ben Graham, Phil Culliton, & Zygmunt Zając",http://blog.kaggle.com/2015/01/02/cifar-10-competition-winners-interviews-with-dr-ben-graham-phil-culliton-zygmunt-zajac/,2015-01-02 20:17:07,"<h2>Dr. Ben Graham</h2>
<a href=""https://www.kaggle.com/users/145599/benjamin-graham"">Dr. Ben Graham</a> is an Assistant Professor in Statistics and Complexity at the <a href=""http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham"">University of Warwick</a>. With a categorization accuracy of 0.95530 he ranked first place.
<h3>Congratulations on winning the CIFAR-10 competition! How do you feel about your victory?</h3>
Thank you! I am very pleased to have won, and quite frankly pretty amazed at just how competitive the competition was.

When I first saw the competition, I did not think the test error would go below about 8%. I assumed 32x32 pixels just wasn't enough information to identify objects very reliably. As it turned out, everyone in the top 10 got below 7%, which is roughly <a href=""http://karpathy.ca/myblog/?p=160"">on a par with human performance</a>.
<h3>Can you tell us about the setup of the network? How many layers?</h3>
It is a deep convolutional network trained using <a href=""http://arxiv.org/abs/1409.6070"">SparseConvNet</a> with architecture:
<pre>input=(3x126x126) -
320C2 - 320C2 - MP2 -
640C2 - 10% dropout - 640C2 - 10% dropout - MP2 -
960C2 - 20% dropout - 960C2 - 20% dropout - MP2 -
1280C2 - 30% dropout - 1280C2 - 30% dropout - MP2 -
1600C2 - 40% dropout - 1600C2 - 40% dropout - MP2 -
1920C2 - 50% dropout - 1920C1 - 50% dropout - 10C1 - Softmax output
</pre>
It was trained taking advantage of:
<ul>
	<li>spatial-sparsity in the 126x126 input layer,</li>
	<li>batchwise dropout,</li>
	<li>(very) leaky rectified linear units, and</li>
	<li>affine spatial and color-space training data augmentation.</li>
</ul>
The same architecture produces a test error of 20.68% for CIFAR-100.

[caption id=""attachment_4741"" align=""aligncenter"" width=""512""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/caterrors.png""><img class=""wp-image-4741 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/caterrors.png"" alt=""caterrors"" width=""512"" height=""256"" /></a> These cats evaded the DeepCNet solution by looking a lot like a fighter jet and a car.[/caption]
<h3>Can you tell us a little about the hardware used to train the nets? How long did it take to train? What was the development cycle like?</h3>
The network took about 90 hours to train on an <a href=""http://timdettmers.wordpress.com/2014/08/14/which-gpu-for-deep-learning/"">NVIDIA GeForce GTX</a> 780 graphics card. I had already written a convolutional neural network for spatially-sparse inputs to learn to <a href=""http://arxiv.org/pdf/1308.0371.pdf"">recognise online Chinese handwriting</a>.

Over the course of the competition I upgraded the program to allow <a href=""http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"">dropout</a> to be applied batchwise, and cleaned up some kernels that were accessing memory inefficiently. That made it feasible to train pretty large networks.

Which papers/approaches authored by other scientists did contribute the most to your top score?

The network architecture is the result of borrowing ideas from a number of recent papers
<ul>
	<li><a href=""http://people.idsia.ch/~juergen/cvpr2012.pdf"">Multi-column deep neural networks for image classification</a>; Ciresan, Meier and Schmidhuber</li>
	<li><a href=""http://arxiv.org/abs/1312.4400"">Network In Network</a>; Lin, Chen and Yan</li>
	<li><a href=""http://arxiv.org/abs/1409.1556"">Very Deep Convolutional Networks for Large-Scale Image Recognition</a>; Simonyan and Zisserman.</li>
</ul>
Reading each of those papers was jaw-dropping as the ideas would not have occurred to me.

[caption id=""attachment_4742"" align=""aligncenter"" width=""592""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/correct.png""><img class=""wp-image-4742 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/correct.png"" alt=""correct"" width=""592"" height=""237"" /></a> These images were all correctly classified. To the net they look the most like their respective classes. From DeepCNet's extremes.[/caption]
<h3>Where do you see convnets in the future? Anything in particular that you are excited about?</h3>
I am very interested in the idea of <a href=""http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham/sparsecnn.pdf"">spatially-sparse</a> 3d convolutional networks. For example, given a length of string, you might be able to pull both ends to produce a straight line. Alternatively, the string might contain a knot which you cannot get rid of no matter how hard you pull. That is an idea that is obvious for humans, but hard to solve by computer as there are so many different kinds on knots.

Hopefully 3d convolutional networks can develop some of the physical intuition humans take for granted.

Besides convnets, I am very interested in machine learning techniques for time-series data, such as recurrent neural networks.
<h3>Thank you very much for sharing your <a href=""https://www.kaggle.com/c/cifar-10/forums/t/10493/train-you-very-own-deep-convolutional-network/56310"">code</a> on the forums. What is your opinion on sharing code?</h3>
My pleasure; it was nice to see a couple of the other teams in the top 10 (""<a href=""https://www.kaggle.com/users/103869/jiki"">Jiki</a>"" and ""<a href=""http://www.kaggle.com/users/115173/phil-culliton"">Phil</a> &amp; <a href=""http://www.kaggle.com/users/114978/triskelion"">Triskelion</a> &amp; <a href=""https://www.kaggle.com/users/111640/kazanova"">Kazanova</a>"") use the code. Another Kaggler, <a href=""http://www.kaggle.com/users/160106/nagadomi"">Nagadomi</a>, also made his <a href=""https://github.com/nagadomi/kaggle-cifar10-torch7"">code</a> available during the competition. It was fascinating to see him implement some of the ideas to come out of the <a href=""http://www.image-net.org/challenges/LSVRC/2014/"">ILSVRC2014 competition</a> such as ""C3-C3-MP2"" layers and <a href=""http://www.image-net.org/challenges/LSVRC/2014/eccv2014"">Inception layers</a>.
<h3>Do you think your convnet could be improved even more on this task, or do you feel it is close to its limit?</h3>
After the competition, I re-ran my top network on the 10,000 images from the original CIFAR-10 test set, resulting in 446 errors.

Here is a confusion matrix for showing where the 446 errors come from:
<pre>         airplane automobile  bird   cat deer  dog frog horse ship truck
airplane        0          3    10     2    2    0    2     0   16     3
automobile      1          0     1     0    0    0    0     0    3    12
bird            8          1     0    14   19    8    9     5    2     0
cat             4          1     8     0    9   57   20     2    5     2
deer            3          1    12     7    0    5    4     8    0     0
dog             4          1     7    39   10    0    1     7    1     1
frog            4          0     7     7    3    1    0     1    0     1
horse           6          0     3     4    7    8    0     0    0     0
ship            2          3     2     0    0    1    0     0    0     3
truck           3         20     0     2    0    0    1     0    7     0
</pre>
Looking at some of the 446 misclassified images, it seems that there is plenty of room for improvement in accuracy. I am sure there is also scope for improving the efficiency of the network.
<h3>Which machine learning scientist inspires you?</h3>
Lots of them: Alan Turing, Yann LeCun, Geoffrey Hinton, <a href=""http://cs.stanford.edu/people/ang/"">Andrew Ng</a>, <a href=""http://people.idsia.ch/~juergen/"">Jürgen Schmidhuber</a>, <a href=""http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html"">Yoshua Bengio</a>, <a href=""https://research.facebook.com/researchers/1522313361316754/rob-fergus/"">Rob Fergus</a>, <a href=""https://code.google.com/p/cuda-convnet/"">Alex Krizhevsky</a>, <a href=""http://www.cs.utoronto.ca/~ilya/"">Ilya Sutskever</a>, ...
<h3>Anything of note on the competition and/or dataset that you found surprising? An approach that worked unexpectedly well, or perhaps did not work for you?</h3>
I was very surprised how much of a difference fine-tuning (finishing off training the network using a small number of training epochs with a low learning rate and without data augmentation) made.
<h3>Again, thank you very much for sharing your code. Our team would not have beaten the estimated human error rate without it!</h3>
My pleasure. Academia can be a bit antisocial, so it is lovely to see so much enthusiasm going into Kaggle competitions.
<h2>Phil Culliton</h2>
<a href=""http://www.kaggle.com/users/115173/phil-culliton"">Phil Culliton</a> is a game developer and Senior Researcher at an NLP startup. With a score of 0.94120 his team scored 6th place.
<h3>Can you tell about the architecture of the net? Number of layers etc.?</h3>
Our 6th place submission used multiple iterations (with varying epoch counts) of a single network architecture.

We also used a ""trick"" suggested by Dr. Graham which incorporated a small number of epochs that used no affine transformations.

The network architecture in question was Dr. Graham's spatially sparse CNN. It used 12 LeNet layers and a final softmax layer - it looked roughly like this (this is modified output from Dr. Graham's code):
<pre>LeNetLayer 128 neurons, VeryLeakyReLU
LeNetLayer 128 neurons, VeryLeakyReLU MP2
LeNetLayer 384 neurons, Dropout 0.0833333 VeryLeakyReLU
LeNetLayer 384 neurons, VeryLeakyReLU MP2
LeNetLayer 768 neurons, Dropout 0.208333 VeryLeakyReLU
LeNetLayer 768 neurons, VeryLeakyReLU MP2
LeNetLayer 1280 neurons, Dropout 0.3 VeryLeakyReLU
LeNetLayer 1280 neurons, VeryLeakyReLU MP2
LeNetLayer 1920 neurons, Dropout 0.4 VeryLeakyReLU
LeNetLayer 1920 neurons, VeryLeakyReLU MP2
LeNetLayer 2688 neurons, Dropout 0.5 VeryLeakyReLU
LeNetLayer 2688 neurons, VeryLeakyReLU
LeNetLayer 10 neurons, Softmax Classification
</pre>
The ""MP"" entries above denote max pooling, and ""VeryLeakyReLU"" denotes a ""leaky"" ReLU with a fairly large (alpha was 0.33) non-zero gradient.

DropOut was implemented in a straightforward manner. I considered adding <a href=""http://cs.nyu.edu/~wanli/dropc/"">DropConnect</a> into the mix but ran out of time to test it.

Input images were distorted using a semi-random system of stretching and flipping - I played around with this but also ran out of time to properly validate it.

Earlier in the competition I did attempt to ensemble multiple network architectures but none of them outperformed the top contender.

[caption id=""attachment_4743"" align=""aligncenter"" width=""512""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/errors2.png""><img class=""wp-image-4743 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/errors2.png"" alt=""errors2"" width=""512"" height=""256"" /></a> The cat on the left looks most like a frog. The cats on the right trick the net into thinking it is a boat. From DeepCNet's extreme errors.[/caption]
<h3>What were the technical challenges to overcome to produce submissions for this challenge?</h3>
I mention this again later, but getting CUDA installed and running properly on various machines turned out to be a much bigger task than I thought it would be - it was difficult and time-consuming. I'm an old hand at getting cranky C code to compile - like, say, porting Windows codebases to OSX - so when I say I saw some weird stuff in trying to get CUDA-based libraries to run, I mean it.

Also - in a normal Kaggle competition I try to make use of all of the submissions available to me, even if it's just to try oddball approaches that may or may not work. However, for CIFAR-10, coming up with the machine time was an issue. I farmed the work out over <a href=""http://aws.amazon.com/ec2/instance-types/"">AWS GPU servers</a> as well as multiple local servers, but AWS quickly became expensive and eventually I had to stop using it.

Finding the right ratio of network size / sample batch size / speed for each server also took some care. I discovered that sample batch size (the number of samples sent to the GPU at a time) actually had an effect on final results, although I haven't yet quantified it. I'd be interested in exploring that further.
<h3>Which libraries did you use? Can you give some of the pros and cons?</h3>
For the top submission's neural networks we used Dr. Graham's reference code in CUDA / C++, with variations in parameters and some extremely minor changes.

The biggest pro was speed - we were training simply enormous networks and it could only have worked using GPGPUs. The cons - complexity of setup and installation. Each machine's CUDA install was a new mini-adventure, some of which didn't turn out so well. I hadn't played with CUDA much before, and frankly I'm not too enamored with it. Getting it working properly - and compiling <a href=""http://opencv.org/"">OpenCV</a> with it! - on OSX was ridiculously hard. Eventually I switched over to all-Linux CUDA servers, where the task was marginally easier.

Luckily Dr. Graham's code was very adaptable and didn't have any strange library requirements - several of the other libraries we attempted to use required very specific / old versions of CUDA and would only work if you had a particular compiler, etc., or weren't amenable to running on one platform or another.
<h3>Did you try anything else besides convolutional nets?</h3>
I also tried simple neural networks using <a href=""http://0xdata.com/h2o/"">H2O</a> in R, kNN with <a href=""http://scikit-learn.org/"">scikit-learn</a>, and <a href=""http://hunch.net/~vw/"">Vowpal Wabbit</a>. I'm a pretty heavy user of the latter two, but H2O was new to me. All produced interesting results, but none ground-breaking.

I did really like H2O's deep learning implementation in R, though - the interface was great, the back end extremely easy to understand, and it was scaleable and flexible. Definitely a tool I'll be going back to.
<h3>Did you read any papers for this competition?</h3>
Several. DropOut, DropConnect, and network architecture papers were heavily featured. I had just been doing some NN work in my day job so I got some dual-purpose reading done.

I heartily recommend Dr. Graham's <a href=""http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham/sparsecnn.pdf"">preprint</a> about the architecture we used - you can find it on <a href=""http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham/"">his website</a>.

I spent a fair bit of time on <a href=""http://fastml.com/"">fastml.com</a> as well - their <a href=""http://fastml.com/object-recognition-in-images-with-cuda-convnet/"">articles on CIFAR-10</a> were beyond useful.

[caption id=""attachment_4744"" align=""aligncenter"" width=""435""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/deepcnet-architecture.png""><img class=""wp-image-4744 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/deepcnet-architecture.png"" alt=""deepcnet-architecture"" width=""435"" height=""157"" /></a> The DeepCNet architecture from ""Graham B. (2014) Spatially-sparse convolutional neural networks""[/caption]
<h3>What did you learn from this competition? First time using convnets for a Kaggle competition? Do you think you can apply any knowledge to future competitions?</h3>
This was my first time using convnets for anything! I was impressed with their power and accuracy. I was also impressed at the number of GPU hours (and expense) it took to run a decent-sized network. It certainly isn't for the impatient or faint of heart.

I strongly suspect that deep learning / NNs will bubble toward the top of my toolbox for some problems. Definitely on anything remotely similar to CIFAR I'll be headed to the code from this competition first - probably with an email to Dr. Graham shortly thereafter.
<h3>I heard you approached Dr. Ben Graham midway during the competition and he released code. Can you tell a little about how this came to be?</h3>
Sure! I noticed that Dr. Graham was consistently on the top of the leaderboard and clicked through his Kaggle profile to find out if he was working for an ML company or using a particular product. There wasn't anything on his profile except for a link that was only partially visible, so I hopped on Google and dug around a bit.

It was a slightly convoluted process, but I eventually made my way to <a href=""http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham/"">his website</a> and noted that he had several sets of sample / reference code for dealing with CIFAR-10 that were freely available and accompanied by (rather excellent) write-ups. I grabbed a set and started trying to work with it, had some problems getting it going, and sent him a question. I figured I wouldn't hear back from him - frankly I wasn't sure whether he'd be willing to help his competition.

However, within a few hours he'd sent me a version of the code with all the issues ironed out and some friendly comments! Shortly thereafter he shared that same code on the forums, which was great as that got even more people using it.

We kept in touch during the competition, whenever he updated the code on the forums he'd send me an email letting me know, and encouraging me to keep trying (although by the end of the competition it was pretty clear to me that he was going to win).

He was a great sport, a tremendous help and I'm looking forward to seeing more of his work in the future.
<h2>Zygmunt Zając</h2>
<a href=""http://www.kaggle.com/users/28038/foxtrot"">Zygmunt Zając</a> is the author of <a href=""http://fastml.com/"">FastML</a> and a Machine Learning Researcher. He used DropConnect to improve his accuracy to 0.90660, good for 18th place.
<h3>Can you tell about the architecture of the net? Number of layers etc.?</h3>
I have used models trained by <a href=""http://cs.nyu.edu/~wanli/"">Li Wan</a>, the author of DropConnect. The details are outlined in the paper: <a href=""http://jmlr.org/proceedings/papers/v28/wan13.html"">Regularization of Neural Networks using DropConnect</a>.

[caption id=""attachment_4745"" align=""aligncenter"" width=""585""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/model-layout.png""><img class=""wp-image-4745 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/model-layout.png"" alt=""model-layout"" width=""585"" height=""249"" /></a> Figure from the paper: ""Wan L., <a href=""http://www.matthewzeiler.com/"">Zeiler M</a>., <a href=""https://cs.nyu.edu/~zsx/"">Zhang S.</a>, LeCun Y., <a href=""http://cs.nyu.edu/~fergus/pmwiki/pmwiki.php"">Fergus R.</a> (2013) Regularization of Neural Networks using DropConnect[/caption]
<h3>What were the technical challenges to overcome to produce submissions for this challenge?</h3>
The challenges were getting the data in and the predictions out, as usual. In this case it meant converting raw images into cuda-convnet format and learning how to get the predictions from the library.

On top of that, getting DropConnect code to work was a bit tricky. You can read about the journey here:
<ul>
	<li><a href=""http://fastml.com/object-recognition-in-images-with-cuda-convnet/"">Object recognition in images with cuda-convnet</a></li>
	<li><a href=""http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/"">Regularizing neural networks with dropout and with dropconnect</a></li>
</ul>
<h3>Which libraries did you use? Can you give some of the pros and cons?</h3>
I used Alex Krizhevsky’s <a href=""https://code.google.com/p/cuda-convnet/"">cuda-convnet</a> extended with <a href=""http://cs.nyu.edu/~wanli/dropc/"">Li Wan's code</a>. Cuda-convnet struck me as a very well designed and implemented library.
<h3>Did you try anything else besides convolutional nets?</h3>
No.
<h3>Did you read any papers for this competition?</h3>
Mainly Hinton's et al. <a href=""http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"">dropout paper</a> and Li Wan's et al. <a href=""http://cs.nyu.edu/~wanli/dropc/dropc.pdf"">DropConnect paper</a>. There are other references in the FastML articles mentioned above.
<h3>What did you learn from this competition? Did any knowledge from previous competitions (cats vs. dogs) transfer? Do you think you can apply any knowledge to future competitions?</h3>
It was my first brush with convolutional networks, I gained a general idea of how they work. Also that it isn't as easy to overfit as I thought. ;)

About DropConnect, it seems to offer results similiar to dropout. State of the art scores reported in the paper come from model ensembling.

I went into the <a href=""https://www.kaggle.com/c/dogs-vs-cats"">cats and dogs competition</a> after CIFAR-10, exposure to convnets certainly helped. Generally the knowledge can be directly applied to contests dealing with images.

[caption id=""attachment_4746"" align=""aligncenter"" width=""512""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/errors3.png""><img class=""wp-image-4746 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/errors3.png"" alt=""errors3"" width=""512"" height=""256"" /></a> The dog on the left looks most like a horse. The dog on the right looks most like a cat. From DeepCNet's extreme errors.[/caption]
<h3>I saw you being mentioned on the <a href="" http://cs.nyu.edu/~wanli/dropc/ "">DropConnect page</a>. Can you tell a little more about how this came to be?</h3>
I exchanged a few emails with Li Wan after I asked him for help with getting his code to work. I mentioned the article I was writing and he saw it fit to post a link.
<h2>About the interviews</h2>
These interviews were conducted over email. I would like to thank everyone for taking part in these interviews, and I hope the resulting article may serve as a resource for convolutional nets, the CIFAR-10 dataset and the Kaggle competition.

<strong>Read our interview with a founding father of convolutional nets, Yann LeCun, <a href=""http://blog.kaggle.com/2014/12/22/convolutional-nets-and-cifar-10-an-interview-with-yan-lecun/"">here &gt;&gt;</a></strong>"
"Kaggle InClass: Stanford's ""Getting a Handel on Data Science"" Winners' Report",http://blog.kaggle.com/2015/01/05/kaggle-inclass-stanfords-getting-a-handel-on-data-science-winners-report/,2015-01-05 18:26:09,"<address>On December 3rd, Stanford Data Mining &amp; Analysis course (STATS202) wrapped up a heated <a href=""https://inclass.kaggle.com/"">Kaggle InClass</a> competition, ""Getting a Handel on Data Science"". Beating out 92 other teams, ""TowerProperty"" came in first place. Below, TowerProperty outlines the competition and their journey to the top of the leaderboard.</address><address> </address><address><a href=""https://inclass.kaggle.com/"">Kaggle InClass</a> is provided free of charge to academics as a statistical and data mining learning tool for students. Instructors from any course dealing with data analysis may get involved!</address><address>

<hr />

</address>

[caption id=""attachment_4755"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/Handel12.png""><img class=""wp-image-4755 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/Handel12.png"" alt="""" width=""500"" height=""200"" /></a> Screen shot of the <a href=""http://biglakedata.com/files/kaggle.html"">interactive map</a> we made for exploratory analysis[/caption]
<h2>Team TowerProperty</h2>
The three of us met as students enrolled in the Data Mining &amp; Analysis graduate course at Stanford University. We’re all working professionals and took the course for credit through Stanford’s Center for Professional Development.
<h3>What was the Stanford Stats 202 InClass challenge?</h3>
We were given a database of several thousand accounts with anonymized information about orchestra patrons and their history of subscriptions and single ticket purchases. The challenge was to predict who would subscribe to the 2014-2015 season and at what subscription level. We were given the output variable (total subscriptions) for a subset of the accounts in order to train our model.
<h3>What processing methods did you use?</h3>
From the very beginning, our top priority was to develop useful features. Knowing that we would learn more powerful statistical learning methods as our Stanford course progressed, we made sure that we had the features ready so we would be able apply various models to them quickly and easily. Broadly, the features we developed are grouped as follows:

<strong>Artistic preferences</strong> – We parsed the historical concerts data for the conductor and composer and artists performing at a given time, as well as the composer who’s work was being performed. We then compared this to the number of subscriptions bought that year by account. Our goal was to see if we could classify accounts as Bach, Haydn or Vivaldi-lovers and use that for predictions (perhaps Bach lovers will be more likely to buy subscriptions this year as well because there is another Bach concert in the performance schedule).

<strong>Spatial</strong> <strong>features </strong>– We computed geodesic distances from each account to the orchestra’s venue locations. Even though we were not given street addresses for each account, we able to approximate distance by assigning latitude and longitude coordinates to each account according to the centroid of its billing zip code.

<strong>Derivative features – </strong>Using Hadley Wickam’s <a href=""http://vita.had.co.nz/papers/tidy-data.pdf"">tidy data methods and R packages</a>, we prepared a large set of predictors based on price levels, number of tickets bought by that account during that year, etc. We also smoothed some features to avoid over-fitting. For most of the competition, we used predictors from the last few years. But in the winning model, we actually used only the last two years and average values over the last five years.

When we later applied the boosted decision trees model, we derived additional predictors that expressed the variance in the number of subscriptions bought – theorizing that the decision tree would be more easily able to separate “stable” accounts from “unstable” ones.

We created 277 features in the end, which we applied in different combinations. Surprisingly, our final model used only 18 of them.
<h3>What supervised learning methods did you use?</h3>
Most importantly – and from the very beginning – we used 10-fold cross validation error as the metric to compare different learning methods and for optimization within models.

We started with multiple linear regression models. These simple models helped us become familiar with the data while we concentrated our initial efforts on preparing features for later use.
<h3>We then applied and tested the following methods:</h3>
<ul>
	<li>support vector machines (SVM),</li>
	<li>Bayesian Additive Regression Trees (BART),</li>
	<li>K-Nearest Neighbors (KNN), and</li>
	<li>boosted regression trees</li>
</ul>
We didn’t have much luck with SVM, BART and KNN. Perhaps we did not put enough effort into that, but since we already had very good results from using boosted trees, the bar was already quite high. Our biggest effort soon turned to tuning the boosted regression tree model parameters.

Using cross validation error, we tuned the following parameters: number of trees, bagfrac, shrinkage, and depth. We then tuned the minobinsnode parameter – we saw significant improvements when adjusting the parameter downwards from its default setting.

Our tuning process was both manual and automated. We wrote R scripts that randomly changed the parameters and set of predictors as then computed the 10-fold cross-validation error on each permutation. But these scripts were usually used only as a guide for approaches that we then further investigated manually. We used this as a kind of modified forward selection process.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/Handel2.png""><img class=""aligncenter size-full wp-image-4750"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/Handel2.png"" alt=""Handel2"" width=""975"" height=""474"" /></a>

In the end, model that gave us the lowest 10-fold CV error and won the competition, was surprisingly simple and small. It had only 18 predictors:

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/Handel3.png""><img class=""aligncenter size-full wp-image-4751"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/Handel3.png"" alt=""Handel3"" width=""613"" height=""363"" /></a>
<h3>How did you choose your final submission?</h3>
The rule we used to choose our final two submissions was simple: select the one that produced the lowest public Kaggle score, and the one that gave us the lowest 10-fold cross validation error.

This actually turned out to be somewhat of a tough decision because we had other submissions with better public Kaggle scores. Nevertheless, we decided to trust our 10-fold cross-validation and it turned out to be the right choice. (It was actually a pretty tough decision to choose the model that did not use all of the additional predictors, which we had spent so much time creating!)

In the end, the more complicated model – the one which resulted in our best public score – was also beaten by the simpler one.
<h3>What was your most important insight into the data?</h3>
Probably our most important insight about the data was that there was a pattern to the order of the data we received. In other words, even though the data was anonymized, it still retained useful information from its record sequence.

We noticed this by analyzing the accounts that were buying subscriptions and changing the level of subscriptions over the years, as well as those that bought last year. We observed that the distribution of these values in the file provided was far from random. In fact, the distribution seemed to have a temporal pattern.

The non-random nature of the data file was also manifested in the “missingness” of some spatial features, which can be seen in this plot:

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/Handel4.png""><img class=""aligncenter size-full wp-image-4752"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/Handel4.png"" alt=""Handel4"" width=""975"" height=""245"" /></a>

We felt that decision trees have a natural property of finding the right splits, which can be useful to separate some accounts from the others. So, based on that, we started including “account.num” as a predictor and we discovered that it did in fact make our cross-validation scores significantly better. Actually, in the winning model, this simple predictor turned out to be the fourth most important!
<h3>Were you surprised by any of your insights?</h3>
As the competition progressed, we realized that we were probably using too many predictors. This was somewhat of a surprise to us newbies. We discovered that it was often beneficial to drop a predictor that had high importance in the splits!

We first noticed this phenomenon with our spatial predictors like “billing.city” and “distance-from-venue.” They were always getting chosen as very influential … and yet they were consistently increasing the final CV error. Once we detected this phenomenon, we grew more and more suspicious of any predictor that we were using.

Similarly, it turned out that the more aggressive we were in eliminating older predictors, the smaller the error. In our final model, we used only subscriptions for the 2013-14 and 2012-13 seasons and values averaged over last 5 years.

It turned out that simplifying the model (by removing predictors) actually resulted in substantial improvements.
<h3>What have you taken away from this competition?</h3>
Keep it simple and build a proper cross validation approach into your work.
<h3>What tools did you use?</h3>
We used GitHub to host the code and collaborate (you can take a look at it <a href=""https://github.com/MatthewSchumwinger/towerProperty.git"">here</a>). This was especially important as our team members lived in distant cities. Our code was written entirely in R (although we sometimes used Excel to derive new predictors).

<hr />

About team “TowerProperty”:

<strong>Michal Lewowski</strong> lives in the Seattle-area and works on <a href=""http://bing.com/"" target=""_blank"">bing.com</a> search engines at Microsoft.

<strong>Matt Schumwinger</strong> lives in Milwaukee, Wisconsin and is a data consultant at <a href=""http://www.biglakedata.com"">Big Lake Data</a>.

<strong>Kartik Trasi</strong> lives in the Seattle-area and works on big data analytic platforms for Telecommunications Industry at IBM."
Reviewing the American Epilepsy Society Seizure Prediction Challenge,http://blog.kaggle.com/2015/01/26/american-epilepsy-society-seizure-prediction-challenge/,2015-01-26 18:35:27,"In 2014 Kaggle completed two seizure predictions challenges, one co-organized by <a href=""http://www.upenn.edu/"">UPenn</a>, <a href=""http://www.mayoclinic.org/"">Mayo Clinic</a> and one by the <a href=""https://www.aesnet.org/"">American Epilepsy Society</a>.

Accurate and fast seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives:
<ul>
	<li>Seizures that are quickly detected can be aborted earlier by using a <a href=""http://www.epilepsy.com/learn/treating-seizures-and-epilepsy/devices/responsive-neurostimulation"">responsive neurostimulation device.</a></li>
	<li>Larger amounts of EEG data can be analyzed by doctors.</li>
	<li>Patients can better plan activities when they are notified of an impending seizure.</li>
</ul>
[caption id=""attachment_4771"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/PET_scan.jpg""><img class=""wp-image-4771 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/PET_scan-300x233.jpg"" alt=""PET_scan"" width=""300"" height=""233"" /></a> Positron emission tomography of the brain[/caption]

&nbsp;

In this blog post, we talk with the top three teams from the <a href=""https://www.kaggle.com/c/seizure-prediction"">American Epilepsy Society Seizure Prediction Challenge</a>.

<strong>""[The winning team's results] blew the top off previous efforts. Accurate seizure detection and prediction are key to building effective devices to treat epilepsy.""</strong>

<em>— <a href=""http://www.med.upenn.edu/apps/faculty/index.php/g275/p6213"">Brian Litt</a>, Professor of neurology and bioengineering at the <a href=""http://www.med.upenn.edu/"">University of Pennsylvania</a> in '<a href=""http://www.npr.org/blogs/health/2014/12/10/369654830/a-crowd-of-scientists-finds-a-better-way-to-predict-epileptic-seizures"">A Crowd Of Scientists Finds A Better Way To Predict Seizures</a>'</em>

<strong>""Seizure detection and seizure prediction are two fundamental problems in the field that are poised to take significant advantage of large data computation algorithms and benefit from the concept of sharing data and generating reproducible results.""</strong>

<em>— <a href=""http://www.ninds.nih.gov/find_people/ninds/bio_walter_koroshetz.htm"">Dr. Walter J. Koroshetz</a>, director at the <a href=""http://www.ninds.nih.gov/"">NINDS</a> in '<a href=""http://www.washingtonpost.com/news/to-your-health/wp/2014/12/06/predicting-epileptic-seizures-with-82-percent-accuracy/"">Predicting epileptic seizures with 82 percent accuracy</a>'</em>

<strong>""Working in different countries, we exchanged ideas via e-mail, and agreed on how to best use our submissions during the final days of the contest.""</strong>

<em>— 1st place team, QMSDP</em>

<strong>""My observation is that with the open source tools and learning experience in Kaggle competition, a person can tackle most of the machine learning problems.""</strong>

<em>— 2nd place team, Jialun</em>

<strong>""All of us hold a PhD in our respective areas, and we are forming a new multidisciplinary research group in data science, that is a field where we have shared interests.""</strong>

<em>— 3rd place team, ESAI CEU-UCH</em>

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/seizure-logos.png""><img class=""aligncenter size-full wp-image-4764"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/seizure-logos.png"" alt=""seizure-logos"" width=""600"" height=""111"" /></a>
<h2>1st place team, QMSDP</h2>
<h3>Biographies</h3>
<a href=""http://www.cai.uq.edu.au/tieng"">Dr. Quang Tieng</a> is a Senior Research Officer at the <a href=""http://www.cai.uq.edu.au/"">Centre for Advanced Imaging</a> (CAI) at the University of Queensland. One of his research projects is super-resolution in MRI.

<a href=""http://www.cai.uq.edu.au/chen"">Dr. Min Chen</a> is a Postdoctoral Research Fellow at the CAI at the at the University of Queensland. Her research projects focus on temporal lobe epilepsy.

<a href=""http://www.cai.uq.edu.au/bosshard"">Dr. Simone Bosshard</a> is a Postdoctoral Research Fellow at the CAI at the University of Queensland. One of her research projects involves studying the structural network responsible for generating epileptic discharges.

<a href=""https://www.kaggle.com/users/224424/drew-abbot"">Drew Abbot</a> is a software engineer at <a href=""http://www.ailive.net/"">AiLive</a> in California. This company has worked closely together with Nintendo to create software for the Wii video game console.

<a href=""https://www.kaggle.com/users/77826/phillip-chilton-adkins"">Phillip Adkins</a> is a mathematician and works at AiLive in California. AiLive uses machine learning to facilitate the development of motion recognition packages.
<h3>Background prior to entering challenge</h3>
Quang, Min, and Simone all work at The University of Queensland in Australia.

Phillip and Drew work together at AiLive in CA, USA.
<h3>Solution Summary (as told by the team)</h3>
To begin, note that our team merged together after working the contest independently and combined different approaches and ideas to achieve the final result.

Our winning submission was a weighted average of three separate models: a Generalized Linear Model regression with Lasso or elastic net regularization (via MATLAB's lassoglm function), a Random Forest (via MATLAB's <a href=""http://nl.mathworks.com/help/stats/treebagger-class.html"">TreeBagger</a> implementation), and a bagged set of linear Support Vector Machines (via Python's <a href=""http://scikit-learn.org"">scikit-learn</a> toolkit).
<h3>Feature selection</h3>
For the Lasso GLM model, the features were as follows:
<ol>
	<li>Spectrum and Shannon's entropy at six frequency bands: delta (0.1-4Hz), theta (4-8Hz), alpha (8-12Hz), beta (12-30Hz), low-gamma (30-70Hz) and high gamma (70-180Hz).</li>
	<li>Spectral edge power of 50% power up to 40Hz.</li>
	<li>Shannon's entropy at dyadic frequency bands.</li>
	<li>Spectrum correlation across channels at dyadic frequency bands.</li>
	<li>Time-series correlation matrix and its eigenvalues.</li>
	<li>Fractal dimensions.</li>
	<li>Hjorth parameters: activity, mobility and complexity.</li>
	<li>Statistical moments: skewness and kurtosis.</li>
</ol>
For the bagged SVM model, the features involved a kernel PCA decomposition of the below features.

The features for the Random Forest model were also a combination of time- and frequency-domain information, and were chosen as:
<ol>
	<li>Sums of FFT power over hand-picked bands spanning frequencies: f0 (fundamental frequency of FFT), 1Hz, 4Hz, 8Hz, 16Hz, 32Hz, 64Hz, 128Hz and Nyquist. DC was also included, yielding 9 bands per channel.</li>
	<li>Time-series correlation matrix.</li>
	<li>Time-series variance.</li>
</ol>
<h3>Tools used</h3>
MATLAB and Python with Scikit-learn.
<h3>Continued advancement</h3>
Once the contest was over, we realized that using 10 windows (or, simply the 1-minute window) for all subjects actually yielded a better private LB score than the 12- and 150-window choice for dogs and humans, respectively.

We decided that interpolating the signal by a factor of K before taking the final p-norm was worth trying, and indeed, marginal public LB improvements were achieved after doing so (using cubic spline interpolation). In the end, we decided to use Random Forest models trained on 31/32 overlapped preictal and interictal features to classify 63/64 overlapped test features (yielding 4732 and 4737 samples for each 10-minute segment), and interpolate and p-norm those scores for our final Random Forest model.

Interestingly, as overlap and interpolation increased, the optimal p used in the p-norm seemed to increase as well, and our final choices for K and p ended up being 8 and 23, respectively.
<h3>Further reading on the winning solution</h3>
To see a detailed description of the solution, together with code, look at this <a href=""https://github.com/drewabbot/kaggle-seizure-prediction/"">Github Repo</a>.
<h2>2nd place team, Jialun He</h2>
<h3>Biography</h3>
<a href=""https://www.linkedin.com/pub/jialun-he/3/94b/a8"">Jialun He</a> received a Ph.D from MIT in 2003 and is currently a senior algorithm engineer at <a href=""http://hemedex.com/"">Hemedex, Inc</a>.
<h3>What was your background prior to entering this challenge?</h3>
I am a senior algorithm engineer at Hemedex, Inc in the past ten years, where I work on the development of a monitoring device used to measure real time tissue blood flow level. The device is used in neurosurgery and neurointensive care, as well as organ transplant, reconstructive surgery and oncology. I have an interdiscipline background in mechanical and biomedical engineering where I got my Ph.D degree from MIT in 2003. In recent years my focus is on extracting useful information from patient data recoded by our device. My interests are in big data application, especially in healthcare and wearable device
<h3>What made you decide to enter?</h3>
Several years ago I worked on a project for seizure detection using cerebral blood flow (CBF) rate recorded by our devices. Seizure is generally detected with EEG data recorded at a frequency ranging from 100 to 1000Hz. Our monitor records CBF at 1Hz. When patient data labelled with seizure came in, I found that the patient is also experienced high fluctuation in CBF. Seizure CBF chart is very similar to seizure EEG chart, at different time scale. The seizure CBF waveform can also be decomposed into various frequency bands similar to EEG’s wave bands. Anyway, the seizure detection project was successful and it has been implemented in an automatic system for analyzing incoming patient data. So when I found out that Kaggle hosted a competition for seizure detection, I would like to see what I could do with EEG data

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/01/DogRecordingSystem.png""><img class=""aligncenter wp-image-4769 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/DogRecordingSystem-1024x787.png"" alt="""" width=""640"" height=""492"" /></a>
<h3>What preprocessing and supervised learning methods did you use?</h3>
This competition is all about feature engineering. The core features are the power spectral band. Other candidates of features are signal correlation between EEG channels and eigenvalue of the correlation matrix. Both in frequency domain and time domain.
Several common classifiers in scikit-learn package have been tested, such as random forest, gradient tree boosting, support vector machine. Most of them had really good CV score for individual subject. But did not get good score in LB. The gaps between CV score and leaderboard (LB) score were very big. One of the reason is that LB score is across all subject. Other possible reason is due to overfitting. My best submissions according to the LB score were based on support vector machine with RBF kernel, which produced better results because of more control in balancing bias and variance
<h3>What was your most important insight into the data?</h3>
Due to very limited amount of training cases, for example, each patient data only has 3 independent seizure occurrences, it is very important to keep a delicacy balance between bias and variance. With this in mind, I added additional signal processing procedures in feature extraction. I resample the signal from 400Hz in dog and 5000Hz in patient to 100Hz. I split the data into longer window of 50 seconds. I also resample the frequency band of power spectral. Those signal processing procedures all helped reducing overfitting
Another challenge of the competition is that the evaluation matrix is based on AUC cross all subjects. I have tried a cross subject classifier but the score is not good compared to classifiers built on individual subject. My best submission is based on individual classifier. Additional calibration is needed to align predict across subjects.
<h3>Were you surprised by any of your insights?</h3>
I was surprised by the final shake up of the leader board when the final scores were revealed. Many competitors’ final scores were reduced dramatically due to overfitting. It was not a surprise for me that I was among the group of competitors that had least amount of overfitting. I had to admitted that luck was also a factor in determine who could be the final prize winner.
<h3>Which tools did you use?</h3>
I use Python and standard packages of numpy, scipy, scikit-learn and matplotlib. I also have a homemade neural network system
<h3>What have you taken away from this competition?</h3>
Start early is my advice for anyone who wants to enter a Kaggle competition. I started a month before the end of the competition. I was in a rush every day. At the end of the competition I still have some ideals that have not been implemented. My guess is that I need at least two months to explore all the possible ideals and have a chance for good ensemble.

Good old school technique in signal processing helps me a lot in this competition. Domain knowledge in patient monitoring also help me understand the nature of the problem. However, what impressed me most is that folks with limited amount of domain knowledge also did pretty well in the competition. My observation is that with the open source tools and learning experience in Kaggle competition, a person can tackle most of the machine learning problems.
<h2>3rd place team, ESAI-CEU-UCH</h2>
<h3>Biographies</h3>
<a href=""http://www.uchceu.es/directorio/fj-munoz-almaraz"">Javier Muñoz-Almaraz</a>: PhD in Mathematics with a dissertation about numerical continuation of periodic orbits. Now, he is interested in optimization problems related with data analysis, dynamical systems in mechanics and neuronal dynamics.

<a href=""http://www.uchceu.es/directorio/francisco-zamora-martinez"">Francisco Zamora-Martínez</a>: PhD in Computational linguistics, application of artificial neural networks to language modeling for handwriting recognition, spoken language understanding and machine translation. He is interested in machine learning, energy efficiency, pattern recognition and data science problems.

<a href=""http://www.uchceu.es/directorio/juaparal"">Juan Pardo</a>: PhD in Computer Science Engineering. He has been working in several European research projects in different fields. He is director of the department of Physics, Mathematics and Computing at university. Interested in data science. Volunteer at ISACA and PMI organizations.

<a href=""http://www.uchceu.es/directorio/pbotella"">Paloma Botella-Rocamora</a>: PhD in Mathematics, specialist in Statistics. She has been working in Health research projects a long time. She was visiting researcher last year at Bio-statistics Dpt. at University of Minnesota. Interested in Bayesian statistics in data science.

<img class=""aligncenter size-full wp-image-4766"" src=""http://blog.kaggle.com/wp-content/uploads/2015/01/seizure-signal.png"" alt=""seizure-signal"" width=""600"" height=""396"" />
<h3>What was your background prior to entering this challenge?</h3>
We are a multidisciplinary research group (ESAI) composed by lecturers at Universidad CEU Cardenal Herrera, in Valencia (Spain).

Paloma Botella-Rocamora and Javi Muñoz-Almaraz are mathematicians, Paloma more focused on statistics and Javi on optimization and dynamical systems. Juan Pardo and Francisco Zamora-Martínez are informatics, Juan more focused on computer engineering and Francisco in computer science.

All of us hold a PhD in our respective areas, and we are forming a new multidisciplinary research group in data science, that is a field where we have shared interests.

We are interested in the application of Bayesian methods, deep learning and optimization methods to solve challenging problems like the proposed in this competition.
<h3>What made you decide to enter?</h3>
From a technical point of view, we wanted to test the team skills and show that it is possible to produce a competitive system combining ideas from different (but related) research areas. Additionally, we tried to apply deep learning techniques to the challenge, whose benefit to this task remains unclear after the competition results analysis.

On the other hand, the competition has been important to let us to work in a common problem and find the way to speak the same language (members work in different areas). And of course the money, as our budget for research is very very limited due to the economical crisis there is in Spain.
<h3>What preprocessing and supervised learning methods did you use?</h3>
We tried different preprocessing techniques. First, we started with <a href=""https://www.youtube.com/watch?v=vQLH7qTeJRM"">Fast Fourier Transform</a>(FFT) of the data over 50% overlapped sliding windows with 60 seconds length.

This transformation produced a very large number of features, and a filter bank with 6 filters has been applied to avoid dimensionality problems. This preprocessing was insufficient to achieve the high AUC results of top10 teams.

We played with eigen values of correlation matrices computed over the same sliding windows as FFT. The combination of both features in the same model was also important to improve the system results, but not enough to be competitive.

The high correlation between windows and filters suggests that our models can be improved by removing these correlations in the data, so we decided to apply Principal Component Analysis (PCA) and Independent Component Analysis (ICA) to the FFT output. Both transformations showed similar performance, and the system achieved the top20 of public test leaderboard.

Just to improve a little bit the results, we decided to compute different bunch of statistics over the whole input data, without windowing, and finally combined different models and preprocessing techniques in an ensemble.

Regarding to supervised learning methods, we start trying logistic regression models, expecting that linear models wouldn't overfit, and they could be used as a nice baseline. However, our surprise was that this simple logistic regression models achieved so high AUC scores in cross-validation (0.93), but in public test data the AUC dropped to very low values (approx. 0.60). We were very confused because of this result, and discussed during the whole competition about why it happened, but we couldn't realize any clear explanation.

Following logistic regression, we tried K-nearest-neighbors (KNNs), but computing class probabilities instead of distances. The drop between our cross-validation AUC and the public test AUC was reduced by using KNNs, but not so much. Finally, we trained Artificial Neural Networks (ANNs) with different number of layers, using dropout to avoid overfitting and ReLU activation functions. After a hard manual optimization of these ANN models, we obtained our best single model result.

Besides the exploration stated above, the combination of KNNs, ANNs using FFT, PCA, correlations, and other statistics, in an ensemble optimized following Bayesian Model Combination (BMC) was our ticket to be in the top15 in public leaderboard, but 4th place in private leaderboard, and 3rd prize after the winner rejected its first prize.

We found that the ensemble of different knowledge sources was a nice way to ensure good stability between public and private AUC. (See the code in the <a href=""https://github.com/ESAI-CEU-UCH/kaggle-epilepsy"">Github Repo</a>)
<h3>What was your most important insight into the data?</h3>
We found that all the channels of the EEG were very correlated, and this correlation could harm the supervised statistical learning. The use of PCA or ICA to reduce this correlation is a way to ensure better performance. However, another ways to exploit channels similarity, and to reduce their dimensionality, could be explored.

As it was discussed in the <a href=""http://www.kaggle.com/c/seizure-prediction/"">forum</a>, a global model, able to learn from all the available subjects, would be a very important step forward, but this exploration remains in the future work of this task, at least for us.
<h3>Were you surprised by any of your insights?</h3>
We were surprised about the logistic regression behavior using our features, the large drop between cross-validation and public test AUC was very disturbing. It has complicated the internal comparison between our different approaches.
<h3>Which tools did you use?</h3>
We used two main tools, R for statistical preprocessing and <a href=""https://github.com/pakozm/april-ann"">APRIL-ANN</a> for FFT and supervised learning. This last tool is a brand new development where members of the research team are involved.
<h3>What have you taken away from this competition?</h3>
We realize that it is very important to stabilize the system results by using ensembles, and that ensembles of different preprocessing pipelines can be even better. Following this methodology, it is easy to share knowledge and skills in multidisciplinary teams, and it resulted in a way to improve the system results to be in the top10.
<h2>Further Reading &amp; Resources</h2>
<ul>
	<li><a href=""http://www.kaggle.com/c/seizure-prediction/forums/t/10945/congratulations-to-the-winners"">Congratulations to the Winners!</a></li>
	<li><a href=""https://www.kaggle.com/c/seizure-detection/forums/t/10079/features-for-seizure-detection"">Features for Seizure Detection</a></li>
	<li>Howbert JJ, Patterson EE, Stead SM, Brinkmann B, Vasoli V, Crepeau D, Vite CH, Sturges B, Ruedebusch V, Mavoori J, Leyde K, Sheffield WD, Litt B, Worrell GA (2014) <a href=""http://www.ncbi.nlm.nih.gov/pubmed/24416133"">Forecasting seizures in dogs with naturally occurring epilepsy</a>. PLoS One 9(1):e81920.</li>
	<li>Cook MJ, O'Brien TJ, Berkovic SF, Murphy M, Morokoff A, Fabinyi G, D'Souza W, Yerra R, Archer J, Litewka L, Hosking S, Lightfoot P, Ruedebusch V, Sheffield WD, Snyder D, Leyde K, Himes D (2013) <a href=""http://www.thelancet.com/journals/laneur/article/PIIS1474-4422%2813%2970075-9/abstract"">Prediction of seizure likelihood with a long-term, implanted seizure advisory system in patients with drug-resistant epilepsy</a>: a first-in-man study. LANCET NEUROL 12:563-571.</li>
	<li>Park Y, Luo L, Parhi KK, Netoff T (2011) <a href=""http://www.ncbi.nlm.nih.gov/pubmed/21692794"">Seizure prediction with spectral power of EEG using cost-sensitive support vector machines.</a> Epilepsia 52:1761-1770.</li>
	<li>Davis KA, Sturges BK, Vite CH, Ruedebusch V, Worrell G, Gardner AB, Leyde K, Sheffield WD, Litt B (2011) <a href=""http://www.ncbi.nlm.nih.gov/pubmed/21676591"">A novel implanted device to wirelessly record and analyze continuous intracranial canine EEG</a>. Epilepsy Res 96:116-122.</li>
	<li>Andrzejak RG, Chicharro D, Elger CE, Mormann F (2009) <a href=""http://www.dtic.upf.edu/~ralph/betterthanchance.pdf"">Seizure prediction: Any better than chance?</a> Clin Neurophysiol.</li>
	<li>Snyder DE, Echauz J, Grimes DB, Litt B (2008) <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2888045/"">The statistics of a practical seizure warning system</a>. J Neural Eng 5: 392-401.</li>
	<li>Mormann F, Andrzejak RG, Elger CE, Lehnertz K (2007) <a href=""https://fenix.tecnico.ulisboa.pt/downloadFile/3779571469443/SeizurePrediction.pdf"">Seizure prediction: the long and winding road.</a> Brain 130: 314-333.</li>
	<li>Haut S, Shinnar S, Moshe SL, O'Dell C, Legatt AD. (1999) <a href=""http://www.ncbi.nlm.nih.gov/pubmed/10612353"">The association between seizure clustering and status epilepticus in patients with intractable complex partial seizures.</a> Epilepsia 40:1832-1834.</li>
</ul>"
Defending Champions Winners' Story: Helping Santa's Helpers,http://blog.kaggle.com/2015/02/24/defending-champions-winners-story-helping-santas-helpers/,2015-02-25 01:36:20,"<h5><a href=""http://blog.kaggle.com/wp-content/uploads/2015/02/santas_helpers.jpg"">
</a>This past December, the defending champions of Kaggle's annual holiday competition swept all three prizes in the <a href=""https://www.kaggle.com/c/helping-santas-helpers"" target=""_blank"">Helping Santa's Helpers</a> optimization challenge and claimed $20,000. In their own words, <a href=""http://www.mimuw.edu.pl/~mucha"" target=""_blank"">Marcin Mucha</a> and <a href=""http://www.mimuw.edu.pl/~cygan"" target=""_blank"">Marek Cygan</a> of team Master Exploder walk us through their winning approach.</h5>
[caption id=""attachment_4775"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/02/santas_helpers.jpg""><img class=""size-medium wp-image-4775"" src=""http://blog.kaggle.com/wp-content/uploads/2015/02/santas_helpers-300x234.jpg"" alt=""How many toys can you buy with $20,000 in prize money?"" width=""300"" height=""234"" /></a> Santa brought team Master Exploder ""the power of the lower bounds"" for Christmas.[/caption]

<b>What was your background prior to entering this challenge?</b>

We are both active researchers in the field of algorithmics. We are particularly interested in ways of dealing with computational hardness: mainly approximation algorithms, and in Marek’s case parametrized complexity. We both work at the University of Warsaw.

We also both have a long history of competing in all kinds of programming contest. Quick Topcoder style contest, ACM ICPC, marathons, 24 hour challenges - we have done all of these many times.

<b>What made you decide to enter this competition?</b>

Considering our fixation with programming contest it is not very surprising that we wanted to try our luck with Kaggle’s Christmas Santa challenges. In fact, these contests seem perfect for us. Not only the problems are computationally hard, but also the long duration gives these contest a bit of a research flavor.

The first one we have entered was last year’s “<a href=""https://www.kaggle.com/c/packing-santas-sleigh"">Packing Santa’s Sleigh</a>”. We have had tons of fun competing and in the end we managed to grab the top spot. After that experience, we have been eagerly awaiting the next challenge and it did not disappoint.

<b>What algorithmic approaches have you used?</b>

The data in this challenge was rather large - we had to schedule 10 million toys for 900 elves. This made it really hard to directly optimize the total production time. Instead, based on investigating the specific features of the problem and the data, we designed a high level structure that our solution would have, and optimized pieces of this structure separately. To solve these subproblems, we used Integer Linear Programs (ILP), as much as we could. When we were not able to find a reasonable ILP formulation, or when the problem seemed too simple to use one, we resorted to local search/simulated annealing.

<b>What was your most important insight into the data?</b>

Investigating the data was key to designing a good high level solution structure. However, most of the observations we made were rather straightforward. In contrast to the ML contest, there are no magical features here, no interesting patterns either. The key observation was probably the following:

There was a huge number of large toys and producing those, even at the highest speed rating, would take many years. Since all toys arrive during the first year, this makes arrival times almost irrelevant (except for the first year, which needs to be processed separately). This observation significantly simplifies the problem and gives it a “bin packing flavor”.

<b>Were you surprised by any of your findings?</b>

For a long time we used elves with maximum speed rating to produce the largest toys. Then we saw the trailer for the new Marion Cotillard movie “Two days, one night” and thought: “Wow, this is what our elves should do! Get rid of their fellow elves to get salary bonuses!”. Well, not exactly. This might have been a good idea, but the actual idea that we had was that they should work for two days and one night straight. That is 34 hours, including 20 working hours and 14  non-working hours. As it turns out, this leads to the speed rating only dropping from 4.0 to about 1.3 and is much better than just producing a very large toy.

<b>Which tools did you use?</b>

We started by analyzing the data using R. Then, for non-ILP parts of the solution we  used  GCC C++ compiler, and for solving the ILPs we used the interactive version of FICO Xpress Optimizer. We were considering using the C++ solver API, but abandoned that idea - for fast development it is usually better to stick with simplicity. We also used some other tools to analyze the logs generated by our programs and to automate many tasks - mainly standard unix utilities like grep, sort, uniq etc., but also occasionally awk or python.

<b>How did your experience help you succeed in this competition?</b>

We already mentioned in an earlier answer how these Christmas optimization contests are perfect for us because of our research experience. This year’s contest’s problem was particularly suitable for us as it was amenable to ILP approaches. As we do ILP modeling regularly in our research and have a lot of experience with related techniques and tricks, this might have given us an edge over the field.

<b>What have you taken away from this competition?</b>

$20,000. Just kidding, of course we are going to have to pay the taxes. But more seriously:

Why use ILPs instead of, say, local search algorithms? It is not really about the quality of solution, or how fast they are found. The key advantage of using ILPs is that they do not only give you a solution, but also a lower bound. This makes it so much easier to decide when to stop the solver, it also gives you hints as to where your solution might be improved. In principle we knew that already, but this contest let us really feel the power of the lower bounds."
Winner's Interview: BCI Challenge @ NER2015,http://blog.kaggle.com/2015/03/23/winners-interview-bci-challenge-ner2015/,2015-03-23 23:17:09,"<h5>The <a href=""https://www.kaggle.com/c/inria-bci-challenge"" target=""_blank"">Brain-Computer Interface (BCI) Challenge</a> used EEG data captured from study participants who were trying to ""spell"" a word using visual stimuli. As humans think, we produce brain waves that can be mapped to actual intentions. In this competition, Kagglers were given the brain wave data of people with the goal of spelling a word by only paying attention to visual stimuli. This competition was proposed as part of the <a href=""http://neuro.embs.org/2015/"" target=""_blank"">IEEE Neural Engineering Conference (NER2015)</a>.</h5>
[caption id="""" align=""aligncenter"" width=""355""]<img src=""https://kaggle2.blob.core.windows.net/competitions/kaggle/4043/media/P300_Speller.gif"" alt="""" width=""355"" height=""266"" /> Participant spelling performance is highly dependent upon the subject’s attentional effort towards the target item and his/her simultaneous effort to ignore the flashes of the irrelevant items.[/caption]

In this blog, fourth place finisher, Dr. Duncan Barrack, shares his approach and some key strategies that can be applied across Kaggle competitions.
<h3>Biography</h3>
Dr. Duncan Barrack received his PhD in applied maths from the University of Nottingham in the UK in 2010 and is currently a research fellow at the <a href=""http://www.horizon.ac.uk/"" target=""_blank"">Horizon Digital Economy Research Institute</a> at the University of Nottingham.
<h3>What was your background prior to entering this challenge?</h3>
My PhD work involved modelling the signalling mechanism which was thought to be responsible for increasing proliferation rates, as well promoting cell cycle synchrony, in clusters of radial glial cells (a type of brain cell). This involved using tools from non-linear dynamical systems theory to study systems of ordinary differential equations. Since 2011, I have been working as a research fellow at the <a href=""http://www.horizon.ac.uk/"" target=""_blank"">Horizon Digital Economy Research Institute</a>, at the University of Nottingham where I apply statistical and machine learning techniques to solve problems in industry and healthcare.
<h3>How did you get started competing on Kaggle?</h3>
Although I had dabbled with the Titanic and Digit Recognizer 101 competitions a while ago, I really got into Kaggle as part of a big data workshop held at Nottingham University where a number of colleagues and I entered the American epilepsy society seizure prediction challenge.
<h3>What made you decide to enter this competition?</h3>
I had really enjoyed the  American epilepsy society seizure prediction challenge. The BCI challenge started shortly after the epilepsy challenge had finished and as it also involved analysing EEG data it seemed natural to enter. Also, I found the notion that it is possible to use brain signals to communicate with a machine (a concept new to me) extremely interesting.

[caption id=""attachment_4798"" align=""aligncenter"" width=""282""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/03/bci_logo_blog2.png""><img class=""wp-image-4798 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/03/bci_logo_blog2-282x300.png"" alt=""bci_logo_blog2"" width=""282"" height=""300"" /></a> 56 passive EEG sensors captured brain wave data of study participants[/caption]
<h3>What preprocessing and supervised learning methods did you use?</h3>
This competition was all about finding the right features. <strong>Because of this I spent a good deal of time reading the BCI literature to find out about the kind of features used to solve similar problems.</strong> The best features I found were based on simply taking the mean of the EEG signal in each channel over windows of various lengths and lags as well as features based on <a href=""http://www.sciencedirect.com/science/article/pii/0168559794900515"" target=""_blank"">template matching</a>.

I threw a lot of machine learning methods at the problem including logistic regression with elastic net regularisation, tree based methods and SVMs.  <strong>In the end my best performing model was a weighted averaged of two SVMs with linear kernels and different feature sets, although the average of two logistic regression models did almost as well.</strong>
<h3>What was your most important insight into the data?</h3>
The data used to calculate the public leaderboard score came from two subjects only.  With such a small number of subjects it was clear to me and, going by the posts in the forums many others as well, that the public leaderboard score was likely a poor estimator of the private score. For this reason, I took care when it came to my cross validation (CV) procedure as I knew I would be relying on it when choosing my final model.  The training data came from 16 subjects and, for my CV procedure, I split it into 4 ‘subject wise’ folds. I then calculated the AUC score (the evaluation metric used in the competition) for the four subjects in the test fold. I repeated this CV procedure 5 times with different splits and took the average of the 20 AUC scores produced (5 repetitions × 4 folds). The CV score of my best model (~0.75) was very close to to the public leaderboard score (~0.77). This model was also the most stable (the CV score variance was the lowest of all my models) which I saw as a desirable property given that the number of subjects in the test set was also relatively small.
<h3>Were you surprised by any of your findings?</h3>
Because I had tried to be careful with my cross validation procedure, I wasn't too surprised by my final leaderboard score. However, I was surprised (and also very impressed) with how much higher the score of the overfitting avengers team, who finished in top spot on leaderboard, was. Reading about their <a href=""http://www.kaggle.com/c/inria-bci-challenge/forums/t/12819/the-overfitting-avengers-solution-code"" target=""_blank"">approach</a> in the forums really opened my eyes to what was possible. I'm just glad they decided not to accept the prize!
<h3>Which tools did you use?</h3>
For the feature extraction I used Matlab. I used Python with scikit-learn for the modelling.
<h3>What have you taken away from this competition?</h3>
Despite the fact that simple models like logistic regression have been around for ages they can still be extremely effective. This is especially true in completions like this one where it's important not to overfit because results must generalise across data from different subjects.
<h3>Do you have any advice for those just getting started in data science?</h3>
<strong>I think sometimes there is a temptation when you're getting in to data science to use the biggest and baddest model you can as soon as you can when simple models may be more effective.</strong> Also, it's really important to carry out some exploratory data analysis first. This may help spark some ideas on what features may be useful ."
"Predicting March Madness: 1st Place Winner, Zach Bradshaw",http://blog.kaggle.com/2015/04/17/predicting-march-madness-1st-place-finisher-zach-bradshaw/,2015-04-17 21:51:36,"We recently wrapped up our second annual <a href=""https://www.kaggle.com/c/march-machine-learning-mania-2015"" target=""_blank"">March Machine Learning Mania competition</a> with an industry insider finishing at the top of the leaderboard.

First place finisher, <a href=""https://www.kaggle.com/users/311573/zdb-88"" target=""_blank"">Zach Bradshaw</a>, is a Sports Analytics Specialist at ESPN. Prior to joining ESPN, he worked in the basketball analytics departments of the <a href=""http://www.nba.com/suns/"" target=""_blank"">Phoenix Suns</a> and Charlotte Bobcats (now renamed the <a href=""http://www.nba.com/hornets/"" target=""_blank"">Charlotte Hornets</a>). Zach received a Masters of Science in Statistics from <a href=""http://home.byu.edu/home/"" target=""_blank"">Brigham Young University</a> in 2014.

[caption id=""attachment_4832"" align=""aligncenter"" width=""660""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/bball-logo-sm.png""><img class=""wp-image-4832 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/bball-logo-sm.png"" alt=""bball-logo-sm"" width=""660"" height=""310"" /></a> March Machine Learning Mania 2015 had 341 teams and 405 players competing to correctly predict winning percentages for the likelihood of each possible matchup, not just the traditional bracket.[/caption]

&nbsp;
<h2>How did you get started in data science and sports analytics?</h2>
From a young age, I was passionate about sports and enjoyed solving interesting problems. However, it was not until later in college that I had an unanticipated opportunity to intern with an NBA team. Thanks to good timing, hard work, and my previous basketball research, I was fortunate enough to get an internship doing what I love, applying data science in basketball.
<h2>What made you enter the March Mania competition on Kaggle?</h2>
As soon as I heard about the March Mania competition, I wanted to participate due to my interest in basketball and predictive modeling. I had previously done some predictive modeling for NBA games and this competition was the perfect opportunity to apply similar techniques to college basketball.
<h2>How did your industry knowledge help you create your dataset and model?</h2>
My previous experience modeling NBA games guided my approach in the competition. Both the dataset and modeling techniques closely resembled my previous work.
<h2>Did you use any intuition or industry knowledge to manually tweak your output probabilities?</h2>
Using a Bayesian framework allowed for the incorporation of prior knowledge or intuition that was not accounted for in the data. However, in hindsight this hurt my predictions slightly more than it helped, at least in the 2015 tournament. There were no tweaks to the output probabilities of my first entry. However, with my winning entry, I manually tweaked the prediction for the Baylor vs. Georgia State game. With a series of unlikely events at the end of that game, I successfully “predicted” the upset.

[caption id=""attachment_4831"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/Hunter-1.jpg""><img class=""wp-image-4831"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/Hunter-1.jpg"" alt="""" width=""300"" height=""450"" /></a> 14th-seeded Georgia State beat 3rd-seeded Baylor with a 3 point buzzer beater.[/caption]
<h2>How did your experience in sports and sports analytics help you succeed in this competition?</h2>
My experience in sports analytics saved a lot of time in the exploratory phase as I already had a sense of what data and techniques might make a good model. My experience with basketball had a small impact on how I modeled a few nuances of the game. Although my experience in sports analytics was helpful in succeeding in the competition, the gains were marginal and I also needed some good luck to succeed.
<h2>Which tools did you use?</h2>
R and SQL
<h2>Do you have any advice for those just getting started in data science and sports analytics?</h2>
All models are wrong but some are useful, don’t get too caught up in trying to create the perfect model. Taking some time to better understand the problem at hand and its underlying structure is an important and oft overlooked step in the modeling process. For those specifically interested in sports analytics, I think the best way to get started is doing your own research. Like any other industry, connections are important and having some of your own research is the first important step in developing relationships with others in the industry."
AXA Winners' Interview: Learning Telematic Fingerprints From GPS Data,http://blog.kaggle.com/2015/04/20/axa-winners-interview-learning-telematic-fingerprints-from-gps-data/,2015-04-20 22:16:35,"Team Driving It <a href=""https://www.kaggle.com/c/axa-driver-telematics-analysis/leaderboard/private"" target=""_blank"">took second place</a> in the hugely popular <a href=""https://www.kaggle.com/c/axa-driver-telematics-analysis"" target=""_blank"">AXA Driver Telematics</a> competition. This blog shares their experience working as a team to build a ""telematic fingerprint"" for drivers, making it possible to distinguish if a given driver was behind the wheel.
<h4>""<em>Our final telematics model consisted of an ensemble of 6 different models. We mainly used Random Forests, in combination with Gradient Boosting and Logistic Regression.</em>""</h4>
<h4><em>""Collaborating in a team is a great experience, we finished 2nd place because we joined forces. I definitely recommend it."" </em></h4>
&nbsp;

[caption id=""attachment_4839"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/Telematics_axa2.png""><img class=""wp-image-4839 size-medium"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/Telematics_axa2-300x281.png"" alt=""Telematics_axa2"" width=""300"" height=""281"" /></a> 1,528 teams and 1,865 players competed to create a ""telematic fingerprint"" for drivers.[/caption]
<h2>What was your background prior to entering this challenge?</h2>
<b>Scott</b>: I work at an aerospace company as a mechanical engineer. I’ve done a fair amount of coding in Fortran, but have only been using python for a few months.

<b>Janto:</b> I'm a Cognitive Science MSc student. I’m interested in Computational Neuroscience and Machine Learning. My work involves modelling behavioural and brain data as well as applying machine learning. While I mostly work in MATLAB during the day I started coding in Python about 1.5 years ago and it has become my first choice for machine learning tasks (and nearly everything else). Currently, I’m working on my thesis at Oxford University.

<b>Andrei:</b> I work as a cloud architect at Sparked (<a href=""http://sparked.nl"" target=""_blank"">http://sparked.nl</a>), mainly focusing on bringing data analytics and machine learning to mainstream. In parallel, I work on finishing up my PhD thesis as a result of 4 years being a part of the Software Languages Team at the University of Koblenz-Landau in Germany.
<h2>How did you get started competing on Kaggle?</h2>
<b>Scott: </b>I got started in the Christmas Challenge, helping Santa’s helper’s. I wanted to use it as a method of learning python. I picked that challenge because it was more algorithm based than data analysis based.

<b>Janto:</b> This was my first competition on Kaggle. I had played with the Titanic and the MNIST data set to benchmark a few algorithms. My main motivation for joining Kaggle was to test how statistics and machine learning algorithms would perform on real datasets and learn about the strengths and weaknesses of different approaches, hands-on.

<b>Andrei: </b>Actually, this was my first competition. I like the automotive and related domains and decided to join.
<h2>What made you decide to enter this competition?</h2>
<b>Janto:</b> I liked the fact that the data was unlabeled which I found challenging. With only the raw GPS data given the competition left room for a lot of creative work in feature engineering, local testing and trip matching which became evident in the diversity of ideas on the forums. I expected the challenge to be quite different from the ""supervised learning toolbox-battles"".

<b>Scott:</b> I chose to do this analysis because it appeared to me that path matching would be a dominant method of ranking the routes. Since there is no out of the box path matching tools that I know of, it would be much more algorithm based and somewhere I could add value. I contrast this with data analysis techniques like feature generation, random forests, gradient boosted trees, etc. Which at the time I was not familiar with.

<b>Andrei:</b> The motivation was twofold. First, I enjoy things happening in the automotive and related fields. Second, I was planning to refresh my data science toolkit, that I used to apply in very special technical domains in the past. For this reason I was looking to join the team, since this is the best way to combine and distill the best approaches out there.
<h2>What preprocessing and supervised learning methods did you use?</h2>
<b>Janto:</b> I focused on the telematics modelling part and spent a great amount of time on feature engineering and data exploration at the beginning of the challenge. First I tried Self-Organizing Maps which didn't work well at the early stage when I only had few features.

For me, the crucial step was to turn the task into a supervised problem and the effort we put into ensembling telematics and trip matching.

We tackled the problem by combining the results of the trip matching that Scott had done and our driver signature models. The idea was to identify frequently taken trips as they were likely to be trips from the respective driver and apply supervised models to telematic features for unmatched trips.

Our final telematics model consisted of an ensemble of 6 different models. We mainly used Random Forests, in combination with Gradient Boosting and Logistic Regression. Some coarse grid searching I had done earlier in the competition yielded the model parameters and we didn’t tune a lot in the end. We trained models on different partially independent feature sets each of us had extracted and combinations of those feature sets.

Random Forests worked pretty well and extremely fast: after decreasing the time spent for input/output routines by converting the data to .npy files, my first ensemble (a simple Random Forest combined with Logistic Regression) scored 0.86 on the leaderboard in about 30 minutes.

<b>Scott:</b> Like many people in the competition, we used a combination of path matching and a telematics approach. My focus was on path matching, and only switched to telematics at the end of the competition.

For the path matching, the most important preprocessing method was the RDP algorithm, which I discovered relatively early in my investigation phase when I stumbled upon this Stack Overflow post

<a href=""http://stackoverflow.com/questions/14631776/calculate-turning-points-pivot-points-in-trajectory-path"" target=""_blank"">http://stackoverflow.com/questions/14631776/calculate-turning-points-pivot-points-in-trajectory-path</a>

The RDP algorithm effectively works by drawing a straight line between the starting and ending points of the path, and finding the point on that route that is the maximum distance from that line segment and comparing it against a provided tolerance. If the largest point is outside that tolerance, is remembered as an RDP point, and the path is split and the RDP point serves as a new starting and ending point for the line segment.

Once I had those RDP points, I assigned every adjacent set of 3 points, and every set of 3 RDP points with one skipped between them, an angle and the distance of the legs of the triangle. I then matched every path against every other path and looked for at least 3 matching angles / leg distances that could be rotated to be in the same locations.

I was very surprised at the end of the competition to see that many teams utilized the RDP algorithm, since it never broke on the forums during the competition, although many other valuable insights did.

<b>Andrei:</b> Gradient boosting proved the reputation of performing well on a small training dataset. There is a perfect in-depth tutorial which I would recommend <a href=""http://topepo.github.io/caret/index.html"" target=""_blank"">http://topepo.github.io/caret/index.html</a> especially about model training and parameters tuning.

[caption id=""attachment_4835"" align=""aligncenter"" width=""636""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/axablog1.png""><img class=""wp-image-4835 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/axablog1.png"" alt=""axablog1"" width=""636"" height=""625"" /></a> Schematic illustration of our modelling process. We extracted 3 feature sets and trained different classifiers on them (RF: Random Forest, GBM: Gradient Boosting Machine, LogReg: Logistic Regression). Each trip was assigned a probability (of being a true trip). In each classifier output we globally sorted the trips according to those probabilities and assigned ranks (across drivers). These were used as input for the final ensemble. Our final telematics model consisted of a weighted sum ensemble of the 6 different models.[/caption]
<h2>What was your most important insight into the data?</h2>
<b>Scott:</b> For me, it was the power of ensembling. Early on I thought that the competition could be won by path matching. However it quickly became apparent that the best would be combination of path matching and telematics. Since I was not familiar with the telematics approach, I found teammates who could do this. As soon as the team formed, combining my ~120th ranked path matching with my teammates ~120th ranked telematics, we immediately jumped up to 25th place.

Ensembling helped us at the end too. We had 6 different telematics solutions scoring between .83 - .89, and combined they scored .91. Basically, anytime we rolled in a independently scored telematics solution we were able to get at least some score boost.

<b>Janto:</b> I agree with Scott, the ensembling was key. Early in the competition it seemed there were two types of competitors: the ""telematic modelling guys"" and the ""trip matchers"", both obtained good results in the 0.9 ballpark. I soon realized that trip matching would be important to perform well in this task, so we got together.

For me, the main insight was the fact that the two distinct components of the task - geometry matching and driver modelling - could be best combined by using ""probability ranks"" for the trips. The probability distributions of the different models often looked quite different and the trip matching basically produced counts as output. So, we had to translate the probabilistic outputs and counts into one common metric.

<b>Andrei</b>: If we get to the origin of the challenge, namely identifying the driver fingerprint, the path matching part originally sounded like actually cracking the competition. I still think it does not contribute the fingerprint, but that was probably the only way to handle the fact, that real coordinates were anonymized and trimmed down. Having some kind of the “driving landscape”, i.e. types of driving areas, would be more useful feature in real life.

[caption id=""attachment_4836"" align=""aligncenter"" width=""642""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/axablog2.png""><img class=""wp-image-4836 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/axablog2.png"" alt=""axablog2"" width=""642"" height=""412"" /></a> Trip matching. The trips are downsampled using the Ramer-Douglas-Peucker (RDP) algorithm. Subsequently, our algorithm searched exhaustively for triples of GPS points within a specified distance that matched in angle and could thereby match routes but also road segments. Varying the search distance, i.e. the sensitivity of the algorithm, and integrating the results we could significantly improve our trip matching.[/caption]

&nbsp;
<h2>Were you surprised by any of your findings?</h2>
<b>Scott: </b>I was surprised at the number of junk runs there were. Approximately 3% of the data was just garbage that never left 100 meters from the starting point. We assumed that these was accidentally turned on GPS or similar. We ended up getting a little bit of AUC value out of them by simply counting the number of junk runs per each driver, and if a driver had a lot of junk runs, moving each of those runs higher in the rankings. (Some of the drivers had 30, 40 or more junk runs!)

<b>Janto</b>: My SOMs performed surprisingly bad.

<b>Andrei:</b> The “real life” nature of the data was indeed the most “uncomfortable” thing, that automatically gets you out of the comfort zone of working with the reference samples. There were many concerns about this in the forums as well.
<h2>Which tools did you use?</h2>
<b>Scott:</b> For the path matching I used mostly python, with numpy and matplotlib. The path matching was nice because the paths could be plotted for immediate feedback on if the algorithm was working or not. With the path matching, I perennially had a problem with run time. For most of the competition, my path matching algorithm took 2 days to get through all 2736 drivers on my 4 core computer. I spent substantial time improving the code to make it run faster, but that was quickly eaten up by making it more complicated to perform better. Eventually, I wrote some of the more simple, but computationally long parts of the algorithm in Fortran, which processes For loops approximately 100 times faster than python, and used F2Py to tie them into the python.

At the end of the competition, we needed more computing power, so I launched 7 two node virtual machines on google compute, as well as another on Azure for a total of 16 nodes running for 14 days. (The cost of this would have been on the order or ~ $100, however Google has a $300 free credit which I utilized.)

For the telematics, we utilized Random Forests and Gradient Based Trees from the Python SciKit Learn toolkit.

<b>Janto:</b> I ran all my analyses in Python, using the great numpy module and scikit learn toolkit for the modelling. For visualizations I used matplotlib. We collaborated via GitHub. As our repository was overflowing with scripts combining those in batch scripts was also quite helpful.

<b>Andrei:</b> I mainly used R with the <i>caret </i>package (<a href=""http://topepo.github.io/caret/index.html"" target=""_blank"">http://topepo.github.io/caret/index.html</a>) Azure cluster was used with R Enterprise from <a href=""http://www.revolutionanalytics.com/revolution-r-enterprise"" target=""_blank"">http://www.revolutionanalytics.com/revolution-r-enterprise</a> and <i>doParallel </i>backend <a href=""http://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf"" target=""_blank"">http://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf</a>
<h2>What have you taken away from this competition?</h2>
<b>Janto:</b> Collaborating in a team is a great experience, we finished 2nd place because we joined forces. I definitely recommend it. Also, we organized our code on Github which I had never used before and it’s a great resource when working together. The competition taught me the power of ensembles and how knowing the evaluation metric can really help to make informed choices in the ensembling process.

<b>Scott:</b> The value in utilizing Github when working with teammates. The basics of using the sci-kit learn toolkit, especially random forests and gradient boosted trees for data analysis. The value of teams, since I would certainly not have scored as highly working on my own.

<b>Andrei: </b>One does not need to be a domain expert to already perform above average. I do not think we really innovated in the feature development, neither we did in the methods. As it was indicated above, many competitors tried the same. But mainly focusing on different machine learning techniques and organized them together helped us to boost the performance. During last week the process was pretty much “wait 4 days to complete” to get 0.0005 performance.
<h2>Do you have any advice for those just getting started in data science?</h2>
<b>Janto: </b>In my opinion, one important point is to be able to test ideas as efficient and reproducible as possible. I like to build pipelines for every necessary task, like ""preprocessing"",""modelling"", ""validation"" and ""submission"" from the beginning. These modules can be easily scaled up at later stages and you don't have to care anymore about the code that produces your predictions if you change your model.

It helps to have one script in which you only specify the model or the ensemble of models, one that loads the data, one that preprocesses the data, etc. If you now wake up in the middle of the night and think ""I should really change my logistic regression to an ensemble of random forests and a gradient boosting machine and change the window length of my moving average preprocessing to 10s"" you just change two lines in the model and your preprocessing script and type ""python validate.py"" in your shell. This helps to keep an overview over your code and avoids errors in testing or output routines. And you can test a new ensemble when you come home from a party at 4 am before you go to sleep.

Validate your models and keep a log. In this competition it turned out that public and private scores correlated nicely but you can’t rely on that. Keep track of every cross-validation and the corresponding LB score.

In this competition it was a bit tricky to validate model performance since the data was unlabeled. That's why I transformed the problem into a supervised one and recreated the task, that is, detecting outliers, with known labels: I took the 200 trips of one driver as ""real"" trips and injected k (usually around 5-10) artificial false trips from another driver into the dataset. Now I had labels and could run a classification scheme. Doing this for around 100 drivers and computing the global AUC score yielded a good indicator of performance.

We observed that the public scores were really close to the local CV scores. This observation justified that we trusted the leaderboard score for some time consuming complex analyses that took a couple of days to compute at later stages of the challenge.

<b>Scott:</b> Watch the forums in your competition. There are a ton of insights to be had in there. Our team got a lot of the features that we ended up using for the telematics competition based on posts from the forums. And our basic method of path matching combined with telematics was all over the forums

Really get to know what the scoring criteria is. The AUC criteria used for this competition essentially only cares about the order the results are ranked in, not the actual scores. We got a large score boost ( ~.005 ) by making sure that no two results were scored the same by sorting our path matching results by the telematics scores and doing a few other tricks to make sure that each of the scores would be unique.

Also, don’t bother with running super computationally expensive runs early on. You can always crank up the number of trees in your random forest for the final few submissions, early on it is valuable to be able to iterate quickly. If I were to do this competition again I would have spent less time running the full 2736 driver data set in January / February and run more iterations of just 10 or 100 drivers that I could check and improve quickly.

<b>Andrei</b>: Use things like google scholar to find few relevant research papers. Especially if you are not a domain expert. Some domains have a better coverage of different machine learning techniques then the others. It is not worth inventing things until the proven methods have not been tried.
<h2>The Team</h2>
<b>Andrei Varanovich <a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/andrei.jpeg""><img class="" wp-image-4838  alignleft"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/andrei-150x150.jpeg"" alt=""andrei"" width=""108"" height=""108"" /></a></b>With a diploma in System Engineering and Masters in Computer Science, Andrei is currently finishing a PhD thesis as a result of 4 years spent as part of the Software Languages Team at the University of Koblenz-Landau in Germany. In his role as cloud architect at Sparked, he mainly focuses on data analytics and machine learning in the Microsoft ecosystem. He is a Microsoft Most Valuable professional since 2008.

<b>Janto Oellrich </b>is a Cognitive Science MSc student specializing in Neuroscience and Machine Learning. Currently, he is working as a research assistant at the Department of Experimental Psychology at the University of Oxford. His research interests include perceptual decision making and feature representation in humans, computational modelling of behaviour and machine learning.

<b>Scott Hartshorn</b> is an aerospace engineer and a graduate of the University of Michigan. His professional work has been in structural engineering. He is currently working to develop an expertise in Machine Learning and Data Analytics."
"Profiling Top Kagglers: KazAnova, Currently #2 in the World",http://blog.kaggle.com/2015/05/07/profiling-top-kagglers-kazanovacurrently-2-in-the-world/,2015-05-07 17:00:23,"There are Kagglers, there are Master Kagglers, and then there are top 10 Kagglers. Who are these people who consistently win Kaggle competitions? In this series we try to find out how they got to the top of the leaderboards.

First up is <a href=""https://www.kaggle.com/users/111640/kazanova"" target=""_blank"">KazAnova</a> -- Marios Michailidis -- the current number 2 out of nearly 300,000 data scientists. Marios is a PhD student in machine learning at <a href=""http://www.cs.ucl.ac.uk/"" target=""_blank"">UCL</a> and a senior data scientist at <a href=""http://www.dunnhumby.com"" target=""_blank"">dunnhumby</a> (organizer of the Kaggle competitions '<a href=""https://www.kaggle.com/c/dunnhumbychallenge"" target=""_blank"">Shopper Challenge</a>' and '<a href=""https://www.kaggle.com/c/hack-reduce-dunnhumby-hackathon"" target=""_blank"">Product Launch Challenge</a>').

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/kazanova.png""><img class=""aligncenter wp-image-4845 size-medium"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/kazanova-300x142.png"" alt=""kazanova"" width=""300"" height=""142"" /></a>
<h2>Marios Michailidis Q&amp;A</h2>
<h3>How did you start with Kaggle competitions?</h3>
I wanted a new challenge in the field and learn from the Grand Masters.

I was doing software development about machine learning algorithms, which also led to creating a <a href=""http://www.kazanovaforanalytics.com/software.html"" target=""_blank"">GUI for credit scoring/analytics</a> by the name KazAnova- a nick name I frequently use in Kaggle to keep reminding myself the passion I have for the field and how it started, <b>but</b> I could only go so far by myself.

Kaggle seemed the right place to learn from the experts.
<h3>What is your first plan of action when working on a new competition?</h3>
<ol>
	<li>First of all to understand the problem and the <a href=""https://www.kaggle.com/wiki/Metrics"" target=""_blank"">metric</a> we are tested on- this is <b>key</b>.</li>
	<li>To as-soon-as possible create a reliable cross-validation process that best would resemble the leaderboard or the test set in general as this will allow me to explore many different algorithms and approaches, knowing the impact they could yield.</li>
	<li>Understand the importance of different algorithmic families, to see when and where to maximize the intensity (is it a linear or non-linear type of problem?)</li>
	<li>Try many different approaches/techniques on a the given problem and seize it from all possible angles in terms of algorithms 'selection, hyper parameter optimization, feature engineering, missing values' treatment- I treat all these elements as <b>hyper</b> parameters of the final solution.</li>
</ol>
<h3>What does your iteration cycle look like?</h3>
<ol>
	<li>Sacrifice a couple of submissions in the beginning of the contest to understand the importance of the different algorithms -- save energy for last 100 meters.</li>
	<li>Do the following process for multiple models
<ul>
	<li>Select a model and do a recursive loop with the following steps:
<ul>
	<li>Transform data (scaling, log(x+1) values, treat missing values, PCA or none)</li>
	<li>Optimize hyper parameters of the model</li>
	<li>Do feature engineering for that model (as in generate new features)</li>
	<li>Do features' selection for that model (as in reducing them)</li>
	<li>Redo previous steps as optimum parameters are likely to have changed slightly</li>
</ul>
</li>
	<li>Save hold-out predictions to be used later (meta-modelling)</li>
	<li>Check consistency of CV scores with leaderboard. If problematic, re-assess cross-validation process and re-do steps</li>
</ul>
</li>
	<li>Create partnerships. Ideally you look for people that are likely to have taken different approaches than you have. Historically (in contrast) I was looking for friends; people I can learn from and people I can have fun with - not so much winning.</li>
	<li>Find a good way to ensemble</li>
</ol>
[caption id=""attachment_4846"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/kazanova-analytics.png""><img class=""wp-image-4846 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/kazanova-analytics.png"" alt=""kazanova-analytics"" width=""500"" height=""520"" /></a> Screen from the KazAnova Analytics GUI.[/caption]

&nbsp;
<h3>What are your favorite machine learning algorithms?</h3>
I like Gradient Boosting and Tree methods in general:
<ol>
	<li>Scalable</li>
	<li>Non-linear and can capture deep interactions</li>
	<li>Less prone to outliers</li>
</ol>
<h3>What are your favorite machine learning libraries?</h3>
<ol>
	<li><a href=""http://scikit-learn.org/stable/"" target=""_blank"">Scikit</a> for forests.</li>
	<li><a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> for GBM.</li>
	<li><a href=""http://www.csie.ntu.edu.tw/~cjlin/liblinear/"" target=""_blank"">LibLinear</a> for linear models.</li>
	<li><a href=""http://www.cs.waikato.ac.nz/ml/weka/"" target=""_blank"">Weka</a> for all.</li>
	<li><a href=""http://www.heatonresearch.com/encog"" target=""_blank"">Encog</a> for neural nets.</li>
	<li><a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a> for nets, although I learnt it very recently.</li>
	<li><a href=""http://people.cs.umass.edu/~vdang/ranklib.html"" target=""_blank"">RankLib</a> for functions like <a href=""http://en.wikipedia.org/wiki/Discounted_cumulative_gain"" target=""_blank"">NDCG</a>.</li>
</ol>
[caption id=""attachment_4847"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/boostedtree.png""><img class=""wp-image-4847 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/boostedtree.png"" alt=""boostedtree"" width=""500"" height=""210"" /></a> Image from slides <a href=""http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf"" target=""_blank"">'Introduction to Boosted Trees'</a> by <a href=""https://www.kaggle.com/users/32300/tianqi-chen"" target=""_blank"">Tianqi Chen</a> (XGBoost)[/caption]

&nbsp;
<h3>What is your approach to hyper-tuning parameters?</h3>
I do this very manually.

I have only tried once to use something like Gridsearch. I feel I learn more about the algorithms and why they work the way they do by doing this manually. At the same time I feel that ""I do something, it's not only the machine!"".

After 40+ competitions I've found that I can get to the top 90% of the best hyper parameters with the first try, so the manual approach has paid off!
<h3>What is your approach to solid CV/final submission selection and LB fit?</h3>
In regards to CV, I try to best resemble what I am being tested on.

In many situations a random split would not work. For example: In the <a href=""http://www.kaggle.com/c/acquire-valued-shoppers-challenge"" target=""_blank"">Acquire valued shoppers' challenge</a> we were mainly tested on different products (offers) than these available on the train set. I made my CV to always try to predict 1 offer using the rest of the offers as this could resemble the test leaderboard better than a random split.

About final selection, I normally go for best Leaderboard submission and best CV submission. In the case of a happy collision, I select something as different as possible with respectable CV result just in case I am lucky!

<small>(A prayer to the god of overfitting is my secret 3rd submission)</small>
<h3>In a few words: What wins competitions?</h3>
<ol>
	<li>Understand the problem well</li>
	<li>Discipline ; To have a well-thorough and documented approach that you follow religiously and defines all the modelling process/framework from how you cross-validate, select models, avoids over fitting (which requires a lot of ...discipline).</li>
	<li>Allow room to try problem-specific things or new approaches within that framework</li>
	<li>The hours you put in</li>
	<li>Have access to the right tools</li>
	<li>Make key partnerships</li>
	<li>Ensembling</li>
</ol>
<h3>What is your favourite Kaggle competition and why?</h3>
The <a href=""www.kaggle.com/c/acquire-valued-shoppers-challenge"" target=""_blank"">Acquire valued shoppers</a>' challenge, not only because I won and it is relevant to what my team does, but I also had the honour to collaborate with <a href=""http://www.kaggle.com/users/9974/gert"" target=""_blank"">Gert Jacobusse</a>.

[caption id=""attachment_4848"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/leaderboard.png""><img class=""size-full wp-image-4848"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/leaderboard.png"" alt=""The final private leaderboard for the Valued Shopper's Challenge"" width=""500"" height=""470"" /></a> The final private leaderboard for the Valued Shopper's Challenge[/caption]
<h3>What was your least favourite Kaggle competition experience?</h3>
<a href=""https://www.kaggle.com/c/decoding-the-human-brain"" target=""_blank"">DecMeg</a>, <a href=""https://www.kaggle.com/c/inria-bci-challenge"" target=""_blank"">BCI</a> and such channel-wave type of competitions. They have big data and are very domain specific.

I found it hard to even make my CV working properly, plus I did quite bad. Hopefully I will improve.
<h3>What field in machine learning are you most excited about?</h3>
I like recommender systems if it can be considered a separate field.

There is a broad spectrum of techniques you can use (which are field specific) and to be able to understand what the customer likes is very challenging and rewarding.
<h3>Which machine learning researchers do you study?</h3>
I study: <a href=""http://www.kaggle.com/users/25112/steffen-rendle"" target=""_blank"">Steffen Rendle</a>, <a href=""https://www.stat.berkeley.edu/~breiman/"" target=""_blank"">Leo Breiman</a>, <a href=""http://www.ci.tuwien.ac.at/~alexis/About.html"" target=""_blank"">Alexander Karatzoglou</a>, <a href=""https://www.kaggle.com/users/1455/michael-jahrer"" target=""_blank"">Michael Jahrer</a> &amp; <a href=""http://www.researchgate.net/profile/Andreas_Toescher"" target=""_blank"">Andreas Töscher</a>, the <a href=""http://www.csie.ntu.edu.tw/~cjlin/mlgroup/"" target=""_blank"">Machine Learning and Data Mining Group</a> at National Taiwan University, and <a href=""http://web4.cs.ucl.ac.uk/staff/jun.wang/blog/about/"" target=""_blank"">Jun Wang</a> &amp; <a href=""http://www0.cs.ucl.ac.uk/staff/p.treleaven/"" target=""_blank"">Philip Treleaven</a>.
<h3>Can you tell us something about the last algorithm you hand-coded?</h3>
It was <a href=""http://www.libfm.org"" target=""_blank"">LibFM</a> for <a href=""https://www.kaggle.com/c/avazu-ctr-prediction"" target=""_blank"">Avazu competition</a> as I believed it could work well in that particular problem. I could not make it work as well as <a href=""http://www.csie.ntu.edu.tw/~cjlin/libffm/"">LibFFM</a> apparently.
<h3>How important is domain expertise for you when solving data science problems?</h3>
For some competitions it is really important.

The fact that I am employed in the recommendation science field and the kind of work that we do within my team, has helped me win the Acquire valued Shoppers challenge.

However I think you can go a long way by following standard approaches even if you don't know the field well, which is also the beauty of machine learning and the fact that some algorithms do a significant job for you.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/ensembling.png""><img class=""aligncenter wp-image-4849 size-medium"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/ensembling-300x272.png"" alt=""ensembling"" width=""300"" height=""272"" /></a>
<h3>What do you consider your most creative trick/find/approach?</h3>
I do multiple ensemble meta-stacking if I can use the term.

During the course of my univariate model tuning I save all the models' outputs. Then I make meta-models with the univariate models selections and most of the times I end up with different ensembles of Meta models.

Sometimes I go to third Meta model with Meta models as inputs (Meta-Meta model).
<h3>How are you currently using data science at work and does competing on Kaggle help with this?</h3>
<b>Classified!</b>

Kaggle does help in optimizing my methods, learning new skills, meet nice people with same passion, be up to date with new tools and generally stay in touch with what's going on in the field.
<h3>What is your opinion on the trade-off between high model complexity and training/test runtime?</h3>
That is a big discussion in principle.

I guess there is a trade-off, but there needs to be an understanding that better models are not necessarily the most interpretable ones and that a more complex model (that is likely to score better) is not necessarily less stable/more dangerous.

I guess the optimum solution should be somewhere in the middle (e.g. not an ensemble of 100 models nor Naive Bayes)

[caption id=""attachment_4850"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/awards.png""><img class=""size-full wp-image-4850"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/awards.png"" alt=""The best 8 finishes for KazAnova."" width=""500"" height=""260"" /></a> The best 8 finishes for KazAnova.[/caption]
<h3>How did you get better at Kaggle competitions?</h3>
Did I ?! :D

I guess what has helped a lot is:
<ol>
	<li>Seeing previous solutions and end-of-competition threads</li>
	<li>Participate in Kaggle forums</li>
	<li>Learn the tools</li>
	<li>Read papers, websites, machine learning tutorials</li>
	<li>Optimize processes (use sparse matrices, cut unnecessary steps, write more efficient code)</li>
	<li>Save everything I've done and reuse (and improve). E.g. I keep a separate folder for each competition I've completed.</li>
	<li>Dedicate time (had to reduce video games)</li>
	<li>Collaborate with others</li>
</ol>
I have found the following resources useful:
<ol>
	<li>This <a href=""http://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/4797/starter-code-in-python-with-scikit-learn-auc-885"" target=""_blank"">benchmark</a> by <a href=""https://www.kaggle.com/users/44623/paul-duan"" target=""_blank"">Paul Duan</a> in the <a href=""https://www.kaggle.com/c/amazon-employee-access-challenge"" target=""_blank"">Amazon competition</a> (my first ever attempt with Python) for a general modelling framework.</li>
	<li>From the same competition : <a href=""http://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/4838/python-code-to-achieve-0-90-auc-with-logistic-regression"" target=""_blank"">Python code to achieve 0.90 AUC with logistic regression</a> from <a href=""https://www.kaggle.com/users/66023/miroslaw-horbal"" target=""_blank"">Miroslaw Horbal</a> to create pairwise interactions with cross-validation.</li>
	<li>For text analysis, this benchmark from <a href=""https://www.kaggle.com/users/5309/abhishek"" target=""_blank"">Abhishek</a>: <a href=""http://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878"" target=""_blank"">Beating the benchmark</a> in <a href=""https://www.kaggle.com/c/stumbleupon"" target=""_blank"">StumbleUpon Evergreen Challenge</a></li>
	<li><a href=""http://www.kaggle.com/c/higgs-boson/forums/t/8184/public-starting-guide-to-get-above-3-60-ams-score"" target=""_blank"">XGBoost benchmark</a> in <a href=""https://www.kaggle.com/c/higgs-boson"" target=""_blank"">Higgs Boson competition</a> by <a href=""https://www.kaggle.com/users/43581/bing-xu"" target=""_blank"">Bing Xu</a></li>
	<li><a href=""https://www.kaggle.com/users/185835/tinrtgu"" target=""_blank"">Tinrtgu</a>'s <a href=""http://jmlr.org/proceedings/papers/v15/mcmahan11b/mcmahan11b.pdf"" target=""_blank"">FTRL</a> Logistic model in Avazu: <a href=""http://www.kaggle.com/c/avazu-ctr-prediction/forums/t/10927/beat-the-benchmark-with-less-than-1mb-of-memory"" target=""_blank"">Beat the benchmark with less than 1MB of memory</a></li>
	<li><a href=""https://www.kaggle.com/c/datasciencebowl"" target=""_blank"">Data science Bowl</a> tutorial for image classification: <a href=""http://nbviewer.ipython.org/github/udibr/datasciencebowl/blob/master/141215-tutorial.ipynb"" target=""_blank"">IPython Notebook Tutorial</a>.</li>
	<li><a href=""http://0xdata.com/"" target=""_blank"">H2O</a> (R) <a href=""https://www.kaggle.com/c/afsis-soil-properties/forums/t/10568/ensemble-deep-learning-from-r-with-h2o-starter-kit"" target=""_blank"">deep learning benchmark</a> from <a href=""https://www.kaggle.com/users/234686/arno-candel"" target=""_blank"">Arno Candel</a> in <a href=""https://www.kaggle.com/c/afsis-soil-properties"" target=""_blank"">Africa Soil competition</a></li>
	<li><a href=""http://nbviewer.ipython.org/github/ottogroup/kaggle/blob/master/Otto_Group_Competition.ipynb"" target=""_blank"">Lasagne and nolearn tutorial</a> for <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge"" target=""_blank"">Otto competition</a> (by the admin) :</li>
	<li>Andrew Ng's <a href=""https://www.coursera.org/course/ml"" target=""_blank"">Coursera course in Machine learning</a></li>
	<li>University of Utah <a href=""http://www.cs.utah.edu/~piyush/teaching/cs5350.html"" target=""_blank"">Machine learning slides</a>.</li>
	<li>Wikipedia and Google</li>
</ol>
<h3>Are partnerships important in achieving good results?</h3>
Very.

Sometimes you cannot measure the impact from one competition only as what you learn from the others may be applicable in the future too. I've been very lucky to have made good and fun collaborations so far and I have learnt from all, especially:
<ol>
	<li>From <a href=""https://www.kaggle.com/users/9974/gert"" target=""_blank"">Gert</a> I've learnt model ensembling, feature engineering and Fourier transforms.</li>
	<li><a href=""https://www.kaggle.com/users/114978/triskelion"" target=""_blank"">Triskelion</a> and <a href=""https://www.kaggle.com/users/115173/phil-culliton"" target=""_blank"">Phil Culliton</a>: <a href=""http://hunch.net/~vw/"" target=""_blank"">Vowpal Wabbit</a></li>
	<li><a href=""https://www.kaggle.com/users/111641/tvs"" target=""_blank"">TVS</a> to avoid over-fitting</li>
	<li><a href=""https://www.kaggle.com/users/2242/bluefool"" target=""_blank"">Bluefool</a> (or Domcastro) 1st derivatives and <a href=""http://www.stat.rice.edu/~jrojo/4th-Lehmann/slides/mcculloch.pdf"" target=""_blank"">BART</a></li>
	<li><a href=""https://www.kaggle.com/users/38878/mike"" target=""_blank"">Mike</a>, Python!</li>
	<li><a href=""https://www.kaggle.com/users/111776/giulio"" target=""_blank"">Giulio</a>, competition management and <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a>.</li>
</ol>
<h2>Bio</h2>
<p style=""text-align: left;""><a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/marios-michailidis.jpg""><img class=""alignleft wp-image-4851"" style=""margin-right: 15px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/marios-michailidis.jpg"" alt=""marios-michailidis"" width=""100"" height=""120"" /></a>
<a href=""https://www.linkedin.com/pub/marios-michailidis/21/665/326/"" target=""_blank"">Marios Michailidis</a> is Senior Data Scientist in <a href=""http://www.dunnhumby.com/"" target=""_blank"">dunnhumby</a> and part-time PhD in machine learning at <a href=""http://www.ucl.ac.uk/"" target=""_blank"">University College London</a> (UCL) with a focus on improving recommender systems. He has worked in both marketing and credit sectors in the UK Market and has led many analytics projects with various themes including: Acquisition, Retention, Uplift, fraud detection, portfolio optimization and more. In his spare time he has created <a href=""http://www.kazanovaforanalytics.com/software.html"" target=""_blank"">KazAnova</a>, a GUI for credit scoring 100% made in Java.</p>
Marios loves competing on Kaggle and learning new machine learning tricks. He told us he will create something good for the ML community soon..."
"Microsoft Malware Winners' Interview: 2nd place, Gert & Marios (aka KazAnova)",http://blog.kaggle.com/2015/05/11/microsoft-malware-winners-interview-2nd-place-gert-marios-aka-kazanova/,2015-05-11 15:04:38,"Marios &amp; Gert, in a team of the same name, <a href=""https://www.kaggle.com/c/malware-classification/leaderboard/private"" target=""_blank"">took 2nd place</a> in the <a href=""https://www.kaggle.com/c/malware-classification"" target=""_blank"">Microsoft Malware Classification Challenge</a>. The two are regular teammates and previously won the <a href=""https://www.kaggle.com/c/acquire-valued-shoppers-challenge"" target=""_blank"">Acquire Valued Shoppers Challenge</a> together.

This blog outlines their approach to the Malware competition and also gives us a closer look at how successful teams come together and collaborate on Kaggle.

<em><strong>""[...]meta features in the asm files turned out to be even more useful than the alphanumeric contents - especially the number of lines in each section, and interpunction characters in each section.""</strong></em>

<em><strong>""Text classification techniques work really well in this problem. Neither I nor Gert had any prior knowledge of the field, yet finding the 'predictive words' in the document files was more than enough to score well.""</strong></em>

[caption id=""attachment_4871"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/malware_logo.png""><img class=""wp-image-4871 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/malware_logo-300x101.png"" alt=""malware_logo"" width=""300"" height=""101"" /></a> 377 teams and 481 players fought back against the malware industry.[/caption]
<h2>What was your background prior to entering this challenge?</h2>
<strong>Gert:</strong> I work as an independent researcher at my own company <a href=""http://www.rogatio.nl/"" target=""_blank"">Rogatio</a>. I have worked on statistical models in epidemiology, fraud detection and product recommendations. My education was in psychometry (<a href=""http://www.leiden.edu/"" target=""_blank"">Leiden University</a>), focused on traditional statistics and significance tests. During about 12 years of data analysis, I added programming skills and cross validation to my toolbox, because they become more important as the nature of data analysis problems changes.

[caption id=""attachment_4865"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-06-at-3.59.44-PM.png""><img class=""wp-image-4865 size-medium"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-06-at-3.59.44-PM-300x145.png"" alt=""Screen Shot 2015-05-06 at 3.59.44 PM"" width=""300"" height=""145"" /></a> <a href=""https://www.kaggle.com/users/9974/gert"" target=""_blank"">Gert's profile</a> on Kaggle[/caption]

<strong>Marios:</strong> I am a data scientist! I have a bachelor's degree in economics, Msc in Risk management and currently do my PhD (Part-Time) in Machine Learning and recommender systems at <a href=""http://www.ucl.ac.uk/"" target=""_blank"">UCL</a>. At the same time I am senior data scientist in dunnhumby and I have worked as analyst in many roles either in credit or marketing. About 4 years ago (2011), I decided I wanted to learn more about programming and machine learning hence I started learning Java. I put together my efforts into an ML package called <a href=""http://www.kazanovaforanalytics.com/"" target=""_blank"">KazAnova</a>. Since then I have picked up a lot of stuff from Kaggle and other means.

[caption id=""attachment_4864"" align=""aligncenter"" width=""300""]<img class=""wp-image-4864 size-medium"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-06-at-3.59.55-PM-300x143.png"" alt=""Screen Shot 2015-05-06 at 3.59.55 PM"" width=""300"" height=""143"" /> <a href=""https://www.kaggle.com/users/111640/kazanova"" target=""_blank"">Marios' (Μαριος Μιχαηλιδης KazAnova) profile</a> on Kaggle[/caption]
<h2>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h2>
<strong>Gert:</strong> No malware specific knowledge, I still don’t know what the different malware types mean or even in what way they are different. I think our skills to handle large datasets were most important for our success: knowing what bits of feature extraction and modeling to invest our time in, and when to move on to the next idea.

<strong>Marios:</strong> No. Only that I do not like Viruses (of any kind) and I will do anything it takes to stop them from continuing to mess up my computer, forcing me re-format every now and then! I treated the problem as text classification though where I have learnt a couple of things from previous (Kaggle) competitions.
<h2>How did you get started competing on Kaggle?</h2>
<strong>Gert:</strong> About four years ago, one of my colleagues pointed me to the Kaggle website. Since then, I am addicted. For me, competing in a Kaggle challenge is at least as much fun as gaming. With one important difference: at the end of the day you haven’t wasted your time!

<strong>Marios:</strong> A friend of mine pointed it out to me too (about 2 years ago). I remember I was doing a logistic regression model for a client and that friend of mine told me, “Why don’t you use Random Forest instead, everybody seems to say its better…” Then I asked, “Who’s everybody?” Then he pointed to <a href=""http://www.slate.com/articles/health_and_science/new_scientist/2012/12/kaggle_president_jeremy_howard_amateurs_beat_specialists_in_data_prediction.html"" target=""_blank"">an interview of Jeremy Howard</a> which mentions just that.
<h2>How did your team start competing together?</h2>
<strong>Marios:</strong> I guess we were both in a quest to get the Master's badge and we were getting almost there... but at the last moment we were always being stripped out of the top 10. In the <a href=""https://www.kaggle.com/c/allstate-purchase-prediction-challenge"" target=""_blank"">Allstate competition</a> Gert finished 11th out of 1550+ teams and I finished 17th. I remember we were hoping there will be cheaters (shame on us!) ahead of us so that we would finally make it, but no luck! Gert was quite vocal about that and <a href=""https://www.kaggle.com/c/allstate-purchase-prediction-challenge/forums/t/8231/cheaters-removed"" target=""_blank"">this forum post</a> was the start of a 2 future-Kaggle-wins combo! Putting that aside, I would see Gert many times ahead of me in competitions, and saw his comments in forums (particularly in the <a href=""https://www.kaggle.com/c/yelp-recruiting"" target=""_blank"">Yelp</a> competition that he was leading for most of phase 1), so I thought it would be a good chance to learn from him. The <a href=""https://www.kaggle.com/c/acquire-valued-shoppers-challenge"" target=""_blank"">Acquire Valued Shoppers Challenge</a> (1st place finish) was our first collaboration...

<strong>Gert:</strong> I can confirm that this is how we started teaming up... and we are still teaming up because we motivate each other a lot to get the best out of the data!
<h2>What made you decide to enter this competition?</h2>
<strong>Gert:</strong> It was Marios who convinced me to enter this competition. I actually thought that the dataset was too large (400 GB). I am not such a patient person and I knew that I would have to sit and wait a significant amount of time.

<strong>Marios:</strong> My curiosity, eagerness to compete and learn, and last but not least, my lust for Kaggle ranking points!
<h2>What preprocessing and supervised learning methods did you use?</h2>
<strong>Gert:</strong> For preprocessing, we wrote our own scripts to go through all the asm files and recognize specific parts, based on keywords (dll extension, function calls) and regular expressions, together with some scripts to compress files or parts of the bytes files. My part of the learning was mainly focused on distinguishing useful features, so I stuck to the same mix of Gradient Boosting and Extremely Randomized Trees from the beginning.

[caption id=""attachment_4858"" align=""aligncenter"" width=""608""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-06-at-3.25.51-PM.png""><img class=""wp-image-4858 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-06-at-3.25.51-PM.png"" alt=""The performance of various subsets of features"" width=""608"" height=""201"" /></a> The performance of various subsets of features.[/caption]
<p class=""HoofdtekstA""><strong>Marios:</strong> For preprocessing I used raw code that basically counts how many times single bytes, 2-gram bytes and 4-gram bytes appear in each file (document). I did the same for full-line bytes.</p>


[caption id=""attachment_4859"" align=""aligncenter"" width=""610""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/malware2_fig2.png""><img class=""wp-image-4859"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/malware2_fig2.png"" alt=""Different Byte’s considered as ngrams, sample line from 0A32eTdBKayjCWhZqDOQ.bytes"" width=""610"" height=""166"" /></a> Different Byte’s considered as ngrams, sample line from 0A32eTdBKayjCWhZqDOQ.bytes[/caption]

In terms of modelling I found XGboost and ExtraTreesClassifier (from Scikit) to combine very well together. Our modelling had 2 layers:
<ol>
	<li>Combine different datasets (e.g. bytes 2-grams + bytes 1-gram) and run a combo of the above models and save holdout predictions.</li>
	<li>Do meta-modelling. Run again a combo of XGBoost and ExtraTrees with the previous models as inputs.</li>
</ol>
[caption id=""attachment_4861"" align=""aligncenter"" width=""610""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-06-at-3.31.42-PM.png""><img class=""wp-image-4861"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-06-at-3.31.42-PM.png"" alt="""" width=""610"" height=""155"" /></a> Different models' performance.[/caption]
<h2 class=""HoofdtekstA"">What was your most important insight into the data?</h2>
<p class=""HoofdtekstA""><i> </i><strong>Gert:</strong> With regard to feature extraction, meta features in the asm files turned out to be even more useful than the alphanumeric contents - especially the number of lines in each section, and interpunction characters in each section.</p>


[caption id=""attachment_4862"" align=""aligncenter"" width=""610""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/malware2_fig4.png""><img class=""wp-image-4862"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/malware2_fig4.png"" alt=""This plot shows that, as an example, the file property ‘compression rate of bytes file’ is quite strongly related to class membership (dots show 5%, 25%, 50%, 75% and 95% percentiles)"" width=""610"" height=""310"" /></a> This plot shows that, as an example, the file property ‘compression rate of bytes file’ is quite strongly related to class membership (dots show 5%, 25%, 50%, 75% and 95% percentiles).[/caption]

[caption id=""attachment_4863"" align=""aligncenter"" width=""610""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/malware2_fig5-e1430951684996.png""><img class=""wp-image-4863"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/malware2_fig5-e1430951684996.png"" alt=""Shows the learning curve for adding features to the Gradient Boosting (Python sklearn) part of the ‘untuned’ model, with just over 300 features - based on the reduced asm features. Almost the same performance can be achieved with less than 50 features - selected using feature importances. In contrast, the performance of xgboost is clearly better when the full 1600 (unreduced) asm features are used."" width=""610"" height=""323"" /></a> Shows the learning curve for adding features to the Gradient Boosting (Python sklearn) part of the ‘untuned’ model, with just over 300 features - based on the reduced asm features. Almost the same performance can be achieved with less than 50 features - selected using feature importances. In contrast, the performance of XGBoost is clearly better when the full 1600 (unreduced) asm features are used.[/caption]

<strong>Marios:</strong> Text classification techniques work really well in this problem. Neither I nor Gert had any prior knowledge of the field, yet finding the “predictive words” in the document files was more than enough to score well.
<h2>Were you surprised by any of your findings?</h2>
<strong>Marios:</strong> I was surprised that we did better than the same guys whose papers we were reading to get ideas about the problem. I guess this puts heat to the discussion about what is better, domain knowledge or traditional ML approaches? On first sight this problem seemed very domain specific…
<h2>Which tools did you use?</h2>
<strong>Gert:</strong> I did everything in Python, with sklearn for modeling.

<strong>Marios:</strong> Python, XGboost, ExtraTreesClassifier (and luck).
<h2>How did your team work together and how did this help you succeed?</h2>
<strong>Gert:</strong> After Marios beat me using my own feature sets, I spent most of the time to extract more features from the asm files, and see if they improved the cross validation score in a model that was kept unchanged throughout the competition. After that I sent the good features to Marios, who created his own (XGBoost) models on each dataset and combined them into a very clever Meta model.

<strong>Marios:</strong> Initially I worked alone and I started creating my own features from the bytes’ file. At some point I had created 65K features and my models where not as good as Gert’s that had less than 300 features with simpler models :( . Then I maximized the intensity to prove I can still be of some value via focusing on modelling... happily it worked out. I got some luck from feature generation too later on. Generally I combine well with Gert because he thinks very unconventionally about the problems and comes up with breaking (the-local-minima) ideas.
<h2>What have you taken away from this competition?</h2>
<strong>Gert:</strong> A good friend and 1500 dollars.

<strong>Marios:</strong> Likewise. Plus bragging rights (and a ton of automated code I can re-use for other competitions)!
<h2>Do you have any advice for those just getting started in data science?</h2>
<strong>Gert:</strong> You learn most if you do things (instead of read about them), if you apply them to many different problems, and if you compete with and team up with others. So start to play on Kaggle!

<strong>Marios:</strong> <em>Knowledge is having the right answer. Intelligence is asking the right question. </em>To improve in this sport (and data modelling in general) it may be good to dedicate time, keep trying new things, learn the tools (in other words level up!), automate a lot, play/collaborate with others and have fun with many competitions to get a gist of different problems."
"TAB Food Winner's Interview: 1st place, Wei Yang (aka Arsenal)",http://blog.kaggle.com/2015/05/18/tab-food-winners-interview-1st-place-wei-yang-aka-arsenal/,2015-05-18 20:14:42,"The <a href=""https://www.kaggle.com/c/restaurant-revenue-prediction"" target=""_blank"">TAB Food Investments (TFI) Restaurant Revenue Prediction </a> competition was the second most popular public competition in Kaggle's history to date. 2,257 teams built models to predict the annual revenue of <a href=""http://tabfoods.com/"" target=""_blank"">TFI</a>'s regional quick service restaurants.

The winning model was a ""single gradient boosting model with simple parameters"". Wei Yang, known as ""<a href=""https://www.kaggle.com/users/70574/arsenal"" target=""_blank"">Arsenal</a>"" on Kaggle, took first place ahead of 2,458 other data scientists. In this blog, he shares what got him to the top of the <a href=""https://www.kaggle.com/c/restaurant-revenue-prediction/leaderboard/private"" target=""_blank"">private leaderboard</a> and what he's learned from competing.

<em><strong>""To me, this competition was mainly about feature engineering.""</strong></em>

<em><strong>""Most of my current machine learning knowledge is theoretical; participating in Kaggle competition is the best way to exercise and reinforce it.""</strong></em>

[caption id=""attachment_4887"" align=""aligncenter"" width=""615""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/TABfood_logos.png""><img class=""wp-image-4887 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/TABfood_logos.png"" alt=""2,257 teams and 2,459 players built models to predict the revenue of regional quick service restaurants."" width=""615"" height=""220"" /></a> With over 1,200 quick service restaurants across the globe, TFI represents some of the world's most well-known brands.[/caption]

Wei created the below <a href=""https://www.kaggle.com/users/70574/arsenal/restaurant-revenue-prediction/geomap-for-average-revenue"" target=""_blank"">interactive geomap</a> in <a href=""https://www.kaggle.com/c/restaurant-revenue-prediction/scripts"" target=""_blank"">scripts</a> during his data exploration. Hover over the map to see the average annual revenue in different regions.

<iframe src=""https://www.kaggle.io/svf/5486/bd6ffd2c5edb475f87311d96420ff947/output.html"" width=""616"" height=""397"" frameborder=""0"" scrolling=""no""></iframe>
<h2>What was your background prior to entering this challenge?</h2>
I earned my master’s degree from the statistics department at Stanford. I have 2 years of working experience as a data scientist in a data consulting company, where I have the opportunities to work on different data sets for different clients.

[caption id=""attachment_4888"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-18-at-11.38.38-AM.png""><img class=""wp-image-4888 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-18-at-11.38.38-AM-300x144.png"" alt=""Screen Shot 2015-05-18 at 11.38.38 AM"" width=""300"" height=""144"" /></a> Wei (aka <a href=""https://www.kaggle.com/users/70574/arsenal"" target=""_blank"">Arsenal</a>) on Kaggle[/caption]
<h2>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h2>
Not in the food restaurant industry. Nevertheless I have been working for client on a similar small data set with age information as well, although that one was a classification problem.
<h2>How did you get started competing on Kaggle?</h2>
The Stanford stat 202 (data mining) InClass competition on the <a href=""https://www.kaggle.com/c/titanic"" target=""_blank"">Titanic problem</a>.
<h2>What made you decide to enter this competition?</h2>
I want to strengthen my machine learning skills. Most of my current machine learning knowledge is theoretical; participating in Kaggle competition is the best way to exercise and reinforce it. What I am truly pursuing is to stabilize my rankings within top 10% for those Kaggle competitions I get involved in. But life is full of surprise.
<h2>What preprocessing and supervised learning methods did you use?</h2>
To me, this competition was mainly about feature engineering. I applied square root transformation to most of the obfuscated P variables (with maximum value &gt;= 10) to make them into the same scale, as well as the target variable “revenue”. I randomly assigned values to the uncommon city levels in both training and test set. This, I believe, has diversified the geo location information contained in the city variable and in some of the obfuscated P variables.

I created one missing value indicator for multiple P variables which, I believe, to some degree helped differentiate synthetic and real test data. Time / Age related information was also extracted. After creating all these new variables I treated zeroes as missing values and used the mice imputation. I read from the forum that dealing with outliers properly could improve scores, although I did not try it out myself. The winning model is just a single gradient boosting model with simple parameters.

[caption id=""attachment_4886"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-18-at-10.50.58-AM-e1431971548218.png""><img class=""wp-image-4886 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-18-at-10.50.58-AM-e1431971548218.png"" alt=""Script by another TAB competitor, piby4 "" width=""600"" height=""323"" /></a> <a href=""https://www.kaggle.com/users/285393/piby4/restaurant-revenue-prediction/histogram-plot-for-1-37-variables"" target=""_blank"">Histogram plot for 1-37 variables script</a> by another TAB competitor, <a href=""https://www.kaggle.com/users/285393/piby4"" target=""_blank"">piby4</a>[/caption]
<h2>What was your most important insight into the data?</h2>
The missing data mechanism. P14 to P18, P24 to P27, P30 to P37 are all zeroes at the same time for 88 out of 137 rows in the training data. Based on this I created an indicator, but I believe this could be utilized more.
<h2>Were you surprised by any of your findings?</h2>
Not really. I just followed my intuition with adjustment from the feedback of two statistics, i.e. training error and training error with outliers removed, as well as the public leaderboard score to choose my final winning model. Since I didn’t over-fit the training data too much, training error was indicative. I tested it post-deadline for several models and 1) training error 2) training error with outliers removed 3) public LB score and 4) private LB score aligned pretty well. I did not use the cv score for this problem since the data set is relatively small. I believe a simple and logical model will be robust.
<h2>Which tools did you use?</h2>
I used R and package <a href=""http://cran.r-project.org/web/packages/mice/index.html"" target=""_blank"">mice</a> for imputation, <a href=""http://cran.r-project.org/web/packages/lubridate/index.html"" target=""_blank"">lubridate</a> for extracting date related features and <a href=""http://cran.r-project.org/web/packages/caret/index.html"" target=""_blank"">caret</a> for modelling (with gbm).
<h2>What have you taken away from this competition?</h2>
As I mentioned, for small data set, one of the important aspects is to choose the right model. Simple and logical was my criteria. I was not doing well on version control of my R codes. I will definitely use GitHub next time.
<h2>Do you have any advice for those just getting started in data science?</h2>
Since I have a mathematical background, I personally prefer understanding the theory thoroughly before the implementation. You can certainly achieve something without deep understanding but your hands are tied. For Kaggle competitions, follow the corresponding forum and you will get great insight about the data.
<h2>Bio</h2>
<b>Wei Yang<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/wei_profilepic.png""><img class="" wp-image-4889 alignleft"" style=""padding-right: 10px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/wei_profilepic.png"" alt=""wei_profilepic"" width=""147"" height=""173"" /></a></b> earned his master's degree from the statistics department at Stanford University in 2013 and received his double Bachelor degrees in Mathematics and Finance from Nankai University, China, in 2011. Wei started his career at Saama Technologies, a data consulting company, which provides data science service for clients, as an associate data scientist in 2013. Besides participating in Kaggle competitions, Wei’s interests lie in theory of computation and mathematical philosophy. Personal Website: <a href=""http://www.wyang.org/"" target=""_blank"">http://www.wyang.org/</a>"
Hacking the Otto Group Challenge in Paris,http://blog.kaggle.com/2015/05/20/hacking-the-otto-group-challenge-in-paris/,2015-05-20 17:37:23,"<p class=""normal"">Last week we organized a <a href=""http://www.meetup.com/fr/Kaggle-Paris-Meetup/events/221973811/"" target=""_blank"">6th meetup</a> around the Otto Group Product Classification Challenge. The event was hosted at <a href=""http://lapaillasse.org/"" target=""_blank"">La Paillasse</a>, a community lab based in the center of Paris that brings together people from various backgrounds and nationalities to work on inspiring projects in the field of Bioscience and related tech fields.</p>


[caption id=""attachment_4788"" align=""aligncenter"" width=""200""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/03/meetup_kaggle.png""><img class=""wp-image-4788"" src=""http://blog.kaggle.com/wp-content/uploads/2015/03/meetup_kaggle-300x218.png"" alt=""The Kaggle Paris Meetup group has 363 Kagglers!"" width=""200"" height=""145"" /></a> The meetup took place on May 5, 2015. 25 Kagglers attended.[/caption]
<p class=""normal"">This was our first hands-on meetup. While our previous meetups concentrated on talks of competition winners and presentation of new ideas, tricks and methods, this time we were working together on an open competition and exchanging ideas in a collaborative format.</p>
<p class=""normal"">The attending Kagglers presented themselves and the motivation behind their participation in the competition. With 25 total attendants, five had a ranking in the top 100 of the leader board. While in previous meetups many of the attendants were newcomers to the field, in this meetup most participants held roles as data scientists or similar in the industry or the academy.</p>
<p class=""normal""><a href=""https://twitter.com/delphinel"" target=""_blank"">Delphine Lê</a> and <a href=""https://fr.linkedin.com/in/brunoseznec"" target=""_blank"">Bruno Seznec</a> started the meetup with a presentation of the Otto challenge and the datasets: classification of products into their correct category (<a href=""http://slides.com/bruno16/kaggle-paris-meetup-2015-0505#/"" target=""_blank"">slides</a>).</p>


[caption id=""attachment_4906"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/tsneprojectionottopiotrek.png""><img class=""wp-image-4906 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/tsneprojectionottopiotrek.png"" alt="""" width=""600"" height=""555"" /></a> T-sne projection for train data as posted by <a href=""https://www.kaggle.com/users/8998/piotrek"" target=""_blank"">piotrek</a> in the <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13122/visualization/69296"" target=""_blank"">competition forum</a>[/caption]
<p class=""normal"">Also, an overview of various methods from the competition’s forum were presented and discussed:</p>

<table style=""height: 220px;"" width=""437"" align=""center"">
<tbody>
<tr>
<td width=""195""><strong>Method</strong></td>
<td width=""69""><strong>Score</strong></td>
<td width=""102""><strong>Language</strong></td>
<td width=""79""><strong>Forum</strong></td>
</tr>
<tr>
<td style=""font-weight: 400;"">Random Forest</td>
<td>0.54</td>
<td>R</td>
<td style=""font-weight: 400;""><a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12932/beat-the-benchmark-with-a-random-forest-in-r"" target=""_blank"">link</a></td>
</tr>
<tr>
<td style=""font-weight: 400;"">XGBoost</td>
<td>0.51</td>
<td>R</td>
<td style=""font-weight: 400;""><a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12947/achieve-0-50776-on-the-leaderboard-in-a-minute-with-xgboost"" target=""_blank"">link</a></td>
</tr>
<tr>
<td style=""font-weight: 400;"">Deep Learning (Keras)</td>
<td style=""font-weight: 400;"">0.48</td>
<td style=""font-weight: 400;"">Python</td>
<td style=""font-weight: 400;""><a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13632/achieve-0-48-in-5-min-with-a-deep-net-feat-batchnorm-prelu/75673"" target=""_blank"">link</a></td>
</tr>
</tbody>
</table>
We also had a discussion around the main open source tools we use and especially new tools that we started using in the last few months: XGBoost, Lasagne, Keras, GraphLab. This competition was a great opportunity to discover and try them.

After the presentation, we had split into smaller work groups around specific topics:
<ul>
	<li>Ensembles and model fusion methods</li>
	<li>Feature Engineering</li>
	<li>Deep Learning and Neural Networks</li>
	<li>Parameter selection</li>
</ul>
Other participants discussed specific tools and a few used the meetup to have first try on these tools. Experience level varied among participants and a few participants got to do their first submissions on Kaggle during the meetup.

We also had a presentation of the parameter selection methods GridSearchCV and RandomizedSearchCV from <a href=""https://twitter.com/chris_bour"" target=""_blank"">Christophe Bourguignat</a> and the hyperopt library by <a href=""https://fr.linkedin.com/in/aminebenhalloum"" target=""_blank"">Amine Benhalloum</a> (code example can be found <a href=""https://github.com/bamine/Kaggle-stuff/blob/master/otto/hyperopt_xgboost.py"" target=""_blank"">here</a>).

The evening was a great opportunity for us to meet again, learn new tricks and improve our methods in the Otto competition. We will surely organize more hands-on meetups in the near future!

We are planning to organize a summer meetup in June. If your company is located in Paris and is interested to host or to sponsor the next meetup please don’t hesitate to contact the organizing team. ☺

<hr />

<i>This article was adapted by <a href=""https://fr.linkedin.com/in/kobykarp"" target=""_blank""><em>Koby Karp</em></a> from a report written by <a href=""https://twitter.com/seiteta"" target=""_blank"">Frédéric Bardolle</a> that first appeared <a href=""http://datatami.com/2015/05/07/kaggleparis6/"" target=""_blank"">here</a> (in French).</i>

<em>The Kaggle Paris Meetup was founded by </em><em>Koby Karp</em><em> and </em><a href=""https://fr.linkedin.com/pub/kenji-lefevre/6b/500/784"" target=""_blank""><em>Kenji Lefevre</em></a><em> in March 2014 and in the past year six meetups were hosted by Equancy, Dataiku, OCTO and the tech institute Télécom ParisTech, all of which have data scientists that participate regularly in Kaggle competitions.</em>"
"Microsoft Malware Winners' Interview: 1st place, ""NO to overfitting!""",http://blog.kaggle.com/2015/05/26/microsoft-malware-winners-interview-1st-place-no-to-overfitting/,2015-05-27 00:37:44,"Team ""say NOOOOO to overfittttting"" did just that and took first place in the <a href=""https://www.kaggle.com/c/malware-classification"" target=""_blank"">Microsoft Malware Classification Challenge</a>.

Sponsored by the <a href=""http://www.www2015.it/"" target=""_blank"">WWW 2015</a> / <a href=""http://wwwusers.di.uniroma1.it/~mei/BIG2015/Home.html"" target=""_blank"">BIG 2015</a> conferences, competitors were given nearly half a terabyte of data (when uncompressed!) and tasked with classifying variants of malware into their respective families. This blog outlines their winning approach and includes key visuals from their analysis.

[caption id=""attachment_4921"" align=""aligncenter"" width=""450""]<img class=""wp-image-4921"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/malware_burglarbanner-300x89.png"" alt=""377 teams and 481 players fought back against the malware industry."" width=""450"" height=""133"" /> 377 teams and 481 players competed to correctly classify malware into 9 families.[/caption]
<h2>How did you get started competing on Kaggle?</h2>
<strong>Little Boat:</strong> I was learning Python by doing the Harvard CS109 online, and was trying to find some practical projects to do. Then Google suggested Kaggle (not surprised!). Learned a lot since then.

[caption id=""attachment_4922"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/littleboat.png""><img class=""wp-image-4922 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/littleboat-300x142.png"" alt=""littleboat"" width=""300"" height=""142"" /></a> Little Boat's Kaggle <a href=""https://www.kaggle.com/xiaozhouwang"" target=""_blank"">profile</a>[/caption]

<strong>rcarson:</strong> I was taking the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">coursera course</a> of Andrew Ng. And I noticed that someone mentioned Kaggle in the course forum. I started competing last April, in the <a href=""https://www.kaggle.com/c/acquire-valued-shoppers-challenge"" target=""_blank"">repeat shopper challenge</a>.

[caption id=""attachment_4923"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-5.32.29-PM.png""><img class=""size-medium wp-image-4923"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-5.32.29-PM-300x143.png"" alt=""rcarson's Kaggle profile"" width=""300"" height=""143"" /></a> rcarson's Kaggle <a href=""https://www.kaggle.com/jiweiliu"" target=""_blank"">profile</a>[/caption]

<strong>Xueer Chen:</strong> rcarson introduced Kaggle to me. We had no machine learning background at that time and we teamed up to compete and learn from scratch.

[caption id=""attachment_4924"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-5.33.40-PM.png""><img class=""size-medium wp-image-4924"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-5.33.40-PM-300x142.png"" alt=""Xueer Chen's Kaggle profile"" width=""300"" height=""142"" /></a> Xueer Chen's Kaggle <a href=""https://www.kaggle.com/users/149507/xueer-chen"" target=""_blank"">profile</a>[/caption]
<h2>What made you decide to enter this competition?</h2>
<strong>Little Boat:</strong> Just thought it was fun to play with 400 GB data, although it turned out to be pretty small after feature extraction.

<strong>rcarson:</strong> The data size. My general impression is that the large data size often come together with rich feature space and stable cross validation performance. I really prefer this kind of contests since my skills are very limited in the opposite type of contests.

<strong>Xueer Chen:</strong> Its similarity to biological virus. Also the chance to go to Italy.

[caption id=""attachment_4911"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/MM_overview1-e1432681311937.png""><img class=""wp-image-4911 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/MM_overview1-e1432681311937.png"" alt=""MM_overview"" width=""600"" height=""333"" /></a> An overview of the team's approach.[/caption]
<h2>What preprocessing and supervised learning methods did you use?</h2>
<strong>Little Boat:</strong> Extracting good features is the key to winning this competition. And XGBoost gives the power to validate how useful the new features are. So thinking and trying new features on XGBoost is basically what we did.

<img class=""aligncenter wp-image-4913 "" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-4.06.13-PM-1024x650.png"" alt=""Screen Shot 2015-05-26 at 4.06.13 PM"" width=""506"" height=""321"" />

<strong>rcarson:</strong> In detail, we extracted three kinds of features: opcode N-gram count, segment line count and asm file pixel intensity features. We wrote our own feature extraction code in plain python which is online and can be accelerated by pypy. For supervised modeling, we use Xgboost and ensemble. Please find details in our <a href=""https://youtu.be/VLQTRlLGz5Y"" target=""_blank"">slides and papers</a>.

<strong>Xueer Chen:</strong> I also tried some image processing techniques such as segmentation and gabor filtering in Matlab but they didn’t contribute to our final feature set.

[caption id=""attachment_4920"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/MM_top10OCode1.png""><img class=""wp-image-4920 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/MM_top10OCode1-1024x459.png"" alt="""" width=""640"" height=""287"" /></a> Random Forest Feature Importance [1][/caption][caption id=""attachment_4919"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/MM_top10segmentcount.png""><img class=""wp-image-4919 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/MM_top10segmentcount-1024x338.png"" alt="""" width=""640"" height=""211"" /></a> Random Forest Feature Importance [2][/caption]
<h2>What was your most important insight into the data?</h2>
<strong>Little Boat:</strong> There are only 10K sample points for training and the model accuracy is very high. Including the predicted labels of test data will double your training data, and the signal noise ratio won’t decrease much.

<strong>rcarson:</strong> The features are sparse and non-linear. It is necessary to select feature first to make it denser otherwise the performance of XGBoost will be degraded and be slowed down. Please find the detailed analysis in the slides.

<strong>Xueer Chen:</strong> There are very fine texture patterns in images generated by bytes file or assembly file. However using image features alone will not give very good performance. The interactions of image features and the opcode N-gram features is most useful.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-4.14.27-PM-e1432682140406.png""><img class=""aligncenter size-full wp-image-4918"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-4.14.27-PM-e1432682140406.png"" alt=""Screen Shot 2015-05-26 at 4.14.27 PM"" width=""600"" height=""261"" /></a>
<h2>Were you surprised by any of your findings?</h2>
<strong>Little Boat:</strong> I was always surprised by any improvement we had from including some new features. But Rcarson found a good explanation for them every time! Such a great teammate!

<strong>rcarson:</strong> I’m very impressed by the asm file pixel density features discovered by Little Boat. It is so powerful yet simple and I have never seen it mentioned anywhere.

<strong>Xueer Chen:</strong> Most of my findings doesn’t improve the performance of the model. I’m very impressed by my teammates’ work.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-4.10.21-PM-e1432681864389.png""><img class=""aligncenter size-full wp-image-4916"" src=""http://blog.kaggle.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-4.10.21-PM-e1432681864389.png"" alt=""Screen Shot 2015-05-26 at 4.10.21 PM"" width=""600"" height=""266"" /></a>
<h2>Which tools did you use?</h2>
<strong>Little Boat:</strong> Code everything with Python. Run feature extraction with Pypy and model them with XGBoost.

<strong>rcarson:</strong> The feature engineering code is in pure python, ever without numpy and pandas package. The benefit is that it is very memory efficient, one line at a time, and can be accelerated by pypy.

<strong>Xueer Chen:</strong> I use matlab to do all the image processing.
<h2>What have you taken away from this competition?</h2>
<strong>Little Boat:</strong> Everything can happen if you have a good team.

<strong>rcarson:</strong> Cross validation is more trustworthy than domain knowledge.

<strong>Xueer Chen:</strong> Simple features, such as pixel intensity, can be more useful than high-level features, such as grey level co-occurrence matrix.
<h2>How did your team form?</h2>
<strong>Little Boat:</strong> We teamed up in the Avazu competition. So it is pretty easy to send them an invitation.

<strong>rcarson:</strong> We teamed up before and we found each other close on the learderboard this time.

<strong>Xueer Chen:</strong> We merged with Little Boat in the early part of the contest. We had great teamwork before.
<h2>How did competing on a team help you succeed?</h2>
<strong>Little Boat:</strong> I am always very excited when I join some new competition. And then the excitement slowly goes away after trying a couple of models, a couple feature engineering tricks, and a couple of blendings. And most of the time I would just give up in the middle of the competition and move on to another one. <strong>But with a good team, they bring in new ideas, and it kind of pushes you to the end. That excitement just follows you along the journey and finds a way better spot on the Leaderboard for you.</strong>

<strong>rcarson:</strong> Teamwork changes everything. <strong>We motivate and inspire each other along the way.</strong> Xiaozhou Wang’s discovery of pixel intensity features of asm file and semi-supervised learning trick is the most impressive modeling skills I have ever seen.

<strong>Xueer Chen:</strong> <strong>Competing in a team really allows me to focus on something that I like and I am good at.</strong> It is a pity that I didn’t find high level image features and bio-inspired models work in this challenge but it may work in future.

&nbsp;

<hr />

For more details on the team's approach, don't miss their video presentation from the BIG 2015 conference:

<iframe src=""https://www.youtube.com/embed/VLQTRlLGz5Y?feature=oembed&amp;showinfo=0"" width=""616"" height=""347"" frameborder=""0"" allowfullscreen=""allowfullscreen""></iframe>
<h2></h2>"
"How Much Did It Rain? Winner's Interview: 2nd place, No Rain No Gain",http://blog.kaggle.com/2015/06/08/how-much-did-it-rain-winners-interview-2nd-place-no-rain-no-gain/,2015-06-08 21:57:14,"Kagglers Sudalai (aka <a href=""https://www.kaggle.com/sudalairajkumar"" target=""_blank"">SRK</a>) and Marios (aka <a href=""https://www.kaggle.com/kazanova"" target=""_blank"">Kazanova</a>) came together to form team ""No Rain No Gain!"" and <a href=""https://www.kaggle.com/c/how-much-did-it-rain/leaderboard/private"" target=""_blank"">take second place</a> in the <a href=""https://www.kaggle.com/c/how-much-did-it-rain"" target=""_blank"">How Much Did it Rain?</a> competition. Sudalai had two goals in competing: to earn a Master's badge and to finish in the top 100. In the blog below, Sudalai shares how he managed to accomplish both (and get a new friend) by being part of a great team.

[caption id=""attachment_4967"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/rainicons_nofreehunch.png""><img class=""size-medium wp-image-4967"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/rainicons_nofreehunch-300x87.png"" alt=""351 players on 321 teams took on the difficult task of predicting probabilistic distributions of hourly rain given polarimetric radar measurements"" width=""300"" height=""87"" /></a> 351 players on 321 teams built models to predict probabilistic distributions of hourly rainfall[/caption]
<h2>The Basics</h2>
[caption id=""attachment_4966"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/srkprofile_nofreehunch.png""><img class=""wp-image-4966 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/srkprofile_nofreehunch-300x143.png"" alt=""Sudalai (aka SRK)'s Kaggle profile"" width=""300"" height=""143"" /></a> Sudalai's Kaggle <a href=""https://www.kaggle.com/sudalairajkumar"" target=""_blank"">profile</a>[/caption]
<h3>What was your background prior to entering this challenge?</h3>
I am a data scientist from India with five years of experience. I have a certification in Business Analytics from <a href=""http://www.iimb.ernet.in/"" target=""_blank"">IIM-Banglore</a> after earning my undergraduate degree. Also I have participated in quite a few Kaggle competitions.
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
Nope. This is an absolutely new domain to me.
<h3>How did you get started competing on Kaggle?</h3>
My Kaggle journey started two years ago. I was looking for an opportunity to learn data science and machine learning concepts through hands-on experimentation. That is when I stumbled up on the “<a href=""https://www.kaggle.com/c/stumbleupon"" target=""_blank"">StumbleUpon</a>” Kaggle competition and started competing with the help of <a href=""https://www.kaggle.com/abhishek"" target=""_blank"">Abhishek</a>’s <a href=""https://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878"" target=""_blank"">benchmark</a> code.
<h3>What made you decide to enter this competition?</h3>
This competition looked quite interesting in various aspects. Understanding the objective itself was quite challenging at the first place. Then the format of the input data was quite complex and different from the other competitions. Also, one could potentially try both regression and classification methodologies in this competition. All these challenges pushed me to take an attempt.
<h2><strong>Let's Get Technical</strong></h2>
<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/norainnogains_Approach-e1433796516857.png""><img class="" size-large wp-image-4959 alignnone"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/norainnogains_Approach-1024x474.png"" alt=""norainnogains_Approach"" width=""640"" height=""296"" /></a>
<h3>What preprocessing and supervised learning methods did you use?</h3>
I think feature extraction is also an important step in this competition along with machine learning. We have spent good amount of time coding up the features. We computed a number of features including mean value of the radarwise means, mean value of the good quality radar, mean value of the radar that scans the longest time, percentage of missing values, interaction between the mean values and time of scan and others.

[caption id=""attachment_4976"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/Cumulative_rainfall_percentage_for_RR1_percentile_bins.png""><img class=""wp-image-4976"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/Cumulative_rainfall_percentage_for_RR1_percentile_bins.png"" alt=""Cumulative rainfall percentages for RR1 percentile bins. See the python code &amp; output on Kaggle scripts"" width=""612"" height=""612"" /></a> Cumulative rainfall percentages for RR1 percentile bins. See the <a href=""https://www.kaggle.com/sudalairajkumar/how-much-did-it-rain/rainfall-of-rr1-percentile-bins"" target=""_blank"">python code</a> &amp; output on Kaggle scripts[/caption]

We have tried different techniques such as Extra trees regressor, linear regression with regularization, Gradient Boosting classifier and Random Forest Classifier. Our Final model is an ensemble of extra-trees regressor and gradient boosting classifier (50-50). The main intuition behind the selection of these 2 models was the gap between cv and LB. The first model (Extra trees regressor) was tuned based on the leaderboard feedback. The second was tuned at initial stages based on validation sample (70-30 split). We think part of the reason we advanced from public to private was the fact that we had a flavour of “unoverfitted” substance in our models!
<h3>Were you surprised by any of your findings?</h3>
We were surprised to see the improvement when we removed the rainfall values greater than 70mm during training. As one could see from the rainfall plot, there are quite a few high values (rainfall greater than 70mm) in the training set. We thought that these rainfall values are mostly errors in rain gauge measurements and so we decided to remove them. It improved our rank all of a sudden. But we realized this only during the last week of the competition and so we couldn't capitalize on it much after that!

[caption id=""attachment_4963"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/614.png""><img class=""wp-image-4963 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/614.png"" alt="""" width=""614"" height=""357"" /></a> See the <a href=""https://www.kaggle.com/sudalairajkumar/how-much-did-it-rain/rainfall-histogram-plot"" target=""_blank"">python code</a> for this histogram on Kaggle scripts[/caption]
<h3>Which tools did you use?</h3>
Python all the way. Coded up the feature extraction part on our own and used scikit-learn for modeling algorithms along with xgboost.
<h3>How did you spend your time on this competition?</h3>
In this competition, I spent about 60 to 70% of the time on feature engineering (creation, selection etc) and the rest on machine learning.
<h2><strong>Teamwork</strong></h2>
<h3>How did your team form?</h3>
In the past, I have seen people forming teams at the later stages of a competition and improving their results. I have not tried that successfully so far. Also previously once I have asked Marios for teaming up in another competition. But unfortunately there we could not form a team. So towards the end of this competition (when I was almost out of all my tricks) I reached out to Marios again and this time we successfully formed a team.
<h3>How did competing on a team help you succeed?</h3>
Since we formed a team at the very later stage of this competition, both of us already have our own set of variables and models. So ensembling two entirely different approaches helped a lot.

Also since the number of submissions is limited to 1 per day in this competition, we used to brainstorm a lot before making each submission and also in the process I learned a lot from Marios.
<h2><strong>Words of Wisdom</strong></h2>
<h3>What have you taken away from this competition?</h3>
I was trying to become a “Master Kaggler” and to get a place in top 100. With this competition, both of them came true. Apart from this, I got a very good friend (Marios) and some bragging rights!

On the data science side, I learnt about the importance of <strong>outlier removal</strong>, bagging, extra-trees classifier and ensembling tricks. (Thanks, Marios!)
<h2><strong>Bio</strong></h2>
<strong>Sudalai Rajkumar<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/Sudalai_nofreehunch.png""><img class=""alignright wp-image-4964"" style=""margin-left: 15px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/Sudalai_nofreehunch.png"" alt=""Sudalai_nofreehunch"" width=""120"" height=""120"" /></a></strong> earned his certification in Business Analytics and Intelligence from <a href=""http://www.iimb.ernet.in/"" target=""_blank"">IIM-Bangalore</a> and his bachelors in engineering from <a href=""http://www.psgtech.edu/"" target=""_blank"">PSG College of Technology</a>, India. He is currently working as a Data Scientist at <a href=""http://www.tigeranalytics.com/"" target=""_blank"">Tiger Analytics</a>, a data science consulting company. He is interested in solving real world data science problems, machine learning and Kaggling."
"Otto Product Classification Winner's Interview: 2nd place, Alexander Guschin ¯\_(ツ)_/¯",http://blog.kaggle.com/2015/06/09/otto-product-classification-winners-interview-2nd-place-alexander-guschin/,2015-06-09 19:05:03,"The <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge"" target=""_blank"">Otto Group Product Classification Challenge</a> made Kaggle history as our most popular competition ever. <a href=""https://www.kaggle.com/aguschin"" target=""_blank"">Alexander Guschin</a> finished <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge/leaderboard/private"" target=""_blank"">in 2nd place</a> ahead of 3,845 other data scientists. In this blog, Alexander shares his stacking centered approach and explains why you should never underestimate the nearest neighbours algorithm.

[caption id=""attachment_4980"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/ottogroup_nofreehunch.png""><img class=""wp-image-4980 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/ottogroup_nofreehunch-300x101.png"" alt="""" width=""300"" height=""101"" /></a> 3,848 players on 3,514 teams competed to classify items across Otto Group's product lines[/caption]
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I have some theoretical understanding of machine learning thanks to my base institute (<a href=""http://mipt.ru/en/"" target=""_blank"">Moscow Institute of Physics and Technology</a>) and our professor <a href=""https://scholar.google.com/citations?user=KIW4fnsAAAAJ"" target=""_blank"">Konstantin Vorontsov</a>, one of the top Russian machine learning specialists. As for my acquaintance with practical problems, another great Russian data scientist who once was Top-1 on Kaggle, <a href=""https://www.kaggle.com/dyakonov"" target=""_blank"">Alexander D’yakonov</a>, used to teach a course on practical machine learning every autumn which gave me very good basis. Kagglers may know this course as <a href=""http://www.machinelearning.ru/wiki/index.php?title=%D0%A1%D0%BF%D0%B5%D1%86%D0%BA%D1%83%D1%80%D1%81_%C2%AB%D0%9F%D1%80%D0%B8%D0%BA%D0%BB%D0%B0%D0%B4%D0%BD%D1%8B%D0%B5_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B8_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85%C2%BB"" target=""_blank"">PZAD</a>.

[caption id=""attachment_4982"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/alexanderguschin_nofreehunch.png""><img class=""size-medium wp-image-4982"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/alexanderguschin_nofreehunch-300x142.png"" alt=""Alexander's Kaggle profile"" width=""300"" height=""142"" /></a> Alexander's <a href=""https://www.kaggle.com/aguschin"" target=""_blank"">profile</a> on Kaggle[/caption]
<h3></h3>
<h3>How did you get started competing on Kaggle?</h3>
I got started in 2014’s autumn in “<a href=""https://www.kaggle.com/c/forest-cover-type-prediction"" target=""_blank"">Forest Cover Type Prediction</a>”. At that time I had no experience in solving machine learning problems. I found excellent benchmarks in “<a href=""https://www.kaggle.com/c/titanic"" target=""_blank"">Titanic: Machine Learning from Disaster</a>” which helped me a lot. After that I understand that machine learning is extremely interesting for me and just tried to participate in every competition I could.
<h3>What made you decide to enter this competition?</h3>
I wanted to check some ideas for my bachelor work. I liked that Otto competition has quite reliable dataset. You can check everything on cross-validation and changes on CV were close enough to leaderboard. Also, the spirit of competition is quite appropriate for checking ideas.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
[caption id=""attachment_4970"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/pic_stacking-e1433873433697.png""><img class=""wp-image-4970 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/pic_stacking-e1433873433697.png"" alt="""" width=""612"" height=""295"" /></a> My solution’s stacking schema[/caption]

<strong>The main idea of my solution is stacking.</strong> Stacking helps you to combine different methods’ predictions of Y (or labels when it comes to multiclass problems) as “metafeatures”. Basically, to obtain metafeature for train, you split your data into K folds, training K models on K-1 parts while making prediction for 1 part that was left aside for each K-1 group. To obtain metafeature for test, you can average predictions from these K models or make single prediction based on all train data. After that you train metaclassifier on features &amp; metafeatures and average predictions if you have several metaclassifiers.

In the beginning of working on the competition I found useful to split data in two groups : (1) train &amp; test, (2) TF-IDF(train) &amp; TF-IDF(test). Many parts of my solution use these two groups in parallel.

<strong>Talking about supervised methods, I’ve found that Xgboost and neural networks both give good results on data.</strong> Thus I decided to use them as metaclassifiers in my ensemble.

Nevertheless, KNN usually gives predictions that are very different from decision trees or neural networks, so I include them on the first level of ensemble as metafeatures. Random forest and xgboost also happened to be useful as metafeatures.
<h3>What was your most important insight into the data?</h3>
Probably the main insight was that <strong>KNN is capable of making very good metafeatures</strong>. Never underestimate nearest neighbours algorithm.

Very important were to combine NN and XGB predictions on the second level. While my final second- level NN and XGB separately scored around .391 on private LB, the combination of them achieved .386, which is very significant improvement. Bagging on the second level helped a lot too.

[caption id=""attachment_4973"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/pic_tsne1.png""><img class=""wp-image-4973 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/pic_tsne1-e1433873262206.png"" alt="""" width=""612"" height=""592"" /></a> TSNE in 2 dimensions[/caption]

Beside this, TSNE in 2 dimensions looks very interesting. We can see on the plot that we have some examples which most likely will be misclassified by our algorithm. It does mean that it won’t be easy to find a way to post-process our predictions to improve logloss.

Also, it seemed interesting that some classes were related closer than others, for example class 1 and class 2. It’s worth trying to distinguish these classes specially.

[caption id=""attachment_4974"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/pic_class_prob-e1433873344956.png""><img class=""wp-image-4974 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/pic_class_prob-e1433873344956.png"" alt="""" width=""612"" height=""615"" /></a> Final model’s predictions for holdout’[/caption]
<h3>Were you surprised by any of your findings?</h3>
Unfortunately, it appears that you won’t necessarily improve your model if you will make your metafeatures better. And when it comes to ensembling, all that you can count on is your understanding of algorithms<strong> (basically, the more diverse metafeatures you have, the better)</strong> and effort to try as many metafeatures as possible.

[caption id=""attachment_4972"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/pic_3x3-e1433873114388.png""><img class=""wp-image-4972 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/pic_3x3-e1433873114388.png"" alt="""" width=""612"" height=""632"" /></a> The more diverse metafeatures you have, the better. Metafeature by Extratrees vs metafeature by Neural Network.[/caption]
<h3>Which tools did you use?</h3>
I only used sklearn, xgboost, lasagne. These are perfect machine learning libraries and I would recommend them to anyone who is starting to compete on Kaggle. Relying on my past experience they are sufficient to try different methods and achieve great results in most Kaggle competitions.
<h2>Words of Wisdom</h2>
<h3>Do you have any advice for those just getting started in data science?</h3>
I think that the most useful advice here is try not to stuck trying to fine-tune parameters or stuck using the same approaches every competition. Read through forums, understand winning solutions of past competitions and all of this will give you significant boost whatever your level is. <strong>In another words, my point is that reading past solutions is as important as solving competitions.</strong>

Also, when you first starting to work on machine learning problems you could make some nasty mistakes which will cost you a lot of time and efforts. Thus it is great if you can work in a team with someone and ask him to check you code or try the same methods on his own. Besides always compare your performance with people on forums. <strong>When you see that you algorithm performs much worse than people report on forum, go and check benchmarks for this and other recent competitions and try to figure out the mistake.</strong>
<h2>Bio</h2>
<strong>Alexander Guschin<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/alexanderguschin.jpeg""><img class=""wp-image-4975 alignright"" style=""margin-left: 15px; margin-top: 5px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/alexanderguschin.jpeg"" alt=""alexanderguschin"" width=""95"" height=""95"" /></a></strong> is 4th year student in <a href=""http://mipt.ru/en/"" target=""_blank"">Moscow Institute of Physics and Technology</a>. Currently, Alexander is finishing his bachelor diploma work about ensembling methods."
"Facebook IV Winner's Interview: 2nd place, Kiri Nichol(aka small yellow duck)",http://blog.kaggle.com/2015/06/19/facebook-iv-winners-interview-2nd-place-kiri-nicholaka-small-yellow-duck/,2015-06-19 17:12:42,"Facebook's fourth recruiting competition, <a href=""https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot"" target=""_blank"">Humans or Robots?</a>, wrapped up on June 8 as the most popular recruiting competition in Kaggle's history. A record number of 985 teams competed for a chance to interview for a machine learning software engineering role at the world's most iconic social media company.

The competition challenged participants to identify human vs. robot bidders in data from on a fictional online auction site. In this blog, second place winner Kiri Nichol (aka <a href=""https://www.kaggle.com/smallyellowduck"" target=""_blank"">small yellow duck</a>) outlines the approach that led to a final RandomForestClassifier ensemble and discusses the value of Kaggle's competition format and forums.

[caption id=""attachment_4995"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/facebook_purplebot2-e1434401714429.jpg""><img class=""wp-image-4995 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/facebook_purplebot2-300x141.jpg"" alt="""" width=""300"" height=""141"" /></a> 985 data scientists competed to identify humans vs. robots[/caption]
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I have a PhD in physics - my thesis was on applying statistical mechanics to granular materials. After that, I worked for a bit as an imaging researcher in the Radiotherapy department of the Dutch Cancer Institute (NKI). Most of my formal background in computer science is from high school and a <a href=""http://laplace.physics.ubc.ca/People/matt/Teaching/03Fall/PHYS410/Notes.html"" target=""_blank"">really excellent course</a> I took as an undergrad on scientific programming in <a href=""https://en.wikipedia.org/wiki/Fortran"" target=""_blank"">Fortran</a> - matrix operations, optimization problems, parallelization, that sort of thing.

[caption id=""attachment_4991"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-1.26.42-PM.png""><img class=""size-medium wp-image-4991"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-1.26.42-PM-300x142.png"" alt=""Kaggle profile for Kiri (aka small yellow ducks)"" width=""300"" height=""142"" /></a> Kaggle <a href=""https://www.kaggle.com/smallyellowduck"" target=""_blank"">profile</a> for Kiri (aka small yellow duck)[/caption]

On the way to the PhD I ended up using pretty much every data analysis tool out there - <a href=""http://www.wolfram.com/mathematica/"" target=""_blank"">Mathematica</a>, <a href=""http://www.maplesoft.com/products/Maple/"" target=""_blank"">Maple</a>, <a href=""https://en.wikipedia.org/wiki/IDL_(programming_language)"" target=""_blank"">IDL</a> and <a href=""http://www.mathworks.com/products/matlab/"" target=""_blank"">Matlab</a> - and I also implemented a lot of toy models in Fortran. My machine learning knowledge was mostly acquired tackling problems on Kaggle and following Andrew Ng's <a href=""https://www.coursera.org/course/ml"" target=""_blank"">Introduction to Machine Learning class</a> on Coursera. And I've had some help from friends from university - one coached me through <a href=""http://aws.amazon.com/"" target=""_blank"">Amazon Web Services</a> and another taught me a lot about natural language processing.
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
&lt;laughs&gt; I swear I've never sniped an auction!
<h3>How did you get started competing on Kaggle?</h3>
Just before I left the Netherlands to move back to Canada a friend showed me Kaggle and I thought ""WOW! All these problems sound really, really interesting!""  I didn't have a job lined up when I moved, so I decided to throw myself into learning about machine learning and just work on whatever happened to spark my interest. I did the <a href=""https://www.kaggle.com/c/titanic"" target=""_blank"">Titanic problem</a> to learn about <a href=""http://scikit-learn.org/stable/"" target=""_blank"">sklearn</a> and then I started tackling competitions.
<h3>What made you decide to enter this competition?</h3>
I'd been working on another project for a while and I was starting to get stuck, so I decided I needed a break from it. There were three weeks left in the Bot vs Human competition and that seemed like the right amount of time for me get somewhere with the data.
<h2>Let's Get Technical</h2>
<h3>What supervised learning methods did you use?</h3>
I tried out pretty much all the sklearn classifiers that generate probabilities -  <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" target=""_blank"">RandomForestClassifier</a> performed the best. <strong>My final submission was a simple average of the probabilities predicted by five instances of RandomForestClassifier </strong>(each initialized with a different random number).
<h3>What was your most important insight into the data?</h3>
<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/robot_banner-e1434400369278.png""><img class=""aligncenter wp-image-4993"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/robot_banner-e1434400369278.png"" alt="""" width=""400"" height=""96"" /></a>

<strong>Robots bid a lot and they bid fast.</strong> Just two features - the mean number of bids per auction and the median time between subsequent bids by each user got me to a score of 0.89ish.
<h3>Were you surprised by any of your findings?</h3>
When I made a histogram of bids per unit time was really surprised that once a day there was a sharp peak in bidding activity by humans. It seemed weird that auctions all over the world would end at the same time, but I couldn't think of any other explanation for the peak.

<img class="" aligncenter"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://small-yellow-duck.github.io/facebook_auction/bids_over_time_zoom.png"" alt="""" width=""800"" height=""600"" />

I also made a histogram of the time between each bid and the last bid recorded for the auction - there were two clusters in this plot, which suggested that some of the auctions actually went on for more than two weeks. I naively used the median time from each bid until the last bid placed in an auction as a feature - it ended up being a much more useful feature than I'd expected, which puzzled me. I also couldn't figure out why there were hardly any bids placed by robots between 11 and 14 days before the end of the auction. I wondered if this behaviour might have helped to explain why we only got to see three days out of every two weeks in the data....

<img class="" aligncenter"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://small-yellow-duck.github.io/facebook_auction/hist_time_before_auction_end_bots_humans_norm.png"" alt="""" width=""800"" height=""600"" />
<h3>Which tools did you use?</h3>
I'm a Python fan. I used sklearn for the classification and <a href=""http://pandas.pydata.org/"" target=""_blank"">pandas</a> for manipulating the bidding data. This competition was great for forcing me to be clever about how I did operations on dataframes.
<h3>How did you spend your time on this competition?</h3>
I spent about three days poking around in the data, bringing my pandas skills up to speed, defining some very simple features (time between bids, mean bids per auction) and trying out various classifiers in the sklearn arsenal. Once I'd settled on RandomForestClassifier, most of the remaining time went into refining features, generating more features and getting more clever about calculating the length of a day.

Once I realized that I had a shot at the top ten, I invested some time trying to squeeze value out of the bidder address and payment_account and using clustering to group bidders with overlapping IPs. I also spent a fair bit of time trying to blend different models  with feature-weighted linear stacking, as well as trying to reduce overfitting using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV"" target=""_blank"">recursive feature elimination</a>, but neither effort improved my cross-validation scores.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
Executing the script for training and predicting only took about three minutes on my modest little laptop. The time-intensive part of the development process was cross-validation because it was necessary to repeatedly test the model on 100+ different train/valid splits in order to get a reasonable estimate for what sort of score I could expect.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from your participation in Kaggle competitions?</h3>
This competition helped me to be smarter about manipulating data with pandas. The other moral that always bears repeating is that it pays to look at sample chunks of data - both the raw data and plots. No matter how many smart things I think I've done, <strong>whenever I make a plot I think ""why did I wait so long to look at this?""</strong>

For me, the main benefit of participating in Kaggle is motivational: I am a person who is driven to learn when there is an interesting problem that I would like to solve. <strong>I've also learned a lot from the clever folks who've shared their strategies and insights on the <a href=""https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot/forums"" target=""_blank"">message board</a>.</strong>

[caption id=""attachment_4997"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-2.05.27-PM-e1434402393647.png""><img class=""wp-image-4997 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-2.05.27-PM-e1434402393647.png"" alt=""Screen Shot 2015-06-15 at 2.05.27 PM"" width=""500"" height=""58"" /></a> Kagglers regularly share their insights and approaches in the <a href=""https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot/forums"" target=""_blank"">forums</a>[/caption]

I think that Kagglefication presents a really valuable way of doing research, especially medical or engineering research of the nature ""we introduce technique X for performing task Y"". Usually these sorts of papers are phenomenally boring to read and they also make it difficult to compare different techniques for performing the same task. Kagglefication has a couple of advantages: first, it forces people to get together and identify a problem whose solution is actually really valuable. Second, Kagglefication obliges people to cooperate to create a decent-sized data set (which in medicine is not always such an easy feat). Third, having everyone work on the same data set makes it much more straightforward to compare the effectiveness of different approaches. <strong>Fourth, I think that people learn better from others' solutions when they have invested effort in trying to figure out how to solve a problem themselves.</strong> Finally, Kagglefication provides a metric other than ""number of publications"" and ""poshness of university"" to assess researchers.
<h2>Just for Fun</h2>
<h3>Why would you like to work at Facebook as a machine learning engineer?</h3>
In the past year I've gotten really excited about using neural networks to do image segmentation, so I'm pretty curious about the ""friend tagger"". I've been collecting ""friend tagger"" failures and I have some ideas about how to generate passive feedback about whether the friend tagger has tagged somebody correctly. I'd also be interested in working on sock-puppet detection and account fraud. And I'm curious about how people with limited computer skills or internet access use Facebook. I had the opportunity to travel a bit in Brazil last year and I found out that EVERYBODY has Facebook, even if they don't have electricity or plumbing. Facebook is in a position to help people learn how to get around on the internet.
<h3>What's your dream job?</h3>
I'm pretty open. Something where I have to answer worthwhile questions and solve interesting problems - something which is a tough technical challenge and requires being creative. Having nice colleagues is important too.

I'd love to work more in medicine. But medicine can be very frustrating. It's very hard for hospitals and research organizations and businesses to share data - and there aren't really any incentives to do so. Even in Canada and the Netherlands, where medical care is socialized, hospitals just don't have the digital infrastructure to move patient records around, to make records available for study. If a patient is treated in Amsterdam and then has complications five years later and has further treatment in Groningen, the hospital in Amsterdam doesn't necessarily find this out. It's the same situation in Canada.

We're throwing away opportunities to answer questions about what makes for effective (and cost-effective) care. In the US there is <a href=""https://www.cancercommons.org/tag/donate-your-data/"" target=""_blank"">one organization</a> that I'm aware of that is trying to initiate a patient-driven system which would allow individuals to make their data available in exchange for treatment advice. But it's very hard to collect complete data this way. <strong>One of my dreams would be to have a box on your organ-donor card that says ""Donate My Data To Science"".</strong>

[caption id=""attachment_4998"" align=""aligncenter"" width=""350""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/cancercommons.png""><img class=""wp-image-4998"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/cancercommons.png"" alt=""Cancer Commons is a nonprofit that unites patients, oncologists and scientists to make sure critical information gets shared with the patients who need it."" width=""350"" height=""183"" /></a> <a href=""https://www.cancercommons.org/"" target=""_blank"">Cancer Commons</a> is a nonprofit that unites patients, oncologists and scientists to make sure critical information gets shared with the patients who need it.[/caption]

Another challenge with doing medical research is that there is a separation between hospitals (which have data) and businesses developing drugs, apparatus and software (which have money and technical expertise). Fifty years ago, medical research could be done in hospitals, but it's reached the point where developing drugs, developing tools for genetic analysis and developing software for delivering radiation has become so sophisticated that it's really hard for a hospital or a single lab to do this sort of development. I'm not sure if there is a better solution to this problem than the status quo, but maybe we need to start having a conversation about it.
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
Ooooh - I'd love to have a competition to do automatic segmentation of organs and tissues in CT or MRI scans. <strong>The problem is basically to build a Facebook friend tagger for internal organs.</strong> Right now, automatic segmentation routines for medical images just aren't good enough, so technicians still delineate all the relevant tissues in a CT scan before a cancer patient can have radiotherapy. But radiotherapy could be improved if there were tools available for rapid, reliable tissue delineation."
"Profiling Top Kagglers: Owen Zhang, Currently #1 in the World",http://blog.kaggle.com/2015/06/22/profiling-top-kagglers-owen-zhang-currently-1-in-the-world/,2015-06-22 20:24:39,"Next up in our series on top Kagglers is the #1: <a href=""https://www.kaggle.com/owenzhang1"" target=""_blank"">Owen Zhang</a> (Zhonghua Zhang). Owen comes from an engineering background and currently works as the Chief Product Officer at <a href=""http://www.datarobot.com/"" target=""_blank"">DataRobot</a>.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/owen-badge.png""><img class=""aligncenter size-medium wp-image-5004"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/owen-badge-300x140.png"" alt=""owen-badge"" width=""300"" height=""140"" /></a>
<h2>Owen Q&amp;A</h2>
<h3>How did you start with Kaggle competitions?</h3>
Back in 2011 I had just switched to analytics as a full time job (after several years working in IT), and was eager to improve my skills and to “prove myself”.

So it was fortuitous that Kaggle came along with the <a href=""https://www.kaggle.com/c/ClaimPredictionChallenge"" target=""_blank"">first Allstate competition</a>.

Being in the same industry I felt I had some advantage as well. I was lucky and got <a href=""http://blog.kaggle.com/2012/01/30/owen-zhang-on-placing-2nd-in-the-claim-prediction-challenge/"" target=""_blank"">2nd place</a>. As a consequence I became a Kaggle addict.
<h3>What is your first plan of action when working on a new competition?</h3>
Submit a “reasonable” model as soon as possible -- to validate my understanding of the problem and the data.
<h3>What does your iteration cycle look like?</h3>
It depends on the competition and I usually go through a few stages.

At the beginning I focus on data exploration and try some basic approaches so I iterate pretty quickly.

Once the obvious ideas are exhausted I usually slow down and do some research into the domain -- reading papers, forum post, etc. If I get an idea I would then implement it and submit it to the public LB.

My iteration cycle usually is short -- I rarely work on feature engineering that requires more than a few hours of coding for a particular feature.

My personal experience is that very complicated features usually do not work well -- possibly because of my buggy code.
<p style=""text-align: center;""><iframe src=""https://www.youtube.com/embed/LgLcfZjNF44?feature=oembed&amp;showinfo=0"" width=""560"" height=""315"" frameborder=""0"" allowfullscreen=""allowfullscreen""></iframe>
<small><i><a href=""http://nycdatascience.com/news/featured-talk-1-kaggle-data-scientist-owen-zhang/"" target=""_blank"">Tips on Modeling Competitions on Kaggle</a> (2015) Owen Zhang</i>, NYC Data Science Academy</small></p>

<h3>What are your favorite machine learning algorithms?</h3>
Again it depends on the problem, but if I have to pick one, then it is GBM (its <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> flavor). It is amazing how well it works in many different problems.

Also I am a big fan of online SGD, <a href=""http://libfm.org/"" target=""_blank"">factorization</a>, neural networks, each for particular types of problems.
<h3>What are your favorite machine learning libraries?</h3>
I used to be a very focus R user until about 1 year ago. But now I am more of a Python user who use whichever library that’s needed, including <a href=""http://scikit-learn.org/stable/"" target=""_blank"">scikit-learn</a>, XGBoost, VW, <a href=""https://code.google.com/p/cuda-convnet2/"" target=""_blank"">cuda-convnet2</a> and others.
<h3>What is your approach to hyper-tuning parameters?</h3>
I mostly tune by hand -- starting with reasonable values and doing trial and error based <a href=""https://en.wikipedia.org/wiki/Hill_climbing"" target=""_blank"">hill climbing</a>. This fits my style -- I like to tinker with the problem in an ad-hoc fashion.

Sometimes I think I should eventually be more disciplined and systematic, using some <a href=""https://en.wikipedia.org/wiki/Bayesian_optimization"" target=""_blank"">Bayesian optimizer</a>. However that would take some fun away from the process, I think.
<h3>What is your approach to solid CV/final submission selection and LB fit?</h3>
This really depends on if there is a public LB <a href=""https://www.kaggle.com/wiki/Leakage"" target=""_blank"">data leak</a>.

For time-sensitive models, it is very common that the public LB gives us “hints” about the private LB. In that case I use almost exclusively the public LB results as validation. If that is the case, I would usually give public LB more weight.

In the case the data is small/noise, I use a combination of CV and public LB.

I also usually choose 2 different models as final results -- one favors public LB and one favors CV.
<h3>In a few words: What wins competitions?</h3>
Luck + hard work + experience

[caption id=""attachment_5006"" align=""aligncenter"" width=""415""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/owen-results.png""><img class=""wp-image-5006 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/owen-results.png"" alt=""owen-results"" width=""415"" height=""285"" /></a> Owen's top 8 finishes[/caption]
<h3>What is your favourite Kaggle competition and why?</h3>
Every competition is different and it is hard to pick a single favorite, so I will pick a few:
<ul>
	<li>The <a href=""https://www.kaggle.com/c/amazon-employee-access-challenge"" target=""_blank"">Amazon access challenge</a> was good because
<ol>
	<li>I got good result (2nd place),</li>
	<li>almost all the different things I tried in the competition worked,</li>
	<li>I did better than many of my long time nemeses, such as <a href=""https://www.kaggle.com/leustagos"" target=""_blank"">Leustagos</a> and <a href=""https://www.kaggle.com/xavierconort"" target=""_blank"">Gxav</a> (Xavier Conort).</li>
</ol>
</li>
	<li>The <a href=""https://www.kaggle.com/c/dogs-vs-cats"" target=""_blank"">Dogs vs. Cats</a> was very interesting because the problem was interesting. Also this is the first time I actually started learning CNN and it was really fascinating.</li>
	<li>The recent <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge"" target=""_blank"">Otto competition</a> -- I did quite badly and it is not because of data noise. From methodology perspective I think I learned a lot from others in this one.</li>
</ul>
<h3>What was your least favourite Kaggle competition experience?</h3>
Usually I avoid competitions with too little data or too much noise. It is just a lottery.

For example one recent competition had only 130 data points in training, so the results are just random. I don’t participate in those so it is not “experience” per se, but they do make the overall Kaggle experience less perfect than it should be.
<h3>What field in machine learning are you most excited about?</h3>
Deep learning is really popular and I am very excited about it, like everyone else.

However I would be really excited if there is a “feature engineering” field. So far it is still mostly ad hoc.
<h3>Which machine learning researchers do you study?</h3>
I learned quite a bit by reading the book <a href=""http://statweb.stanford.edu/~tibs/ElemStatLearn/"" target=""_blank"">“Elements of Statistical Learning”</a>, and I also dabble in all the deep learning publications.

However I learned most from my fellow <a href=""https://www.kaggle.com/users"" target=""_blank"">Kagglers</a>.
<h3>Can you tell us something about the last algorithm you hand-coded?</h3>
I usually use other people’s code. The last “algorithm” I wrote was just generalized linear models with elastic net penalty.

What I found is that it is usually not “efficient” (from time budget) perspective to write my own algorithm for a competition.

Almost always I can find open source code for what I want to do, and my time is much better spent doing research and feature engineering.
<p style=""text-align: center;""><iframe src=""https://www.youtube.com/embed/AR7IcHENFj0?feature=oembed&amp;showinfo=0"" width=""560"" height=""315"" frameborder=""0"" allowfullscreen=""allowfullscreen""></iframe>
<small><i><a href=""http://www.opendatascience.com/blog/videos/owen-zhang-talk-odsc-boston-2015/"" target=""_blank"">Open Source Tools and Data Science Competitions</a> (Owen Zhang) - 2015 Boston <a href=""http://www.slideshare.net/odsc"" target=""_blank"">ODSC</a></i></small></p>

<h3>How important is domain expertise for you when solving data science problems?</h3>
It is quite important because often that is an important source of feature engineering. In addition it is not easy or efficient to re-invent the wheels.

The domain can be very broad. For example, I worked in insurance companies, which make me comfortable with not only insurance problems, but business oriented problems in general. On the other hand I can see physicists doing better with more mathematical or scientific problems.

Also it is worth noting that domain expertise has different meaning in different context. To a degree, knowing “overfitting is bad” is also domain expertise.
<h3>What do you consider your most creative trick/find/approach?</h3>
I don’t think anything I do is particularly “creative”.

If I have to name one, in the <a href=""https://www.kaggle.com/c/allstate-purchase-prediction-challenge"" target=""_blank"">Allstate purchase option competition</a> (with 7 correlated target variables), I built sequential conditional models, from the most “independent” target to the least “independent” target. However I am pretty sure I was reinventing the wheel.
<h3>How are you currently using data science at work and does competing on Kaggle help with this?</h3>
Our company DataRobot’s product is built to help data scientists build generalizable models, and what I learned from Kaggle is very helpful.

For example, if I find a particular approach is effective in certain modeling context, I would make a suggestions to the team to implement that as a feature in our product.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/datarobot.png""><img class=""aligncenter size-full wp-image-5005"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/datarobot-e1434996999675.png"" alt=""datarobot"" width=""600"" height=""207"" /></a>
<h3>What is your opinion on the trade-off between high model complexity and training/test runtime?</h3>
I assume this is about the trade-off between complexity vs prediction performance. In Kaggle competitions there is no trade-off, we only care about accuracy.

However in reality we rarely implement the most complicated model, for at least 2 reasons:
<ol>
	<li>It takes too much effort to train/execute.</li>
	<li>More complicated models tend to be more brittle.</li>
</ol>
On the other hand, even if we don’t implement the best performing/most complicated model, such a model still has inherent value:
<ol>
	<li>It can provide a benchmark to evaluate simplified models against,</li>
	<li>it gives us insights that will help us make simplified models better,</li>
	<li>it helps us detect potential data problems.</li>
</ol>
<h3>What is your view on automating Machine Learning?</h3>
We are in this business so certainly I believe it is doable (and valuable) to a degree.

The mechanical part of machine learning (like CV, hyper parameter tuning, model algorithm selection) will be almost completely automated, so that data scientists can focus on problem selection, model definition, and feature engineering.
<h3>How did you get better at Kaggle competitions?</h3>
To be honest I don’t know for sure. I am not even sure that I “got better” or even that I “am good”.

I was definitely lucky. Learning from others on Kaggle certainly improves my skills over time, but the same is true for everyone.
<h3>What did you learn from doing Kaggle competitions?</h3>
Too many things to list, but here are a few that come to mind:
<ol>
	<li>Nothing is certain until the private leaderboard is revealed.</li>
	<li>There are always more things that I don’t know than I do.</li>
	<li>Relying on the public LB is dangerous.</li>
	<li>There is usually a simple elegant approach that does equally well.</li>
</ol>
<h2><small><i></i></small>Bio</h2>
<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/owen.jpg""><img class="" wp-image-5007 alignright"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/owen.jpg"" alt=""owen"" width=""130"" height=""130"" /></a><a href=""https://www.linkedin.com/pub/owen-zhang/51/aa0/363"" target=""_blank"">Owen Zhang</a> currently works as a data scientist at <a href=""http://www.datarobot.com/"" target=""_blank"">DataRobot</a> a Boston based startup company. His education background is in engineering, with a master’s degree from <a href=""http://www.utoronto.ca/"" target=""_blank"">U of Toronto</a>, Canada, and bachelor’s from <a href=""http://en.ustc.edu.cn/"" target=""_blank"">U of Science and Technology of China</a>. Before joining DataRobot, he spent more a decade in several U.S. based property and casualty insurance companies, last one being <a href=""http://www.aig.com/"" target=""_blank"">AIG</a> in New York."
"Facebook IV Winner's Interview: 1st place, Peter Best (aka fakeplastictrees)",http://blog.kaggle.com/2015/06/23/facebook-iv-winners-interview-1st-place-peter-best-aka-fakeplastictrees/,2015-06-23 23:40:00,"Peter Best (aka <a href=""https://www.kaggle.com/fakeplastictrees"" target=""_blank"">fakeplastictrees</a>) took 1st place in <a href=""https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot"" target=""_blank"">Human or Robot?</a>, our fourth Facebook recruiting competition. Finishing ahead of 984 other data scientists, Peter ignored early results from the public leaderboard and stuck to his own methodology (which involved removing select bots from the training set). In this blog, he shares what led to this winning approach and how the competition helped him grow as a data scientist.
<h2><a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/facebook_botblog.png""><img class=""aligncenter wp-image-5016"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/facebook_botblog.png"" alt="""" width=""250"" height=""161"" /></a></h2>
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
After studying chemistry for my first degree, I worked for twenty years in quantitative asset management culminating in being a partner in my own firm. After this, I went back to university to obtain a second degree, in maths this time. Recently, I have been looking for the right project to commit to. My programming experience extends all the way back to a <a href=""https://en.wikipedia.org/?title=ZX81"" target=""_blank"">ZX81</a>. Whilst I would never describe myself as a professional programmer, I seem to be able to program.

[caption id=""attachment_5014"" align=""aligncenter"" width=""378""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-23-at-4.45.02-PM.png""><img class=""size-full wp-image-5014"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-23-at-4.45.02-PM.png"" alt=""fakeplastictrees_profile"" width=""378"" height=""179"" /></a> <a href=""https://www.kaggle.com/fakeplastictrees"" target=""_blank"">Kaggle profile</a> for Peter Best (aka fakeplastictrees)[/caption]
<h3>How did you get started competing on Kaggle?</h3>
With all my experience, I decided my area of greatest interest lay in analysing complex data. I also quickly realised that my coding skills were from the previous century and thus I opted to learn python. Perhaps the best way to learn a programming language is to actually do something in it and this led me to Kaggle.

[caption id=""attachment_5017"" align=""aligncenter"" width=""525""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-25-at-11.37.52-AM.png""><img class=""size-full wp-image-5017"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-25-at-11.37.52-AM.png"" alt=""Kaggle_badges"" width=""525"" height=""113"" /></a> Peter's top competition finishes[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
Not really. Financial markets are somewhat like auctions and the people who work in financial markets are somewhat like bots (occasionally!), but I don’t think I used any specialist knowledge in this competition. Many examples of data analysis have similarities despite relating to completely different fields.
<h3>What made you decide to enter this competition?</h3>
The key to this competition looked like it was going to be feature engineering, something I regard as a strength. In addition, the size of the data set was small enough to manage easily on my Mac with short run times.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
Most features were composed by manipulating the data in the bids database then transferring some aggregate measure into a smaller database uniquely indexed by bidder. This smaller database was then used as input to a boosted trees model.
<h3>What was your most important insight into the data?</h3>
As soon as I started doing some cross-validation on an early version of my model, I found that the statistical error on the estimate of the Area Under the ROC Curve (the evaluation metric used in the competition) was going to be quite high. This led me to undertake extensive resampling in my cross-validation to ensure my estimate of the AUC was accurate enough to base decisions upon. In addition, it suggested that the public leaderboard score was likely to be a poor guide to the eventual private leaderboard score and it was going to be much better to trust my own cross-validation. As a consequence of this I only actually submitted three entries into the competition.
<h3>Were you surprised by any of your findings?</h3>
One of the most straightforward features to envisage was simply the number of bids made by each bidder. Ex ante, one could suppose that a bot could place far more bids than the average human. When a histogram of the number of bids placed by each bot was considered, however, there was a striking anomaly. Five bots were identified as having only one bid each in the data set, which looks unusual in the distribution, as shown.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/histogram-1.png""><img class=""aligncenter wp-image-5013"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/histogram-1-1024x625.png"" alt=""histogram (1)"" width=""612"" height=""374"" /></a>

Having only one bid per bot suggests that either there are some links with other bots in the data or that they have been labelled as bots using data which we don’t know. Investigation yielded no obvious links to other bots and the latter point implies that the data from these bots won’t generalise well to a different data set. Thus, the algorithms that I used may be negatively affected by trying to learn from these bot examples. I decided then to create a solution that removed these five observations from the training data set and submitted this as one of my two selected entries. It was this entry that won the competition.

Also, note that this picture was on the home page of the competition. There are five bots! Was it a clue?

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/robot_banner-e1434400369278.png""><img class=""aligncenter size-full wp-image-4993"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/robot_banner-e1434400369278.png"" alt=""robot_banner"" width=""500"" height=""120"" /></a>
<h3>Which tools did you use?</h3>
I did everything in python, using the excellent Scikit-learn package for the boosted trees algorithm.
<h3>How did you spend your time on this competition?</h3>
I tend to flit to and fro between the two [feature engineering &amp; machine learning] as I’m going along. I tend to start with some feature engineering, then perform some gross parameter tuning for the machine learning algorithm, then return to some more feature engineering and so on. In this competition the feature engineering took up a lot more time than the machine learning. I spent quite a lot of time lovingly hand-crafting features that didn’t improve the model in cross-validation, but this is normal for this sort of competition. Just because you think a feature will be useful doesn’t mean it will be.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
Just a few minutes, though each cross-validation run took over an hour.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
Enhanced self-belief in persevering with what you know to be the correct methodology and not getting distracted by things that don’t really matter (in this competition, that meant the public leaderboard!).

Also, a realisation that even though I think my methods gave me the best chance of doing well, actually winning the competition required the cards to fall my way this time.
<h3>Do you have any advice for those just getting started in data science?</h3>
Firstly, look through the Kaggle website, especially <a href=""https://www.kaggle.com/forums"" target=""_blank"">the forums</a>, to pick up a vast amount of good advice on how to improve your data science.

Mostly though, my advice is to just have a go, make mistakes and learn from these.
<h3>Bio</h3>
<strong><a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-23-at-4.35.03-PM.png""><img class=""alignright wp-image-5012"" style=""margin-left: 15px; margin-top: 5px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-23-at-4.35.03-PM.png"" alt="""" width=""115"" height=""117"" /></a>Peter Best</strong> is a graduate in both chemistry and mathematics. He worked for many years in the quantitative asset management industry where he performed extensive data analysis. He is currently competing in Kaggle competitions and trying to decide on his next project."
"How Much Did It Rain? Winner's Interview: 1st place, Devin Anzelmo",http://blog.kaggle.com/2015/07/01/how-much-did-it-rain-winners-interview-1st-place-devin-anzelmo/,2015-07-01 16:00:29,"An early insight into the importance of splitting the data on the number of radar scans in each row helped Devin Anzelmo take first place in the <a href=""https://www.kaggle.com/c/how-much-did-it-rain"" target=""_blank"">How Much Did It Rain?</a> competition. In this blog, he gets into details on his approach and shares key visualizations (with code!) from his analysis.

[caption id=""attachment_5030"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/rainblog2.png""><img class=""wp-image-5030 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/rainblog2-300x128.png"" alt="""" width=""300"" height=""128"" /></a> 351 players on 321 teams built models to predict probabilistic distributions of hourly rainfall[/caption]
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
My background is primarily in cognitive science and biology, but I dabbled in many different areas while in school. My particular interests are in human learning and behavior and how we can use human activity traces to learn to shape future actions.

[caption id=""attachment_5028"" align=""aligncenter"" width=""340""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-07-01-at-9.23.24-AM.png""><img class=""wp-image-5028 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/Screen-Shot-2015-07-01-at-9.23.24-AM.png"" alt=""Devin's profile on Kaggle"" width=""340"" height=""161"" /></a> Devin's <a href=""https://www.kaggle.com/devinanzelmo"" target=""_blank"">profile</a> on Kaggle[/caption]

My interest in gaming as a means of teaching and competitive nature has made Kaggle a great fit for my learning style. I started competing seriously on Kaggle in October 2014. I did not have much experience with programming or applied machine learning, and thought entering a competition would provide a structured introduction. Once I started competing I found I had difficult time stopping.
<h3>What made you decide to enter this competition?</h3>
I thought there was a decent chance I could get into the top five in the competition and this drove me to enter. After finishing the BCI competition I had to decide between <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge"" target=""_blank"">Otto group product challenge</a> and this one. I chose How Much Did it Rain because the dataset was difficult to process, and it wasn't obvious how to approach the problem. These factors favored my skills. I didn't feel like I could compete in Otto where the determining factor was going to primarily rely on ensembling skills.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
Most of the preprocessing was just feature generation. Like most other competitors I used descriptive statistics and counts of the different error codes. These made up the bulk of my features and turned out to be enough to get first place. We were given QC'd reflectivity data, but instead of using this information to limit the data used in feature generation I included it as a feature and let learning algorithm (Gradient Boosted Decision Trees) use it as needed.

The most important decision with regard to supervised learning was how to model the output probability distribution. I decided to model it by transforming the problem into a multi-class classification problem with soft output. Since there was not enough data to perform classification with the full 70 classes the problem had to be reduced further. It turned out there were many different ways that people solved this problem, and I highly recommend reading the <a href=""https://www.kaggle.com/c/how-much-did-it-rain/forums/t/14242/congratulations"" target=""_blank"">end of competition thread</a> for some other approaches.

[caption id=""attachment_5024"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/figure1.png""><img class=""wp-image-5024"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/figure1.png"" alt=""figure1"" width=""612"" height=""205"" /></a> See the <a href=""https://www.kaggle.com/devinanzelmo/how-much-did-it-rain/component-cdf-s-and-sample-predictions"" target=""_blank"">code</a> on scripts[/caption]

I ended up using a simple method in which basic component probability distributions were combined using the output of a classification algorithm. For classes that had enough data a step function was used for a CDF. When there was less data the several labels were combined and replaced by a single value. In this case an estimation of the empirical distribution for that class was used as a component CDF. This method worked well and I used it for most of the competition. I did try regression and classification just on the data from the minority classes but it never performed quite as well as just using the empirical distribution.
<h3>What was your most important insight into the data?</h3>
Early in the competition I discovered that it was helpful to split the data based on the number of radar scans in each row. Each row has data spanning the hour previous to the rain gauge reading. In some cases there was only one radar scan in others there was more then 50. There are over one hundred thousand rows in the training set with more then 17 radar scans. For this data I wanted to create features which take into account the changing of weather conditions over time. In doing this I realized it was not possible to make these features for the rows that had only 1 or 2 radar scans. This was the initial reason for splitting the dataset. When I started looking for places to split it I found that there was also a strong positive correlation between the number of radar scans and the average rain amount. Those rows with 1 scan had 95% 0mm of rain, while the subset with 17 or more scans only 48% of the data had 0mm of rain. Interestingly for the data with few radar scans many of the most important features were the counts of the error codes.

[caption id=""attachment_5025"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/figure2.png""><img class=""wp-image-5025"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/figure2.png"" alt=""figure2"" width=""612"" height=""204"" /></a> See the <a href=""https://www.kaggle.com/devinanzelmo/how-much-did-it-rain/log-histogram-of-label-values-version1"" target=""_blank"">code</a> on scripts[/caption]

In contrast the most important features in the data with many scans were derived from Reflectivity and HybridScan which have a physical relationship to rain amount. Splitting the data allowed me to use many more features for the higher scan data which gave a large boost to the score. Over 65% of the error came from the data with more then 7 scans. The data with low scans contributed to very small amount of the final score and I was able to spend less time modeling these subsets.
<h3>Were you surprised by any of your findings?</h3>
The most mysterious aspect of the competition was the 5000 rows in the training data that had Expected rain amount over 70mm. The requirements of the competition only asked us to model up to 69mm of rain in an hour but the evaluation metric punished large classification errors so severely that I felt compelled to figure out how to predict these large values. A quick calculation showed that of the 1.1 million rows in the training set these 5000 large values, if mis-predicted, would account for half of my error.

It turned out that many of the samples with labels above 70mm did not have reflectivity values indicating heavy rain. I was still able to improve my local validation score by treating the large rain amount samples as their own class and using an all zero CDF in generating the final prediction. Unfortunately this also worsened my public leaderboard score by a large amount.

[caption id=""attachment_5026"" align=""aligncenter"" width=""547""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/figure3.png""><img class=""wp-image-5026 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/figure3.png"" alt=""figure3"" width=""547"" height=""508"" /></a> See the <a href=""https://www.kaggle.com/devinanzelmo/how-much-did-it-rain/kde-and-scatter-plot"" target=""_blank"">code</a> on scripts[/caption]

Through leaderboard feedback I was able to determine that there were differences in the distribution of these large values in the 2013 training set and the 2014 test set. Removing the rows with large values from the training set turned out to be the best course of action.

My hypothesis about the large values is that they were generated by specific rain gauges, which the learning algorithm was able to detect using features based on DistanceToRadar and the -99903 error code. The -99903 error code can correspond to physical blockage of a radar beam by mountains or other physical objects. Both of these features can help identify specific rain gauges which would lead to overfitting the train set if there were fixes to the malfunction before the start of 2014. As I don't have access to the 2014 labels this will remain speculation for now.
<h3>Which tools did you use?</h3>
I used python for this competition relying heavily on pandas for data exploration and direct numpy implementations when I needed things to be fast. This was my first competition using Xgboost, and I was very pleased with the ease of use and speed.
<h3>How did you spend your time on this competition?</h3>
I probably spent 50% percent of my time coding, and then having to refactor when I realized my implementation was not flexible enough to incorporate my new ideas. I also tried several crazy things that required substantial programming time that I didn't end up using.

The other 50% percent was split pretty equally between feature engineering, data<span class=""s1""> exploration and tweaking my classification framework.</span>
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
I spent many hours coding and refactoring in this competition. Since I had to do nearly the same thing on five different datasets having manually code everything made it difficult to try new ideas. Having a flexible framework to try out many ideas is critical and this is one of the things I spent time learning how to do in this competition. The effort has already payed off in other competitions.

With only one submission a day it was important to try out things in a systematic way. What worked best was changing one aspect of my method and seeing whether it improved my score. I needed to keep records of everything I did or it was possible waste time redoing things I already tried. Having the discipline to keep on track and and not try too many things at once is critical for doing well and this competition put me to the test on this.
<h3>Do you have any advice for those just getting started competing on Kaggle?</h3>
Read the Kaggle <a href=""http://blog.kaggle.com/2015/05/07/profiling-top-kagglers-kazanovacurrently-2-in-the-world/"" target=""_blank"">blog post</a> profiling KazAnova for a great high level perspective on competing. I read this about two weeks before the end of the competition and I started saving my models and predictions, and automating more of my process which allowed for some late improvements.

Other then this I think its very helpful to read <a href=""https://www.kaggle.com/forums"" target=""_blank"">the forums</a> and follow up on hints given by those at the top of the leaderboard. Very often people will give small hints, and I have gotten in habit of following up on even the smallest clues. This has taught me many new things and helped me find critical insights into problems.
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
With the introduction of <a href=""https://www.kaggle.com/scripts"" target=""_blank"">Kaggle scripts</a> it seems it will now be possible to have solution code evaluated remotely instead of requiring competitors to submit a CSV submission file. I think having this functionality opens up the possibility of solving new types of problems that were not feasible in the past.

With this in mind I would like to run a problem that favors reinforcement learning based solutions. As a simple example we could teach an agent to explore mazes. The training set would consist of several different mazes (perhaps it would be good generate their own training data) and the test set could be another set of unseen mazes hosted on Kaggle. All the training code would be required to run directly on scripts making transition to an evaluation server easy. I don't think this type of problem would have worked without scripts, and I think it would be fun to see if it is possible to turn agent learning problems into Kaggle competitions.

Another possibility with remote execution of solutions would be a Rock Paper Scissors programming tournament. There are already some RPS tournaments available online. Perhaps hosting a variant as a knowledge competition would be possible as these types of competitions are really fun.
<h3>What is your dream job?</h3>
Ideally I would like to work with neural and behavioural data to help improve human performance and alleviate problems related to mental illness. There are many very challenging problems in this area. Unfortunately most of the current classification frameworks for mental illness are deeply flawed. My dream job would allow for the application of diverse descriptions, methods, and sensors, without the need to push a product out immediately.

My sense is that the amount of theoretical upheaval needed is holding back research in academia, and the ineffectiveness of most current techniques is hampering the development of new businesses (plus the legal issues of the health industry). I would be interested in any project that is making progress through this mire ;)

Beyond these interests what motivates me is the ability to explore complex problems and the freedom to try new solutions. Any job in which I can tackle a difficult problems with the ability to actually try to solve it is fundamentally interesting to me.
<h2>Bio</h2>
Devin Anzelmo is a recent graduate from University of California Merced where he received a B.S in Cognitive Science and a minor in Applied Mathematics. His research interests include machine learning, neuroscience, and animal learning and behavior. He enjoys immersion in complex problems and puzzles. He currently spends most of his time competing on Kaggle but is also interested in employment opportunities in the San Luis Obispo area."
"CrowdFlower Winners' Interview:  3rd place, Team Quartet",http://blog.kaggle.com/2015/07/22/crowdflower-winners-interview-3rd-place-team-quartet/,2015-07-22 18:50:49,"The goal of the <a href=""https://www.kaggle.com/c/crowdflower-search-relevance"" target=""_blank"">CrowdFlower Search Results Relevance competition</a> was to come up with a machine learning algorithm that can automatically evaluate the quality of the search engine of an e-commerce site. Given a query (e.g. ‘tennis shoes’) and an ensuing result (‘adidas running shoes’), the goal is to score the result on relevance, from 1 (least relevant) to 4 (most relevant). To train the algorithm, teams had access to a set of 10,000 queries/result pairs that were manually labeled by CrowdFlower using the aforementioned classes.

[caption id=""attachment_5061"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/crowdflowerbanner.png""><img class=""wp-image-5061 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/crowdflowerbanner-300x202.png"" alt=""1,424 data scientists on 1,326 teams competed to build a better model for evaluating search results relevance."" width=""300"" height=""202"" /></a> Team ""Quartet"" finished 3rd out of 1,424 data scientists on 1,326 teams. The competition closed on July 6, 2015.[/caption]
<h2>The Basics</h2>
<em>""Do not try to climb Everest all by yourself - doing it alongside a team is more fun and will probably win you more money in the end."" - Team Quartet</em>
<h3>Tell us about your team</h3>
Our team ‘Quartet’ was formed by Maher Harb, Roman Khomenko, Sergio Gamez, and Alejandro Simkievich. This was a very multi-faceted and multi-cultural team indeed, with folks ranging from physicists to computer scientists to business majors and nationalities from Canada, Ukraine, Spain, and Brazil. While diverse, we are guilty of being an all-male team and yet we hope we can fix that in the next competition (ladies, join our team next time!).
<h2>Let's Get Technical</h2>
<h3>What was your overall approach to the challenge?</h3>
Our approach consisted of implementing the following steps:
<ol>
	<li>Cleaning the original datasets (e.g. removing stopwords, removing special characters, converting characters to lowercase, stemming, etc.) and applying the TF-IDF (term frequency, inverse document frequency) algorithm to a concatenation of the query, product title, and product description.</li>
</ol>
<ol start=""2"">
	<li>Applying dimensionality reduction to the resulting datasets to work with roughly 250 instead of thousands of features.</li>
</ol>
<ol start=""3"">
	<li>Augmenting the resulting datasets with custom engineered features.</li>
</ol>
<ol start=""4"">
	<li>Building three different model families, including support vector machines (<a href=""https://en.wikipedia.org/wiki/Support_vector_machine"" target=""_blank"">SVM</a>), artificial neural networks (<a href=""https://en.wikipedia.org/wiki/Artificial_neural_network"" target=""_blank"">ANN</a>), and gradient boosting machine (<a href=""https://en.wikipedia.org/wiki/Gradient_boosting"" target=""_blank"">GBM</a>).</li>
</ol>
<ol start=""5"">
	<li>Rounding the regression predictions to integers using specially developed thresholds that optimize the kappa score.</li>
</ol>
<ol start=""6"">
	<li>Ensembling the individual models and building second-layer models. <em>The winning model was an ensemble of a single-layer ANN, a single-layer SVM, and a second-layer ANN (with features derived from over 20 single-layer models)</em>.</li>
</ol>
<h3>What processing methods did you use?</h3>
The original dataset was raw text. A number of tasks, discussed extensively at the Kaggle forum and that most competing teams carried out, were performed to convert the raw data to one on which machine learning algorithms can be trained. First, the datasets were cleaned by applying steps that included:
<ul>
	<li>Removing html tags and other non-text elements. These elements were present since, presumably, the datasets were generated programmatically by parsing the web.</li>
</ul>
<ul>
	<li>Removing stop-words such as common articles, prepositions, etc. (e.g. ‘the’, ‘a’, ‘with’,’on’).</li>
</ul>
<ul>
	<li>Using word-stemmers so that similar terms such as ‘root’ and ‘roots’, or ‘game’ and ‘gaming’ are converted to the same base word.</li>
</ul>
Basic features were generated by applying the TF-IDF operation on the resulting text. TF-IDF is an algorithm that assigns a value between 0 and 1 to every word in a given document, which in turn is part of an existing corpus. The more frequent a term is in the document and less frequent in the corpus, the higher the value for such term. For more details on TF-IDF, please check this <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" target=""_blank"">wikipedia article</a>.

Because the TF-IDF creates a feature for every token in the corpus, we used dimensionality reduction to end up with a more workable dataset of 225-250 features, depending on the model. The particular algorithm we used for dimensionality reduction is truncated singular value decomposition (SVD). More information on SVD is available <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html"" target=""_blank"">here</a>.
<h3>Were there any innovative aspects to your approach?</h3>
As it turned out, the quality of the models could be enhanced significantly by creating ad-hoc features that allow all machine learning models alike to make better decisions. Therefore, feature engineering was an important factor in this competition. The following set of features were added to the training and test datasets:

<strong>N-gram similarity features</strong> - A total of 14 features were added through the evaluation of similarity between unigrams, bigrams, and trigrams extracted from the query and the product title &amp; description strings. Some of those features are boolean (e.g. 1 if unigram or bigram is found in search string, 0 if it is not) while others are similarity scores between the n-gram and the search string. To fully understand these features, we recommend taking a look at the <a href=""https://github.com/skrypka/crowdflower-search/blob/master/ngramMatches07.R"" target=""_blank"">R code</a> used to generate the features. However, as a representative example of the general idea, the following demonstrates how one of the 14 features was created:
<p style=""padding-left: 30px;""><strong>Query</strong>: “<em>led christmas lights</em>”</p>
<p style=""padding-left: 30px;""><strong>Title</strong>: “<em>Set of 10 Battery Operated Multi LED Train Christmas Lights - Clear Wire</em>”</p>
6 possible n-grams can be generated from the query string. Those are [“<em>led christmas lights</em>”, “<em>led christmas</em>”, “<em>christmas lights</em>”, “<em>led</em>”, “<em>christmas</em>”, “<em>lights</em>”]. Searching the product title for exact match of the query n-grams yields [0, 0, 1, 1, 1, 1]. The sum of the result (4) normalized by the maximum number of possible matches (6) becomes a feature (0.67).

<strong>Query-product name similarity features</strong> - While checking the cross-validation results, we noticed that our solution performed particularly bad whenever the main noun in the query was present in the product title, but it was not the actual product (i.e. it was an adjective in the title rather than the main noun). An example would be: query = “<em>night gown</em>” and product title = “<em>night gown necklace</em>”. Thus, we developed an algorithm to extract the main noun from the product title and performed a similarity measure between the query and the extracted noun. This was accomplished by implementing a set of rule-based regular expressions that were carefully crafted to remove all unnecessary descriptions, prefixes, suffixes, etc. For example any phrase following the words “for”, “by”, and “with” was removed, sizes &amp; colors were removed,and so on. Finally, the last two words in the cleaned title were regarded as the main product. The following is a good example of what is achieved through this algorithm:
<p style=""padding-left: 30px;""><strong>Query:</strong> “<em>reusable straws”</em></p>
<p style=""padding-left: 30px;""><strong>Title:</strong> “<em>Itzy Ritzy Snack Happens Reusable Snack Bag with Thermos Straw Bottle, Jungle”</em></p>
<p style=""padding-left: 30px;""><strong>Cleaned title:</strong> “<em>Itzy Ritzy Snack Happens Reusable Snack Bag”</em></p>
<p style=""padding-left: 30px;""><strong>Product name:</strong> “<em>Snack bag”</em></p>
<p style=""padding-left: 30px;""><strong>Code: </strong>For additional details refer to the <a href=""https://github.com/skrypka/crowdflower-search/blob/master/extractProductName_NEW.R"" target=""_blank"">R code</a> used for generating this feature.</p>
<strong>Alternative query similarity features</strong> - For each of the 261 unique queries, we created a corpus by combining all titles associated with that query, extracted the top trigram, and used it as an alternative query. This basically allowed us to potentially double our engineered features. The idea behind this approach was to capture the correct product implicated when the query did not have similarity with the product title. This is especially an issue when the search query is a brand name. As an example, figure 1 below is a word cloud visualization of the trigrams in the title corpus associated with query = ‘<em>samsonite</em>’. The top trigram is <em>‘piece luggage set</em>’. Because class 4 is the most common class (accounting for ~60% of the all labels), a reasonable assumption was made that the intended product is indeed a ‘<em>piece luggage set</em>’. Thus, we considered ‘<em>piece luggage set</em>’ as an alternative query and subsequently generated similarity features between it and the product title/description. For additional details refer to the <a href=""https://github.com/skrypka/crowdflower-search/blob/master/alt_query.R"" target=""_blank"">R code</a> used for generating this feature.

[caption id=""attachment_5058"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/crowdflower3_wordcloud.png""><img class=""wp-image-5058 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/crowdflower3_wordcloud.png"" alt=""crowdflower3_wordcloud"" width=""612"" height=""554"" /></a> Word cloud of trigrams extracted from the titles corpus corresponding to query ‘samsonite’. See the full code on <a href=""https://www.kaggle.com/madcap/crowdflower-search-relevance/alternative-query-from-title-corpus?tab=code/files"" target=""_blank"">scripts</a>.[/caption]

<strong>Intra-title similarity features - </strong> The idea behind this feature is to use the group of titles with the same relevance and within the same query as reference point and measure similarity of all individual titles within that query to that reference point. So, for instance, if a certain query has 40 labeled titles (10 per relevance class), then various similarity measures between each title (both labeled and unlabeled) and the 4 groups are constructed. In practice, this is carried out by measuring similarity between the title and each title within the group, then aggregating the result. Three different similarity measures were used: cosine similarity, count of same words, count of same words after extracting the product name, and two different aggregation functions were applied (mean and max), yielding a total of 4✕3✕2= 24 features. For additional details refer to the <a href=""https://github.com/skrypka/crowdflower-search/blob/master/RelevanceGroup.py"" target=""_blank"">python code</a> used for generating this feature.

<strong>Antonym feature</strong> - Noticing that some queries performed particularly bad when compared to hand-labeled predictions in some particular cases, we developed some additional rules to address this issue. One such case was when an antonym of a noun in the query was in the product title or description (e.g. query: “<em>men shoes</em>”, product title: “<em>beautiful women shoes</em>”). To solve for that, we created a boolean variable indicating whether an antonym is present (1) or not (0). This was done only for the most common antonyms.

[caption id=""attachment_5059"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/crowdflower2_feat_corr.png""><img class=""size-full wp-image-5059"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/crowdflower2_feat_corr.png"" alt=""Bar Plot Image"" width=""612"" height=""355"" /></a> Bar plot of the correlation between selected engineered features and label.[/caption]
<h3>What supervised learning methods did you use?</h3>
We used a variety of models that included support vector machines, artificial neural networks, gradient boosted regressors, k-nearest neighbor classifiers, and random forest classifiers. For some of the models, we created both classifications and regression versions (e.g. svm classifiers and svm regressors). We also used alternative implementations of the same model (e.g. using both <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBOOST</a> package and <a href=""https://github.com/h2oai/h2o-2"" target=""_blank"">H2O</a> to build gradient boosted regressors). As a result we had a wealth of different models to be used in ensembling and building of second-layer models (in specific, the second-layer ANN was based on features from over 20 different models).

Implementation wise, SVM was built using the excellent machine learning library of scikit learn (hyperlink). The artificial neural networks were built using keras (which in turn outsources some tasks to theano) on python. Whereas GBM models were built using the XGBOOST and H2O R packages.
<h3>How did you round the regression outputs to yield classes?</h3>
By running experiments on the cross validation folds, we determined the optimal rounding thresholds for rounding predictions obtained through regression. In the end, we found that the following thresholds produced the best kappa score: 1.9, 2.7, 3.5. (i.e. if prediction is lower than 1.9 then label 1 is assigned, if bigger than 1.9 but lower than 2.7, then label 2 is assigned, and so on).
<h3>Describe your winning ensemble</h3>
Our final model (depicted in the diagram below) is a 2-step ensemble, which can be described as follows:

<strong>Step 1</strong> - Ensemble of first-layer ANN (30% weight) and second-layer ANN (70% weight). The averaging is applied on the unrounded predictions and the optimal rounding thresholds were applied after averaging.

<strong>Step 2</strong> - Ensemble of the predictions obtained from step 1 and SVM classifier, according to the following expression: floor(0.8*step1 + 0.35*SVM). The expression looks rather arbitrary, but when carefully examining the possible outcomes of combining the two models according to this formula, we noticed that this simply applies a slight correction to predictions of step 1. Most notably, when step 1 predicts class 2 and the SVM predicts class 1, class 1 is chosen. Our guess for why this operation boosts the overall score is due to the SVM being better at classifying class1 than the step1 ensemble.

[caption id=""attachment_5060"" align=""aligncenter"" width=""677""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/crowdflower2_ensemble.png""><img class=""size-full wp-image-5060"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/crowdflower2_ensemble.png"" alt=""Ensemble Image"" width=""677"" height=""307"" /></a> Block diagram depicting the overall solution[/caption]
<h2>Word of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
As can be seen from this blog, many different ideas were applied. Those ideas can be grouped into a number of initiatives such as feature engineering, testing different models, testing different machine learning tasks such as regression vs. classification, and trying different ensembling and rounding strategies. Not only does it take a lot of time to test so many different ideas, but a lot of ingenuity needs to be put into idea generation. While different team members contributed differently to different initiatives, everyone was actively looking for ways to improve the overall solution and the competition score. This leads us to one issue that is often discussed in machine learning and management overall: the power of teams and teamwork.

[caption id=""attachment_5063"" align=""aligncenter"" width=""622""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/Quartetbadges.png""><img class=""wp-image-5063 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/Quartetbadges.png"" alt=""Quartetbadges"" width=""622"" height=""74"" /></a> Kaggle profiles for <a href=""https://www.kaggle.com/sgamez"" target=""_blank"">Sergio</a>, <a href=""https://www.kaggle.com/dowakin"" target=""_blank"">Roman</a> (aka Dowakin), <a href=""https://www.kaggle.com/madcap"" target=""_blank"">Maher</a> (aka madcap), &amp; <a href=""https://www.kaggle.com/asimkievich"" target=""_blank"">Alejandro</a>.[/caption]

In machine learning, it is well known that the product of model ensembling is usually better than any individual model. The simple reason is that every model suffers from errors due to variance, bias, and irreducible errors. Ensembling models allows for correcting of those errors, at least in part, and as long as the models are independent. Subsequently, a team of data scientists with different approaches to the same problem can create a diversity of models more than an individual, and thus can ensemble more models. So, all things being equal, the product of those ensembles will be better the larger the number of data scientists engaged in creating models.

But that is not all. Idea generation is a tiresome process, and at different points in time, different team members can suffer from burnout. When that happened among us, someone was always willing and ready to take the lead. Some of us are 100% convinced that in a real-life setting, where companies compete aggressively for their customers’ dollars, individuals cannot compete with teams, no matter how good the individual. This is a lesson to all of us.

While exceptional data scientists do win competitions by themselves - and this particular competition is one of those - more often than not the top 10 in Kaggle competitions is comprised of teams. When you move to real life, where serious money is involved, you can expect individuals to team up to stand a higher chance to win in the marketplace. Do not try to climb Everest all by yourself - doing it alongside a team is more fun and will probably win you more money in the end.
<h3>Bios</h3>
<strong>Maher Harb</strong> - Physicist and data scientist. Starting sept 2015 will take on a new position as Assistant Professor at the Dept. of Physics at Drexel University (Philadelphia).

<strong>Roman Khomenko</strong> - Senior software developer and security researcher from Ukraine.

<strong>Sergio Gámez</strong> - Computer vision researcher from Spain.

<strong>Alejandro Simkievich</strong> - Technology entrepreneur and CEO at Statec, a machine learning consulting company in Sao Paulo, Brazil

<hr />

Read other blogs on the CrowdFlower Search Results Relevance competition by clicking the tag below."
"Taxi Trip Time Winners' Interview: 3rd place, BlueTaxi",http://blog.kaggle.com/2015/07/24/taxi-trip-time-winners-interview-3rd-place-bluetaxi/,2015-07-24 21:18:25,"This spring, Kaggle hosted two competitions with the <a href=""http://www.ecmlpkdd2015.org/"" target=""_blank"">ECML PKDD conference</a> in Porto, Portugal. The competitions shared a dataset but focused on different problems. <a href=""https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i"" target=""_blank"">Taxi Trajectory</a> asked participants to predict where a taxi would drop off a customer given partial information on their journey, while <a href=""https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii"" target=""_blank"">Taxi Trip Time</a>'s goal was to predict the amount of time a journey would take given the same dataset. You can read a write-up from the first place team in Taxi Trajectory <a href=""http://blog.kaggle.com/2015/07/27/taxi-trajectory-winners-interview-1st-place-team-%F0%9F%9A%95/"" target=""_blank"">here</a>.

[caption id=""attachment_5073"" align=""aligncenter"" width=""322""]<img class=""wp-image-5073"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/taxi_triptime.png"" alt=""taxi_triptime"" width=""322"" height=""122"" /> 418 players on 345 teams competed to predict the time a taxi journey would take.[/caption]

Team BlueTaxi finished 3rd in Taxi Trip Time and 7th in Taxi Trajectory. This blog outlines how their team of data scientists from five different countries came together, and their winning approach to the Taxi Trip Time competition.
<h2>The Basics</h2>
<h3>The BlueTaxi Team</h3>
The BlueTaxi team is very multicultural, we are <a href=""https://www.kaggle.com/bluebalam/account"" target=""_blank"">Ernesto</a> (El Salvador), <a href=""https://www.kaggle.com/lamthuy"" target=""_blank"">Lam</a> (Vietnam), <a href=""https://www.kaggle.com/alessandra"" target=""_blank"">Alessandra</a> (Italy), <a href=""https://www.kaggle.com/turtledonkey"" target=""_blank"">Bei</a> (China), and <a href=""https://www.kaggle.com/yiannisg"" target=""_blank"">Yiannis</a> (Greece). We had great fun participating and winning the third place in this ECML/PKDD Discovery Challenge which was organized as the Kaggle, Taxi Trip Time, competition. In this post we would like to share with you how we did it.
<h3>What made you decide to enter this competition?</h3>
<b>Ernesto</b>: The Discovery Challenges, organized annually by ECML/PKDD, are always very interesting and this year was not the exception, especially this one organized on top of Kaggle. After a small chat with Lam, we decided to go for it and enter as the BlueTaxi team for both tracks of the challenge.

I took the lead for the trip time prediction and Lam led the destination prediction.

We invited Alessandra, Bei, and Yiannis to consolidate the final team of five.

<b>Lam</b>: I decided to enter because the competition was hosted by Kaggle and the ECML/PKDD 2015 conference will be a great a opportunity for researchers to benefit from exchanging ideas and their experience during the workshop session being held in conjunction with the main conference in Porto next September.

<b>Alessandra</b>: Lam and Ernesto presented the challenge to me and I thought it was very interesting so I decided to join.

<b>Bei</b>: For me that was also the case.

<b>Yiannis</b>: The problem of predicting the destination and trip time for a taxi route seemed very challenging and that's why I decided to participate and join the BlueTaxi team.
<h3>What was your background prior to entering this challenge?</h3>
<b>Lam</b>: I did my PhD in pattern mining for data streams at Technische Universiteit Eindhoven (TU/e) and joined IBM Research Lab in Ireland about a year and a half ago. My research interests include mining big and fast data on big data platforms with applications in telcos, transportation under the smarter city project.

<b>Alessandra</b>: My background is in transportation analytics. Specifically I work on estimation and prediction of traffic and urban traffic control systems. I hold a PhD degree in Information Technology from Politecnico di Milano and currently I am a Research Scientist at IBM Research - Ireland (also known as the IBM Smarter Cities Technology Centre in Dublin, Ireland.)

<b>Bei</b>: I am a statistician with primary interests in time series analysis, forecasting, resampling/subsampling methods for dependent data and financial econometrics. My recent work focuses on statistical methods in urban applications. I received my PhD in Statistics from the University of Waterloo, Canada. I joined IBM Research, Ireland, in late 2012.

<b>Yiannis</b>: I am a Research Software Engineer at Smarter Cities Technologies Center, IBM Research - Ireland. I hold a Masters Degree from Athens University of Economics and Business in Computer Science. Lately, I have been working with spatio-temporal data on various projects focusing on data curation, efficient storing and analysis. Moreover, I have experience with visualisation of similar type of data, helping data scientists to gain insights.

<b>Ernesto</b>: I hold a PhD in Computer Science from the L3S Research Center in the University of Hannover, Germany. My background is in supervised machine learning applied to Web Science, Social Media Analytics, and Recommender Systems. I joined IBM Research, Ireland, early 2014.
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
<b>Lam</b>: In the IBM Research lab I have been working on several projects with similar data, e.g. with GPS traces from buses used for prediction of bus arrival time at bus stop, e.g., see our related paper regarding this topic (Flexible Sliding Window for Kernel Regression Based Bus Arrival Time Prediction) in the industry track at ECML/PKDD 2015.

<b>Ernesto</b>: I did not have any particular domain knowledge in transportation systems, but my experience in machine learning, data science and analytics were of course valuable for the competition.

<b>Alessandra</b>: Yes, my knowledge in the transportation field helped me in the challenge.

<b>Yiannis</b>: My experience with spatio-temporal data helped me in the competition.

<b>Bei</b>: I had some experience analyzing transportation data, which helped me in this challenge.
<h3>How did you get started competing on Kaggle?</h3>
<b>Ernesto</b>: I joined Kaggle a couple of years ago during my PhD. I did enter in some competitions before. The datasets available in Kaggle are usually very interesting and some of them were very useful for my research. But to be honest, I never got good traction in a competition until this time.

<b>Lam</b>: I also joined Kaggle 4 years ago when I was a PhD student. But I haven't tried to compete since then.

<b>Bei</b>: I joined Kaggle in 2012, and this was my second competition since then.

<b>Alesandra and Yiannis</b>: For us this was the first time that we had entered a Kaggle competition :)
<h2>Let's get technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
First we created our local training set by selecting the cut-off times the same as the five snapshots on the test set (the same week-date as well). We also observe that 14th of August is the day before a big holiday in Portugal and 21th of December is the last Sunday before Christmas, both very particular days.

We created a bunch of <b>features</b> as follows:

1. Features from 10-NN. For every test trip we find 10 nearest neighbours w.r.t the Euclidean distance and consider the durations of those trips as predictors.

2. Features from Kernel Regression. Similar to 10-NN, kernel regression was used to predict the duration. Remember that kernel regression is a smooth version of kNN and these features yield very good results.

3. Some features from the partial trips: travelled distance, number of GPS updates, last GPS coordinates, average speed at different part of the trips and accelerations at different part of the trips.

When matching a test trip with the training trips, we only consider to match the last 100, 200, 300, 400, 500, 1000 meters and the full trips as well. This is because the later part of the trip is more important in some cases. Our model show that the last 500 meters of the trip is very important.

[caption id=""attachment_5066"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/trip-match.png""><img class=""wp-image-5066"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/trip-match.png"" alt=""trajectory visualization"" width=""612"" height=""428"" /></a> Two trips with different starting points but with the same destination (Porto Airport). The later part of the trajectories are very close to each other. Therefore via trip matching we can guess destination of the other trip if we can guess the destination of trips with similar route.[/caption]

We also considered contextual matching (match only trips with same taxi id, the same week date, the same call id, etc.) because we observed different distributions of destination for these contexts. The kernel regression on taxi id context produced the best results.

When <b>modelling</b>, we did not predict the duration of the whole trip but instead predict the <i>additional</i> delta travel time with respect to the cut-off timestamp. Since the evaluation metric was RMSLE, we log-transformed the time target labels, i.e., the log of the additional delta time.

<b>Outlier handling</b>: we found that trips with missing values (identified at speed limits 160, 140, 100 Km/h) are more difficult to predict, we try to recover this information on the test set by looking at the gap between the cut-off timestamp and the start timestamp. Unfortunately, this information is not reliable so we decided to remove outliers based on the number of GPS updates based on an absolute deviation from the median of 3.5.

The test dataset for this competition was very small (320 instances), which makes it very prone to overfitting. Our final solution was a robust <b>Ensemble</b> of several models that included: Random Forests, Gradient Boosted Trees, and Extremely Randomized Trees.

<b>BlueTaxi the overall winner approach ;)</b> We expected some variation in the top ranking positions given the small test set, but were quite surprised on all the shuffling we observed from the public to the final private leaderboard. Note that if we average the performance in the 100% of the test set (public and private scores), the BlueTaxi approach is actually the most robust and with the best overall RSMLE score ;)
<h3>What was your most important insight into the data?</h3>
There are many interesting patterns found during data exploration, e.g., some trips with the same call id are very regular, trips to the airport are very easy to predict as well.

We used some open source journey planners to predict travel time but the results, using for example the destination prediction results from the other track of the challenge, but we found that the results were not very good for the taxi trips. This tells us that journey planners should include the real-time traffic information to improve its estimation of travel time.

[caption id=""attachment_5067"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/taxi_id_59708.png""><img class=""wp-image-5067"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/taxi_id_59708-1024x482.png"" alt=""Destination visualization"" width=""612"" height=""288"" /></a> Heatmap of destinations of 155 trips with call id 59708. There are only 8 major destinations. Using call id information one can narrow down the possible destinations of the given taxi.[/caption]
<h3>Which tools did you use?</h3>
R, Python (Numpy, Scipy, Sklearn and Pandas), Matlab, and Javacript and Leaftlet for visualisation.
<h3>How did you spend your time on this competition?</h3>
A rough estimate would be 80% for feature engineering, 10% in data exploration, and 10% in modelling.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
Most features were based on kNN search, which was very time consuming in general, but we implemented a novel indexing technique to solve the efficiency issues, the running time was reduced significantly with the index.

For the modelling part, the most time consuming step is always the model parameter selection, which we conducted using cross validation. The final training is relatively fast. We included in our ensemble Extremely Randomized Trees, which given their nature are much faster to train with respect to Random Forest or Gradient Boosted Trees, and their individual results were quite competitive in our validation set.
<h2>Words of wisdom</h2>
<h3>What have you taken away from this competition?</h3>
Our data-driven and machine learning solution in the end led to significant better results than other models based solely on transportation domain knowledge.

We learnt that data exploration, feature extraction and engineering are very important. Unfortunately, this task still requires a lot of human effort. Modelling is much easier now with many open-source off-the-shelf machine learning tools.

This fact encourages us to continue working on research directions that try to find a generic solution to automate or at least semi-automate this process, i.e. make ML and AI closer to each other.
<h3>Do you have any advice for those just getting started in data science?</h3>
Be always curious and open your mind to what the data has to tell you.

Make data science your passion!

In Kaggle competitions, what it really matters is your <i>passion</i> to learn and discover new things and not what you already know.
<h2>Teamwork</h2>
<h3>How did competing on a team help you succeed?</h3>
We think that competing as a team was the fundamental key for success. Remember that we were participating in both tracks of the ECML/PKDD challenge. Basically two Kaggle competitions at the same time and this was our side project and hobby.

In the end, our BlueTaxi finished 3rd in the taxi trip time prediction and 7th in the destination prediction track. We would not be able to achieve such high rankings if we were not competing as a team.
<h3>Bio</h3>
<b>Ernesto<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/ernesto.jpg""><img class=""alignright wp-image-5068"" style=""margin-left: 15px; margin-bottom: 5px; margin-top: 5px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/ernesto.jpg"" alt=""ernesto headshot"" width=""120"" height=""151"" /></a></b>: Ernesto is passionate about Web Science, Big Data analytics and the power of Machine Learning to support decision making processes. His research focus is to contribute novel and intelligent filtering approaches that leverage social interactions, multidimensional relationships, and other ubiquitous data in the Social Web in order to relieve people's information overload. Ernesto's research has advanced the state-of-the-art in personalized ranking and recommender systems and has demonstrated to be valuable for industrial practitioners as well. Ernesto holds a PhD from the L3S Research Center in the University of Hannover, Germany, and currently is a Research Scientist in Machine Learning and Data Science at IBM Research - Ireland.

<b>Lam<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/lam.jpg""><img class=""alignright wp-image-5069"" style=""margin-left: 15px; margin-bottom: 5px; margin-top: 5px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/lam.jpg"" alt=""Lam headshot"" width=""146"" height=""134"" /></a></b>: Lam did his PhD in pattern mining in data stream at Technische Universiteit Eindhoven in December 2012. He joined IBM research lab in Dublin Ireland right after that and has been working on different research projects, proposing novel machine learning, statistical modelling approaches to solve real-world problems to improve city life.

<b>Alessandra<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/alessandra.jpg""><img class=""alignright wp-image-5070"" style=""margin-left: 15px; margin-bottom: 5px; margin-top: 5px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/alessandra.jpg"" alt=""Alessandra headshot"" width=""120"" height=""153"" /></a></b>: Alessandra Pascale received the B.Sc. degree (with honors) from Politecnico di Bari in 2006 and the M.Sc. degree (mark 110/110) from Politecnico di Milano in 2009, both in Telecommunication Engineering, and the Ph.D. degree in Information Technology from Politecnico di Milano in 2013. She was also enrolled in the ASP (Alta Scuola Politecnica) program, a school of excellence between Politecnico di Milano and Politecnico di Torino. She was visiting researcher at University of California, Berkeley, in the TOPL group, from February to July 2012. In 2013, she worked for 6 months in the framework of the European research project DIWINE, ""Dense cooperative wireless cloud network"".

At the present time she works as Research Scientist at the IBM Smarter Cities Technology Centre in Dublin, Ireland. Her research interests are in the field of signal processing, in particular statistical estimation and prediction of vehicular traffic, distributed/cooperative estimation approaches and urban traffic control systems.

<b>Bei<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/bei.jpg""><img class=""alignright wp-image-5071"" style=""margin-left: 15px; margin-bottom: 5px; margin-top: 5px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/bei.jpg"" alt=""Bei headshot"" width=""120"" height=""120"" /></a></b>: Bei Chen received her BMath in Statistics and Actuarial Science (with Dean's Honour's list and President's Award), MMath and PhD in Statistics (with Outstanding Achievement in Graduate Studies Award) from the University of Waterloo, Canada. She is the recipient of the 2011 Pierre Robillard Award of the Statistical Society of Canada, which recognizes the best thesis in probability and statistics defended at a Canadian university.

Prior to joining IBM, Bei Chen was an tenure-track Assistant Professor (September 2011 - October 2012) in the Department of Mathematics and Statistics at McMaster University, Canada. Her research was funded by various grants, including NSERC, ECR and MITACS.

<b>Yiannis<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/yiannis.jpg""><img class=""alignright wp-image-5072"" style=""margin-left: 15px; margin-bottom: 5px; margin-top: 5px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/yiannis.jpg"" alt=""Yiannis headshot"" width=""112"" height=""112"" /></a></b>: Yiannis Gkoufas holds a Masters Degree from Athens University of Economics and Business in Computer Science. He is working as a Research Software Engineer on the High-Performance Systems Group in Dublin,Ireland since 2012. His main focus is building domain-specific applications and architectures on top of distributed computing frameworks. Moreover, he is responsible for developing the middleware supporting web-based applications. He has been involved in a vast variety of funded projects evolving around Energy and Demand Forecast Analytics, Transportation Monitoring and Planning and Event Detection from Social Media.
<h3>Disclaimer</h3>
<b>Disclaimer from the authors: This post is our own and doesn't necessarily represent IBM's positions, strategies or opinions.</b>

<hr />

Read other blogs on the Taxi Trip Time &amp; Trajectory competitions by clicking the tags below."
"CrowdFlower Winner's Interview: 1st place, Chenglong Chen",http://blog.kaggle.com/2015/07/27/crowdflower-winners-interview-1st-place-chenglong-chen/,2015-07-27 19:18:13,"The <a href=""https://www.kaggle.com/c/crowdflower-search-relevance"" target=""_blank"">Crowdflower Search Results Relevance</a> competition asked Kagglers to evaluate the accuracy of e-commerce search engines on a scale of 1-4 using a dataset of queries &amp; results. Chenglong Chen finished ahead of 1,423 other data scientists to take first place. He shares his approach with us from his home in Guangzhou, Guangdong, China. (To compare winning methodologies, you can read a write-up from the third place team <a href=""http://blog.kaggle.com/2015/07/22/crowdflower-winners-interview-3rd-place-team-quartet/"" target=""_blank"">here</a>.)

[caption id=""attachment_5082"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/cfbanner.png""><img class=""wp-image-5082 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/cfbanner-300x233.png"" alt=""cfbanner"" width=""300"" height=""233"" /></a> The competition ran from May 11-July 6, 2015.[/caption]
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I was a Ph.D. student in Sun Yat-sen University, Guangzhou, China, and my research mainly focused on passive digital image forensics. I have applied various machine learning methods, e.g., SVM and deep learning, to detect whether a digital image has been edited/doctored, or how much has the image under investigation been resized/rotated.

[caption id=""attachment_5076"" align=""aligncenter"" width=""380""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/ChenglongChen_Profile.jpg""><img class=""wp-image-5076 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/ChenglongChen_Profile.jpg"" alt=""ChenglongChen_Profile"" width=""380"" height=""180"" /></a> Chenglong's <a href=""https://www.kaggle.com/chenglongchen"" target=""_blank"">profile</a> on Kaggle[/caption]

I am very interested in machine learning and have read quite a lot of related papers. I also love to compete on Kaggle to test out what I have learnt and also to improve my coding skill. Kaggle is a great place for data scientists, and it offers real world problems and data from various domains.
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
I have a background of image proecssing and have limited knowledge about NLP except BOW/TF-IDF kinda of things. During the competition, I frequently refered to the book <em><a href=""https://www.goodreads.com/book/show/9802408-python-text-processing-with-nltk-2-0-cookbook"" target=""_blank"">Python Text Processing with NLTK 2.0 Cookbook</a></em> or Google for how to clean text or create features from text.

I did read the paper about ensemble selection (which is the ensembling method I used in this competition) a long time ago, but I haven't have the opportunity to try it out myself in real word problem. I previously only tried simple (weighted) averaging or majority voting. This is the first time I got so serious about the model ensembling part.
<h3>How did you get started competing on Kaggle?</h3>
It dates back a year and a half ago. At that time, I was taking Prof. Hsuan-Tien Lin's Machine Learning Foundations <a href=""https://www.coursera.org/course/ntumlone"" target=""_blank"">course</a> on Coursera. He encouraged us to compete on Kaggle to apply what we have learnt to real world problems. From then on, I have occasionally participated in competitions I find interesting. And to be honest, most of my programming skills about Python and R are learnt during Kaggling.
<h3>What made you decide to enter this competition?</h3>
After I passed my Ph.D. dissertation defense early in May, I have had some spare time before starting my job at an Internet company. I decided that I should learn something new and mostly get prepared for my job. Since my job will be about advertising and mostly NLP related, I thought this challenge would be a great opportunity to familiarize myself with some basic or advanced NLP concepts. This is the main reason that drove me to enter.

Another reason was that this dataset is not very large, which is ideal for practicing ensemble skills. While I have read papers about ensembling methods, I haven't got very serious about ensembling in previous competitions. Usually, I would try very simple (weighted) averaging. I thought this is a good chance to try some of the methods I have read, e.g., stacking generalization and ensemble selection.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
The documentation and code for my approach are available <a href=""https://github.com/ChenglongChen/Kaggle_CrowdFlower"" target=""_blank"">here</a>. Below is a high level overview of my method.

[caption id=""attachment_5077"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/FlowChart.jpg""><img class=""size-large wp-image-5077"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/FlowChart-1024x526.jpg"" alt=""Flowchart of my method"" width=""640"" height=""329"" /></a> Figure 1. Flowchart of my method[/caption]

For preprocessing, I mainly performed HTML tags dropping, word replacement, and stemming. For a supervised learning method, I used ensemble selection to generate an ensemble from a model library. The model library was built with models trained using various algorithms, various parameter settings, and various feature sets. I have used <a href=""http://hyperopt.github.io/hyperopt/"" target=""_blank"">Hyperopt</a> (usually used in parameter tuning) to choose parameter setting from a pre-defined parameter space for training different models.

I have tried various objectives, e.g., MSE, softmax, and pairwise ranking. MSE turned out to be the best with an appropriate decoding method. The following is the decoding method I used for MSE (i.e., regression):
<ol>
	<li>Calculate the pdf/cdf of each median relevance level, 1 is about 7.6%, 1 + 2 is about 22%, 1 + 2 + 3 is about 40%, and 1 + 2 + 3 + 4 is 100%.</li>
	<li>Rank the raw prediction in an ascending order.</li>
	<li>Set the first 7.6% to 1, 7.6% - 22% to 2, 22% - 40% to 3, and the rest to 4.</li>
</ol>
In CV, the pdf/cdf is calculated using the training fold only, and in the final model training, it is computed using the whole training data.

Figure 2 shows some histograms from my reproduced best single model for one run of CV (only one validation fold is used). Specifically, I plotted histograms of 1) raw prediction, 2) rounding decoding, 3) ceiling decoding, and 4) the above cdf decoding, grouped by the true relevance. It's most obvious that both rounding and ceiling decoding methods have difficulty in predicting relevance 4.

[caption id=""attachment_5078"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/compare_MSE_Decoding.jpg""><img class=""wp-image-5078 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/compare_MSE_Decoding-1024x882.jpg"" alt=""Figure 2. Histograms of raw prediction and predictions using various decoding methods grouped by true relevance. (The code generated this figure is available here.)"" width=""640"" height=""551"" /></a> Figure 2. Histograms of raw prediction and predictions using various decoding methods grouped by true relevance. (The code generated this figure is available <a href=""https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/Fig/compare_MSE_decoding.py"" target=""_blank"">here</a>.)[/caption]

Following are the kappa scores for each decoding method (using all 3 runs and 3 folds CV). The above cdf decoding method exhibits the best performance among the three methods we considered.
<table>
<tbody>
<tr>
<td>Method</td>
<td>CV Mean</td>
<td>CV Std</td>
</tr>
<tr>
<td>Rounding</td>
<td>0.404277</td>
<td>0.005069</td>
</tr>
<tr>
<td>Ceiling</td>
<td>0.513138</td>
<td>0.006485</td>
</tr>
<tr>
<td>CDF</td>
<td>0.681876</td>
<td>0.005259</td>
</tr>
</tbody>
</table>
<h3>What was your most important insight into the data?</h3>
I have found that the most important features for predicting the search results relevance is the <em>correlation</em> or <em>distance</em> between query and product title/description. In my solution, I have features like interset word counting features, Jaccard coefficients, Dice distance, and cooccurencen word TF-IDF features, etc. Also, it’s important to perform some word replacements/alignments, e.g., spelling correction and synonym replacement, to align those words with the same or similar meaning.

While I didn't have much time exploring word embedding methods, they are very promising for this problem. During the competition, I came across a paper entitled “<em><a href=""http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf"" target=""_blank"">From word embeddings to document distances</a></em>”. The authors of this paper used Word Mover’s Distance (WMD) metric together with <a href=""https://code.google.com/p/word2vec/"" target=""_blank"">word2vec</a> embeddings to measure the distance between text documents. This metric is shown to have superior performance than BOW and TF-IDF features.
<h3>Were you surprised by any of your findings?</h3>
I have tried optimizing kappa directly uisng XGBoost (see below), but it performed a bit worse than plain regression. This might have something to do with the hessian, which I couldn't get to work unless I used some scaling and change it to its absolute value (see <a href=""https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/Code/Model/utils.py#L247-L252"" target=""_blank"">here</a>).
<h3>Which tools did you use?</h3>
I used Python for this competition. For feature engineering part, I heavily relied on pandas and Numpy for data manipulation, TfidfVectorizer and SVD in Sklearn for extracting text features. For model training part, I mostly used <a href=""https://github.com/dmlc/XGBoost/releases/tag/v0.40"" target=""_blank"">XGBoost</a>, <a href=""http://scikit-learn.org/stable/"" target=""_blank"">Sklearn</a>, <a href=""https://github.com/fchollet/keras/releases/tag/0.1.1"" target=""_blank"">keras</a> and <a href=""http://stat.rutgers.edu/home/tzhang/software/rgf/"" target=""_blank"">rgf</a>.

I would like to say a few more words about XGBoost, which I have been using very often. It is great, accurate, fast and easy of use. Most importantly, it supports customized objective. To use this functionality, you have to provide the gradient and hessian of your objective. This is quite helpful in my case. During the competition, I have tried to optimize quadratic weighted kappa directly using XGBoost. Also, I have implemented two ordinal regression algorithms within XGBoost framework (both by specifying the customized objective.) These models contribute to the final winning submission too.
<h3>How did you spend your time on this competition?</h3>
Where I spent my time on the competition changed during the competition.
<ol>
	<li>In the early stage, I mostly focused on data preprocessing. I have spent quite a lot of time on researching and coding down the methods to perform text cleaning. I have to mention that quite a lot of effort was spent on exploring the data (e.g., figuring out misspellings and synonyms etc.)</li>
	<li>Then, I spent most of my time on feature extraction and trying to figure out what features would be useful for this task. The time was split pretty equally between researching and coding.</li>
	<li>In the same period, I decided to build a model using ensemble selection and realized my implementation was not flexible enough to that goal. So, I spent most of the time refactoring my implementation.</li>
	<li>After that, most of my time was spent on coding down the training and prediction parts of various models. I didn't spend much time on tuning each model's performance. I utilized Hyperopt for parameter tuning and model library building.</li>
	<li>With the pipeline for ensemble selection being built, most of my time was spent on figuring out new features and exploring the provided data.</li>
</ol>
In short, I would say I have done a lot of researching and coding during this competition.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
Since the dataset is kinda of a small size and kappa is not very stable, I utilized bagged ensemble selection from a model library containing hundreds or thousands of models to combat overfitting and stabilize my results. I don't have an exact number of the hours or days, but it should take quite a large amount of time to train and make prediction. Furthermore, this also depends on the trade-off between the size of the model library (computation burden) and the performance.

That being said, you should be able to train the best single model (i.e., XGBoost with linear booster) in a few hours. It will give you a model of kappa score about <strong>0.708</strong> (Private LB), which should be enough for a top<strong>15</strong> place. For this model, feature extraction occupied most of the time. The training part (using the best parameters I have found) should be a few minutes using multi-threads (e.g., 8).
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
<ul>
	<li>Ensembling of a bunch of diverse models helps a lot. Figure 3 shows the CV mean, Public LB, and Private LB scores of my 35 best Public LB submissions generated using ensemble selection. As time went by, I have trained more and more different models, which turned out to be helpful for ensemble selection in both CV and Private LB.</li>
	<li>Do not ever underestimate the power of linear models. They can be much better than tree-based models or SVR with RBF/poly kernels when using raw TF-IDF features. They can be even better if you introduce appropriate nonlinearities.</li>
	<li>Hyperopt is very useful for parameter tuning, and can be used to build model library for ensemble selection.</li>
	<li>Keep your implementation flexible and scaleable. I was lucky to refactor my implementation early on. This allowed me to add new models to the model library very easily.</li>
</ul>
[caption id=""attachment_5079"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/35lb_subs-figure0.jpg""><img class=""wp-image-5079"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/35lb_subs-figure0.jpg"" alt=""35lb_subs-figure0"" width=""612"" height=""521"" /></a> Figure 3. CV mean, Public LB, and Private LB scores of my 35 best Public LB submissions. One standard deviation of the CV score is plotted via error bar. (The code generated this figure is available <a href=""https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/Fig/35lb_subs.tex"" target=""_blank"">here</a>.)[/caption]
<h3>Do you have any advice for those just getting started in data science?</h3>
<ul>
	<li>Use things like Google to find a few relevant research papers. Especially if you are not a domain expert.</li>
	<li>Read the winning solutions for previous competitions. They contain lots of insights and tricks, which are quite inspired and useful.</li>
	<li>Practice makes perfect. Choose one competition that you are interested in on Kaggle and start Kaggling today (and every day)!</li>
</ul>
<h2>Bio</h2>
<strong>Chenglong Chen <a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/ChenglongChen_Biography.jpg""><img class=""alignright wp-image-5080"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/ChenglongChen_Biography.jpg"" alt=""ChenglongChen_Biography"" width=""150"" height=""150"" /></a></strong> is a recent graduate from <a href=""http://www.sysu.edu.cn/2012/en/index.htm"" target=""_blank"">Sun Yat-sen University</a> (SYSU), Guangzhou, China, where he received a B.S. degree in Physics in 2010 and recently got a Ph.D. degree in Communication and Information Systems. As a Ph.D. student, his research interests included image processing, multimedia security, pattern recognition, and in particular digital image forensics. He will be starting his job career at <a href=""http://www.tencent.com/en-us/index.shtml"" target=""_blank"">Tencent</a> this August, working on advertising. Chenglong can be reached at: c.chenglong@gmail.com

<hr />

Read other blogs on the CrowdFlower Search Results Relevance competition by clicking the tag below."
"Taxi Trajectory Winners' Interview: 1st place, Team ?",http://blog.kaggle.com/2015/07/27/taxi-trajectory-winners-interview-1st-place-team-%f0%9f%9a%95/,2015-07-27 23:44:01,"<a href=""https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i"" target=""_blank"">Taxi Trajectory Prediction</a> was the first of two competitions that we hosted for the <a href=""http://www.ecmlpkdd2015.org/"" target=""_blank"">2015 ECML PKDD conference</a> on machine learning. Team ? took first place using deep learning tools developed at the <a href=""http://www.mila.umontreal.ca/"" target=""_blank"">MILA</a> lab where they currently study. In this post, they share more about the competition and their winning approach.

For more write-ups from our 2015 ECML PKDD taxi competitions, be sure to read the story of BlueTaxi, third place team in <a href=""https://www.kaggle.com/c/pkdd-15-taxi-trip-time-prediction-ii"" target=""_blank"">Taxi Trip Time Prediction</a>, <a href=""http://blog.kaggle.com/2015/07/24/taxi-trip-time-winners-interview-3rd-place-bluetaxi/"" target=""_blank"">here</a>.

[caption id=""attachment_5092"" align=""aligncenter"" width=""427""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/portugal_map4_crop.png""><img class=""size-full wp-image-5092"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/portugal_map4_crop.png"" alt=""459 data scientists on 381 teams competed to best predict where taxi riders would be dropped off."" width=""427"" height=""222"" /></a> 459 data scientists on 381 teams competed to best predict where taxi riders would be dropped off.[/caption]
<h2>The Basics</h2>
The task was particularly simple: we had to predict the destination of a taxi given the beginning of its trajectory (GPS points) and a few pieces of meta information (date, time, taxi id, client information).

The training data was composed of all the taxi rides that took place in 2013/2014 in the city of Porto, Portugal. This represents around 1.7 million taxi rides, run by 442 taxis. You can find the competition homepage on Kaggle <a href=""https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i"" target=""_blank"">here</a>.

Before going further, here are two videos showing the predictions of our model as two taxis run:

<em>Predicted destinations by our best model as the taxis run towards their destination. For the first video, we can clearly see that our model has learned the position of the airport.</em>

<iframe src=""http://ejls.fr/taxi/prediction_1098_20fps.mp4"" width=""616"" height=""347"" frameborder=""0"" allowfullscreen=""allowfullscreen""></iframe>

<iframe src=""http://ejls.fr/taxi/prediction_42402_24fps.mp4"" width=""616"" height=""347"" frameborder=""0"" allowfullscreen=""allowfullscreen""></iframe>
<h3>What were the main difficulties you faced in the dataset?</h3>
There are several difficulties in the dataset, such as:
<ul>
	<li>The taxi trajectories are variable-length sequences, the shortest being of length 0 (i.e. when the data is missing) and the longest being of length around 5000 GPS points (i.e a 20 hour ride!).</li>
	<li>The features are very heterogeneous: in particular, certain metadata is discrete (taxi and client ids), others are highly structured (date, time), and the GPS trajectories are sequences of continuous coordinates.</li>
	<li>The testing dataset is unbelievably small: 320 taxi rides only, whereas the training dataset is composed of more than 1.7 million rides. To win the competition with confidence, the only way was to design a method that would surpass those of our competitors by far.</li>
	<li>As a real-world dataset, there are several inconsistencies in the data, such as taxi rides that last more than 16 hours or a taxi driver who went to Iran :). The datapoint in Iran is certainly due to a badly calibrated GPS, but in general, training trajectories can be quite noisy due to the imprecision of the GPS.</li>
</ul>
<h3>What was your background prior to entering this challenge?</h3>
We are three students at the <a href=""http://www.mila.umontreal.ca/"" target=""_blank"">Montréal Institute for Learning Algorithms</a> (MILA, formerly known as LISA), a research lab specialised in (deep) neural networks and led by Professor Yoshua Bengio. We are two interns (Alex and Étienne) with backgrounds in computer science and one first-year PhD student (Alexandre) with more background in machine learning (ML). The ML techniques, skills and tools used during this competition are those of our every-day work at the MILA lab: (exotic) (deep) neural networks implemented with programming frameworks developed by our lab (Theano, Blocks).
<h3>Did you have any domain knowledge that helped you succeed?</h3>
No, we did not have any prior domain knowledge about Porto and its taxis!
<h3>How did you get started competing on Kaggle?</h3>
For the three of us, this was our first (serious) Kaggle competition. As students of the MILA lab, we were interested in trying out Deep Learning (DL) techniques on less mainstream applications than those in which DL already rocks (such as computer vision, NLP, speech).
<h2>Let's Get Technical</h2>
<h3>What was your general approach?</h3>
We wanted to design a full machine-learning method with as little hand-engineering as possible: ideally no pre-processing/feature engineering, no post-processing and no model ensembling.

We tried several models, all neural network based. It turned out that our more sophisticated models did not perform as well as our simpler models, strangely enough!
<h3>Can you describe your winning model?</h3>
[caption id=""attachment_5084"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/winning_model.png""><img class=""size-large wp-image-5084"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/winning_model-1024x492.png"" alt=""Figure 1: Architecture of our winning model. We tried replacing the MLP by a recurrent neural network, but it did not work as well. Deeper networks also did not improve the results."" width=""640"" height=""308"" /></a> Figure 1: Architecture of our winning model. We tried replacing the MLP by a recurrent neural network, but it did not work as well. Deeper networks also did not improve the results.[/caption]

Our winning architecture is based on a <a href=""https://en.wikipedia.org/wiki/Multilayer_perceptron"" target=""_blank"">multi-layer perceptron</a> (MLP), one of the simplest neural network architectures. More precisely, a MLP is composed of an input layer that takes a fixed-size representation of the input, one or several hidden layers, which compute intermediate representations of the input, and an output layer that gives a prediction.

Of course, we had to adapt a few things to make the MLP work with the task:
<ul>
	<li>The known part of the trajectory is of varying length, whereas an MLP takes a fixed-length input. We tackle this by taking only the 5 first and 5 last points of the trajectory as an input for the MLP. It turns out that these are indeed the most important parts of the trajectory, what happens in the middle matters less.</li>
	<li>To consider the metadata (time and client information), we use an approach commonly used in natural language models in which word embeddings are trained. Instead of words, we learn embedding vectors for each of the metadata values (quarter hour of the day, day of the week, week of the year, client ID, etc.) and use those embeddings as supplementary inputs to the MLP.</li>
	<li>We initially tried to predict the output position x, y directly, but we actually obtain significantly better results with another approach that includes a bit of pre-processing. More precisely, we first used a mean-shift clustering algorithm on the destinations of all the training trajectories to obtain around 3,392 popular destination points. The penultimate layer of our MLP is a softmax that predicts the probabilities of each of those 3,392 points to be the destination of the taxi. As the task requires to predict a single destination point, we then calculate the mean of all our 3,392 targets, each weighted by the probability returned by the softmax layer.</li>
</ul>
Our model is then trained to minimize directly the error distances between our predictions and the targets. We use stochastic gradient descent and momentum for that.
<h3>What was your most important insight into the data?</h3>
[caption id=""attachment_5085"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/xyhmap2.png""><img class=""size-large wp-image-5085"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/xyhmap2-910x1024.png"" alt=""Figure 2: Heatmap of all the GPS points of all the training rides. Note that there is no map printed here, only GPS points, which reveal the road network of Porto."" width=""640"" height=""720"" /></a> Figure 2: Heatmap of all the GPS points of all the training rides. Note that there is no map printed here, only GPS points, which reveal the road network of Porto.[/caption]

This challenge was quite interesting from a visualization point of view. At the beginning of the competition, we computed a heatmap of the most visited zones of Porto by the taxis (Figure 2). As you can see, we can clearly identify the main highways, the airport, the train station, the city centre and zones with low urban concentration. These insights made us think that we should provide our model with a prior that corresponds to the distribution of the data. And this led us to consider clusters for the destinations.
<h3>Were you surprised by any of your findings?</h3>
Just out of curiosity, we used <a href=""https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"" target=""_blank"">t-SNE</a> to project the embeddings that we learnt for each metadata into 2D dimensional spaces (our original embedding spaces have 10 dimensions each) and this gave us a nice visual idea of the way each metadata value influences the prediction (if one wants to quantitatively assess the importance of a single metadata, one could train a model with only this specific metadata as input). In particular, Figures 3 and 4 show the projections of, respectively, the embedding of the quarter of hour through the day and the embedding of the week of the year. These visual patterns confirm that these two metadata are important in the prediction. We did not find such obvious patterns in the other metadata t-SNE visualizations.

[caption id=""attachment_5086"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/qhour_of_day.W.png""><img class=""size-full wp-image-5086"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/qhour_of_day.W.png"" alt=""Figure 3: t-SNE 2D projection of the embeddings learnt for the quarter of hour at which the taxi departed (there are 96 quarters in one day, so 96 points, each one representing a particular quarter). This suggests that each quarter is pretty important on its own."" width=""640"" height=""529"" /></a> Figure 3: t-SNE 2D projection of the embeddings learnt for the quarter of hour at which the taxi departed (there are 96 quarters in one day, so 96 points, each one representing a particular quarter). This suggests that each quarter is pretty important on its own.[/caption]

[caption id=""attachment_5087"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/week_of_year.W.png""><img class=""size-full wp-image-5087"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/week_of_year.W.png"" alt=""Figure 4: t-SNE 2D projection of the embeddings learnt for week of the year during which the taxi departed (there are 52 weeks in one year, so 52 points, each one representing a particular quarter)."" width=""640"" height=""525"" /></a> Figure 4: t-SNE 2D projection of the embeddings learnt for week of the year during which the taxi departed (there are 52 weeks in one year, so 52 points, each one representing a particular quarter).[/caption]
<h3>How did you process the data?</h3>
The training dataset is composed of complete trajectories, whereas the test set is composed of only partial trajectories. This means that the GPS sequences had to be cut before being fed to our networks. The ideal solution would have been to cut all the trajectories at all different times, which would have resulted in 100.000.000 training examples and shuffle them so that we could do better stochastic gradient descent but this would have been too expensive. Instead we randomly iterate through the 1.700.000 full training examples, and dynamically generate 100 random cuts for each of them.
<h3>Which tools did you use?</h3>
We used the libraries developed at the MILA lab, namely <a href=""http://deeplearning.net/software/theano/"" target=""_blank"">Theano</a>, a numpy-like library for GPU accelerated computation, and <a href=""https://github.com/mila-udem/blocks"" target=""_blank"">Blocks</a>, a fairly recent deep learning framework built on top of Theano. We would like to thank all their developers!

Our code and instructions to reproduce our results are available on our github repo <a href=""https://github.com/adbrebs/taxi"" target=""_blank"">here</a>.
<h3>How did you spend your time on this competition?</h3>
As described previously, our approach uses only a little feature engineering. We did spend a bit of time trying to understand the data by visualization, but most of our time was spent on thinking about neural network architectures and implementing them with Blocks.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
Our model was trained by stochastic gradient descent, meaning that it would consider all the examples in the dataset one after another - or, more precisely, by batches of 200 examples - several times. One epoch, i.e. one pass through all the data, takes varying time depending on the complexity of the model. It takes a few hours on a GTX 680 to do a single epoch with our best model, and around half a day to train it.
<h3>What are the other models that you tried?</h3>
[caption id=""attachment_5091"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/memory_taxi1.png""><img class=""wp-image-5091 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/memory_taxi1-1024x227.png"" alt="""" width=""640"" height=""142"" /></a> Figure 5: Memory-like network. The RELU layers could be replaced by recurrent neural networks. In our experiments, the candidates were extracted randomly but a hand-coded similarity function could be used.[/caption]

We tried more sophisticated neural net architectures, such as recurrent neural networks and something that relates to memory networks. Surprisingly, recurrent architectures, that take the full trajectory prefix as input, did not improve our results. Our memory network architecture (Figure 5) is based on learning a similarity function, which weights candidate trajectories extracted from the training dataset.
<h2>Bios</h2>
<strong>Alexandre de Brébisson <a href=""http://blog.kaggle.com/wp-content/uploads/2015/07/alexandre_headshot.png""><img class=""alignright wp-image-5089"" style=""margin-left: 5px; margin-top: 1px; margin-bottom: 1px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/07/alexandre_headshot-150x150.png"" alt="""" width=""84"" height=""84"" /></a></strong> is a PhD student at the MILA lab in Montréal under the supervision of Professors Pascal Vincent and Yoshua Bengio.

<strong>Étienne Simon </strong>is a computer science student at ENS Cachan, and currently doing an internship at the MILA lab in Montréal under the supervision of Professor Yoshua Bengio.

<strong>Alex Auvolat</strong> is a computer science student at ENS Paris, and currently doing an internship at the MILA lab in Montréal under the supervision of Professors Pascal Vincent and Yoshua Bengio.

<hr />

Read other blogs on the Taxi Trip Time &amp; Trajectory competitions by clicking the tags below."
Detecting Diabetic Retinopathy in Eye Images,http://blog.kaggle.com/2015/08/10/detecting-diabetic-retinopathy-in-eye-images/,2015-08-10 17:17:37,"<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_1.png""><img class=""aligncenter wp-image-5098"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_1.png"" alt=""dr5_1"" width=""612"" height=""136"" /></a>

The past almost four months I have been competing in a <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection"" target=""_blank"">Kaggle competition</a> about diabetic retinopathy grading based on high-resolution eye images. In this post I try to reconstruct my progression through the competition; the challenges I had, the things I tried, what worked and what didn't. This is not meant as a complete documentation but, nevertheless, some more concrete examples can be found at the <a href=""#code"">end</a> and certainly in the <a href=""#code"">code</a>. In the end I finished <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection/leaderboard"" target=""_blank"">fifth of the almost 700 competing teams</a>.

<strong>Update 02/08/2015:</strong> <a href=""https://github.com/JeffreyDF/kaggle_diabetic_retinopathy"" target=""_blank"">Code and models (with parameters)</a> added.<a name=""top""></a>
<h2>Table of Contents</h2>
<ol>
	<li><a href=""#introduction"">Introduction</a></li>
	<li><a href=""#overview"">Overview / TL;DR</a></li>
	<li><a href=""#opening"">The opening (processing and augmenting, kappa metric and first models)</a></li>
	<li><a href=""#middlegame"">The middlegame (basic architecture, visual attention)</a></li>
	<li><a href=""#endgame"">The endgame (camera artifacts, pseudo-labeling, decoding, error distribution, ensembling)</a></li>
	<li><a href=""#other"">Other (not) tried approaches and papers</a></li>
	<li><a href=""#conclusion"">Conclusion</a></li>
	<li><a href=""#code"">Code, models and example activations</a></li>
</ol>
<a name=""introduction""></a>
<h2>Introduction</h2>
<strong>Diabetic retinopathy (DR)</strong> is the leading cause of blindness in the working-age population of the developed world and is estimated to affect over 93 million people. (From the <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection"" target=""_blank"">competition description</a> where some more background information can be found.)

The grading process consists of recognising very fine details, such as <em>microaneurysms</em>, to some bigger features, such as <em>exudates</em>, and sometimes their position relative to each other on <em>images of the eye</em>. (This is not an exhaustive list, you can look at, for example, the<a href=""https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/402294/Revised_Grading_Definitions_V1_3_1Nov12_SSG.pdf"" target=""_blank""> long list of criteria used in the UK to grade DR</a>.)

Some annotated examples from the literature to get an idea of what this really looks like (the medical details/terminology are not very important for the rest of this post):

[caption id=""attachment_5099"" align=""aligncenter"" width=""446""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_2.png""><img class=""wp-image-5099 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_2.png"" alt=""dr5_2"" width=""446"" height=""348"" /></a> Example of <em>non-proliferative diabetic retinopathy</em> (NPDR): <strong>Thin arrows</strong>: hard exudates; <strong>Thick arrow</strong>: blot intra-retinal hemorrhage; <strong>Triangle</strong>: microaneurysm. (<a href=""http://openi.nlm.nih.gov/detailedresult.php?img=3263167_OJO-4-135-g001&amp;query=null&amp;req=4&amp;npos=-1"" target=""_blank"">Source</a>)[/caption]

[caption id=""attachment_5100"" align=""aligncenter"" width=""512""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_3.png""><img class=""wp-image-5100 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_3.png"" alt=""dr5_3"" width=""512"" height=""568"" /></a> Some (sub)types of diabetic retinopathy. The competition grouped some together to get 5 ordered classes. (<a href=""http://openi.nlm.nih.gov/detailedresult.php?img=3284208_opth-6-269f3&amp;query=&amp;it=xg&amp;req=4&amp;npos=4"" target=""_blank"">Source</a>)[/caption]

Now let's look at it as someone who simply wants to try to model this problem.

You have <strong>35,126 images in the training set</strong> that look like this...

[caption id=""attachment_5101"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_4.png""><img class=""wp-image-5101"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_4.png"" alt=""dr5_4"" width=""612"" height=""413"" /></a> Some pseudorandom samples from the training set. Notice the black borders and different aspect ratios.[/caption]

...annotated by a patient id and ""left"" or ""right"" (each patient has two images, one per eye) and divided into <strong>5 fairly unbalanced classes</strong> (<em>per eye/image, not per patient</em>)!

[table]
Class,Name,Number of images,Percentage
0,Normal,25810,73.48%
1,Mild NPDR,2443,6.96%
2,Moderate NPDR,5292,15.07%
3,Severe NPDR,873,2.48%
4,PDR,708,2.01%
[/table]

You are asked to predict the class (thus, one of the 5 numbers) for each of the <strong>53,576 test images</strong> and your predictions are scored on the <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection/details/evaluation"" target=""_blank"">quadratic weighted kappa metric</a>. For the public leaderboard that was updated during the competition, 20 percent of the test data was used. The other 80 percent are used to calculate the final ranking.

Not surprisingly, all my models were <strong>convolutional networks</strong> (convnets) adapted for this task. I recommend reading the <a href=""http://benanne.github.io/2015/03/17/plankton.html"" target=""_blank"">well-written blogpost</a> from the ≋ Deep Sea ≋ team that won the <a href=""https://www.kaggle.com/c/datasciencebowl"" target=""_blank"">Kaggle National Data Science Bowl</a> competition since there are a lot of similarities in our approaches and they provide some more/better explanation.

<strong>A small but important note:</strong> notice that 20 percent of the test images is roughly 11k images and the training set is only 35k images. Hence, the leaderboard score can actually provide some more stable scoring than only using, for example, a 90%/10% split on the training set which would result in only about a third of the size of the public leaderboard dataset used for validation. Normally I don't like to make many submissions during a competition but because training and evaluating these models is quite time-consuming and the public leaderboard could provide some interesting and somewhat reliable information, I did try to make use of this at the end.

Also note that competing in a competition such as this requires you -- unless you have a huge amount of resources and time -- to interpret a stream of fairly complex and noisy data as quickly and efficiently as possible and thus, the things I have ""learned"" are not really tested rigorously and might even be wrong. In the same mindset, a lot of the code is written very quickly and even obvious optimisations might be postponed (or never done!) because the trade-off is not worth it, there are more immediate priorities, you don't want to risk breaking some things, I don't know how (to do it quickly), etc. I started working on this competition somewhere in the beginning of April because before that I was in the process of moving to London.
<p style=""text-align: right;""><a href=""#top"">top</a>
<a name=""overview""></a></p>

<h2>Overview / TL;DR</h2>
This post is written in an approximate chronological order, which might be somewhat confusing to read through at first. Therefore, I'll try to sketch in a very concise fashion what my eventual models looked like. (It's not completely independent of the rest of the text, which does try to provide some more explanation.)

My best models used a <strong>convolutional network</strong> with the following relatively basic architecture (listing the output size of each layer)

[table]
Nr,Name,batch,channels,width,height,filter/pool
0,Input,64,3,512,512,
1,Conv,64,32,256,256,7//2
2,<em>Max pool</em>,64,32,127,127,3//2
3,Conv,64,32,127,127,3//1
4,Conv,64,32,127,127,3//1
5,<em>Max pool</em>,64,32,63,63,3//2
6,Conv,64,64,63,63,3//1
7,Conv,64,64,63,63,3//1
8,<em>Max pool</em>,64,64,31,31,3//2
9,Conv,64,128,31,31,3//1
10,Conv,64,128,31,31,3//1
11,Conv,64,128,31,31,3//1
12,Conv,64,128,31,31,3//1
13,<em>Max pool</em>,64,128,15,15,3//2
14,Conv,64,256,15,15,3//1
15,Conv,64,256,15,14,3//1
16,Conv,64,256,15,15,3//1
17,Conv,64,256,15,15,3//1
18,<em>Max pool</em>,64,256,7,7,3//2
19,Dropout,64,256,7,7,
20,Maxout (2-pool),64,512, , ,
21,Concat with image dim,64,514, , ,
22, <strong>Reshape</strong> (merge eyes),32,1028, , ,
23,Dropout,32,1028, , ,
24,Maxout (2-pool),32,512, , ,
25,Dropout,32,512, , ,
26,Dense (linear),32,10, , ,
27, <strong>Reshape</strong> (back to one eye),64,5, , ,
28,Apply softmax,64,5, , ,
[/table]
<p style=""text-align: center;""><em>Where a//b in the last column denotes pool or filter size a x a with stride b x b.</em></p>
where the reshape was done to <strong>combine the representations of the two eyes belonging to the same patient</strong>. All layers were initialised with the SVD variant of the orthogonal initialisation (based on <a href=""http://arxiv.org/abs/1312.6120"" target=""_blank"">Saxe et al.</a>) and the non-linear layers used <strong>leaky rectify</strong> units <em>max(alpha*x, x)</em> with <em>alpha=0.5</em>.

The inputs were 512x512 images which were augmented in real-time by

1. <em>Cropping</em> with certain probability
2. <em>Color balance</em> adjustment
3. <em>Brightness</em> adjustment
4. <em>Contrast</em> adjustment
5. <em>Flipping images</em> with 50% chance
6. <em>Rotating images</em> by x degrees, with x an integer in [0, 360]
7. <em>Zooming</em> (equal cropping on x and y dimensions)

together with their original image dimensions. Because of the great class imbalance, some classes were oversampled to get a more uniform distribution of classes in batches. Somewhere in the middle of training this oversampling stopped and images were sampled from the true training set distribution to
<ol>
	<li>Try to control the overfitting, which is particularly difficult when the network sees some images almost ten times more often than others.
Have the network fine-tune the predictions on the true class distribution.</li>
	<li>Training was done with Stochastic Gradient Descent (SGD) and Nesterov momentum for almost 100k iterations on a loss which was a combination of a continuous kappa loss together with the cross-entropy (or log) loss:</li>
</ol>
<pre> kappalogclipped = cont_kappa + 0.5 * T.clip(log_loss, log_cutoff, 10**3)

</pre>
An important part was <strong>converting the softmax probabilities for each label to a discrete prediction</strong>. Using the label with the highest probability (i.e. <em>argmax</em>) did quite well but is unstable and a significant improvement comes from converting these probabilities to one continuous value (by weighted sum), ranking these values and then using some boundaries to assign labels (e.g., first 10% is label 0, next 20% is label 1, etc.).

Doing all this gets you to somewhere around <strong>+0.835</strong> with a single model.

A final good improvement then came from <strong>ensembling</strong> a few models using the mean of their log probabilities for each class, converting these to normal probabilities in [0, 1] again and using
<pre> weighted_probs = probs[:, 1] + probs[:, 2] * 2 + probs[:, 3] * 3 + probs[:, 4] * 4

</pre>
to get one vector of predictions on which we can apply the ranking procedure from the previous paragraph to assign labels. A few candidate boundaries were determined using <a href=""http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.optimize.minimize.html"" target=""_blank"">scipy's minimize</a> function on the kappa score of some ensembles.
<p style=""text-align: right;""><a href=""#top"">top</a>
<a name=""opening""></a></p>

<h2>The opening</h2>
The patients are always split into a training set (90%) and a validation set (10%) by stratified sampling based on the maximum label of the two eyes per patient.
<h3>Processing and augmenting</h3>
First of all, since the original images are fairly large (say, 3000x2000 pixels on average) and most contained a fairly significant black border, I started by downscaling all the images by a factor of five (without interpolation) and trying to remove most of these black borders.

[caption id=""attachment_5102"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_5.png""><img class=""wp-image-5102"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_5.png"" alt=""dr5_5"" width=""612"" height=""413"" /></a> The same images from before but now the black borders are cropped.[/caption]

This made it computationally much more feasible to do many <strong>augmentations</strong>: we augment the training set with seemingly similar images to increase the number of training examples.

These augmentations (transformations) were:

1. <i>Cropping</i> with certain probability
2. <i>Color balance</i> adjustment
3. <i>Brightness</i> adjustment
4. <i>Contrast</i> adjustment
5. <i>Flipping</i> images with 50% chance
6. <i>Rotating</i> images by x degrees, with x an integer in [0, 360[
7. <i>Zooming</i> (equal cropping on x and y dimensions)

Most of these were implemented from the start. The specific ranges/parameters depend on the model (some examples can be found at the end). During training random samples are picked from the training set and transformed before queueing them for input to the network. The augmentations were done by spawning different threads on the CPU such that there was almost no delay in waiting for another batch of samples. This worked very well for most of the competition, which is why I did not spend much time trying to optimise this.

<strong>Resizing</strong> was done after the cropping and before the other transformations, because it makes some of other operations computationally way too intensive, and can be done in two ways:

1. Rescale while <em>keeping the aspect ratio</em> and doing a <em>center crop</em> on the resulting image
2. Normal bilinear rescaling, <em>destroying the original aspect ratio</em>

The method chosen also depends on the model. Method 1 was used a lot in the beginning, then 2 in the middle and in the end I revisited the choice again. Eventually I stuck with method 2 since my best performing models in the end used that and I did not want to have another hyperparameter to take into account, and I was afraid of losing too much information with the center crops from the first method.

[caption id=""attachment_5103"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_6.png""><img class=""wp-image-5103"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_6.png"" alt=""dr5_6"" width=""612"" height=""306"" /></a> An older batch of augmented 512x512 samples as input to a network. The augmenting process went through some subtle changes since then. You can also see that the images are paired by patient but with independent augmentations (more about that later).[/caption]

During the training the <strong>input is normalised</strong> by subtracting the total mean and dividing by the total standard deviation estimated on a few hundred samples before training (a pixel based normalisation was also used for a lot of models, at the moment of writing I have not figured out if there is a big difference).

The original image dimensions (before they are rescaled to a square input) are also always concatenated as features with the other dense representations (I never removed this or tested if it was of any use).
<h3>The kappa metric</h3>
The first few nets were initially trained with SGD and Nesterov momentum on the cross-entropy (or log) loss function. Thus, as a <em>classification problem</em>. However, the competition used the (quadratic weighted) kappa metric to score submissions and not the log loss or accuracy. The <strong>(quadratic weighted) kappa score</strong> of a class prediction <em>y</em> for a ground truth <em>t</em> can be given by the Python code (not optimised for edge cases and assuming one-hot encoded):
<pre> import numpy as np
 
def quadratic_kappa(y, t, eps=1e-15):
 # Assuming y and t are one-hot encoded!
 num_scored_items = y.shape[0]
 num_ratings = y.shape[1]
 ratings_mat = np.tile(np.arange(0, num_ratings)[:, None],
 reps=(1, num_ratings))
 ratings_squared = (ratings_mat - ratings_mat.T) &amp;amp;amp;amp;lt;b&amp;amp;amp;amp;gt; 2
 weights = ratings_squared / (float(num_ratings) - 1) &amp;amp;amp;amp;lt;b&amp;amp;amp;amp;gt; 2</pre>
<pre># We norm for consistency with other variations.
 y_norm = y / (eps + y.sum(axis=1)[:, None])</pre>
<pre># The histograms of the raters.
 hist_rater_a = y_norm.sum(axis=0)
 hist_rater_b = t.sum(axis=0)</pre>
<pre># The confusion matrix.
 conf_mat = np.dot(y_norm.T, t)</pre>
<pre># The nominator.
 nom = np.sum(weights * conf_mat)
 expected_probs = np.dot(hist_rater_a[:, None],
 hist_rater_b[None, :])
 # The denominator.
 denom = np.sum(weights * expected_probs / num_scored_items)</pre>
<pre>return 1 - nom / denom</pre>
Alternatively, for some more information about the metric, see the <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection/details/evaluation"" target=""_blank"">evaluation page of the competition</a>. The kappa metric tries to represent the <em>agreement between two different raters</em>, where the score typically varies from 0 (random agreement) to 1 (complete agreement). The downside of this metric is that it is <em>discrete</em> -- forcing you to pick only one class of the five per sample instead of allowing you to use the probabilities for each class -- which necessarily gives you some more variance when evaluating models. There already is a decent amount of noise in the dataset (<a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection/forums/t/14402/a-rogue-s-gallery-of-training-cases"" target=""_blank"">very questionable ground truth classifications</a>, noisy/broken images, etc.) so there certainly was a noticeable amount of variance throughout the competition to take into account.

I only trained one or two models on the cross-entropy loss since the competition was scored on the kappa metric and I felt it was important to quickly change to a loss function closely based on this metric. Several variations of a <em>continuous kappa loss</em> -- you simply use your softmax prediction matrix <em>y</em> as input to the <i>quadratic_kappa</i> function above -- were tried but using only a kappa loss seemed too unstable in the beginning of training to learn well. Combining the continuous kappa with the cross-entropy loss seemed to fix this. Eventually this ""<strong>kappalogclipped</strong>"" loss was used
<pre>kappalogclipped = cont_kappa + 0.5 * T.clip(log_loss, log_cutoff, 10**3)</pre>
where there was an additional <em>y_pow</em> parameter which determined the power to which to raise the predictions before calculating the continuous kappa (squashing lower probabilities, making the predictions more discrete). This <em>y_pow</em> was 2 for a lot of the later experiments and has a significant influence on the model. By using <em>y_pow=2</em> the continuous kappa approximates the discrete kappa very well, which will give you some more variance in the updates. The <em>log_cutoff</em> determined from which point to start ignoring the log loss, this was 0.80 for most of the experiments. The scaling by 0.5 is something left behind by many other experiments with losses.

There has been a lot of discussion about <strong>optimising the kappa metric</strong>, partly because of the recent <a href=""https://www.kaggle.com/c/crowdflower-search-relevance"" target=""_blank"">CrowdFlower Search Results Relevance competition</a> which also used this metric to score the submissions. Most of the competitors in that competition used regression on the Mean Squared Error (MSE) objective together with some <em>decoding strategy</em> to convert the continuous predictions to discrete ones. The first place winner has an <a href=""https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/Doc/Kaggle_CrowdFlower_ChenglongChen.pdf"" target=""_blank"">excellent write-up</a> where he compared many different methods to optimise the kappa metric and concludes that MSE together with a ranking based discretisation worked best. I also considered such an approach but since training my models took quite a while and I did not want to lose too much time testing all these different methods -- <em>and</em> it already worked quite well! -- I stuck with the <em>kappalogclipped</em> loss. These other losses also don't take the distribution of the predictions into account, which I thought might be important (even though you could optimise for that <em>after training</em>). The <em>kappalogclipped</em> loss had the benefit of allowing me to easily monitor a relatively reliable kappa score during the training by using the label with the highest probability (the same <i>argmax</i> strategy was used for almost all the models but is revisited at the end). Also note that the fact that these labels are <em>ordered</em> is implicitly defined by the kappa loss itself.

I did test the MSE objective very briefly at the end and got somewhat similar performance.

This was still trained with SGD with Nesterov momentum using some learning rate schedule (decreasing the learning rate 3-5 times). Most of the time some L2 regularisation on the network parameters, or <strong>weight decay</strong>, was added.
<h3>First models</h3>
My first models used <em>120x120 rescaled input</em> and I stayed with that for a decent amount of time in the beginning (first 3-4 weeks). A week or so later my first real model had an architecture that looked like this (listing the output size of each layer)
<p style=""text-align: left;"">[table]
Nr,Name,batch,channels,width,height,filter/pool
0,Input,32,3,120,120,
1,Cyclic slice,128,3,120,120,
2,Conv,128,32,120,120,3//1
3,Conv,128,16,120,120,3//1
4, <em>Max pool</em>,128,16,59,59,3//2
5,Conv roll,128,64,59,59,
6,Conv,128,64,59,59,3//1
7,Conv,128,32,59,59,3//1
8,<em>Max pool</em>,128,32,29,29,3//2
9,Conv roll,128,128,29,29,
10,Conv,128,128,29,29,3//1
11,Conv,128,128,29,29,3//1
12,Conv,128,128,29,29,3//1
13,Conv,128,64,29,29,3//1
14,<em>Max pool</em>,128,64,14,14,3//2
15,Conv roll,128,256,14,14,
16,Conv,128,256,14,14,3//1
17,Conv,128,256,14,14,3//1
18,Conv,128,256,14,14,3//1
19,Conv,128,128,14,14,3//1
20,<em>Max pool</em>,128,128,6,6,3//2
21,Dropout,128,128,6,6,
22,Maxout (2-pool),128,512, , ,
23,Cyclic pool,32,512, , ,
24,Concat with image dim,32,514, , ,
25,Dropout,32,514, , ,
26,Maxout (2-pool),32,512, , ,
27,Dropout,32,512, , ,
28,Softmax,32,5, , ,
[/table]</p>
<p style=""text-align: center;""><em>Where a//b in the last column denotes pool or filter size a x a with stride b x b.</em></p>
which used the cyclic layers from the <a href=""http://benanne.github.io/2015/03/17/plankton.html"" target=""_blank"">≋ Deep Sea ≋ team</a>. As nonlinearity I used the <strong>leaky rectify</strong> function, <i>max(alpha*x, x)</i>, with <em>alpha=0.3</em>. Layers were almost always initialised with the SVD variant of the orthogonal initialisation (based on <a href=""http://arxiv.org/abs/1312.6120"" target=""_blank"">Saxe et al.</a>). This gave me around <strong>0.70</strong> kappa. However, I quickly realised that, given the grading criteria for the different classes (think of the microaneurysms which are pretty much impossible to detect on 120x120 images), I would have to use bigger input images to get anywhere near a decent model.

Something else that I had already started testing in models somewhat, which seemed to be quite critical for decent performance, was <strong>oversampling the smaller classes</strong>. I.e., you make samples of certain classes more likely than others to be picked as input to your network. This resulted in more stable updates and better, quicker training in general (especially since I was using small batch sizes of 32 or 64 samples because of GPU memory restrictions).
<p style=""text-align: right;""><a href=""#top"">top</a>
<a name=""middlegame""></a></p>

<h2>The middlegame</h2>
First I wanted to take into account the fact that for each <em>patient</em> we get two retina images: the left and right eye. By <strong>combining the dense representations of the two eyes</strong> before the last two dense layers (one of which being a softmax layer) I could use both images to classify each image. Intuitively you can expect some pairs of labels to be more probable than others and since you always get two images per patient, this seems like a good thing to do.

This gave me the <em>basic architecture for 512x512 rescaled input</em> which was used pretty much until the end (except for some experiments):

[table]
Nr,Name,batch,channels,width,height,filter/pool
0,Input,64,3,512,512,
1,Conv,64,32,256,256,7//2
2,<em>Max pool</em>,64,32,127,127,3//2
3,Conv,64,32,127,127,3//1
4,Conv,64,32,127,127,3//1
5,<em>Max pool</em>,64,32,63,63,3//2
6,Conv,64,64,63,63,3//1
7,Conv,64,64,63,63,3//1
8,<em>Max pool</em>,64,64,31,31,3//2
9,Conv,64,128,31,31,3//1
10,Conv,64,128,31,31,3//1
11,Conv,64,128,31,31,3//1
12,Conv,64,128,31,31,3//1
13,<em>Max pool</em>,64,128,15,15,3//2
14,Conv,64,256,15,15,3//1
15,Conv,64,256,15,14,3//1
16,Conv,64,256,15,15,3//1
17,Conv,64,256,15,15,3//1
18,<em>Max pool</em>,64,256,7,7,3//2
19,Dropout,64,256,7,7,
20,Maxout (2-pool),64,512,,,
21,Concat with image dim,64,514,,,
22,<strong>Reshape</strong> (merge eyes),32,1028,,,
23,Dropout,32,1028,,,
24,Maxout (2-pool),32,512,,,
25,Dropout,32,512,,,
26,Dense (linear),32,10,,,
27,<strong>Reshape</strong> (back to one eye),64,5,,,
28,Apply softmax,64,5,,,
[/table]
<p style=""text-align: center;""><em>Where a//b in the last column denotes pool or filter size a x a with stride b x b.</em></p>
Some things that had also been changed:
<ol>
	<li>Using <strong>higher leakiness</strong> on the leaky rectify units, <em>max(alpha*x, x)</em>, made a big difference on performance. I started using <em>alpha=0.5</em> which worked very well. In the small tests I did, using <em>alpha=0.3</em> or lower gave significantly lower scores.</li>
	<li>Instead of doing the initial downscale with a factor five before processing images, I only downscaled by a factor two. It is unlikely to make a big difference but I was able to handle it computationally so there was not much reason not to.</li>
	<li>The oversampling of smaller classes was now done with a <strong>resulting uniform distribution</strong> of the classes. But now it also switched back somewhere during the training to the <em>original</em> training set distribution. This was done because initially I noticed the distribution of the predicted classes to be quite different from the training set distribution. However, this is not necessarily because of the oversampling (although you would expect it to have a significant effect!) and it appeared to be mostly because of the specific kappa loss optimisation (which takes into account the distributions of the predictions and the ground truth). It is also much more prone to overfitting when training for a long time on some samples which are 10 times more likely than others.</li>
	<li>Maxout worked slightly better or at least as well as normal dense layers (but it had fewer parameters).</li>
</ol>
<h3>Visual attention</h3>
Throughout I also kept trying to find a way to work better with the high resolution input. I tried splitting the image into four (or sixteen) non-overlapping (or only slightly overlapping) squares, passing these through a smaller convnet in parallel and then combining these representations (by stacking them or pooling over them) but this did not seem to work. Something I was a little more hopeful about was using the <strong>spatial transformation layers</strong> from the <a href=""http://arxiv.org/abs/1506.02025"" target=""_blank"">Spatial Transformer Networks paper</a> from <a href=""http://www.deepmind.com/"" target=""_blank"">DeepMind</a>. The intention was to use some coarse input to make the ST-modules <b>direct their attention to some parts of the image in higher resolution</b> (for example, potential microaneurysms!) and hopefully they would be able to detect those finer details.

However, training this total architecture end-to-end, without initialising with a pre-trained net, was incredibly difficult at first. Even with norm constraints, lowered learning rates for certain components, smaller initialisation, etc., it was quite difficult, for one, to not have it diverge to some limiting values for the transformation parameters (which sort of makes sense). However, this might be partly because of the specific problem and/or my implementation. In the paper they also seem to work with pre-trained nets and this does seem like the way to go. Unfortunately, when I first tried this, it started overfitting quite significantly on the training set. I wish I had more time to explore this but since my more basic networks already worked so well and seemed to still allow for a decent amount of improvement, I had to prioritise those.

Some small things I have learned:
<ol>
	<li>In general I found it is hard to have different ""networks"" competing with each other in one big architecture. One tends to smother the other. Even when both were initialised with some trained network. This is possibly partly because of my L2 regularisation on the network weights. I had to keep track of the activations of certain layers during training to make sure this didn't happen.</li>
	<li>It wasn't really mentioned anywhere in the paper but I used a sort of normalisation of my transformation parameters for the ST-modules using some sigmoid.</li>
	<li>I think some parts of the network might still need to be fixed (i.e., fixing the parameters) during training to get it working.</li>
</ol>
In general, I think these layers are very interesting but they certainly didn't seem that straightforward to me. However, this might be partially because of this specific problem.
<p style=""text-align: right;""><a href=""#top"">top</a></p>
<a name=""endgame""></a>
<h2>The endgame</h2>
In the last two to three weeks I was trying to wrap up any (very) experimental approaches and started focusing on looking more closely at the models (and their predictions) to see where I could maybe still improve. The ""basic architecture"" from the previous section barely changed for more than a month and most of my improvements came from optimising the learning process. My best (leaderboard) score (of about <strong>0.828</strong>, which was top 3 for a few weeks) three weeks before the end of the competition came from a simple log mean ensemble from 2-3 models scoring individually around <strong>0.82</strong>.
<h3>Camera artifacts</h3>
When looking at the two images of certain patients next to each other I noticed something which is harder to see when looking at single images: <strong>camera artifacts</strong>.

[caption id=""attachment_5104"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_7.png""><img class=""wp-image-5104"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_7.png"" alt=""dr5_7"" width=""612"" height=""246"" /></a> Sample camera artifacts: tiny black dots on the outer left center and bigger black spots and stripes on the outer right.[/caption]

Sometimes they can even resemble microaneurysms (well, they can be tiny black dots, the size of a microaneurysm) and it can be very hard to distinguish between the two unless you have the other image. Even more, these artifacts seemed to be fairly common as well! The problem is: it is very unlikely my models at the time would be able to figure this out because
<ol>
	<li>Although they take both eyes into account by merging the dense representations, these are very high level representations (fully connected layers get rid of a lot of the spatial information, see e.g. <a href=""http://arxiv.org/abs/1506.02753"" target=""_blank"">Dosovitskiy and Brox</a>).</li>
	<li>Augmentations were done on each eye separately, independent of patient. I did have a ""paired transformations"" option but at the time I did not see any big improvements using that.</li>
</ol>
One thing I tried to take this into account was to <em>merge</em> the outputs of the first convolutional or pooling layer for each of the two eyes (or maybe even the input layer). Then theoretically the convolutional layer could be able to detect similar patterns in the left <em>and</em> right eye. However, then I felt I was reducing the input space way too much after the merging (which was -- and had to be -- done in the first few layers) and thus, I instead took the outputs <i>a</i> and <i>b</i> from the output of some layer (for the two eyes of a patient) and replaced them with <i>ab</i> and <i>ba</i> by <em>stacking</em> them on the channel dimension (instead of simply replacing them both by <i>ab</i>). This way the net still had access to the other low level representations but I was not losing half the input space.

Unfortunately this did not seem to help that much and I was running out of time such that I put this aside.
<h3>Pseudo-labeling</h3>
Nearer to the end of the competition I also started testing the clever pseudo-labeling idea from the <a href=""http://benanne.github.io/2015/03/17/plankton.html"" target=""_blank"">≋ Deep Sea ≋</a> team which uses the predictions from other (ensembles of) models on the test set to help <em>guide</em> or regularise new models. Essentially, during training I added some test images to the batches, such that on average roughly 25% of the batch was comprised of images from the test set, together with the softmax predictions for those images from my best ensemble. This probably helped to push me to about <strong>0.83</strong> for a single model.
<h3>Better decoding</h3>
For a long time I simply used the class with the highest probability from my softmax output as my prediction (i.e., <i>argmax</i>). Even though this allowed me to get this far, it was obvious that this method is quite unstable since it doesn't take the magnitude of the (other) probabilities into account, only their size relative to each other. To get around that, I used a similar <em>ranking decoding</em> strategy as was used by some people in the <a href=""https://www.kaggle.com/c/crowdflower-search-relevance"" target=""_blank"">CrowdFlower Search Results Relevance competition</a>: first we convert the probabilities from the softmax output to one value by weighing each probability by the class label {0, 1, 2, 3, 4}
<pre>weighted_probs = probs[:, 1] + probs[:, 2] * 2 + probs[:, 3] * 3 + probs[:, 4] * 4

</pre>
Next we rank the weighted probabilities from low to high and try to find the most optimal boundaries <i>[x_0, x_1, x_2, x_3]</i> to assign labels. I.e., the images with a weighted probability in <i>[0, x_0]</i> we assign the label 0, the images with a weighted probability in <i>[x_0, x_1]</i> we assign the label 1, etc. I used <a href=""http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.optimize.minimize.html"" target=""_blank"">scipy's minimize</a> function to quickly find some boundaries that optimise the kappa score on the train set of some ensemble. This probably could have been optimised better but I did not want to overtune and have the risk of badly overfitting.

This helped quite a bit, pushing some of my single models from 0.83 to <strong>0.835</strong>. However, I was kind of surprised that changing the prediction distribution quite significantly with different boundaries did not result in bigger differences in scores.
<h3>Error distribution</h3>
Something that stood out was the fact that the models were highly variable on the validation sets. Although this was mostly because of the discrete metric, the noise in the dataset itself and my small validation set, I still wanted to try to find out more. If you look at the <i>quadratic_kappa</i> metric/loss above you can see that it is determined by two matrices: <i>nom</i> and <i>denom</i>. The kappa score is then given by <i>1 - sum(nom) / sum(denom)</i>. Hence, you want to minimise the <i>nom</i> sum and maximise the <i>denom</i> sum. The denominator <i>denom</i> is fairly stable and captures the distributions of the predictions and the targets. The nominator <i>nom</i> is the one which will give us the most insight into our model's predictions. Let's look at the nom and denom for some random well-performing model (+0.82 kappa) when I was using the simple <i>argmax</i> decoding:

[caption id=""attachment_5105"" align=""aligncenter"" width=""475""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_8.png""><img class=""wp-image-5105 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_8.png"" alt=""dr5_8"" width=""475"" height=""192"" /></a> Normalised nom (left) and denom (right) for some well performing model with highest probability decoding (+0.82 kappa). Position (i, j) = predicted j for true label i.[/caption]

Whereby the matrices are normalised and show the percentage of the total sum located in that position. You immediately notice something: <em>the error is almost dominated by the errors of predicting class 0 when it was really class 2</em> (i.e., you predict a normal eye but it is really a moderate NPR eye). Initially this gives you some hope, since you think you should be able to bring down some misclassifications which are 2 classes apart and they would have an important impact on your score. However, the problem with this, and in general, was that the ground truth was not sure at all (since I'm not an ophthalmologist, not even a doctor!) and some <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection/forums/t/14402/a-rogue-s-gallery-of-training-cases"" target=""_blank"">really questionable classifications had already come to light</a>.

Because the other ranking decoding strategy is only applied <em>after training</em> and was done quite late in the competition, I don't have any error distribution pictures for those yet. But I do remember the nom behaving quite similarly and most of the error (30-40%) coming from predicting class 0 when it was really class 2.
<h3>Ensembling</h3>
A good improvement then came from <strong>ensembling</strong> a few models using the mean of their log probabilities for each class, converting these to normal probabilities in [0, 1] again and using the <em>ranking decoding </em>strategy from one of the previous paragraphs to assign labels to the images. A few candidate boundaries were determined using <a href=""http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.optimize.minimize.html"" target=""_blank"">scipy's minimize</a> function on the kappa score of some ensembles. Doing this on the best models at the time pushed my models to <strong>+0.84</strong>.
<p style=""text-align: right;""><a href=""#top"">top</a></p>
<a name=""other""></a>
<h2>Other (not) tried approaches and papers</h2>
Some other things that <em>did not seem to work</em> (well enough):
<ul>
	<li><a href=""http://arxiv.org/abs/1502.03167"" target=""_blank"">Batch Normalisation</a>: although it allowed for higher learning rates, it did not seem to speed up training all that much. (But I do believe the training of convnets should be able to be much faster than it is now.)</li>
	<li>Other types of updates (e.g., <a href=""http://arxiv.org/abs/1412.6980"" target=""_blank"">ADAM</a>): especially with the special loss based on the kappa metric I was not able to get it working well.</li>
	<li><a href=""http://arxiv.org/abs/1412.6806"" target=""_blank"">Remove pooling layers</a>: replacing <em>all</em> the 3//2 (pool size 3x3 and stride 2x2) max pooling layers with a 3//2 <em>convolutional layer</em> made the training quite a bit slower but more importantly seemed to have pretty bad performance. (I was more hopeful about this!)</li>
	<li><a href=""http://arxiv.org/abs/1502.01852"" target=""_blank"">PReLU</a>: tried it only briefly but it seemed to only worsen performance / result in (more) overfitting.</li>
	<li>Using 2//2 max pooling instead of 3//2 in the hope of being able to distinguish finer details better (since, for example, microaneurysms are most of the time located closely to some vessel).</li>
	<li>Several architecture changes: more convolutional layers after the first, second or third pooling layer, bigger filter size in the first layer, replace the first 7//2 convolutional and 3//2 pooling layer by two 5//2 convolutional layers, etc. But it did not seem to help that much and I was struggling with the variance of the models.</li>
	<li>Second level model: at the very, very end I tried using second level models (such as ridge regression, Lasso, etc.) on the softmax outputs of many different models (instead of just using a log mean ensemble). Unfortunately, in my case, it was too messy to do this at the last minute with many models with small and different validation sets. But I would expect it to potentially work really well if you are able to regularise it enough.</li>
</ul>
Some things <em>I haven't tried</em>:
<ul>
	<li><i>Specialist networks</i> makes everything much more complicated and there was still the problem of some classes being very small. I also wasn't sure at all that it would have helped.</li>
	<li><i>Higher resolution input</i> 512x512 was about the limit for networks that could train in 1-2 days on a GTX 980. One of the bigger bottlenecks then actually became the augmentation. I estimated from looking at some images (and hoped) that 512x512 would be able to capture most of the information.</li>
	<li><i>Implementing one of the many more manual approaches</i> there were many <a href=""http://www.ncbi.nlm.nih.gov/pubmed/20703740"" target=""_blank"">papers</a> reviewing the different approaches tried in the literature for detecting diabetic retinopathy on eye images and most of them were quite manual (constructing several filters and image processing pipelines to extract features). I believe(d) quite firmly that a convnet should be able to do all of that (and more) but supplementing the network with some of these manual approaches might have been helpful since you could simply run it only once on higher resolution input (maybe helping you detect, for example, microaneurysms). However, I couldn't bring myself to do all this manual image processing and wanted a more elegant solution.</li>
	<li><i>Unsupervised training</i> it would have been a bit of a gamble since I haven't read that many recent experiences where unsupervised training was very helpful and it would have taken a lot of time. I certainly wanted to use the test set and was more than happy with the pseudo-labeling approach.</li>
	<li><i>Multiple streams / scales / crops</i> I felt I had more immediate problems to overcome and this would have made the model too big (for my resources).</li>
</ul>
I have read quite a few papers (and skimmed a lot more) and even though some of them were very interesting, I was quite limited in time and resources to try them all out. Some I have read and found interesting (not necessarily directly related to this competition):
<ul>
	<li><a href=""http://arxiv.org/abs/1504.07947"" target=""_blank"">Efficient Multiple Instance Convolutional Neural Networks for Gigapixel Resolution Image Classification</a>: in the paper they iteratively select <em>discriminative patches</em>, patches whose hidden label equals the true label of the image, from a gigapixel resolution image and train a convnet on them. I decided the retina images did not need to be that high res to capture almost all the low level information to have to do something like this (I would guess maximum 1024x1024, probably smaller, which is still very big but more doable). It also felt a little unsuitable for these eye images that have so many different types of low level information (not the best explanation).</li>
	<li><a href=""http://arxiv.org/abs/1412.7755"" target=""_blank"">Multiple Object Recognition with Visual Attention</a> and <a href=""http://arxiv.org/abs/1506.02025"" target=""_blank"">Spatial Transformer Networks</a>: these are two different methods to have visual attention on images: the first uses recurrent nets, the second normal, end-to-end convnets. This was all when I was searching for a way to be able to work with higher resolution images.</li>
	<li><a href=""http://arxiv.org/abs/1506.02753"" target=""_blank"">Inverting Convolutional Networks with Convolutional Networks</a>, <a href=""http://arxiv.org/abs/1311.2901"" target=""_blank"">Visualizing and Understanding Convolutional Networks</a> and <a href=""http://arxiv.org/abs/1412.6856"" target=""_blank"">Object Detectors Emerge in Deep Scene CNNs</a>: all of these papers try to deduce what kind of information each layer holds, how well you can reconstruct the original image just from this information and some try to see how invariant this information is under certain transformations of the original image. If you've heard or read about <em>DeepDream</em> (kind of hard not to have), there are some related things going on in the first paper but more theoretical. The more direct inspiration for DeepDream was the paper <a href=""http://arxiv.org/abs/1312.6034"" target=""_blank"">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</a> which is also very interesting!</li>
</ul>
<p style=""text-align: right;""><a href=""#top"">top</a>
<a name=""conclusion""></a></p>

<h2>Conclusion</h2>
The actual process was quite a bit more lengthy and chaotic (especially at the end) but hopefully this captures the most important elements of my competition experience. All in all, a relatively basic architecture was able to achieve top scores in this competition. Nevertheless, the predictions always felt quite ""weak"" and my feeling is that there is still quite a bit of room for improvement. Without a doubt the biggest difficulty for me was dealing with the <strong>large amount of variance</strong> resulting from
<ol>
	<li>the noisy labels</li>
	<li>the extremely small classes (the two highest gradings together represent less than 5% of the samples!)</li>
	<li>the discrete metric which then very heavily punishes extreme misclassifications (and this in combination with the previous point!)</li>
</ol>
Maybe I should have worked harder on taking 2. into account, although, when checking the misclassifications of my models, I still felt that 1. was a very significant problem as well. It would be very interesting (and important!) to get some scores of other (human) expert raters.

In hindsight, <strong>optimising learning on the special kappa metric seemed to be much more important than optimising the architecture</strong> and I did lose a lot of time trying to work with some more special architectures because I thought they might be needed to be able to finish high enough. It is also possible that the MSE objective was the better choice for this competition. I tested it briefly at the end and the performance seemed somewhat similar but I would expect it to be more stable than using my <em>kappalogclipped</em> loss. I should also have explored different train/validation splits instead of always using 90%/10% splits. This possibly could have made evaluating the models more stable.

Personally, I very much enjoyed the competition. I learned a lot, am left with a whole bunch of ideas to work on and many pages of ugly code to rework. Hopefully next time I can compete together with other smart, motivated people since I miss having those interactions and doing everything on my own while also working a full time job was quite challenging and exhausting! Congratulations to the winners and all the people who contributed!

Also, when comparing results from this competition to other approaches or software, take into account that it does not necessarily make any sense because they may be training and/or evaluating on different datasets!
<p style=""text-align: right;""><a href=""#top"">top</a>
<a name=""code""></a></p>

<h2>Code, models and example activations</h2>
Everything was trained on a NVIDIA GTX 980 in the beginning; this was the GPU for the desktop I was also working on, which wasn't ideal. Therefore, later I also tried using the GRID K520 on AWS (even though it was at least two times slower). The <a href=""https://github.com/JeffreyDF/kaggle_diabetic_retinopathy"" target=""_blank"">code</a> is based on the <a href=""https://github.com/benanne/kaggle-ndsb"" target=""_blank"">code</a> from the <a href=""http://benanne.github.io/2015/03/17/plankton.html"" target=""_blank"">≋ Deep Sea ≋</a> team that won the <a href=""https://www.kaggle.com/c/datasciencebowl"" target=""_blank"">Kaggle National Data Science Bowl competition</a> and uses mostly <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a> (which uses <a href=""https://github.com/Theano/Theano"" target=""_blank"">Theano</a>). Models (including static and learned parameters and data from the training) were dumped via <a href=""https://docs.python.org/2/library/pickle.html"" target=""_blank"">cPickle</a> and a quickly written script was used to produce images for each of these dumps summarising the model and its performance. For example:

[caption id=""attachment_5106"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_9.png""><img class=""wp-image-5106 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_9-1024x965.png"" alt=""dr5_9"" width=""640"" height=""603"" /></a> Example of a model image. It isn't the prettiest but it has almost all the information I need. This model got about <strong>0.824</strong> on the public leaderboard. The long training time is mostly because of AWS. (Click to enlarge.)[/caption]

This way it was much easier to compare models and keep track of them. (This is just an example to give you an idea of one method I used to compare models. If you are curious about the specifics, you can find all the information in the code itself.)

<strong>Example activations</strong> for one of the better models with the basic 512x512 architecture (each vertical block represents the output of one channel of the layer):

[caption id=""attachment_5107"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_10.png""><img class=""wp-image-5107 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_10-1024x512.png"" alt=""dr5_10"" width=""640"" height=""320"" /></a> Input layer without augmentations and normal rescaling. Labels (from left to right, up to down): 0, 0, 0, 2, 1, 0, 4, 0 -- 1, 0, 0, 2, 2, 0, 4, 0 -- 0, 0, 0, 0, 0, 2, 2, 0 -- 0, 0, 0, 0, 0, 2, 3, 0.[/caption]

[caption id=""attachment_5108"" align=""aligncenter"" width=""242""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_11.png""><img class=""wp-image-5108 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_11-242x1024.png"" alt=""First layer, 7//2 convolutional layer activations. Click for larger image (24MB)."" width=""242"" height=""1024"" /></a> First layer, 7//2 convolutional layer activations. <a href=""https://github.com/JeffreyDF/jeffreydf.github.io/blob/master/images/dr_comp/outputs/2015_07_20_204321_1.png"" target=""_blank"">Larger image</a>.[/caption]

[caption id=""attachment_5109"" align=""aligncenter"" width=""242""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_12.png""><img class=""wp-image-5109 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_12-242x1024.png"" alt=""dr5_12"" width=""242"" height=""1024"" /></a> Second layer, 3//2 pooling layer activations. <a href=""https://github.com/JeffreyDF/jeffreydf.github.io/blob/master/images/dr_comp/outputs/2015_07_20_204321_2.png"" target=""_blank"">Larger image</a>.[/caption]

[caption id=""attachment_5110"" align=""aligncenter"" width=""242""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_13.png""><img class=""size-large wp-image-5110"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_13-242x1024.png"" alt=""Third layer, 3//1 convolutional layer activations. Larger image."" width=""242"" height=""1024"" /></a> Third layer, 3//1 convolutional layer activations. <a href=""https://github.com/JeffreyDF/jeffreydf.github.io/blob/master/images/dr_comp/outputs/2015_07_20_204321_3.png"" target=""_blank"">Larger image</a>.[/caption]

[caption id=""attachment_5111"" align=""aligncenter"" width=""242""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_14.png""><img class=""size-large wp-image-5111"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_14-242x1024.png"" alt=""Fourth layer, 3//1 convolutional layer activations. Larger image."" width=""242"" height=""1024"" /></a> Fourth layer, 3//1 convolutional layer activations. <a href=""https://github.com/JeffreyDF/jeffreydf.github.io/blob/master/images/dr_comp/outputs/2015_07_20_204321_4.png"" target=""_blank"">Larger image</a>.[/caption]

[caption id=""attachment_5112"" align=""aligncenter"" width=""242""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_15.png""><img class=""size-large wp-image-5112"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/dr5_15-242x1024.png"" alt=""Fifth layer, 3//2 pooling layer activations. Larger image. "" width=""242"" height=""1024"" /></a> Fifth layer, 3//2 pooling layer activations. <a href=""https://github.com/JeffreyDF/jeffreydf.github.io/blob/master/images/dr_comp/outputs/2015_07_20_204321_5.png"" target=""_blank"">Larger image</a>.[/caption]
<p style=""text-align: right;""><a href=""#top"">top</a></p>

<article class=""post-content"">
<h2 id=""thanks"">Thanks</h2>
I would like to thank <a href=""https://kaggle.com/"" target=""_blank"">Kaggle</a>, <a href=""http://www.chcf.org/"" target=""_blank"">California Healthcare Foundation</a> and <a href=""http://eyepacs.com/"" target=""_blank"">EyePACS</a> for organising and/or sponsoring this challenging competition. Also many thanks to the wonderful developers and contributors of <a href=""https://github.com/Theano/Theano"" target=""_blank"">Theano</a> and <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a> for their continuous effort on these libraries.

</article>

<hr />

This blog was originally published <a href=""http://jeffreydf.github.io/diabetic-retinopathy-detection/"" target=""_blank"">here</a> on Jeffrey De Fauw's <a href=""http://jeffreydf.github.io/index.html"" target=""_blank"">site</a>.

Read other posts on the Diabetic Retinopathy competition by clicking the tag below."
"Diabetic Retinopathy Winners' Interview: 4th place, Julian & Daniel",http://blog.kaggle.com/2015/08/14/diabetic-retinopathy-winners-interview-4th-place-julian-daniel/,2015-08-14 18:01:17,"The <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection"" target=""_blank"">Diabetic Retinopathy (DR) competition</a> asked participants to identify different stages of the eye disease in color fundus photographs of the retina. The competition ran from February through July 2015 and the results were outstanding. By automating the early detection of DR, many more individuals will have access to diagnostic tools and treatment. Early detection of DR is key to slowing the disease's progression to blindness.

Fourth place finishers, Julian De Wit and Daniel Hammack, share their approach here (including a simple recipe for using ConvNets on a noisy dataset).
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
<b>Julian De Wit: </b>I studied software engineering at the Technical University of Delft in the Netherlands. I've always loved to implement complex (machine learing) algorithms. Nowadays I work as a freelancer and mainly do machine learning projects. I use Kaggle to battle-test ideas and try new algorithms and frameworks.

[caption id=""attachment_5129"" align=""aligncenter"" width=""382""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-13-at-2.56.37-PM.png""><img class=""size-full wp-image-5129"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-13-at-2.56.37-PM.png"" alt=""Julian's Kaggle profile"" width=""382"" height=""180"" /></a> Julian's Kaggle <a href=""https://www.kaggle.com/juliandewit"" target=""_blank"">profile</a>[/caption]

<b> Daniel Hammack: </b>I have been involved in the machine learning field for a few years, starting with science fair in High School. I thought machine learning was pretty neat and mostly taught myself using the great resources online these days. Andrew Ng, Geoff Hinton, Steven Boyd, and Michael Collins all deserve a shoutout for having excellent lectures available online for free.

[caption id=""attachment_5130"" align=""aligncenter"" width=""378""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-13-at-2.56.54-PM.png""><img class=""size-full wp-image-5130"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-13-at-2.56.54-PM.png"" alt=""Daniel's Kaggle profile"" width=""378"" height=""182"" /></a> Daniel's Kaggle <a href=""https://www.kaggle.com/dhammack"" target=""_blank"">profile</a>[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
<b> Daniel: </b>I had never done any image processing before, and also not worked with any medical data, so this competition was a great learning experience for me. I have been keeping up with the research on deep learning for computer vision, but I wanted the chance to try that knowledge out on some real data.

<b> Julian: </b>I have always been following advances in Neural networks and biologically inspired computing. Since the big breakthroughs with convnets I've been trying to solve practical problems with this technology. I was just building a feature trainer/localizer for a customer and used/tested this software to find DR features. However... in the end this was only a small part of the solution.
<h3>What made you decide to enter this competition?</h3>
<b> Daniel: </b>In the last 3-5 years there has been incredible progress in computer vision mainly through the application of deep convolutional neural networks. I have been excitedly following this research for a few years and lately decided that I'd like to try it out. I was only taking one class during the summer and I knew I'd have some extra time so I decided to give the competition a go as a fun side project.

<b> Julian: </b>I was working on a project that required me to find small objects in big images. This sounded very similar to the DR competition. I thought that competing on the challenge would give me insights for my project and the other way around. I started out making a classifier that localized and counted DR symptoms. However, in the end, an end-to-end convnet dominated the solution.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
Like the other top performing solutions, we used deep convolutional neural networks (CNNs) trained on consumer GPUs. CNNs have been shattering state-of-the-art records on a wide variety of computer vision datasets recently, and they do so without much domain knowledge required (which is great!).

[caption id=""attachment_5124"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/DR4_1.png""><img class=""wp-image-5124"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/DR4_1.png"" alt=""DR4_1"" width=""612"" height=""457"" /></a> Note the differences in image color, brightness, contrast, etc. in some of the random training images.[/caption]

For preprocessing we both had similar approaches. There was a lot of noise in the data, so our normalization pipeline was designed to combat this.

We ended up first centering the eye, cropping out the extra blank space, downsizing, then applying brightness and contrast normalization. We trained models with varying input size - Daniel used 256x256 for the duration of the competition and Julian experimented more with different sizes (larger and smaller). We found larger input sizes to work better, but at the cost of longer time to learn and the usage of more precious GPU memory (the biggest constraint to performance).

[caption id=""attachment_5125"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/DR4_2.png""><img class=""size-full wp-image-5125"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/DR4_2.png"" alt=""Some examples of preprocessed images. Note that color, size, eye location, brightness, and contrast are now more uniform."" width=""600"" height=""600"" /></a> Some examples of preprocessed images. Note that color, size, eye location, brightness, and contrast are now more uniform.[/caption]
<h3>What was your most important insight into the data?</h3>
<b> Daniel: </b>The importance of symmetry. Given that the eye is a sphere, there are several classes of transformations we can apply that should not affect the label of the image. The major ones we used were mirroring and random rotations.

[caption id=""attachment_5126"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/DR4_3.png""><img class=""wp-image-5126 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/DR4_3.png"" alt=""DR4_3"" width=""600"" height=""600"" /></a> An example of how rotations of the eye should not affect the diagnosis.[/caption]

<b> Julian: </b>Again I realized just how good convnets work. I am really convinced that having a convnet as an extra pair of eyes in medical diagnosis would really be useful if not plain mandatory. With a tool I built <a href=""https://github.com/juliandewit/kaggle_retinopathy"" target=""_blank"">here</a>, you can compare the model predictions to the official diagnoses yourself. When the doctor and the model disagree, often it looks like the model is correct!
<h3>Were you surprised by any of your findings?</h3>
<b> Daniel: </b>Yes! I was surprised to find that the ADAM learning rule seemingly made overfitting much easier (but also better training set performance). I was also very impressed by the performance of <a href=""http://arxiv.org/abs/1502.03167"" target=""_blank"">Batch Normalization</a>. In the models where I used Batch Normalization, I observed much quicker convergence and was able to remove dropout.

<b> Julian: </b>I used a 2nd level classifier over the output of the convnet(s) and some extra features. I thought this was a unique approach but somehow this was a necessary step to come to the the 80+ kappa scores.
<h3>Which tools did you use?</h3>
<b> Julian: </b>I started out with my own convnet but halfway the competition I switched to <a href=""https://github.com/dmlc/cxxnet"" target=""_blank"">CXXNet</a> with the CuDNN library. In the last week I tried <a href=""https://github.com/btgraham/SparseConvNet"" target=""_blank"">SparseConvnet</a> by <a href=""https://www.kaggle.com/btgraham"" target=""_blank"">Ben Graham</a>. I really like his ideas and his implementation but, due to my inexperience with his software, I could not get a good score with it at such short notice.

<b> Daniel: </b>I started with <a href=""https://github.com/lisa-lab/pylearn2"" target=""_blank"">Pylearn2</a> and switched to <a href=""https://github.com/fchollet/keras"" target=""_blank"">Keras</a> with about a month left in the competition. Keras is a great library - the code is very easy to understand and customize. I ended up doing quite a bit of customization during the competition, so having that ability was very important.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
Our solution was an ensemble of several models, of course, but the training time for a strong model was typically about 36 hours. We used SGD + momentum which worked pretty well. To generate predictions it's about .02 seconds per image. We found test-time augmentation to be beneficial, so we actually ended up generating predictions on several different views of the same image (from 4-16).
<h2>Words of Wisdom</h2>
<h3>What are your thoughts on teaming up?</h3>
Teaming up with another Kaggler is a great learning experience. Not only do you get the benefit from ensembling your solutions, but often you can share complementary approaches to the same problem.
<h3>What have you taken away from this competition?</h3>
<b> Daniel: </b>One interesting thing about this competition was the quick drop-off in performance on the leaderboard. This is probably because deep convnets are still relatively niche, but they are getting much closer to becoming useful without an intimate knowledge of deep learning. With that said, here is my convnet recipe. It's heavily inspired from <a href=""http://arxiv.org/abs/1409.1556"" target=""_blank"">OxfordNet</a> and the <a href=""http://benanne.github.io/2015/03/17/plankton.html"" target=""_blank"">results</a> of the Deep Sea team in the NDSB competition:

1. Normalize your data. This means removing irrelevant attributes of the input data. If brightness and contrast are not important, then normalize them out.

2. Set up your network. Start by alternating layers of 3x3 convolutions with ReLU activations and 2x2 stride 2 pooling. Each time you pool, increase the number of convolutional filters. I like to double the number of filters, others increase it linearly. Keep alternating these layers until the result is small enough to deal with in fully connected layers. Domain knowledge comes into play here, you need to know how far apart two pixels need to be before you can ignore their interaction. Stack a few fully connected ReLU or MaxOut layers on top.

3. Initialize your network with a tested (theoretically sound) method. I like sqrt(2) scaled <a href=""https://github.com/Lasagne/Lasagne/blob/master/lasagne/init.py#L329"" target=""_blank"">orthogonal initialization</a>, but Julian had good results with the <a href=""http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf"" target=""_blank"">Xavier method</a> so I think either is fine. Good initialization is extremely important.

4. Train with SGD + momentum. Exploit any label-preserving transformations to artificially enlarge your dataset. Use dropout, weight decay, and weight norm penalties if necessary.

A single network following this scheme should have ended up in the top 10% on the leaderboard. Improving the result then takes some work, but the major things to try are: more convolutional layers, different activations, different numbers of filters, different pooling, different preprocessing, and other recent research (e.g. Batch Normalization). Intuition plus trial and error worked well for me in this competition.
<h2>Bios</h2>
<strong>Julian de Wit</strong> is a freelance software engineer. His main interest is to implement theoretical machine learning ideas into practical applications.

<strong>Daniel Hammack</strong> is a researcher at Voloridge Investment Management and a student at the University of Central Florida. He is interested in unsupervised learning, natural language processing, computer vision, and recurrent neural networks.

<hr />

Read other posts on the Diabetic Retinopathy competition by clicking the tag below."
"Avito Winner's Interview: 2nd place, Changsheng Gu (aka Gzs_iceberg)",http://blog.kaggle.com/2015/08/21/avito-winners-interview-2nd-place-changsheng-gu-aka-gzs_iceberg/,2015-08-21 23:09:00,"The <a href=""https://www.kaggle.com/c/avito-context-ad-clicks"" target=""_blank"">Avito Context Ad Click competition</a> asked Kagglers to predict if users of Russia's largest general classified website would click on context ads while they browsed the site. The competition provided a truly robust <a href=""https://www.kaggle.com/c/avito-context-ad-clicks/data"" target=""_blank"">dataset</a> with eight comprehensive relational tables of data on historical user browsing and search behavior, location, and more. Changsheng Gu (aka <a href=""https://www.kaggle.com/gzsiceberg"" target=""_blank"">Gzs_iceberg</a>) finished in second place by using a combination of custom and public tools. <strong>You can read about Owen Zhang's first place approach <a href=""http://blog.kaggle.com/2015/08/26/avito-winners-interview-1st-place-owen-zhang/"" target=""_blank"">here</a>.</strong>

[caption id=""attachment_5145"" align=""aligncenter"" width=""200""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/target_avito300.png""><img class=""wp-image-5145"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/target_avito300.png"" alt=""target_avito300"" width=""200"" height=""202"" /></a> 456 data scientists on 414 teams competed to predict if users would click context ads[/caption]
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I’m working as a software engineer at Bytedance, a Chinese company focusing on news recommendation. My main work is on advertising. Before that, I worked at Amazon doing optimization about warehouse capacity planning.

[caption id=""attachment_5146"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-21-at-4.02.34-PM.png""><img class=""wp-image-5146 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-21-at-4.02.34-PM-300x141.png"" alt=""Kaggle profile"" width=""300"" height=""141"" /></a> Changsheng Gu's Kaggle <a href=""https://www.kaggle.com/gzsiceberg"" target=""_blank"">profile</a>[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
My work helps me, but past competitions about CTR prediction also make me learn a lot. The book “<a href=""http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738"" target=""_blank"">Pattern Recognition And Machine Learning</a>” is also very useful.
<h3>How did you get started competing on Kaggle?</h3>
Two years ago, I was looking for a place to practice what I learned, then I found Kaggle in some online course.
<h3>What made you decide to enter this competition?</h3>
The reason is very simple, I lost in Search Results Relevance competition. Seriously, I’m very interested in CTR prediction.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
For preprocessing, hash tricks and negative down sampling.

For learning methods, FFM, FM and XGBoost.
<h3>Which tools did you use?</h3>
I like to reinvent the wheel, if there’s enough time, I tend to write the tools by myself. But I also used <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a>, <a href=""http://pypy.org/"" target=""_blank"">pypy</a> and <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">lasagne</a> in this competition.
<h3>How did you spend your time on this competition?</h3>
In the early stage, I focused on designing the data pipeline and validation set.

Then, I spent a lot of time doing feature extraction.

After that, I tried to tune the hyperparameters, I used <a href=""https://github.com/hyperopt/hyperopt"" target=""_blank"">hyperopt</a> but didn't see much improvement. Then I decided to train different models by using different feature sets for model ensembling.

Finally, tried to find a good way to do model ensembling.
<h2>Words of wisdom</h2>
<h3>Do you have any advice for those just getting started in data science?</h3>
I believe the devil is in the detail, I often found same methods but somebody always did better. So I encourage people reinventing the tools you used if you have enough time, it helps you understand why and how it works.
<h2>Bio</h2>
Changsheng Gu earned his bachelor in <a href=""http://en.xidian.edu.cn/"" target=""_blank"">Xidian University</a>, China, in 2013. He started his career at Amazon, working on optimization about warehouse. Now he's working as a software engineer in Bytedance, a Chinese company focusing on news recommendation. He’s interested in optimization and prediction problem.

<hr />

Read other posts on the Avito Context Ad Click Prediction competition by clicking the tag below."
"Avito Winner's Interview: 1st place, Owen Zhang",http://blog.kaggle.com/2015/08/26/avito-winners-interview-1st-place-owen-zhang/,2015-08-26 20:04:15,"It was no surprise to see Owen Zhang, currently ranked #1 on Kaggle, take first place in the <a href=""https://www.kaggle.com/c/avito-context-ad-clicks"" target=""_blank"">Avito Context Ad Click competition</a>. Owen used previous competition experience, domain knowledge, and a fondness for XGBoost to finish ahead of 455 other data scientists. The competition gave participants plenty of <a href=""https://www.kaggle.com/c/avito-context-ad-clicks/data"" target=""_blank"">data</a> to explore, with eight comprehensive relational tables on historical user browsing and search behavior, location, and more.

[caption id=""attachment_5151"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/avito_updatedlogo5.png""><img class=""wp-image-5151 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/avito_updatedlogo5-300x141.png"" alt=""avito_updatedlogo5"" width=""300"" height=""141"" /></a> The competition ran June through July 2015[/caption]

In this blog, Owen shares what surprised him, what gave him an edge, and some words of wisdom for all expert and aspiring data scientists. You can read about Changsheng Gu's second place approach <a href=""http://blog.kaggle.com/2015/08/21/avito-winners-interview-2nd-place-changsheng-gu-aka-gzs_iceberg/"" target=""_blank"">here</a>.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I am a data scientist and probably should be considered a veteran Kaggler even before joining this challenge

[caption id=""attachment_5148"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-26-at-11.43.48-AM.png""><img class=""wp-image-5148 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-26-at-11.43.48-AM-300x142.png"" alt=""owen's kaggle badge"" width=""300"" height=""142"" /></a> Owen's <a href=""https://www.kaggle.com/owenzhang1"" target=""_blank"">profile</a> on Kaggle[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
Yes, the two previous CTR challenges (<a href=""https://www.kaggle.com/c/criteo-display-ad-challenge"" target=""_blank"">Criteo</a> and <a href=""https://www.kaggle.com/c/avazu-ctr-prediction"" target=""_blank"">Avazu</a>) certainly help. Also, experience gained from some recent competitions helped.

[caption id=""attachment_5150"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-26-at-11.48.47-AM1.png""><img class=""wp-image-5150"" src=""http://blog.kaggle.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-26-at-11.48.47-AM1.png"" alt="""" width=""500"" height=""125"" /></a> Owen's finishes in the Avazu and Criteo competitions[/caption]
<h3>How did you get started competing on Kaggle?</h3>
I started to learn more about predictive modeling in 2011. The community has been great and I learned so much since then.
<h3>What made you decide to enter this competition?</h3>
I like competition with lots of data (so less leaderboard shakeup), in a domain that I understand, with an interesting structure. This competition is a perfect fit in those aspects.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
I did quite a bit of manual feature engineering and my models are entirely based on <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">xgboost</a>. Feature engineering is a combination of “brutal force” (trying different transformations, etc that I know) and “heuristic” (trying to think about drivers of the target in real world settings). One thing I learned recently was entropy based features, which were useful in my model.
<h3>What was your most important insight into the data?</h3>
There was some “soft leakage”, such as how many other ads are displayed for a given query. Those features are always very powerful, but provides only limited impact in real world applications.

Unlike Criteo and Avazu, where FFM and VW outperformed GBM, in this competition GBM (xgboost) easily outperformed FFM and VW.
<h3>Were you surprised by any of your findings?</h3>
Yes, I thought FFM and VW were required for click through rate, but apparently this is not the case.
<h3>Which tools did you use?</h3>
My solution was entirely written in R. I used packages including <a href=""https://cran.r-project.org/web/packages/data.table/index.html"" target=""_blank"">data.table</a>, <a href=""https://cran.r-project.org/web/packages/tau/index.html"" target=""_blank"">tau</a>, <a href=""https://cran.r-project.org/web/packages/irlba/index.html"" target=""_blank"">irlba</a>, and <a href=""https://cran.r-project.org/web/packages/xgboost/index.html"" target=""_blank"">xgboost</a>.
<h3>How did you spend your time on this competition?</h3>
About ⅔ of time on feature engineering and ⅓ of time on model tuning.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
It takes about 20 hours.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
- With a good computer, R can process “big data” too
- Always write data processing code with scalability in mind
- When in doubt, use xgboost
<h3>Do you have any advice for those just getting started in data science?</h3>
- Don’t be afraid to try things and ask questions
- Get the fastest computer you can afford
- Try to understand the problem/domain, don’t build models “blindly” unless you have to
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
It would be fun to predict future performance of Kagglers.

We could make past performance available and then predict ranking for the next N competitions. The only down side of this set up is that we have to wait for several competitions to start and finish to evaluate the results. But I am sure it would be fun.

Also some recruiting related comp might be very interesting as well. For example, we can try to predict which <a href=""https://www.kaggle.com/jobs"" target=""_blank"">job posts</a> on Kaggle generate most interest.
<h2><small><i></i></small>Bio</h2>
<a href=""http://blog.kaggle.com/wp-content/uploads/2015/06/owen.jpg""><img class="" wp-image-5007 alignright"" src=""http://blog.kaggle.com/wp-content/uploads/2015/06/owen.jpg"" alt=""owen"" width=""130"" height=""130"" /></a><a href=""https://www.linkedin.com/pub/owen-zhang/51/aa0/363"" target=""_blank"">Owen Zhang</a> currently works as a data scientist at <a href=""http://www.datarobot.com/"" target=""_blank"">DataRobot</a> a Boston based startup company. His education background is in engineering, with a master’s degree from <a href=""http://www.utoronto.ca/"" target=""_blank"">U of Toronto</a>, Canada, and bachelor’s from <a href=""http://en.ustc.edu.cn/"" target=""_blank"">U of Science and Technology of China</a>. Before joining DataRobot, he spent more a decade in several U.S. based property and casualty insurance companies, last one being <a href=""http://www.aig.com/"" target=""_blank"">AIG</a> in New York.

<hr />

Read other posts on the Avito Context Ad Click Prediction competition by clicking the tag below."
"Diabetic Retinopathy Winner's Interview: 1st place, Ben Graham",http://blog.kaggle.com/2015/09/09/diabetic-retinopathy-winners-interview-1st-place-ben-graham/,2015-09-09 23:59:36,"Ben Graham finished at the top of the leaderboard in the high-profile <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection"" target=""_blank"">Diabetic Retinopathy competition</a>. In this blog, he shares his approach on a high-level with key takeaways. Ben finished 3rd in the <a href=""https://www.kaggle.com/c/datasciencebowl"" target=""_blank"">National Data Science Bowl</a>, a competition that helped develop many of the approaches used to compete in this challenge.

[caption id=""attachment_5174"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-10-at-11.33.04-AM.png""><img class=""wp-image-5174 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-10-at-11.33.04-AM-300x143.png"" alt=""Screen Shot 2015-09-10 at 11.33.04 AM"" width=""300"" height=""143"" /></a> Ben's Kaggle <a href=""https://www.kaggle.com/btgraham"" target=""_blank"">profile</a>[/caption]
<h2>The Basics</h2>
<h3>What made you decide to enter this competition?</h3>
I wanted to experiment with training CNNs with larger images to see what kind of architectures would work well. Medical images can in some ways be more challenging than classifying regular photos as the important features can be very small.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
For preprocessing, I first scaled the images to a given radius. I then subtracted local average color to reduce differences in lighting.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-10-at-11.12.53-AM.png""><img class=""aligncenter wp-image-5172"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-10-at-11.12.53-AM.png"" alt=""Screen Shot 2015-09-10 at 11.12.53 AM"" width=""612"" height=""538"" /></a>

For supervised learning, I experimented with convolutional neural network architectures. To map the network predictions to the integer labels needed for the competition, I used a random forest so that I could combine the data from the two eyes to make each prediction.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-10-at-11.14.11-AM.png""><img class=""aligncenter wp-image-5173"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-10-at-11.14.11-AM.png"" alt=""Screen Shot 2015-09-10 at 11.14.11 AM"" width=""612"" height=""487"" /></a>
<h3>Were you surprised by any of your findings?</h3>
I was surprised by a couple of things. First, that increasing the scale of the images beyond radius=270 pixels did not seem to help. I was expecting the existence of very small features, only visible at higher resolutions, to tip the balance in favor of larger images. Perhaps the increase in processing times for larger images was too great.

I was also surprised by the fact that ensembling (taking multiple views of each image, and combining the results of different networks) did very little to improve accuracy. This is rather different to the case of normal photographs, where ensembling can make a huge difference.
<h3>Which tools did you use?</h3>
Python and OpenCV for preprocessing. <a href=""https://github.com/btgraham/SparseConvNet"" target=""_blank"">SparseConvNet</a> for processing. I was curious to see if I could sparsify the images during preprocessing; however, due to time constraints I didn't get that working. SparseConvNet implements fractional max-pooling, which allowed me to experiment with different types of spatial data aggregation.
<h2>Bio</h2>
<b>Ben Graham<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/ben_graham.jpg""><img class=""wp-image-5170 alignright"" style=""margin-top: 0px; margin-left: 10px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/ben_graham.jpg"" alt="""" width=""92"" height=""120"" /></a></b> is an Assistant Professor at the University of Warwick, UK. His research interests are probabilistic spatial models such as percolation, and machine learning.

<hr />

Read other posts on the Diabetic Retinopathy competition by clicking the tag below.

&nbsp;"
"ICDM Winner's Interview: 3rd place, Roberto Diaz",http://blog.kaggle.com/2015/09/16/icdm-winners-interview-3rd-place-roberto-diaz/,2015-09-16 16:53:52,"This summer, the <a href=""http://icdm2015.stonybrook.edu/"" target=""_blank"">ICDM 2015</a> conference sponsored a <a href=""https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections"" target=""_blank"">competition</a> focused on making individual user connections across multiple digital devices. Top teams were invited to submit a paper for presentation at an ICDM workshop.

Roberto Diaz, competing as team ""CookieMonster"", <a href=""https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections/leaderboard/private"" target=""_blank"">took 3rd place</a>. In this blog, he shares how he became a Kaggle addict, what he values in a competition, and most importantly, details on his approach to this unique dataset. Congrats to Roberto for achieving his goal of becoming a top 100 Kaggle user!

[caption id=""attachment_5179"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/multichannel_banner1.png""><img class=""wp-image-5179"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/multichannel_banner1.png"" alt=""multichannel_banner"" width=""500"" height=""120"" /></a> 407 players on 340 teams competed in <a href=""https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections"" target=""_blank"">ICDM 2015: Drawbridge Cross-Device Connections</a>[/caption]
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
In addition to being a Kaggle addict, I am a researcher at <a href=""http://www.treelogic.com/"" target=""_blank"">Treelogic</a> working in the machine learning area. In parallel I work on my PhD thesis at the <a href=""http://www.uc3m.com/Home"" target=""_blank"">University Carlos III de Madrid</a> focused on the parallelization of Kernel Methods.

[caption id=""attachment_5183"" align=""aligncenter"" width=""342""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-16-at-4.43.14-PM.png""><img class=""wp-image-5183 size-full"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-16-at-4.43.14-PM.png"" alt="""" width=""342"" height=""163"" /></a> Roberto's Kaggle <a href=""https://www.kaggle.com/robertod"" target=""_blank"">profile</a>[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
I didn't have any knowledge about this domain. The topic is quite new and I couldn't find any papers related to this problem, most probably because there are not public datasets.
<h3>How did you get started competing on Kaggle?</h3>
I started on the first <a href=""https://www.kaggle.com/c/FacebookRecruiting"" target=""_blank"">Facebook competition</a> a long time ago. A friend of mine was taking part in the challenge and he encouraged me to compete. That caught my initial curiosity so I accessed the challenge's forum and I read a post with a solution that scored quite well on the leaderboard and I thought ""I think I can do better than that"". At the end I scored 9th on the leaderboard.

For my second challenge (<a href=""https://www.kaggle.com/c/emc-data-science"" target=""_blank"">EMC Israel Data science challenge</a>) I was on a team with my PhD mates. We finished 3rd receiving a prize.

After that it was too late for me, I had become an addict.
<h3>What made you decide to enter this competition?</h3>
The things I value most in a challenge are:
<ul>
	<li><strong>A conference associated to the challenge:</strong> It is a good opportunity to publish your results. For example, my solution in the <a href=""https://www.kaggle.com/c/higgs-boson"" target=""_blank"">Higgs Boson Machine Learning Challenge</a>:</li>
</ul>
<pre>DÌaz-Morales, R., &amp; Navia-V·zquez, A. (2015, September). <a href=""http://jmlr.org/proceedings/papers/v42/diaz14.pdf"" target=""_blank"">Optimization of AMS using Weighted AUC optimized models</a>. In *JMLR: Workshop and Conference Proceedings*, Vol. 42, pp. 109-127.</pre>
<ul>
	<li><strong>A domain unknown to me:</strong> It is the best way to learn about how to work with a different kind of data.</li>
	<li><strong>The need to preprocess and extract the features from raw data to build the dataset:</strong> It gives you the chance to use your intuition and imagination.</li>
</ul>
This challenge looked very interesting to me because all the conditions were met.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
In this challenge we had a list of devices and a list of cookies and we had to tell what cookies belonged to the person using the device.

The most important part was the feature extraction procedure, they had to contain information about the relation between devices and cookies (for example, the number of IP addresses visited by each one and by both of them).

Once I had the features I tried simple supervised machine learning algorithms and complex ones (my winning methodology was Semi-Supervised learning procedure using Gradient Boosting + Bagging) and the score just grew up from 0.865 to 0.88.
<h3>What was your most important insight into the data?</h3>
A key part of the solution was the initial selection of candidates and the post processing:
<ul>
	<li><strong>Initial selection:</strong> It was not possible to create a training set containing every combination of devices and cookies due to the high number of them. In order to reduce the initial complexity of the problem and to create an affordable dataset, some basic rules were created to obtain an initial reduced set of candidate cookies for every device. The rules are based on the IP addresses that both device and cookie have in common and how frequent they are in other devices and cookies.</li>
	<li><strong>Supervised Learning:</strong> Every pattern in the training and test set represents a device/candidate cookie pair obtained by the previous step and contains information about the device (Operating System (OS), Country, ...), the cookie (Cookie Browser Version, Cookie Computer OS,...) and the relation between them (number of IP addresses shared by both device and cookie, number of other cookies with the same handle than the cookie,...).</li>
	<li><strong>Post Processing:</strong> If the initial selection of candidates did not find a candidate with enough likelihood (logistic output of the classifier) we choose a new set of candidate cookies selecting every cookie that shares an IP address with the device and we score them using the classifier.</li>
</ul>
The initial selection of candidates reduces the complexity of the problem and the post processing step find out most of the device/cookie pairs lost by that initial selection strategy.
<h3>Were you surprised by any of your findings?</h3>
Yes. When I sorted the scores obtained by the classifier for every candidate I saw that if the first score is high and the second is very low, is extremely likely that the first cookie belongs to the device. I made use of this information to create semi-supervised learning procedure updating some features in the training set and retraining the algorithm again with this new information to improve the results.

This picture shows the F05 score and the percentage of devices that fulfill the condition when we match devices and the first cookies candidate when the second candidate scores less than a threshold:

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/grafica.jpg""><img class=""aligncenter size-full wp-image-5181"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/grafica.jpg"" alt=""grafica"" width=""480"" height=""360"" /></a>
<h3>Which tools did you use?</h3>
This solution has been implemented in python and uses the external software <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a>.

The libraries of python used were:
<ul>
	<li><a href=""http://www.numpy.org/"" target=""_blank"">Numpy</a></li>
	<li><a href=""http://www.scipy.org/"" target=""_blank"">Scipy</a></li>
	<li><a href=""http://scikit-learn.org/"" target=""_blank"">Sklearn</a></li>
</ul>
<h3>How did you spend your time on this competition?</h3>
I spent about 20% of the time in feature engineering, 10% in the supervised learning part and 70% eagerly awaiting for the results.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
Too much, the training procedure takes around 9 hours using 12 cores.

The prediction procedure takes around 30 minutes, it is necessary to extract some features from the relational database.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
I was trying to reach a place in top 100 of the users global ranking and I finally got it.

Regarding the challenge:
<ul>
	<li>I have learned how useful it is to save intermediate results in order to not repeat the full training procedure only to change the last steps of the algorithm.</li>
	<li>A paper with my approach to the problem in the next ICDM 2015 workshop dedicated to the challenge.</li>
</ul>
<h3>Do you have any advice for those just getting started in data science?</h3>
""All hope abandon, ye who enter here"".

No, seriously, at the beginning you may feel frustrated because it is difficult area but you are in the correct place if:
<ul>
	<li>You love statistics more than other software engineers</li>
	<li>You love software engineering more than other statisticians.</li>
</ul>
<h2>Bio</h2>
<a href=""https://www.kaggle.com/robertod"" target=""_blank"">Roberto Diaz</a><a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/roberto_profile.jpeg""><img class="" wp-image-5176 alignright"" style=""margin-left: 5px; margin-top: 10px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/roberto_profile.jpeg"" alt=""roberto_profile"" width=""155"" height=""155"" /></a> is a researcher in the R&amp;D department of <a href=""http://www.treelogic.com/"" target=""_blank"">Treelogic</a>, a SME Spanish company focused on Machine Learning, Computer Vision and Big Data that takes part in many <a href=""http://ec.europa.eu/programmes/horizon2020/"" target=""_blank"">EU Research and Innovarions programmes</a>. In parallel he works on his PhD thesis in the University Carlos III de Madrid focused on the parallelization of Kernel Methods."
"Caterpillar Winners' Interview: 1st place, Gilberto | Josef | Leustagos | Mario",http://blog.kaggle.com/2015/09/22/caterpillar-winners-interview-1st-place-gilberto-josef-leustagos-mario/,2015-09-22 17:24:04,"The <a href=""https://www.kaggle.com/c/caterpillar-tube-pricing"" target=""_blank"">Caterpillar Tube Pricing</a> competition asked teams to use detailed tube, component, and volume data to predict the price a supplier would quote for the manufacturing of different tube assemblies. Team ""Gilberto | Josef | Leustago | Mario"" finished in first place, bringing in new players (with new models) near the team merger deadline to create a strong ensemble. Feature engineering played a key role in developing their individual models, and team discussions in the last week of the competition brought them to the top of the leaderboard.

[caption id=""attachment_5185"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/dozer_banner-e1442941881405.png""><img class=""wp-image-5185 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/dozer_banner-e1442941881405.png"" alt=""dozer_banner"" width=""500"" height=""169"" /></a> 1,452 players on 1,323 teams competed from June 19 through August 21, 2015[/caption]
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
<b>Mario:</b> Since February 2014 I have been taking MOOCs, reading papers, watching lectures about data science/machine learning. Now I work as a freelance data scientist in projects from startups and consulting companies, but I am currently looking for a data scientist position (preferably remote) at a company.

[caption id=""attachment_5189"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.28.28-PM.png""><img class=""size-medium wp-image-5189"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.28.28-PM-300x141.png"" alt=""Mario's profile on Kaggle"" width=""300"" height=""141"" /></a> Mario's <a href=""https://www.kaggle.com/mariofilho"" target=""_blank"">profile</a> on Kaggle[/caption]

<b>Josef:</b> I'm working as a Data Scientist for the <a href=""https://www.otto.de/"" target=""_blank"">Otto Group</a> in Hamburg. Additionally, I'm currently working on my PhD thesis in Machine Learning at the <a href=""https://www.zv.uni-leipzig.de/en/"" target=""_blank"">University of Leipzig</a>.

[caption id=""attachment_5190"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.29.20-PM.png""><img class=""size-medium wp-image-5190"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.29.20-PM-300x142.png"" alt=""Josef's profile on Kaggle"" width=""300"" height=""142"" /></a> Josef's <a href=""https://www.kaggle.com/joseffeigl"" target=""_blank"">profile</a> on Kaggle[/caption]

<b>Lucas:</b> I work as a Senior Data Scientist at <a href=""http://www.niddel.com/"" target=""_blank"">Niddel</a>. At Niddel we're building a system to identify security threats based on machine learning. We are building something very exciting and advanced there.

[caption id=""attachment_5191"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.30.10-PM.png""><img class=""size-medium wp-image-5191"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.30.10-PM-300x142.png"" alt=""Leustagos' profile on Kaggle"" width=""300"" height=""142"" /></a> Lucas' (aka Leustagos) <a href=""https://www.kaggle.com/leustagos"" target=""_blank"">profile</a> on Kaggle[/caption]

<b>Gilberto:</b> I am an electronic engineer and have experience with software for at least 18 years. Since 2012 I've participated in Kaggle competitions.

[caption id=""attachment_5192"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.31.04-PM.png""><img class=""wp-image-5192 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.31.04-PM-300x141.png"" alt=""Gilberto's profile on Kaggle"" width=""300"" height=""141"" /></a> Gilberto's <a href=""https://www.kaggle.com/titericz"" target=""_blank"">profile</a> on Kaggle[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
<b>Mario:</b> I have worked in companies that sold items that looked like tubes, but nothing really relevant for the competition.
<b>Josef:</b> Well, I have a basic understanding of what a tube is.
<b>Lucas:</b> Not a clue.
<b>Gilberto:</b> No.
<h3>How did you get started competing on Kaggle?</h3>
<b>Mario:</b> In 2013 I entered the <a href=""https://www.kaggle.com/c/battlefin-s-big-data-combine-forecasting-challenge"" target=""_blank"">Big Data Combine</a>, but I didn’t know what I was doing. In 2014 I participated for the first time, knowing what I was doing, in the <a href=""https://www.kaggle.com/c/avito-prohibited-content"" target=""_blank"">Avito</a> competition, and got my first Top 25%.
<b>Josef:</b> I joined Kaggle about 3 years ago because I had some theoretical knowledge about machine learning and wanted to apply it to some interesting problems. I've been quite active since then and I usually learn something new in every competition, which is great fun.
<b>Lucas:</b> I joined Kaggle in the end of 2011, just after doing the wonderful Andrew Ng Machine Learning <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">course</a> available on Coursera. After completing that I have been learning by myself by reading ML forums, FAQs, previous winners' posts on Kaggle and such.
<b>Gilberto:</b> Despite being an engineer I have always been interested in machine learning algorithms. In 2012 I found Kaggle via a Google search.
<h3>What made you decide to enter this competition?</h3>
<b>Mario:</b> It looked like a competition that could benefit from feature engineering and understanding the data. I am trying to get better at this, which I consider one of the core skills of a good data scientist.
<b>Josef:</b> I like competitions were feature engineering is a major part of the problem. This competition looked exactly like that, given all the different input files.
<b>Lucas:</b> I like competitions that have a time series component and this one also looked like it was possible to build a proper validation set to try out ideas.
<b>Gilberto:</b> I always try to participate in all competitions and test the performance of my algorithms. I usually choose the competitions that I feel more comfortable programming.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
<b>Mario:</b> There were many different files about tubes, so the only preprocessing was joining relevant files together. After this step, I did a lot of feature engineering, and focused on using <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> and <a href=""http://stat.rutgers.edu/home/tzhang/software/rgf/"" target=""_blank"">Regularized Greedy Forests</a>.
<b>Josef:</b> I focused on feature engineering, which was very important in this competition. For example, aggregated statistics for the tubes, the supplier or the material proved to be useful.
It was also very important to predict different transformations of the cost, like the logarithm.
As for the supervised learning methods, I achieved the best results with gradient boosted tree models, like the awesome XGBoost. I also spend some time tinkering with deep neural nets, but couldn't get near the best XGBoost model.
<b>Lucas:</b> For the first part of the competition I focused on feature engineering. I tried to extract the proper relationship between the train and test sets to build a validation and tried to at least beat the famous '<a href=""https://www.kaggle.com/kumareshd/caterpillar-tube-pricing/xgbooost/run/26489"" target=""_blank"">beat the benchmark</a>' script using my own features and code. Finding some soft leaks also get our scores on par with the other teams on lb. For this task I used the pandas python module to check statistics and xgboost to train a simple model and try out ideas fast.
<b>Gilberto:</b> Preprocessing steps was basically setting up the dataset in a proper way like one-hot encoding categorical variables and calculating some physical characteristics of the tubes based in quantity. The preferred supervised methods were <a href=""http://www.libfm.org/"" target=""_blank"">libFM</a> and XGboost.
<h3>What was your most important insight into the data?</h3>
<b>Mario:</b> Calculating the total component weight for a tube, finding that expensive tubes had a smaller “max quantity” when their prices varied with quantity, and the fact that the tube ids were not random, and had predictive power.
<b>Josef:</b> Leakage played an important part again. Using the tube IDs as a feature turned out to be crucial.
<b>Lucas:</b> Finding some soft leaks related to tubes in the same action pool being both on train and test sets. Understanding a bit of the highly non linear relationship between quantity and cost helped too.
<b>Gilberto:</b> Training over transformed cost function improved a lot the final model performance.

[caption id=""attachment_5186"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.17.38-PM.png""><img class=""wp-image-5186"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.17.38-PM.png"" alt=""Visualizing Important Variables script by another competition participant, saihttam"" width=""614"" height=""534"" /></a> <a href=""https://www.kaggle.com/c/caterpillar-tube-pricing"" target=""_blank"">Visualizing Important Variables script</a> by another competition participant, <a href=""https://www.kaggle.com/saihttam"" target=""_blank"">saihttam</a>[/caption]
<h3>Were you surprised by any of your findings?</h3>
<b>Mario:</b> I have been reading about data leaks for some time, so I was happy about finding the tube id pattern. Besides that, I love single models that do well, and my best single model was an XGBoost that could get the 10th place by itself.
<b>Gilberto:</b> Yes, the assembly_id feature improved a lot the performance. Also some physical features like volume, area and weight made an important role.
<h3>Which tools did you use?</h3>
<b>Mario:</b> XGBoost, Regularized Greedy Forests, <a href=""http://scikit-learn.org/stable/"" target=""_blank"">scikit-learn</a> and <a href=""https://github.com/fchollet/keras"" target=""_blank"">Keras</a>.
<b>Josef:</b> That was the first competition for me were I used <a href=""https://www.python.org/download/releases/3.4.0/"" target=""_blank"">Python 3.4</a> for everything and it worked out very well. I especially liked the pipelines and feature unions of sklearn.
<b>Lucas:</b> Used python3 all the way, and modules like <a href=""http://pandas.pydata.org/"" target=""_blank"">pandas</a>, sklearn, Keras and XGboost.
<b>Gilberto:</b> Basically R.
<h3>How did you spend your time on this competition?</h3>
<b>Mario:</b> When I was on my own I did a lot of feature engineering, when I teamed up, it was all about creating new models and ensembling.
<b>Josef:</b> I spend about 1/2 of my time on feature engineering and 1/2 on model selection, fine-tuning and blending.
<b>Lucas:</b> Half of my time on building diverse models by using different outputs transformations, training parameters, and modeling algorithms. The other half I spent building a proper ensemble stack that blended all models from our team.
<b>Gilberto:</b> Most of the time I spent testing different algorithms over some prebuilt datasets.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
<b>Mario:</b> Each of my models took, on average, 2h 30m to run. To generate the predictions for stacking and the test set it should take around 20h.
<b>Josef:</b> The training of my models took quite some time. Creating all the out-of-fold predictions for the final blending and creating the predictions for the test set took up to 15h.
<b>Lucas:</b> Depends on the model. We have many. Some take just a few minutes, others can take up to 10 hours. We used many combinations of output transformations and parameters jammed together.
<b>Gilberto:</b> It depends of the model. LibFM model runs fast even bagging it many times. Bagged XGboost takes a little more time, it's about 3~4 hours per model on a 8 core cpu.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
<b>Mario:</b> Teaming up teaches you a lot. I always wanted to team up with more experienced data scientists, and this was a great opportunity. And a good model can take you far in Kaggle, but ensembling makes you win.
<b>Josef:</b> Team work and blending seem to get more and more important for winning Kaggle competitions.
All of the Top 7 teams consisted of at least three participants. In some of my previous competitions, you could win by finding all the important features and training some solid models. That was simply not enough for this competition. You also had to build a very strong ensemble of different models.
<b>Lucas:</b> Team work is playing a very important role in winning Kaggle competitions. The diversity of approaches and ideas usually leads to better models and data processing.
<b>Gilberto:</b> Not much.

[caption id=""attachment_5187"" align=""aligncenter"" width=""562""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.21.54-PM.png""><img class=""size-full wp-image-5187"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-22-at-12.21.54-PM.png"" alt=""The Top 100 Users With Most Team Memberships shows high ranked Kagglers perform frequently on teams"" width=""562"" height=""324"" /></a> The <a href=""https://www.kaggle.com/mlandry/meta-kaggle/top-100-users-with-most-team-memberships"" target=""_blank"">Top 100 Users With Most Team Memberships script</a> by <a href=""https://www.kaggle.com/mlandry"" target=""_blank"">mlandry</a> shows high ranked Kagglers perform frequently on teams[/caption]
<h3>Do you have any advice for those just getting started in data science?</h3>
<b>Mario:</b> Take MOOCs, read papers, and watch lectures to understand the theory. But consider Kaggle as a “Projects course”, and learn from experience too.
<b>Josef:</b> When I started with Data Science, I simply choose a competition and spent all the time I had on it.
I think, there is a strong positive correlation between the time spent for a competition and the final rank: The more time you invest, the better your final rank will be.
<b>Lucas:</b> Do online courses, read the forums and the previous winners' posts. If that isn’t enough, ask on the forums. Some people aren't cheap on advice. Just make sure you are asking a question that doesn't have an easy to find answer, because that would be just laziness and not curiosity.
<b>Gilberto:</b> Take some online training, read the specific forums and start coding. Learning by hits and mistakes is the best way to improve knowledge.
<h3>How did competing on a team help you succeed?</h3>
<b>Josef:</b> I joined our team very late, about 10 days before the end of the competition. Simply adding my models to our ensemble helped us to achieve our final score.
We also merged some of our data sets and trained some new models on our combined features, which helped too.
I guess, we wouldn't have been able to win without our collaborative effort.
<b>Lucas:</b> Competing on a team helped to build distinct approaches for the same problem and thats is very useful when ensembling. We also had an online discussion chat that surely guided each of us to improve even more on our solutions.
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
<b>Josef:</b> I already did that! You can find the result at the “<a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge"" target=""_blank"">Otto Group Product Classification Challenge</a>”.
<b>Lucas:</b> If I had the proper dataset and patient history, I would try to build a model to match cancer patients with the proper medicine to improve their chances.
<b>Gilberto:</b> I would start a competition to find out who is going to win a new competition based just on users' competitions historical data ;-)
<h3>What is your dream job?</h3>
<b>Gilberto:</b> My dream job has a dash of machine learning, a handful of money, a lot of free time to enjoy my family and the opportunity to help save the world.
<h2>Bio</h2>
<b>Mario Filho</b> is a self-taught data scientist. He currently works as a freelance data scientist, doing projects for startups and consulting companies. He is interested in validation techniques, feature engineering and tree ensembles.
<b>Josef Feigl</b> is currently working as a Data Scientist for the Otto Group in Hamburg and writing his PhD thesis in Machine Learning at the University of Leipzig. He holds a diploma in Business Mathematics and is interested in recommender systems and neural networks.
<b>Lucas</b> is a Senior Data Scientist currently working to improve network security. He is also an enthusiastic Kaggler that loves to learn something new and tackle new challenges.
<b>Gilberto</b> is an electronics engineer with a M.S. in telecommunications. For the past 16 years he's been working as an engineer for big multinationals like Siemens and Nokia and later as an automation engineer for Petrobras Brazil. His main interests are in machine learning and electronics areas.

<hr />

Read an interview with the 3rd place team in the Caterpillar Tube Pricing competition by clicking the tag below."
"Liberty Mutual Property Inspection, Winner's Interview: 1st place, Qingchen Wang",http://blog.kaggle.com/2015/09/28/liberty-mutual-property-inspection-winners-interview-qingchen-wang/,2015-09-28 19:16:57,"The hugely popular Liberty Mutual Group: Property Inspection Prediction <a href=""https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction"" target=""_blank"">competition</a> wrapped up on August 28, 2015 with Qingchen Wang at the top of a crowded leaderboard. A total of 2,362 players on 2,236 teams competed to predict how many hazards a property inspector would count during a home inspection.
<h2><a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/libertymutual_homeinspection_banner4.png""><img class=""aligncenter wp-image-5203"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/libertymutual_homeinspection_banner4.png"" alt="""" width=""400"" height=""101"" /></a></h2>
This blog outlines Qingchen's approach, and how a relative newbie to Kaggle competitions learned from the community and ultimately took first place.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I did my bachelor’s in computer science. After working for a few months at EA Sports as a software engineer I felt the strong need to learn statistics and machine learning as the problems that interested me the most were about predicting things algorithmically. Since then I’ve earned master’s degrees in machine learning and business and I’ve just started a PhD in marketing analytics.

[caption id=""attachment_5196"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-28-at-11.57.43-AM.png""><img class=""wp-image-5196 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-28-at-11.57.43-AM-300x142.png"" alt=""Qingchen's profile badge"" width=""300"" height=""142"" /></a> Qingchen's <a href=""https://www.kaggle.com/qwang88"" target=""_blank"">profile</a> on Kaggle[/caption]
<h3>How did you get started competing on Kaggle?</h3>
I had an applied machine learning course during my master’s at UCL and the course project was to compete on the <a href=""http://www.heritagehealthprize.com/c/hhp"" target=""_blank"">Heritage Health Prize</a>. Although at the time I didn’t really know what I was doing it was still a very enjoyable experience. I’ve competed briefly in other competitions since, but this was the first time I’ve been able to take part in a competition from start to finish and it turned out to have been quite a rewarding experience.
<h3>What made you decide to enter this competition?</h3>
I was in a period of unemployment so I decided to work on data science competitions full-time until I found something else to do. I actually wanted to do the <a href=""https://www.kaggle.com/c/caterpillar-tube-pricing"" target=""_blank"">Caterpillar competition</a> at first but decided to give this one a quick go since the data didn’t require any preprocessing to start. My early submissions were not very good so I became determined to improve and ended up spending the whole time doing this.

<strong>What made this competition so rewarding was how much I learned.</strong> As more or less a Kaggle newbie, I spent the whole two months trying and learning new things. I hadn’t known about methods like gradient boosting trees or tricks like stacking/ blending and the variety of ways to handle categorical variables. At the same time, it was probably the intuition that I developed through previous education that set my model apart from some of the other competitors so I was able to validate my existing knowledge as well.
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
I have zero prior experience or domain knowledge for this competition. It’s interesting because during the middle of the competition I hit a wall and a number of the top-10 ranked competitors have worked in the insurance industry so I thought maybe they had some domain knowledge which gave them an advantage. It turned out to not be the case. As far as data science competitions go, I think this one was rather straightforward.

[caption id=""attachment_5198"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-28-at-12.06.53-PM-e1443467333270.png""><img class=""size-full wp-image-5198"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-28-at-12.06.53-PM-e1443467333270.png"" alt=""Histogram of all fields in the dataset with labels. Script by competition participant, Rajiv Shah"" width=""614"" height=""489"" /></a> Histogram of all fields in the dataset with labels. <a href=""https://www.kaggle.com/rshah4/liberty-mutual-group-property-inspection-prediction/histogram-of-all-fields-with-labels"" target=""_blank"">Script</a> by competition participant, <a href=""https://www.kaggle.com/rshah4"" target=""_blank"">Rajiv Shah</a>[/caption]
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
I used only <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> (tried others but none of them performed well enough to end up in my ensemble). <strong>The key to my result was that I also did binary transformation of hazards which turned the regression problem into a set of classification problems.</strong> I noticed that some other people also tried this method through the forum thread but it seems that they didn’t go far enough with the binary transformation as that was the best performing part of my ensemble.

I also played with different encodings of categorical variables and interactions, nothing sophisticated, just the standard tricks that many others have used.
<h3>Were you surprised by any of your findings?</h3>
I’m surprised by how poor our prediction accuracies were. This seemed like a problem that was well suited for data science algorithms and it was both disappointing and exciting to see such high prediction errors. I guess that’s the difference between real life and the toy examples in courses.
<h3>Which tools did you use?</h3>
I only used XGBoost. It’s really been a learning experience for me as I entered this competition having no idea what gradient boosted trees was. After throwing random forests at the problem and getting nowhere near the top of the leaderboard, I installed XGBoost and worked really hard on tuning its parameters.

[caption id=""attachment_5197"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-28-at-12.00.20-PM.png""><img class=""size-medium wp-image-5197"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-28-at-12.00.20-PM-300x223.png"" alt=""Check out this great blog on the math behind boosting and why it doesn't overfit"" width=""300"" height=""223"" /></a> XGBoost fans or those new to boosting, check out this <a href=""http://jeremykun.com/2015/09/21/the-boosting-margin-or-why-boosting-doesnt-overfit/"" target=""_blank"">great blog</a> by Jeremy Kun on the math behind boosting and why it doesn't overfit[/caption]
<h3>How did you spend your time on this competition?</h3>
Since the variables were anonymous there wasn’t much feature engineering to be done. Instead I treated feature engineering as just another parameter to tune and spent all of my time tuning parameters. My final solution was an ensemble of different specifications so there were a lot of parameters to tune.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
The combination of training and prediction of my winning solution takes about 2 hours on my personal laptop (2.2ghz Intel i7 processor).
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
One thing that I learned which I’ve always overlooked before is that <strong>parameter tuning really goes a long way in performance improvements</strong>. While in absolute terms it may not be much, in terms of leaderboard improvement it can be of great value. Of course, without the community and the public scripts I wouldn’t have won and may still not know about gradient boosted trees, so a big thanks to all of the people who shared their ideas and code. I learned so much from both sources so it’s been a worthwhile experience.

[caption id=""attachment_5199"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-28-at-12.14.04-PM.png""><img class=""wp-image-5199"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-28-at-12.14.04-PM.png"" alt=""For an animated view of the community's leaderboard progression over time, click through to Walter Reade's script"" width=""500"" height=""361"" /></a> Click through to an animated view of the community's leaderboard progression over time, and the influence of benchmark code sharing. <a href=""https://www.kaggle.com/inversion/liberty-mutual-group-property-inspection-prediction/leaderboard-progression"" target=""_blank"">Script</a> by competition participant, <a href=""https://www.kaggle.com/inversion"" target=""_blank"">inversion</a>[/caption]
<h3>Do you have any advice for those just getting started in data science?</h3>
For those who don’t already have an established field, I strongly endorse education. All of my data science experience and expertise came from courses taken during my bachelor’s and master’s degrees. I believe that without already having been so well educated in machine learning I wouldn’t have been able to adapt so quickly to the new methods used in practice and the tricks that people have talked about.

There are now a number of very good education programs in data science which I suggest that everyone who wants to start in data science to look into. For those who already have their own established fields and are doing data science on the side, I think their own approaches could be very useful when combined with the standard machine learning methods. It’s always important to think outside the box and it’s all the more rewarding when you bring in your own ideas and get them to work.

Finally, don’t be afraid to hit walls and grind through long periods of trying out ideas that don’t work. A failed idea gets you one closer to a successful idea, and having many failed ideas often can result in a string of ideas that work down the road. <strong>Throughout this competition I tried every idea I thought of and only a few worked. It was a combination of patience, curiosity, and optimism that got me through these two months.</strong> The same applies to learning the technical aspects of machine learning and data science. I still remember the pain that my classmates and I endured in the machine learning courses.
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
I’m a sports junkie so I’d love to see some competitions on sports analytics. It’s a shame that I missed the one on <a href=""https://www.kaggle.com/c/march-machine-learning-mania-2015"" target=""_blank"">March Madness</a> predictions earlier this year. Maybe one day I’ll really run a competition on this stuff.

<em>Editor's note: March Machine Learning Mania is an annual competition so you can catch it again in 2016!</em>
<h3>What is your dream job?</h3>
My dream job is to lead a data science team, preferably in an industry that’s full of new and interesting prediction problems. I’d be just as happy as a data scientist though, but it’s always nice to have greater responsibilities.
<h2>Bio</h2>
<strong>Qingchen Wang</strong> is a PhD student in marketing analytics at the <a href=""http://abs.uva.nl/"" target=""_blank"">Amsterdam Business School</a>, <a href=""http://www.vu.nl/en/"" target=""_blank"">VU Amsterdam</a>, and ORTEC. His interests are in applications of machine learning methods to complex real world problems in all domains. He has a bachelor’s degree in computer science and biology from the <a href=""https://www.ubc.ca/"" target=""_blank"">University of British Columbia</a>, a master’s degree in machine learning from <a href=""http://www.ucl.ac.uk/"" target=""_blank"">University College London</a>, and a master’s degree in business administration from <a href=""http://www.insead.edu/home/"" target=""_blank"">INSEAD</a>. In his free time Qingchen competes in data science competitions and reads about sports."
"Grasp-and-Lift EEG Detection Winner's Interview: 2nd place, daheimao",http://blog.kaggle.com/2015/09/29/grasp-and-lift-eeg-detection-winners-interview-2nd-place-daheimao/,2015-09-29 17:17:09,"The <a href=""https://www.kaggle.com/c/grasp-and-lift-eeg-detection"" target=""_blank"">Grasp-and-Lift EEG Detection</a> competition asked participants to identify when a hand was grasping, lifting, and replacing an object using EEG data that was taken from healthy subjects as they performed these activities. The competition was sponsored by the <a href=""http://www.wayproject.eu/"" target=""_blank"">WAY Consortium</a> (Wearable interfaces for hAnd function recoverY) as part of their work towards developing better prosthetic devices for patients with amputation or neurological disabilities that have lost hand function.

Team daheimao finished in <a href=""https://www.kaggle.com/c/grasp-and-lift-eeg-detection/leaderboard/private"" target=""_blank"">second place</a> using <a href=""http://www.xlhu.cn/papers/Liang15-cvpr.pdf"" target=""_blank"">recurrent convolutional neural networks</a> (RCNN). In this blog Ming Liang outlines his competition approach, including RCNNs architecture and inspiration.

[caption id=""attachment_5208"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/EEGgrasp.png""><img class=""wp-image-5208 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/EEGgrasp-300x141.png"" alt=""EEGgrasp"" width=""300"" height=""141"" /></a> 452 players &amp; 379 teams competed[/caption]
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I am a Ph.D student in the department of computer science at <a href=""http://www.tsinghua.edu.cn/publish/newthu/index.html"" target=""_blank"">Tsinghua University</a>. My research interests include neural networks and computer vision.
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
No. My past research projects were all related to vision. Basically I know little about EEG, although I spent several years in the School of Medicine.

[caption id=""attachment_5207"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-10-01-at-10.37.12-AM.png""><img class=""size-medium wp-image-5207"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-10-01-at-10.37.12-AM-300x141.png"" alt=""Kaggle profile for Ming Liang (aka daheimao) "" width=""300"" height=""141"" /></a> Kaggle <a href=""https://www.kaggle.com/daheimao"" target=""_blank"">profile</a> for Ming Liang (aka daheimao)[/caption]
<h3>How did you get started competing on Kaggle?</h3>
This is my first Kaggle competition. I knew Kaggle from the winners’ blogs. In my view, the winners were cool data magicians. I wanted to be a member of that so I registered for my Kaggle account about 9 months ago. But I did not have enough time for a competition until recently.
<h3>What made you decided to enter this competition?</h3>
I recently proposed a neural network model for computer vision tasks. The model is called <a href=""http://www.xlhu.cn/papers/Liang15-cvpr.pdf"" target=""_blank"">recurrent convolutional neural network</a> (RCNN). I entered this challenge to evaluate the performance of this model in processing time series data, which has very different statistics from image data. If a good score was achieved, it would be a good advertisement for my work. :)
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
As I said, I joined this competition to evaluate the performance of RCNN. So most of the models are RCNN, except for some convolutional neural network (CNN) models used as baseline. It seems that RCNN performs well on this dataset. The best single model achieves a private LB score of 0.97661. I guess this may be the best single model of this competition?

RCNN is a natural integration of CNN and recurrent neural network (RNN). It is composed of a stack of recurrent convolutional layers (RCL), which is obtained by incorporating recurrent connections into a convolution layer. This idea is inspired by the anatomical findings on the visual system, where intra-layer recurrent connections are abundant. In the following figure, the red arrows denote feed-forward connections and the blue the red arrows denote recurrent connections. As a consequence, a feed-forward convolutional layer which is typically feed-forward can be unfolded for several iterations just like a RNN.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-29-at-3.42.01-PM-e1443566639803.png""><img class=""aligncenter size-full wp-image-5205"" src=""http://blog.kaggle.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-29-at-3.42.01-PM-e1443566639803.png"" alt=""RCNN visualization"" width=""612"" height=""284"" /></a>

I used very simple preprocessing steps, just removing the per-channel average for each sample. Low-pass and high-pass filters were not used. This is because I am not familiar with the general processing procedure of EEG signals. So I decided to keep as much information as possible and let the network learn it.
<h3>What was your most important insight into the data?</h3>
I assume the data is locally correlated along the time axis and this correlation is stationary, so that 1D convolution can be used. But this is a simple assumption and adopted by many other teams.
<h3>Which tools did you use?</h3>
Lasagne. It is a wonderful toolkit and fits neatly to Kaggle competitions.
<h3>How did you spend your time on this competition?</h3>
I spent about 2/3 time on training single models and the other time on trying model ensembles. The single models performed well, but some mistakes happened with model ensembles. As a result, the performance of my ensemble stopped improving several days before the deadline.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
Many hours are needed to train a model, depending on the model size. About an hour is needed for a model to make the predictions for all the test data.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
Although I made some mistakes in the stage of ensemble, I earned precious experience in how to combine neural network models. Moreover, I got an exciting time in preparing the competition."
"Grasp-and-Lift EEG Detection Winners' Interview: 3rd place, Team HEDJ",http://blog.kaggle.com/2015/10/05/grasp-and-lift-eeg-detection-winners-interview-3rd-place-team-hedj/,2015-10-05 22:39:34,"This is our 3rd place solution to the <a href=""http://www.kaggle.com/c/grasp-and-lift-eeg-detection"" target=""_blank"">Grasp-and-Lift EEG Detection Competition</a> on <a href=""https://www.kaggle.com/"" target=""_blank"">Kaggle</a>. The main aim of the competition was to identify when a hand is grasping, lifting, and replacing an object using EEG data that was taken from healthy subjects as they performed these activities. Better understanding the relationship between EEG signals and hand movements is critical to developing a BCI device that would give patients with neurological disabilities the ability to move through the world with greater autonomy.

<em><b>""Our final solution was an ensemble of 34 nets.""</b></em>

We would like thank the competition sponsor: <a href=""http://www.wayproject.eu/"" target=""_blank"">The WAY Consortium (Wearable interfaces for hAnd function recoverY; FP7-ICT-288551)</a>.

[caption id=""attachment_5211"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/grasp_eeg.png""><img class=""size-medium wp-image-5211"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/grasp_eeg-300x141.png"" alt=""Fig. 1 Grasp-and-Lift EEG Detection, 379 teams and 452 players competed."" width=""300"" height=""141"" /></a> <strong>Fig. 1</strong> Grasp-and-Lift EEG Detection, 379 teams and 452 players competed.[/caption]
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
<b>Tim</b> : I had a rather meandering academic career-I started as an applied physics major at Caltech, wandered off and got a masters in nuclear engineering doing <a href=""https://en.wikipedia.org/wiki/Fusor"" target=""_blank"">fusor</a> work at the University of Illinois and finally got Ph.D. in electrical engineering doing computational electromagnetics, also at Illinois. These days I create tools for linear electronic component characterization (mostly cables), focusing on making inexpensive test equipment do things it wasn't designed for.

[caption id=""attachment_5218"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/tim.png""><img class=""wp-image-5218 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/tim-300x142.png"" alt=""tim"" width=""300"" height=""142"" /></a> Kaggle <a href=""https://www.kaggle.com/bitsofbits"" target=""_blank"">profile</a>, Tim Hochberg[/caption]

<b>Esube</b>: I am a Ph.D. student with research areas of assistive (Robot-mediated and VR-based) technology for children with autism and adults with schizophrenia.

[caption id=""attachment_5219"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/esube.png""><img class=""wp-image-5219 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/esube-300x142.png"" alt=""esube"" width=""300"" height=""142"" /></a> Kaggle <a href=""https://www.kaggle.com/deepcnn"" target=""_blank"">profile</a>, Esube Bekele aka ""Deep""[/caption]

<b>Elena</b>: I am a physicist working on data analysis for many years. I have a Ph.D. in physics and I work as data analyst for the <a href=""http://public.virgo-gw.eu/language/en/"" target=""_blank"">Virgo experiment</a> in Italy for the detection of Gravitational Waves. My fields of interest are noise analysis, transient detection and data cleaning. I'm rather new to Machine Learning techniques, but have some experience with time series analysis.

[caption id=""attachment_5220"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/elena.png""><img class=""wp-image-5220 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/elena-300x142.png"" alt=""elena"" width=""300"" height=""142"" /></a> Kaggle <a href=""https://www.kaggle.com/elenacuoco"" target=""_blank"">profile</a>, Elena Cuoco[/caption]

<b>Jing</b>: I am a Ph.D. student working on socially assistive robotic technology for elderly care.

[caption id=""attachment_5221"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/jing.png""><img class=""wp-image-5221 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/jing-300x142.png"" alt=""jing"" width=""300"" height=""142"" /></a> Kaggle <a href=""https://www.kaggle.com/jingfan2015"" target=""_blank"">profile</a>, Jing Fan[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
<b>Tim</b>: I have zero domain knowledge, but I have some experience with neural networks gained while participating in Kaggle's <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection"" target=""_blank"">Diabetic Retinopathy</a> and <a href=""https://www.kaggle.com/c/datasciencebowl"" target=""_blank"">Plankton Identification</a> contests. I also have quite a bit of Python experience, which helped working with <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a>.

<b>Esube</b>: My research involves electrophysiological signal processing for affective state prediction. This entails various signal pre-processing, feature extraction, feature level analysis and modeling. I already had good experience in peripheral physiological signals such as EKG, EMG, etc..., which are very similar to EEG. Recently, I also incorporated EEG among the modalities for my virtual reality (VR) based assistive system. So, this experience helped me in exploring the data more and coming up ways to diversify our nets to eventually arrive at stronger ensemble using only simple weighted averaging. Moreover, I also have some experience with convolutional nets gained by participating in the <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection"" target=""_blank"">Diabetic Retinopathy</a> and <a href=""https://www.kaggle.com/c/datasciencebowl"" target=""_blank"">National Data Science Bowl</a> (plankton) competitions.

<b>Elena</b>: I have no knowledge about this specific domain. No idea of EKG, EGG etc., but I have knowledge of time series analysis and transient signals detection. I've a bit of experience with python and related library for ML, in particular scikit-learn, but very little experience with neural networks.

<b>Jing</b>: I have some experience analyzing EEG signals recorded from Emotiv EEG headset. The EEG signals were used to build classification models for affective computing.
<h3>How did you get started competing on Kaggle?</h3>
<b>Tim</b>: Someone I know through our local Python users group suggested I check out Kaggle if I was interested in data science. So I did.

<b>Esube</b>: When I was taking machine learning course, our professor introduced us to Kaggle. However, it took me a couple of years till I decided to join.

<b>Elena</b>: 2 years ago I decided to follow an online course on Machine Learning at Caltech by Professor Yaser Abu-Mostafa. I was very enthusiastic about the course. In the forum discussion I read about Kaggle site and its competitions, so I decided to join. I started with 2 basic competition: the first on <a href=""https://www.kaggle.com/c/data-science-london-scikit-learn"" target=""_blank"">scikit-learn</a> and the second on <a href=""https://www.kaggle.com/c/titanic"" target=""_blank"">Titanic</a>. Only after more than one year I decided to participate to featured competitions.

<b>Jing</b>: The <a href=""http://www.kaggle.com/c/grasp-and-lift-eeg-detection"" target=""_blank"">Grasp-and-Lift EEG Detection</a> problem is my first Kaggle competition. Esube told me about this competition. I thought it was a good chance to improve my understanding of EEG signals as well as machine learning strategies.
<h3>What made you decide to enter this competition?</h3>
<b>Tim</b>: It looked like something it might be fun trying out neural nets out on. I hadn't attempted anything with 1D CNN before and it seemed like an interesting challenge.

<b>Esube</b>: As I described above in my prior experience, introducing EEG into my research pipeline was a major motivating factor to join this competition. Although I had good experience with peripheral physiological signals, this is my first time predicting from EEG. Also, this is my first time of using conv nets in 1D. Yoshua Bengio published a paper on using deep conv nets for peripheral physiological signals, although the results were not very good. After reading that paper, I was quite interested to apply CNNs to my physiological dataset for quite some time. After realizing there is a shortage of literature of applying CNNs to electrophysiology, I thought this competition would be the best opportunity for me to test if indeed CNNs can perform as well as traditional paradigm.

<b>Elena</b>: I love time series analysis and I was pretty confident I could have done a good job with these data.

<b>Jing</b>: Detecting hand and finger movements from EEG signals seems very interesting.
<h2>Let's Get Technical</h2>
<h3>What was your overall approach to the challenge?</h3>
Our primary approach to the <a href=""http://www.kaggle.com/c/grasp-and-lift-eeg-detection"" target=""_blank"">Grasp-and-Lift EEG Detection</a> problem was convolutional neural networks (convnets). We used very little pre-processing of the data, primarily filtering out the very low and high frequencies to reduce noise and wander, and relied on the convnet itself for feature generation and selection. The predictive value of the individual convnets was quite strong and in the end we simply averaged together the results of our better performing nets to achieve our final submission.
<h3>What pre-processing methods did you use?</h3>
We used very limited pre-processing. The major pre-processing that we applied were only filtering (several band pass and sometimes low pass filtering to exploit the seemingly important low frequency component in this dataset) and dropping the first two channels after we discovered that these two channels are corrupted by ocular artifacts the most and they have very little to do with predicting motor actions (See the data insights section for more details on this).
<h3>What supervised learning methods did you use?</h3>
We used convolutional neural networks (CNNs). The CNNs took sliding, 8 second (4096 point) long slices of the 32 input channels and produced as output the probabilities of each of the 6 possible events. The CNNs were built using a somewhat customized version of <a href=""https://github.com/dnouri/nolearn"" target=""_blank"">nolearn</a>, which is a thin layer on top of the neural networking toolkit <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a>, which is in turn built on top of <a href=""https://github.com/Theano/Theano"" target=""_blank"">Theano</a>.

The specifics of the successful nets varied but they were all similar to net <strong>stf7</strong>, shown in Fig. 2. Other than using 1D convolutional layers, much of the net is similar to a typical image recognition net, with a section of convolutional / maxpooling layers followed by a section of dense / dropout layers. However there are two regions that are atypical:
<ol>
	<li>The initial, linear convolutional layers.
<ul>
	<li>The first convolutional layers reduces the number of channels from 32 to 6. This gives the net a chance to learn a spatio-temporal filter to reduce the noisiness of the data being fed into the net.</li>
	<li>The second convolutional layer with a stride of 16 allowed the net to learn a strategy for down sampling by a factor of 16.</li>
</ul>
</li>
	<li>The stride: 8 maxpooling layer and accompanying bypass.
<ul>
	<li>The stride: 8 maxpooling helped reduce overfitting dramatically. However it had the side effect of making the location of the start of events fuzzy to the dense portion of the net.</li>
	<li>By duplicating the most recent 8 time points and bypassing the maxpooling, the fuzziness can be greatly reduced while still reducing overfitting.</li>
</ul>
</li>
</ol>
[caption id=""attachment_5212"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/stf7.jpeg""><img class=""size-full wp-image-5212"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/stf7.jpeg"" alt=""Fig. 2 Structure of the net stf7"" width=""600"" height=""1600"" /></a> <strong>Fig. 2</strong> Structure of the net stf7[/caption]

During training, we used two types of validation, which we will refer to as `[3&amp;6]` and `3i`. It is clear from Table I, which shows the validation scores for nets trained using both validation scheme for various structures, that `3i` definitively outperforms `[3&amp;6]`. Unfortunately, this didn't become clear till the end of the competition and the vast majority of the nets in our submitted ensemble used `[3&amp;6]` validation.

<strong>Table I</strong>: accuracy comparisons of various net architectures

[table]
Net,Validation,Public LB,Private LB
net34,[3&amp;6],0.96555,0.96291
nds4,[3&amp;6],0.96534,0.96438
stf2,[3&amp;6],0.96821,0.96643
stf3,[3&amp;6],0.96558,0.96786
stf3,3i,0.97204,0.97109
stf7,[3&amp;6],0.97117,0.97088
stf7b,[3&amp;6],0.97138,0.96917
stf7b,3i,0.97216,0.97147
stf7m,[3&amp;6],0.97138,0.96917
stf7m,3i,0.97216,0.97158
[/table]

<em>Note: `[3&amp;6]` means series `3` and `6` were used for validation while the rest of the training series were used for training and `3i` means that only `3%` of the training data were used for validation while the remaining `97%` were used for training.</em>
<h3>The Winning Ensemble</h3>
Our winning ensemble was very simple weighted averaging. Since the individual nets were strong in performance, a simple averaging of diversified nets was sufficient to come up with a strong ensemble. We started on stacking, but, didn't pursue it much due to lack of time during the competition. Fig. 3 shows the pipeline of our solution. We tried to diversify the individual nets by training with different known EEG frequency bands such as delta, theta, alpha, beta, and gamma which were implemented as filter banks and trained separately as shown in Fig. 3. We also dropped the first two channels in some of the nets, i.e. Fp1 and Fp2 to reduce the effect of ocular artifacts. The other method we used to diversify our nets was varying the validation methods.

[caption id=""attachment_5213"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/block_diagram-e1444082440229.png""><img class=""size-full wp-image-5213"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/block_diagram-e1444082440229.png"" alt=""g. 3 The overall solution diagram"" width=""614"" height=""511"" /></a> <strong>Fig. 3</strong> The overall solution diagram[/caption]

&nbsp;
<h3>What was your most important insight into the data?</h3>
As described in the ensemble section above, we tried to explore the dataset in an effort to come up with diversified models by varying filter frequencies and changing the validation strategies. However, in the process of that exploration we stumbled up on the idea that the first two channels were corrupted by very large artifacts and baseline wander (Fig. 4), which we associated with ocular artifacts due to the position of the channels. Based on literature, these two channels have little to do with motor imagery, visual evoked and motor related potentials. Therefore, we decided to drop the two channels in training some of the nets. This is in essence sort of channels selection. Although we could not explore the effect of dropping these two channels, proper treatment of these artifacts could have helped.

[caption id=""attachment_5214"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/raw_eeg-e1444082536361.png""><img class=""size-full wp-image-5214"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/raw_eeg-e1444082536361.png"" alt=""Fig. 4 Raw EEG from 1st series of 1st subject with large artifacts and baseline wander."" width=""614"" height=""321"" /></a> <strong>Fig. 4</strong> Raw EEG from 1st series of 1st subject with large artifacts and baseline wander[/caption]

&nbsp;

Another data exploration we attempted was to convert the raw EEG channels into common spatial pattern (CSP) space to maximize discriminability. However, the events given in this dataset were overlapping (Fig. 5) and that made the conversion difficult. Therefore, we abandoned this idea due to shortage of time.

[caption id=""attachment_5215"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/eeg_label-e1444082629540.png""><img class=""size-full wp-image-5215"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/eeg_label-e1444082629540.png"" alt=""Fig. 5 Raw EEG (top) and overlapping events (bottom)."" width=""614"" height=""350"" /></a> <strong>Fig. 5</strong> Raw EEG (top) and overlapping events (bottom)[/caption]

We also tried at the very end of the competition to do more relevant channels selection and whitening the signal in an effort to reduce the effect of the artifacts and large baseline wanders (Fig. 6). The whitening had undesirable effect of minimizing the evoked potentials (red circles in Fig. 6). However, due to shortage of time, these ideas were not used in the training of the nets that made it to the final ensemble.

[caption id=""attachment_5216"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/white-e1444082690900.png""><img class=""wp-image-5216 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/white-e1444082690900.png"" alt=""white"" width=""614"" height=""461"" /></a> <strong>Fig. 6a</strong> Effect of whitening on baseline wander[/caption]

[caption id=""attachment_5217"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/artifact_white-e1444082719355.png""><img class=""size-full wp-image-5217"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/artifact_white-e1444082719355.png"" alt=""Fig. 6 Effect of whitening on baseline wander (top) and ocular artifacts (bottom)."" width=""614"" height=""461"" /></a> <strong>Fig. 6b</strong> Effect of whitening on ocular artifacts[/caption]
<h3>Were you surprised by any of your findings?</h3>
We were indeed very surprised by how well CNNs were able to perform on this dataset. There is shortage of literature that applied CNNs on electrophysiology in general and EEG in particular. The most prominent paper on electrophysiology is <a href=""http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6496209"" target=""_blank"">Learning deep physiological models of affect</a> by Yoshua Bengio in which he applied CNNs to 2 channels of peripheral physiological signals. The results in this paper and other EEG with CNN papers were not very impressive.

This is the first time to our knowledge CNN performed so well on electrophysiological signals. With single model performance, no traditional feature extraction and modeling comes close. The best reported single traditional model in this competition performed less than 95%. Our single model CNN performance was a little more than 97%. What was even more surprising was the data exploration we were doing in the last week of the competition was so useful that the performance of the ensemble kept improving up until the deadline. We had some nets running that didn't make it by the deadline and we ended up with public/private LB 98.130%/98.015% post deadline once those nets finished. The public LB score would have been #1 while the private LB seemed to preserve our 3rd place, albeit with slightly better score.

We were even more surprised to learn that the 2nd ranked team used recurrent CNNs (RCNN) and their single model performance was more than 97.6%! The surprise is beyond expectation and sparked significant curiosity that we have now formed a team of 5 people (4 of whom are from teams that finished 1st, 2nd, and 3rd in this competition) to continue working on this with a target of publishing at least a conference paper using the larger dataset of which the data for this competition came from and/or another standard EEG motor imagery dataset.
<h3>Which tools did you use?</h3>
We used customized <strong><a href=""https://github.com/dnouri/nolearn"" target=""_blank"">NoLearn</a></strong> which is a wrapper on top of <strong><a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a></strong> which in turn is a library on top of <strong><a href=""https://github.com/Theano/Theano"" target=""_blank"">Theano</a></strong>.
<h3>How did you spend your time on this competition?</h3>
As the CNNs learned end-to-end, meaning extracting features in addition to features to output labels mapping, we spent little time on feature engineering. Originally, we were focused on traditional feature extraction as well. However, once we realized the performance of the CNNs was superior we abandoned the feature extraction completely and focused more on training more CNNs. We spent quite some time exploring the dataset in an effort to diversify the CNNs, however. So, we could say 20% time for pre-processing and feature extraction (again this is just to account for the time spent initially for traditional feature extraction) and 80% model training.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
The run time for the final 3rd place ensemble was about 4 days using 2x GTX 980 GPUs on a machine with 32GB RAM, 220GB swap space and 6 cores/12 threads Intel Xeon CPU running 4 models in parallel with each GPU training 2 models at the same time.
<h2>Teamwork</h2>
<h3>How did your team form?</h3>
<b>Esube</b>: The three (excluding Jing) of us started alone. As I was working on my dissertation while competing, I knew I needed someone to work with. I encouraged Jing to join me first and asked Elena to join us after she posted her <a href=""https://www.kaggle.com/elenacuoco/grasp-and-lift-eeg-detection/simple-grasp-with-sklearn"" target=""_blank"">script</a> (which, btw, was <a href=""http://blog.kaggle.com/tag/scripts-of-the-week/"" target=""_blank"">script of the week</a>). I saw that she wanted to finish top 10 as I was.

Around the same time, I also asked Tim (Again after tinkering with his awesome <a href=""https://www.kaggle.com/bitsofbits/grasp-and-lift-eeg-detection/naive-nnet"" target=""_blank"">script</a>) to join us whenever he feels like joining a team. He said he wanted to test most of his ideas alone before joining a team, first. He eventually came along and joined our team two weeks before the end of the competition. After the fact, I wished he joined us earlier as we were making progress until the end.

[caption id=""attachment_5223"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-05-at-4.07.17-PM-e1444087737880.png""><img class=""wp-image-5223 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-05-at-4.07.17-PM-e1444087737880.png"" alt=""Top competition scripts Grasp-and-Lift"" width=""612"" height=""398"" /></a> Top competition <a href=""https://www.kaggle.com/c/grasp-and-lift-eeg-detection/scripts?sortBy=votes"" target=""_blank"">scripts</a>, including those by Elena &amp; Tim[/caption]

<b>Tim</b>: Esube contacted me and asked about teaming up fairly early. At the time I still had several ideas I wanted to try and I thought it would be easier to test them out on my own since I'd have more submissions available. Once I'd worked through those ideas, which sadly were mostly unsuccessful, I contacted Esube to see if he was still interested in me joining his team.

Being part of team turned out to be very helpful: there were a lot of ideas being batted around and these helped me come up with ideas for new nets. In addition, once we'd settled on neural nets as our primary approach, Esube in particular worked tirelessly helping to train variations on our nets to improve our resulting ensemble. In retrospect, we probably could have done better if I'd joined up sooner since we were making quite a bit a progress as the competition ended.
<h3>How did your team work together?</h3>
Jing was trying traditional feature extraction of linear classifiers, although eventually we decided not to include them in the ensemble due to the poor performance compared to CNNs. Elena started out with linear classifiers and her own implementation of CNN. But, again due to performance, we decided to stick to Tim's CNN implementation. Tim was a crucial member of the team by coming up with CNN structures in fast iterations and training almost half of the nets that eventually made it to the ensemble. Esube trained nets by exploring different parts of the data and trying different pre-processing and validation strategies.
<h3>How did competing on a team help you succeed?</h3>
If it wasn't for the team, we all are convinced we wouldn't be able to surge like that in the last weeks of the competition and end up placing 3rd. The competition was so fierce and was so close that any additional model to the ensemble was very crucial.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
<b>Tim</b>: First, even simple ensembling helps a lot. In previous competitions I'd used single models, and the best finish I'd managed was 14th place. In this competition, had we used our best single model we would have finished in, interestingly enough, 14th place! However, using an average of our top models moved us up 11 places. Second, organize your code so that you're ready to submit it if you place. We did not do this and as a result, it took a lot of extra work to get the code ready to submit at the end of the competition.

<b>Esube</b>: My biggest take home point is the exceptional performance of CNNs on 1D time-series signals. I have learned quite a lot from my teammates especially Tim on how to quickly prototype CNN architectures. I also learned quite a lot from other competitors in the forums especially from Alexander for EEG processing domain knowledge.

<b>Elena</b>: I was really surprised of the performance of Neural Networks! I've learned a lot by the Tim's prototype of CNN applied to these time series. I'm only sorry to have had so few time to be more active and help my teammates during the last weeks of competitions.

<b>Jing</b>: I learned how powerful CNN is. With very little pre-processing and no feature extraction, it performs so well. I also learned from my teammates to try out different methods, do not solely trust on literature.
<h3>Do you have any advice for those just getting started in data science?</h3>
<b>Tim</b>: Dive in and tackle real problems as soon as possible. Kaggle is great for that since you get a chance to try challenging problems, see how you can do relative to other, more experienced data scientists, and see how the best results were obtained.

<b>Esube</b>: I strongly advice newbies like me to participate in teams and learn by doing, meaning there is no substitute (shortcut) for an experience that comes from exercising and trying out different methods.

<b>Elena</b>: The online course I followed was very useful, but it is also important to have good background on statistics, time series analysis, and data cleaning before going to ML techniques.

<b>Jing</b>: Work in a team and learn by solving problems.
<h2>Bios</h2>
<strong><a href=""https://www.linkedin.com/pub/timothy-hochberg/56/310/b99"" target=""_blank"">Tim Hochberg</a></strong>: received a B.S. in applied physics from Caltech in 1989. He went on to earn an M.S. and Ph.D. in Nuclear Engineering and Electrical Engineering respectively from the University of Illinois. His is currently Chief Scientist for atSpeed Technologies where he focuses on coaxing accurate, frequency-domain measurements out of inexpensive, time-domain measurement instruments using a variety of software and hardware techniques.

<strong><a href=""https://www.linkedin.com/in/esubalewbekele"" target=""_blank"">Esube Bekele</a></strong>: received the M.S. degree in electrical engineering, in 2009, from Vanderbilt University, Nashville, TN, USA, where he is currently working towards the Ph.D. degree in electrical engineering. He served as junior faculty in Mekelle Uiversity, Ethiopia, before joining Vanderbilt University. His current research interests include human–machine interaction, robotics, affect recognition, machine learning, and computer vision. He will join the Naval Research Laboratory (NRL), Washington, D.C. as research associate upon completion of his PhD at the end of October. His research at NRL will focus on convolutional neural networks and cognitive architecture for semantic context learning for visual object recognition.

<strong><a href=""https://it.linkedin.com/pub/elena-cuoco/25/615/51"" target=""_blank"">Elena Cuoco</a></strong>: received a Master degree in Physics in 1993 and a Ph.D in Physics in 1997 from Pisa University, Italy. Working as Post Doc for 2 years at 'Osservatorio Astronomico di Arcetri', Firenze, Italy and for 3 years as Researcher at INFN Firenze. Staff member from 2004 at <a href=""http://www.ego-gw.it"" target=""_blank"">European Gravitational Observatory</a> in Italy and member of Virgo Collaboration from 1995. Now I'm Scientific coordinator for <a href=""http://www.grawiton-gw.eu/"" target=""_blank"">GraWIToN project</a>, EGO referent for Data Analysis and the EPO activities coordinator for <a href=""http://public.virgo-gw.eu"" target=""_blank"">EGO and Virgo</a>.

<strong>Jing Fan</strong>: received the M.S. degree in electrical engineering, in 2014, from Vanderbilt University, Nashville, TN, USA, where she is currently working toward the Ph.D. degree in electrical engineering. Her research interests include human-robot interaction, robotics, machine learning, and cognitive computing.

<hr />

Code for team HEDJ's solution can be found <a href=""https://github.com/bitsofbits/kaggle_grasp_and_lift_eeg_detection"" target=""_blank"">here</a>.

Read other blogs on the Grasp-and-Lift EEG Detection competition by clicking the tag below."
"Grasp-and-Lift EEG Detection Winners' Interview: 1st place, Cat & Dog",http://blog.kaggle.com/2015/10/12/grasp-and-lift-eeg-winners-interview-1st-place-cat-dog/,2015-10-12 22:42:39,"Team Cat &amp; Dog took <a href=""https://www.kaggle.com/c/grasp-and-lift-eeg-detection/leaderboard"" target=""_blank"">first place</a> in the <a href=""https://www.kaggle.com/c/grasp-and-lift-eeg-detection"" target=""_blank"">Grasp-and-Lift EEG Detection</a> competition ahead of 378 other teams. The pair also comprised 2/3 of the first place team from another recent EEG focused competition on Kaggle, <a href=""https://www.kaggle.com/c/inria-bci-challenge"" target=""_blank"">BCI Challenge @ NER 2015</a>. Domain knowledge and a strong collaborative relationship have made Alexandre Barachant (aka Cat) and Rafał Cycoń (aka Dog) successful in both competitions.

In this blog, they share best practices for working with EEG data, as well as the tools and code that took them to the top of the Grasp-and-Lift EEG Detection leaderboard. They also tip their hat to all the Kagglers who shared <a href=""https://www.kaggle.com/c/grasp-and-lift-eeg-detection/scripts"" target=""_blank"">scripts</a> during the competition. The code shared and models developed during this challenge were huge contributions to the <a href=""http://www.wayproject.eu/"" target=""_blank"">WAY Consortium</a>'s work in developing prosthetic devices for patients who have lost hand function due to neurological disabilities or amputation.

[caption id=""attachment_5211"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/grasp_eeg.png""><img class=""size-medium wp-image-5211"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/grasp_eeg-300x141.png"" alt=""The Grasp-and-Lift EEG Detection competition ran from June 29 - August 31, 2015."" width=""300"" height=""141"" /></a> The Grasp-and-Lift EEG Detection competition ran from June 29 - August 31, 2015[/caption]
<h2>The Basics</h2>
<a name=""top""></a>
<h3>What was your background prior to entering this challenge?</h3>
<strong><a href=""#cat"">Cat:</a></strong> I hold a PhD in signal processing and an electrical engineering degree from the <a href=""http://www.univ-grenoble-alpes.fr/"" target=""_blank"">Grenoble University</a>, France. I’m currently working as a post-doc fellow at the <a href=""http://www.burke.org/research"" target=""_blank"">Burke Medical Research Institute</a> (Cornell University), where I study the effect of <a href=""https://en.wikipedia.org/wiki/Transcranial_direct-current_stimulation"" target=""_blank"">transcranial direct current stimulation</a> (tDCS) as a clinical treatment for pediatric hemiplegia. My background is more about signal processing than machine learning. But I caught the virus and now I can’t live without my daily dose of data science.

I’m not saying I was a complete noob, and I was already playing with ML during my PhD, but I’ve always been more comfortable with feature engineering than modelisation.

[caption id=""attachment_5238"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-13-at-3.12.14-PM.png""><img class=""size-medium wp-image-5238"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-13-at-3.12.14-PM-300x142.png"" alt=""Kaggle profile for Alexandre Barachant (aka Cat)"" width=""300"" height=""142"" /></a> Kaggle <a href=""https://www.kaggle.com/alexandrebarachant"" target=""_blank"">profile</a> for Alexandre (aka Cat)[/caption]

<strong><a href=""#dog"">Dog:</a></strong> I studied Mechatronics at <a href=""http://www.agh.edu.pl/en"" target=""_blank"">AGH UST</a> in Krakow, Poland, which gave me a solid background in signal processing. I was interested in Data Science and Machine Learning since early years of studies, which resulted in basing both Bachelor and Master theses in these topics. Right now I’m running a startup that provides custom Data Science solutions.

[caption id=""attachment_5237"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-13-at-3.10.17-PM.png""><img class=""size-medium wp-image-5237"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-13-at-3.10.17-PM-300x143.png"" alt=""Kaggle profile for Rafal (aka blaine)"" width=""300"" height=""143"" /></a> Kaggle <a href=""https://www.kaggle.com/rafalcycon/account"" target=""_blank"">profile</a> for Rafał (aka Dog or blaine)[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
<b>Cat:</b> I won’t lie, this challenge was right in my comfort zone. I work with EEG signal on a daily basis. As a matter of fact, my current jobs mainly consist of recording and analyzing EEG signal to decode hand movement in order to understand the physiology of sensory-motor cortex on children with cerebral palsy. In addition, this challenge was more or less the topic of my PhD, where I built a brain computer interface to decode asynchronous hand movement in order to drive an home automation system (control the light or your TV with your thoughts).

Domain knowledge is important for EEG. There are multiple ways to deal with the problem, and tons of different features to investigate. However, it can also blind you during the exploratory phase. For example, if it wasn’t for my teammate and the scripts posted by other contestants, I would never have used very low frequency time-domain signal as a feature (they are not reported that useful in the literature), and it turned out to be one of the most important feature.

<b>Dog:</b> I was always extremely interested in Brain Computer Interfaces and processing brain-related signals, but my knowledge was very limited. Thanks to Alex I learnt a lot during the previous BCI competition. Also of course previous experience with signal processing and machine learning was useful.
<h3>How did you get started competing on Kaggle?</h3>
<b>Cat:</b> The first real challenge I took part in was the <a href=""https://www.kaggle.com/c/decoding-the-human-brain"" target=""_blank"">DecMeg2014</a> challenge. Back then, I was between two post-doc, and was looking for something to keep me busy. Over my years of research, I developed some very good classification methods for EEG signal. They are working really well for online applications, and seemed to be competitive when compared to state-of-the-art methods. However, the field of BCI lacks a standard benchmark. It was the perfect opportunity to find out how these methods were ranking in a very competitive and standardized environment. After a few submissions, I took first place on the leaderboard, and was definitely hooked.

<b>Dog:</b> I entered Kaggle with the challenge of getting a Master badge, and thanks to lots of luck I was able to get it quite fast. But tasting the thrill and fun of competing, and realizing the possibilities of self-development that this platform offers made me stay for longer. Zero regrets.
<h3>What made you decide to enter this competition?</h3>
<b>Cat:</b> This challenge is such a match to my background that I had the moral obligation to enter...

<b>Dog:</b> “Oooh yesss, a competition about processing EEG to control a hand prosthetic! That is so cool !” :-)
<h2>Let's Get Technical</h2>
<h3>What domain-specific features did you use ?</h3>
There are two major kinds of event related activity that we can extract from EEG.

The first one is called <a href=""https://en.wikipedia.org/wiki/Event-related_potential"" target=""_blank"">Event Related Potential</a> (ERP), and is characterized by a phase-locked, time-domain waveform that appears in response to a stimulation. ERP are without any doubt the most studied type of activity in EEG. The <a href=""https://www.kaggle.com/c/inria-bci-challenge"" target=""_blank"">BCI challenge</a> and the <a href=""https://www.kaggle.com/c/decoding-the-human-brain"" target=""_blank"">DecMeg2014 challenge</a> were about classification of ERP. Typical features are time-domain signal, generally averaged across several repetitions of the stimulation in order to increase the signal to noise ratio.

[caption id=""attachment_5232"" align=""aligncenter"" width=""608""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-12-at-3.02.07-PM.png""><img class=""size-full wp-image-5232"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-12-at-3.02.07-PM.png"" alt=""Fig. 1 : Example of Visual Evoked Potential (VEP) present in the dataset. The graph on top represents the individual time course for each electrode, with respect to their position on the scalp. The second figure represents the topographical map of the amplitude of the VEP for different timing. Link to script"" width=""608"" height=""561"" /></a> Fig. 1 : Example of Visual Evoked Potential (VEP) present in the dataset. The graph on top represents the individual time course for each electrode, with respect to their position on the scalp. The second figure represents the topographical map of the amplitude of the VEP for different timing. <a href=""https://www.kaggle.com/alexandrebarachant/grasp-and-lift-eeg-detection/visual-evoked-potential-vep/notebook"" target=""_blank"">Link to script</a>[/caption]

The other one is called <a href=""https://en.wikipedia.org/wiki/Neural_oscillation"" target=""_blank"">neural oscillation</a>, and is characterized by change in signal power in specific frequency bands. These oscillations appear naturally in ongoing EEG activity, and are representative of a wide range of different cognitive state (e.g sleep stage, meditation, etc.) or can be induced by a specific task, for example a hand movement, or mental calculus. This was the case for the two seizure detection challenge organized last year. Typical features are FFT-based, or more simply log-variance / covariance of the signal after frequential filtering.

[caption id=""attachment_5233"" align=""aligncenter"" width=""608""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-12-at-3.04.00-PM-e1444687477273.png""><img class=""size-full wp-image-5233"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-12-at-3.04.00-PM-e1444687477273.png"" alt=""Fig. 2 : Example of induced neural oscillation related to hand movement in the grasp-and-lift dataset. During a right Hand movement, the power in the mu (10 Hz) and beta (20 Hz) frequency bands decrease over the left sensory-motor cortex. Link to script"" width=""608"" height=""174"" /></a> Fig. 2 : Example of induced neural oscillation related to hand movement in the grasp-and-lift dataset. During a right Hand movement, the power in the mu (10 Hz) and beta (20 Hz) frequency bands decrease over the left sensory-motor cortex. <a href=""https://www.kaggle.com/alexandrebarachant/grasp-and-lift-eeg-detection/common-spatial-pattern-with-mne/files"" target=""_blank"">Link to script</a>[/caption]

Both of these activities have parameters (exact spatial location, latency, shape, frequency band, etc.) specific to each subject, and can be seen as some kind of brain fingerprint. This is the reason why models trained on a subject does not generalize well to other subjects and why optimal parameters of features extraction must be subject-specific.

The Grasp-and-Lift EEG dataset contains both types of activities: induced power related to hand movements, and visual evoked potential elicited by a visual stimulus instructing the subject to start the task. As a results, we used as feature time domain low-pass filtered signal, and covariance matrices estimated in different frequency bands.
<h3>What preprocessing and supervised learning methods did you use?</h3>
On the basis of above domain-specific features and a generic feature called a Filter Bank we built a 3-level classification pipeline:
<ul>
	<li>Level1 models are subject-specific, i.e. trained independently on each subject (but with global hyper-parameters). Most of them are also event-specific. Their main goal is to provide support and diversity for level2 models by embedding subject and events specificities using different types of features.</li>
	<li>Level2 models are global models (i.e. not subject-specific) that are trained on level1 predictions (metafeatures). Their main goal is to take into account the temporal structure and relationship between events. Also the fact that they are globally significantly helps to calibrate predictions between subjects.</li>
	<li>Level3 models ensemble level2 predictions via an algorithm that optimizes level2 models' weights to maximize Area under the ROC Curve, which was the target metric in this competition.</li>
</ul>
[caption id=""attachment_5234"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/catdog1-e1444687807425.png""><img class=""size-full wp-image-5234"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/catdog1-e1444687807425.png"" alt=""Fig. 3 : Classification pipeline of the solution."" width=""612"" height=""663"" /></a> Fig. 3 : Classification pipeline of the solution.[/caption]
<h3>What was your most important insight into the data?</h3>
There was a clearly defined temporal structure of events (i.e. the 6 target classes), so including a signal time course history of several seconds was beneficial in all models. Especially Recurrent Neural Networks were able to “naturally” catch the pattern.
In addition, ensembling was very powerful in this challenge. The amount of data, and the stability across time (which is something rare in a EEG dataset) was allowing us to build a high level of supervised ensembling, without too much risk of overfitting.
<h3>Were you surprised by any of your findings?</h3>
Quite a lot of predictive power was contained in very low frequency content of the data, which is quite unusual for BCI/EEG. We can also observe relatively good performances with a covariance model estimated on the 70-150 Hz frequency range. This frequency range is too high to contain EEG, underlying the possible presence of task-related EMG activity.

We were very surprised by the fact that simply heavily decimating the input to XGBoost served as a much better regularization than many ideas for parameter tuning that we tried. What’s also interesting is that this fact surfaced only due to Alex’s <del>laziness</del> impatience in waiting for XGBoost’s results :-)
<h3>How did you spend your time on this competition?</h3>
We merged a few days after the competition reset (there was a timing error in initial data). We spent the first weeks exchanging ideas about feature extraction and testing different ensembling strategies. We then spent 2 weeks for a complete code refactoring in order to merge our solutions, cross-checking each other’s code for leakage and rules infringements in the process. This gave us a very powerful and robust codebase, and we spent another few days generating many level 1 models. The last 2 weeks was entirely dedicated to ensembling, with a specific focus on insuring the robustness of our solution against overfitting.

[caption id=""attachment_5235"" align=""aligncenter"" width=""612""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/dogcat2-e1444689507492.png""><img class=""size-full wp-image-5235"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/dogcat2-e1444689507492.png"" alt=""Fig. 4 : Leaderboard progress."" width=""612"" height=""460"" /></a> Fig. 4 : Leaderboard progress.[/caption]
<h3>Which tools did you use?</h3>
Everything was done in Python. We used <a href=""http://www.scipy.org/"" target=""_blank"">Scipy</a>, <a href=""http://martinos.org/mne/stable/index.html"" target=""_blank"">MNE</a> and Alex’s <a href=""https://github.com/alexandrebarachant/pyRiemann"" target=""_blank"">pyRiemann packages</a> for signal processing, and <a href=""http://scikit-learn.org/stable/"" target=""_blank"">scikit-learn</a>, <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a>, <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">Theano</a>, <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a>, <a href=""https://github.com/dnouri/nolearn"" target=""_blank"">Nolearn</a>, <a href=""https://github.com/fchollet/keras"" target=""_blank"">Keras</a> and <a href=""https://github.com/hyperopt/hyperopt"" target=""_blank"">hyperopt</a> for machine learning. We would like to thank the authors of these packages for their dedication and excellent work.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
Lots of fun, new knowledge and skills, and $5,000.
<h3>Do you have any advice for those just getting started in data science?</h3>
Practice, practice, practice. And the best way to do that is through Kaggle. Jump in even if you think you lack knowledge (yet!). Possibility to learn from others, to learn with others, and the competitive spirit will drive you to boost your skills considerably. And it’s for free!...well, <a href=""https://www.kaggle.com/khyh00/introducing-kaggle-scripts/xkcd-style-test"" target=""_blank"">almost</a> :-)

One of the most important skills of a data scientist is to validate solutions correctly. Test your work with a bulletproof validation procedure that you are 100% sure is reasonable. You’re going to create features, build models, implement crazy-sounding ideas, but in the end you have to know if you should keep them, throw them away or change them in one way or another. The public leaderboard alone is often very deceptive. We learned that the hard way in the Seizure Prediction challenge, where we dropped from 2nd place on public LB to 26th on private LB. And the reason was a flawed cross-validation procedure. It was used because it gave really well correlated scores with public LB and it happened they were uncorrelated with private LB, which gave us lots of disappointment in the end.

Finally, team-up, and do it early to share discussions and insights, combine skills and points of view - it can be extremely valuable.
<h2>Teamwork</h2>
<h3>How did your team work together?</h3>
<b>Cat:</b> I’m generally more focused on feature engineering, and Rafal on modeling and ensembling. For this challenge, there was no specific role, except for ensembling where I mainly worked on XGBoost while Rafal was playing with RNN (no GPU for me).
We communicated by email, and shared code through gitlab.

[table]
,Average per day (41 days as a team),Total
emails,13.6,560
commits,7.3,300
beers,0,0 :-(
[/table]

<b>Dog:</b> Here we can observe the joy of intercontinental cooperation… :-)
<h3>How did competing on a team help you succeed?</h3>
<b>Dog:</b> Our team was a really good synergy - Alex’s domain expertise and excellent feature engineering skills were leveraged with machine learning stuff that I was able to provide. We both learned a lot. And I think it’s highly unlikely either of us would have won this challenge alone.
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
<b>Dog:</b> More BCI/EEG competitions would be awesome of course :-) Besides that it’d be great to see Kagglers’ knowledge and imagination be put into development of more medical applications, like previous seizure competitions or the recent diabetic retinopathy competition. For example identifying and/or predicting arrhythmia attacks on the basis of ECG signals or perhaps identifying cancer in radiographs.

It would also be fun to try to forecast stock prices of some companies on the basis of lots of various data, like fuel prices, mean salaries for certain jobs, number of positive/negative tweets about a brand, google search trends etc.

<b>Cat:</b> I have been very surprised of the high level of the solutions (and their performance) developed during this challenge. For a problem that is considered as one of the most difficult in the field (asynchronous BCI), it’s amazing how fast Kagglers were able to beat the state-of-the-art methods. It proves once again that the Kaggle community is able to tackle any problem with bright and imaginative solutions that can potentially unlock or boost research. So I would love to see how Kagglers will crack other challenges in BCI/EEG. If i could host a challenge, it will be a Big Data BCI challenge, with hundreds of subjects and data recorded in a real life context i.e. not in very controlled laboratory conditions. This is where the field has to move.
<a name=""cat""></a>
<h2>Bios</h2>
<strong><a href=""http://alexandre.barachant.org/"" target=""_blank"">Alexandre Barachant</a></strong> (“Cat”) is a French Researcher and expert in Brain computer interfacing and Biosignal analysis. In 2012, he received his Ph.D. degree in signal processing from the Grenoble University, France. During his Ph.D. thesis, he developed a robust and adaptive brain computer interface based on self-paced motor imagery. Between 2012 and 2013, he has been a post-doc fellow at the Centre National de la Recherche Scientiﬁque (CNRS) in the GIPSA Laboratory, Grenoble, where he developed a calibration-less P300 brain computer interface for video-game control and assistive communication. Since November 2014, he joined the Burke Medical Research Institute, New York, to study the effects of non-invasive brain stimulation for rehabilitation. His research interests include statistical signal processing, Machine learning, Riemannian geometry and classification of neurophysiological recordings.
<p style=""text-align: right;""><a href=""#top"">top</a></p>
<a name=""dog""></a><strong><a href=""https://www.linkedin.com/pub/rafal-cycon/65/411/832"" target=""_blank"">Rafał Cycoń</a></strong> (“Dog”) is co-founder and CTO of a Data Science consultancy startup <a href=""http://www.fornax.co/"" target=""_blank"">FORNAX</a><a name=""dog""></a>, where he is enjoying solving challenging problems and building intelligent systems. He holds a MSc diploma in Mechatronics received from AGH-UST in Krakow, Poland. During the last 5 years he has completed a variety of research and industrial projects, such as development of image processing pipelines for an intercellular manipulator, detecting early signs of autism in children on the basis of their interaction with tablets during playing mobile games, or development of a software system controlling high-speed cameras for military applications. His scientific interests include Brain-Computer Interfaces and all kinds of stuff related to Data Science and Machine Learning - in particular validation methods, ensembling, (Deep) Neural Networks, Gradient Boosting Machines and signal/image processing techniques. He has just learned that writing about himself in 3rd person is a strange experience.
<p style=""text-align: right;""><a href=""#top"">top</a></p>


<hr />
<p style=""text-align: left;"">Read other posts on the Grasp-and-Lift EEG Detection competition by clicking the tag below.</p>
<p style=""text-align: left;"">The code and documentation for team Cat &amp; Dog's solution can be found on <a href=""https://github.com/alexandrebarachant/Grasp-and-lift-EEG-challenge"" target=""_blank"">this repo</a>.</p>"
"Caterpillar Winners' Interview: 3rd place, Team Shift Workers",http://blog.kaggle.com/2015/10/20/caterpillar-winners-interview-3rd-place-team-shift-workers/,2015-10-20 17:00:59,"The <a href=""https://www.kaggle.com/c/caterpillar-tube-pricing"" target=""_blank"">Caterpillar Tube Pricing</a> competition challenged Kagglers to predict the price a supplier would quote for the manufacturing of different tube assemblies using detailed tube, component, and volume data. Team Shift Workers <a href=""https://www.kaggle.com/c/caterpillar-tube-pricing/leaderboard/private"" target=""_blank"">finished in 3rd place</a> by combining a diverse set of approaches different members of the team had used before joining forces. Like other teams in the competition, they found <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> to be particularly powerful on this dataset.

[caption id=""attachment_5258"" align=""aligncenter"" width=""251""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/cat_logo.png""><img class=""wp-image-5258 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/cat_logo.png"" alt=""cat_logo"" width=""251"" height=""145"" /></a> 1,323 teams and 1,451 players competed[/caption]
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
<b>Shize:</b> I am currently a phd student in the department of electrical and computer engineering at <a href=""http://www.virginia.edu/"" target=""_blank"">University of Virginia</a>, United States. My research interests lie in 1) Large Scale Network Modeling, Analysis, Optimization and Control; 2) Complex Systems; 3) Machine Learning/Data Mining with Application to Big Data. I had already participated in a couple of Kaggle competitions prior to entering this CAT competition.

[caption id=""attachment_5249"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-19-at-12.14.57-PM.png""><img class=""size-medium wp-image-5249"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-19-at-12.14.57-PM-300x142.png"" alt=""Shize Su's profile on Kaggle"" width=""300"" height=""142"" /></a> Shize Su's <a href=""https://www.kaggle.com/sushize"" target=""_blank"">profile</a> on Kaggle[/caption]

<b>Matias:</b> I currently work as a data analyst for <a href=""http://www.aimia.com/"" target=""_blank"">Aimia</a>. Previously I was a database admin, and previous to that a software developer. I started doing data science courses last year through <a href=""https://www.edx.org/"" target=""_blank"">EDx</a> and <a href=""https://www.coursera.org/"" target=""_blank"">Coursera</a>. Over there I became familiar with Python and R, which is my principal tool now for Kaggle.

[caption id=""attachment_5250"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-19-at-12.16.05-PM.png""><img class=""wp-image-5250 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-19-at-12.16.05-PM-300x142.png"" alt=""Matias Thayer profile badge"" width=""300"" height=""142"" /></a> Matias Thayer's <a href=""https://www.kaggle.com/chechir"" target=""_blank"">profile</a> on Kaggle[/caption]

<b>Naokazu:</b> I took my Ph.D. in mathematics and have been worked on data analysis in several fields.

[caption id=""attachment_5251"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-19-at-12.17.22-PM.png""><img class=""size-medium wp-image-5251"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-19-at-12.17.22-PM-300x142.png"" alt=""Naokazu Mizuta's profile on Kaggle"" width=""300"" height=""142"" /></a> Naokazu Mizuta's <a href=""https://www.kaggle.com/naokazumizuta"" target=""_blank"">profile</a> on Kaggle[/caption]

<b>Yannick:</b> I am working in a media company as a data analyst, focusing on turning transactional data into marketing reports, which are used for marketing consultant and publication. Since my major of master degree is information intelligence, I am quite familiar with data mining theory, database, data manipulating softwares like R and SAS, and coding skills like Python and Java.

[caption id=""attachment_5252"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-19-at-12.18.11-PM.png""><img class=""size-medium wp-image-5252"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-19-at-12.18.11-PM-300x144.png"" alt=""Yannick's profile on Kaggle"" width=""300"" height=""144"" /></a> Yannick's <a href=""https://www.kaggle.com/breezeyuner"" target=""_blank"">profile</a> on Kaggle[/caption]

<b>Arto:</b> I've a worked in the Business Intelligence field for some years now. Currently I'm Senior Consultant at <a href=""http://www.affecto.com/"" target=""_blank"">Affecto</a>. My expertise lies in data modelling, architecture and integrations. Lately I've been getting more and more into data science and machine learning.

[caption id=""attachment_5253"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-19-at-12.19.04-PM.png""><img class=""size-medium wp-image-5253"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-19-at-12.19.04-PM-300x143.png"" alt=""Arto's profile on Kaggle"" width=""300"" height=""143"" /></a> Arto's <a href=""https://www.kaggle.com/mammal"" target=""_blank"">profile</a> on Kaggle[/caption]
<h3>How did you get started competing on Kaggle?</h3>
<b>Shize:</b> I knew Kaggle when I took a graduate Machine Learning course at UVA -- the instructor held a <a href=""https://inclass.kaggle.com/"" target=""_blank"">Kaggle in Class</a> competition for the course, and luckily, I finished in 1st place in the competition. Then, I took the <a href=""https://www.kaggle.com/c/pakdd-cup-2014"" target=""_blank"">Kaggle 2014 PAKDD Cup</a> as my course project for that Machine Learning course, and, fortunately, I won the 3rd place prize (which was my 1st public Kaggle competition). It was definitely a surprise for me. I was quite encouraged by the experience, and from then on, I attended a couple of more Kaggle competitions in my spare time, and I have learned a lot from the Kaggle community. Kaggle is definitely a wonderful place for sharping your skills on machine learning and data mining.

<b>Matias:</b> My first competition was on October 2014, as part of the “<a href=""https://www.kaggle.com/c/15-071x-the-analytics-edge-competition-spring-2015"" target=""_blank"">15.071x - The Analytics Edge (Spring 2015)</a>” challenge. I was doing a MIT course through EDx, and this competition was part of the assignments. I really got very engaged during this competition and since then I'm an addict to Kaggle.

<b>Yannick:</b> Well, this is my second Kaggle competition, I still have a long way to go (what good luck to meet with other guys!). I guess the attraction for me is: I can learn a lot of cutting-edge skills and feel happy at the same time.

<b>Arto:</b> I found Kaggle’s tutorial competitions through Coursera. After I had done few of those, my first real competitions were <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge"" target=""_blank"">Otto Group</a> and <a href=""https://www.kaggle.com/c/predict-west-nile-virus"" target=""_blank"">West Nile Virus</a> this spring. In every competition I’ve learned a lot, achieved slightly better results than previously, and have gotten more hooked on Kaggle.
<h3>What made you decide to enter this competition?</h3>
<b>Shize:</b> The problem is interesting to me, and the size of data is ok. Second, I had some spare time during the competition period.

<b>Matias:</b> I liked the fact that the amount of data was relatively small which means you can do many experiments on your laptop without having to wait for ages for the processes to finish. Also, I could use <a href=""https://www.libreoffice.org/"" target=""_blank"">libreoffice</a> solver to optimize the different predictions, which is quite easy and quick. In addition, I enjoy when the data is not “hidden” like in other competitions, and you can make logical and common sense inferences.

<b>Naokazu:</b> The datasets were small enough to explore quickly, diverse enough to try several ideas.

<b>Yannick:</b> The size of the data is suitable for my Internet condition! Luckily, I have already changed the ISP so it wont be a problem anymore.

<b>Arto:</b> The summer in Finland was cold and rainy so it was perfect for coding and learning new stuff. Also, the dataset consisted of many relational tables, which meant home ground advantage for me.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
<b>Shize:</b> In the early stage of the competition, I explored a couple of different models, including <a href=""https://github.com/dmlc/XGBoost"" target=""_blank"">XGBoost</a>, <a href=""https://cran.r-project.org/web/packages/randomForest/index.html"" target=""_blank"">randomforest</a>, <a href=""https://github.com/fchollet/keras"" target=""_blank"">keras</a>, nn, knn, etc. But later I found that XGBoost outperformed all other method significatly in this CAT competition, and so mostly focused on improving XGBoost models. After teaming up, my role mostly turned to coordinate team work plans, set up cv experiments for ensemble, and suggest different ideas &amp; provides instructions for new directions (e.g., meta-features and 2 level models) to explore for further team improvement.

<b>Matias:</b> Early in the competition I realized that XGBoost performed considerably better than other models. Also, I noticed that exactly the same XGBoost model produced considerable different results depending on the seed. In this way I started bagging as many XGBoost models as I could. My strategy was very simple: Just run 100 or more XGBoost models overnight, using different parameters and against different portions of the train data (columns and rows) and then average the predictions. Also I worked a lot on feature engineering in the meantime, so my new models progressively included new features as well. In this way, as more models stacked together, the more the RMSLE score improved (decreased). That was very cool. This is how I managed to be between the 6th and 8th position on the public LB, and then I received an invitation to form part of the “Shift Workers” team, which I accepted happily.

[caption id=""attachment_5256"" align=""aligncenter"" width=""640""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/CAT_competition_diagram.png""><img class=""wp-image-5256 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/CAT_competition_diagram-1024x489.png"" alt="""" width=""640"" height=""306"" /></a> Diagram of final solution. Click image for full size.[/caption]

Then the approach switched a bit. As a team, we used fold 2 validations, and validated the models locally before submitting to Kaggle, so progressively I abandoned the “bagged” approach and used a weighted blend approach instead. As it was hard to improve further, I started building meta features on a CV-Fold 5 subset. Shize Su gave me very good guidelines to build those meta features – Many thanks to him! Here I used the following techniques: KNN, XGBoost, standard linear regression, CART, and Random forest predictions. The predictions obtained with those 2nd level models didn’t perform better than the previous ones, but they provided some diversity that helped to improve our score.

<b>Naokazu:</b> First I tried plain XGBoost model and then tried ensembling. My ensembling method, ""quantity""-wise modeling, was somewhat peculiar for this competition. For each ""quantity"" I dropped certain records from the training data and built models.

<b>Yannick:</b> At first I tried to use <a href=""https://en.wikipedia.org/wiki/AdaBoost"" target=""_blank"">Adaboost</a> and <a href=""https://cran.r-project.org/web/packages/randomForest/index.html"" target=""_blank"">Randomforest</a>, but lately I've found that XGBoost has an excellent performance.

Matias has talked a lot about the methods to the team, and to be honest, I was not familiar with these methods at first (like blending, meta-bagging and complex CV mechanism). For myself, all these techniques I learned from others are even more important and precious than winning this competition.

<b>Arto:</b> I used <a href=""https://www.kaggle.com/hsperr/otto-group-product-classification-challenge/finding-ensamble-weights"" target=""_blank"">ensemble learning</a> from the start of the competition. Feature engineering was done fast with <a href=""http://www.pentaho.com/"" target=""_blank"">Pentaho’s Kettle</a>. After I had one model with a good set of features, I broke it down into multiple models with different feature sets and parameters.

These models were then added to the ensemble with calculated weights and also dropped from the ensemble according to cross validation scores. Later on I did the same work with the datasets of my team mates. Early ensembles had many different algorithms, but in the end all but one of the models (<a href=""https://cran.r-project.org/web/packages/extraTrees/index.html"" target=""_blank"">extratrees</a>) were using XGBoost.
<h3>What was your most important insight into the data?</h3>
<b>Shize:</b> Comprehensive and reliable cv experiment and XGBoosts ensemble were the key for success in this CAT competition. Developing many different variants of the XGBoost model (different parameters, different feature data sets, etc.) and blending based on the cv performs fairly well.

<b>Matias:</b> The power of team work and diversity. Also, I was a bit surprised about the bad performance of regressions. Probably because we were trying to predict 2 costs at the same time (set up cost and product cost).

<b>Naokazu:</b> I usually do not do lots of parameter tunings on XGBoost, especially row and col sampling since they usually have minor effects on performance. This time I got a drastic performance change for tuning these parameters on this dataset.

<b>Yannick:</b> I found that the number of components used in a tube may have strong affect on the prediction. We used a method which was quite similar to <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" target=""_blank"">TFIDF</a> to deal with it, and it worked well.

<b>Arto:</b> At first the competition was focused on feature engineering and in the end it was all about building an ensemble. So key points to success were cross validation, model diversity, and big team size.
<h3>Were you surprised by any of your findings?</h3>
<b>Shize:</b> I was a bit surprised that the XGBoost method was so dominant in this CAT competition. Adding other method models into the ensemble only provided quite marginal or no improvement (except in the 2 level model with other method model's predictions as meta features).

<b>Matias:</b> I was a bit surprised at how different approaches from my team mates worked together, so our final work was basically a big ensemble of many, many different trials.

<b>Naokazu:</b> Same as above.

<b>Yannick:</b> After the competition, I believe the key point to winning on Kaggle is teamwork. Techniques are important, but collaboration is the decisive fact.

<b>Arto:</b> I was a bit surprised that the ensemble score could be improved by adding diversity with deliberately made weaker models.
<h3>Which tools did you use?</h3>
<b>Shize:</b> R and Python

<b>Matias:</b> R and LibreOffice

<b>Naokazu:</b> R and PostgreSQL

<b>Yannick:</b> R and LibreOffice

<b>Arto:</b> Python and Pentaho's Kettle
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
<b>Shize:</b> First, and most importantly, 4 new good friends^_^ Second, a wonderful experience for me to train to be more experienced in coordinating the work and plans of a team.

<b>Matias:</b> That I need to care more about reproducibility from the beginning. At first I started stacking many different XGBoost models and sometimes I forgot to include the seed. Because of that we had to ignore some of those models when doing ensembling after we teamed up.

<b>Naokazu:</b> Ensembling diverse models especially built by different people often gives you improvements on some datasets. I realized it is quite important for winning on Kaggle to team up in certain challenges. Most top teams on this competition teamed up and a <a href=""http://blog.kaggle.com/2015/05/13/improved-kaggle-rankings/"" target=""_blank"">recent change</a> on Kaggle's ranking metric seems to encourage participants to team up.

<b>Yannick:</b> One thing is collaboration, another one is knowing lots of new techniques. It will absolutely help me in other competitions.

<b>Arto:</b> Just like Matias, I got into trouble with reproducibility because I juggled with so many models without a decent source control system. So key learning points for me were better coding practices.
<h2>Teamwork</h2>
<h3>How did competing on a team help you succeed?</h3>
The close collaboration between 5 teammates was the key for our success. Each teammate had his own ideas, strengths &amp; weaknesses, and our final blending included portions of everyone's brain. We believe that, 1+1&gt;2 in such a successful collaboration.

<hr />

Read an interview with the 1st place team in the Caterpillar Tube Pricing competition by clicking the tag below."
"Recruit Coupon Purchase Winner's Interview: 2nd place, Halla Yang",http://blog.kaggle.com/2015/10/21/recruit-coupon-purchase-winners-interview-2nd-place-halla-yang/,2015-10-21 17:32:17,"Recruit <a href=""http://ponpare.jp/"" target=""_blank"">Ponpare</a> is Japan's leading joint coupon site, offering huge discounts on everything from hot yoga, to gourmet sushi, to a summer concert bonanza. The <a href=""https://www.kaggle.com/c/coupon-purchase-prediction"" target=""_blank"">Recruit Coupon Purchase Prediction</a> challenge asked the community to predict which coupons a customer would buy in a given period of time using past purchase and browsing behavior.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/recruitchallenge_logo.png""><img class=""aligncenter size-medium wp-image-5268"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/recruitchallenge_logo-300x141.png"" alt="""" width=""300"" height=""141"" /></a>

Halla Yang <a href=""https://www.kaggle.com/c/coupon-purchase-prediction/leaderboard/private"" target=""_blank"">finished 2nd</a> ahead of 1,191 other data scientists. His experience working with time series data helped him use unsupervised methods effectively in conjunction with gradient boosting. In this blog, Halla walks through his approach and shares key visualizations that helped him better understand and work with the dataset.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I've worked almost a decade in finance as a quantitative researcher and portfolio manager. I've also competed in several Kaggle contests, placing first in the <a href=""https://www.kaggle.com/c/RxVolumePrediction"" target=""_blank"">Pfizer Volume Prediction</a> Masters competition, sixth in the <a href=""https://www.kaggle.com/c/MerckActivity"" target=""_blank"">Merck Molecular Activity Challenge</a>, and ninth in the <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection"" target=""_blank"">Diabetic Retinopathy Detection</a>.

[caption id=""attachment_5266"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-21-at-10.43.26-AM.png""><img class=""size-medium wp-image-5266"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-21-at-10.43.26-AM-300x143.png"" alt=""Halla Yang's profile on Kaggle"" width=""300"" height=""143"" /></a> Halla Yang's <a href=""https://www.kaggle.com/hallayang"" target=""_blank"">profile</a> on Kaggle[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
Predicting prices for thousands of stocks and predicting purchases by thousands of Japanese internet users are loosely similar problems. You can forecast stock returns by looking at time series data such as past returns and cross-sectional data such as industry averages. You can forecast coupon purchases by looking at time series features based on past purchases and cross-sectional features based on peer group averages.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
For each (user, coupon) pair, I calculated the probability that the user would purchase that coupon during the test period using a gradient boosting classifier. I sorted the coupons for each user by probability, composing the ten highest probability coupons into my submission.

To train my classifier, I constructed training data for 24 ""train periods"" that simulated the test period. Train period 1 is the week from 2012-01-08 through 2012-01-14, and includes all coupons with a DISPFROM date - the date on which they're supposed to be first displayed - in that week. Train period 2 is the week from 2012-01-15 through 2012-01-21, and includes all coupons with a DISPFROM date in that week. Train period 24 is the week from 2012-06-17 through 2012-06-23, and includes all coupons with a DISPFROM date in that week.

For each of these training periods, I built a set of features for each (user, relevant coupon) pair. This set of features includes user-specific data, e.g. gender, days on site, and age; coupon-specific data, e.g. catalog price, genre, and price rate; as well as user-coupon interaction data, e.g. how often has the user viewed coupons of the same genre. The target for each observation is set to 1 if the user purchased that coupon during the training week, and 0 otherwise.

To calibrate the parameters of my model, I first trained a model on the first twenty-three weeks of data, and estimated my log loss and confusion matrix on the twenty-fourth week. I then trained a model on the full twenty-four weeks of data to generate my competition submission.

The only supervised learning method I used was gradient boosting, as implemented in the excellent <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">xgboost</a> package. I cycled through other algorithms at the start of my analysis to get a feel for their relative performance - logistic regressions, random forests, SVMs, as well as deep neural networks - but found that gradient boosting was the single best classifier for my approach.
<h3>What was your most important insight into the data?</h3>
First, many test set and training set coupons were viewed prior to their DISPFROM, the date on which they're supposed to be first displayed, and so one could use direct views as a forecasting variable. The violin plot below shows the distribution of first view times relative to DISPFROM. A negative x-value indicates the coupon was viewed prior to its DISPFROM. Over a quarter of coupons are first viewed more than twelve hours before their DISPFROM, and five percent of coupons are first viewed more than ninety hours before their DISPFROM.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/recruit2_view_delta.png""><img class=""aligncenter size-full wp-image-5261"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/recruit2_view_delta.png"" alt=""recruit2_view_delta"" width=""552"" height=""448"" /></a>
​
Simply counting the number of times a user has viewed a test set coupon is tremendously helpful in forecasting test set purchases. As shown in the left panel of the figure below, users are 2.5% likely to buy a coupon if they've viewed it exactly once prior to its DISPFROM, but that probability rises to 32% if they've viewed the coupon four or more times.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/recruit2_conditions-e1445448589721.png""><img class=""aligncenter size-full wp-image-5262"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/recruit2_conditions-e1445448589721.png"" alt="""" width=""614"" height=""266"" /></a>

Second, users tend to buy the same coupons over and over. As shown in the middle panel of the above figure, a user who has purchased a coupon with a given prefecture, genre, and catalog price four or more times has a 38% chance of buying a matched coupon again in the next week if it is offered for sale.

Third, peer group averages can help forecast the behavior of users with little or no history. The right panel of the above figure shows that a user's probability of buying a coupon increases from less than 0.1% to above 0.6% if more than ten percent of age, sex, and geography-matched peers have bought a coupon with the same characteristics.

Fourth, it's important to consider the geographic coverage of each coupon. To be specific, a coupon is relevant for the multiple prefectures listed in coupon_area_train.csv, not just the single prefecture listed for that coupon in coupon_list_train.csv. In the kernel density plots below, I show the purchase intensity for users based in four prefectures: Tokyo, Kanagawa, Osaka, and Aichi, using the geographic data in coupon_list_train.csv. The purchases for Osaka and Aichi users appear strongly bimodal, with an unusually large number of purchases occurring in the Tokyo region.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/recruit2_lat_lon_pref_naive.png""><img class=""aligncenter size-full wp-image-5263"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/recruit2_lat_lon_pref_naive.png"" alt="""" width=""475"" height=""521"" /></a>

On the other hand, if we look at all the prefectures that map to a given coupon, we find that Osaka users purchased Tokyo coupons not because they planned to travel to Tokyo, but because these coupons were also local to Osaka. If we plot the geographic intensity of ""nearest-to-user"" prefecture rather than a coupon's primary listing prefecture, we see much more localized purchase behavior.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/recruit2_lat_lon_pref_improved.png""><img class=""aligncenter size-full wp-image-5264"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/recruit2_lat_lon_pref_improved.png"" alt="""" width=""474"" height=""513"" /></a>
<h2>Words of Wisdom</h2>
<h3>Do you have any advice for those just getting started in data science?</h3>
Focus on understanding the problem. Without understanding the problem, it's impossible to develop a solution.

Start with simple approaches and models. A fast development cycle is key to testing out ideas and learning what works. Don't start building computationally expensive ensembles until you have iterated through most of your best ideas.
<h2>Bio</h2>
<strong><a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-21-at-10.54.25-AM.png""><img class="" wp-image-5267 alignright"" style=""margin-left: 5px; margin-top: 6px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-21-at-10.54.25-AM.png"" alt="""" width=""118"" height=""119"" /></a>Halla Yang</strong> has worked as a quantitative researcher, portfolio manager and trader at <a href=""https://assetmanagement.gs.com/content/gsam/worldwide/en/gateway.html"" target=""_blank"">Goldman Sachs Asset Management</a>, <a href=""http://www.jumptrading.com/"" target=""_blank"">Jump Trading</a> and <a href=""http://www.arrowstreetcapital.com/"" target=""_blank"">Arrowstreet Capital</a>. He holds a Ph.D. in Business Economics from <a href=""http://www.harvard.edu/"" target=""_blank"">Harvard</a>, and a B.A. in Physics, summa cum laude, also from Harvard. He is about to start a new position as data scientist at a management consulting firm."
Passing the Tests: Strategies Used in CERN's Flavour of Physics,http://blog.kaggle.com/2015/10/27/passing-the-tests-strategies-used-in-cerns-flavour-of-physics/,2015-10-27 23:26:46,"<h5><a href=""https://www.kaggle.com/vicensgaitan"" target=""_blank"">Vicens Gaitan</a> participated in the <a href=""https://www.kaggle.com/c/flavours-of-physics"" target=""_blank"">Flavours of Physics: Finding τ → μμμ</a> challenge, finishing near 14th place (final competition results are still being validated). After the competition close, he spent time researching how other participants handled this complex challenge.</h5>
<h5>In this blog, Vicens walks us through a series of scripts he created that share different methods competitors used to pass the <a href=""https://www.kaggle.com/c/flavours-of-physics/details/agreement-test"" target=""_blank"">Agreement</a> and the <a href=""https://www.kaggle.com/c/flavours-of-physics/details/correlation-test"" target=""_blank"">Correlation / CVM</a> tests while achieving a high overall score. Vicen's background in physics (including time with the <a href=""http://home.cern/about/experiments/aleph"" target=""_blank"">ALEPH experiment</a> at LEP) has helped him understand and explain certain nuances of the competition data and design. We'll let Vicens take it from here...</h5>
[caption id=""attachment_5284"" align=""aligncenter"" width=""177""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/taumumumu.png""><img class=""size-full wp-image-5284"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/taumumumu.png"" alt=""706 data scientists on 673 teams competed"" width=""177"" height=""116"" /></a> 706 data scientists on 673 teams competed[/caption]

<a href=""https://www.kaggle.com/c/flavours-of-physics"" target=""_blank"">Flavours of Physics: Finding τ → μμμ</a> was a challenging problem from both a machine learning perspective (due the side constraint of trying to preserve the model's “physical soundness”), as well as from the pure physics analysis methodology (of mixing real data and Monte Carlo data for event selection).

The scope of this project probably goes beyond a reasonable setup for an open competition (not only for high energy people) and this justifies the simplification in physics tests made by the organizers.

During the first weeks of the competition, it was clear that the control signal can be separated from the train sample with high efficiency making the <a href=""https://www.kaggle.com/c/flavours-of-physics/details/agreement-test"" target=""_blank"">agreement test</a> useless. (Meaning, it would not be hard to ""cheat"" and get around the agreement test.) Because of this the organizers of the competition announced they would manually confirm models physical soundness. So the question then was: how can you perform well without discriminating in an explicit way between both samples?

This problem was looming large during the competition, raising doubts about the coherence of the stated problem, but a few days before the competition’s end some submissions with a lot of 9’s stated that something more fundamental had gone unnoticed by most of us: The mother particle's mass is a strong feature and boosted the classifier performance. In fact, it can be calculated with the available information. Therefore the question is now: If this is such a strong feature, why isn't the classifier discovering the formula for the tau mass? This is the topic covered in the first <a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/why-is-so-hard-to-learn-the-tau-mass"" target=""_blank"">notebook</a>:
<h3><a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/why-is-so-hard-to-learn-the-tau-mass"" target=""_blank"">Why is it so hard to 'learn' the tau mass from kinematics?</a></h3>
[caption id=""attachment_5277"" align=""aligncenter"" width=""614""]<a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/why-is-so-hard-to-learn-the-tau-mass"" target=""_blank""><img class=""size-full wp-image-5277"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/bigtrap-e1445980419100.png"" alt=""This plot shows the “Big Trap”: mass separates simultaneously signal vs background, and train vs agreement, but prevents you from passing the CVM test"" width=""614"" height=""435"" /></a> This plot shows the “Big Trap”: mass separates simultaneously signal vs background and train vs agreement, but prevents you from passing the CVM test. See the full <a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/why-is-so-hard-to-learn-the-tau-mass"" target=""_blank"">script</a>[/caption]

If we use the tau mass as a variable, then the result will be highly correlated with mass, preventing us from passing the <a href=""https://www.kaggle.com/c/flavours-of-physics/details/correlation-test"" target=""_blank"">CVM test</a>. (Curiously, nobody complained about this test until the last days of the competition). That makes the question: How can we bypass the CVM test (and the agreement test).

The second <a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/clipping-spreading"" target=""_blank"">workbook</a> shows how to hack both tests with a single trick. (The trick is not mine. I realized it after analyzing what my solution had in common the winning solutions post-competition :) :
<h3><a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/clipping-spreading"" target=""_blank"">Clipping &amp; Spreading…</a></h3>
[caption id=""attachment_5278"" align=""aligncenter"" width=""633""]<a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/clipping-spreading/code"" target=""_blank""><img class=""wp-image-5278 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/cvmdrop.png"" alt="""" width=""633"" height=""394"" /></a> Raising the probability to a high power (clipping to 0 small probabilities &amp; spreading high probabilities between 0-1) adds a small amount of noise &amp; the CVM value drops drastically. See the full <a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/clipping-spreading"" target=""_blank"">script</a>[/caption]

After discovering this exploit, the next question is: what can be done with this data that neither uses the previous hack nor discriminates agreement events from the training ones.

The solution is what I call ""<a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/or-doping"" target=""_blank"">doping</a>"":
<h3><a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/or-doping"" target=""_blank"">…or Doping?</a></h3>
One promising possibility (outside of the competition rules) is to discourage the classifier from learning “imperfect simulated” features as real discriminating variables by “doping” the training sample with a small set of Monte Carlo events from a different channel. You need to label the events as “background” (so, as real data in this case).

[caption id=""attachment_5281"" align=""aligncenter"" width=""630""]<a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/or-doping"" target=""_blank""><img class=""wp-image-5281 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/nodope2.png"" alt="""" width=""630"" height=""450"" /></a> Without doping, we can reach a weighted AUC of ~.98, but we would fail the agreement test with KS because the KS score is larger than .09. See the full <a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/or-doping"" target=""_blank"">script</a>[/caption]

[caption id=""attachment_5282"" align=""aligncenter"" width=""628""]<a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/or-doping"" target=""_blank""><img class=""wp-image-5282 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/dope.png"" alt=""dope"" width=""628"" height=""451"" /></a> By adding 3,460 doping events to the training set, we can get a weighted AUC score of ~.98 and we will pass the KS test with a KS score smaller than .09. See the full <a href=""https://www.kaggle.com/vicensgaitan/flavours-of-physics/or-doping"" target=""_blank"">script</a>[/caption]

This procedure obtains a modest 0.98912 on the public leaderboard, but provides a classifier not relying on MC imperfections and is not correlated with the mass. It is probably useful for event selection.
<h3>How did your background in physics help you understand the problem and the competition objective?</h3>
It was very useful. Probably the features have no meaning to people outside High Energy Physics, and as it has been shown, domain knowledge can provide insights (for example, the role of mass in this problem) that very sophisticated classifiers are not able to catch. Deep neural networks probably had an opportunity to build a mass-like relation, (this is an exercise I want to try), but remember that the mass of a tau particle in an experiment with energies of Tev is a tiny effect and is hard to discover if you don’t know where to look at.
<h3>What are your key takeaways from the competition?</h3>
A lot of them. Probably the most dramatic is that you cannot rely only in the power of machine learning: looking at the data through the glass of a model can expose facts hard to discover by general function approximation. Another one: a nostalgic feeling of similar problems, many years ago, using neural networks for event selection in the <a href=""http://home.cern/about/experiments/aleph"" target=""_blank"">ALEPH experiment</a> at LEP. And the most important: the lessons learned from my fellow competitors.
<h3>What was your key insight into the data?</h3>
Don’t trust the Monte Carlo simulations. At least not fully. Monte Carlo calibration is the key step to use machine learning tools for event selection. “Weak” learners are relatively insensitive to this little imperfections in the simulation, but “hard” ones (like Gradient Boosting Machines) will pick discrepancy as a real separating fact.
<h3>Why is it so important to calculate the mass of the tau particle?</h3>
No, no… ;) The important fact is to not to calculate the tau mass. Not in this competition because the classifier efficiency is fully based on an artifact in the training set, allowing us to separate signal and background only by the mother particle's mass.

I’m joking. The only way to perform well in this competition is to realize this fact and to be able to bypass the side band test with clever tricks.

Congratulation to the winners, they realized all of this before the competition ended. I’m only analyzing their findings post-mortem. It has been very fun!
<h3>Bio</h3>
<strong>Vicens Gaitan</strong> is  R&amp;D director in the <a href=""http://www.aia.es/"" target=""_blank"">Grupo AIA</a> innovation area. He studied physics and get a PhD in Machine Learning applied to experimental High Energy Physics in 1993 with the ALEPH collaboration at CERN. Since 1992 he has been working in AIA in complex problem solving and algorithmic development applied to model estimation, simulation, forecasting and optimization, mainly in the energy, banking and telecommunication sectors.

<hr />



<hr />

&nbsp;

Check out more posts on the Flavour of Physics competition by clicking the tag below!

&nbsp;"
"Dato Truly Native? Winner's Interview: 2nd place, mortehu",http://blog.kaggle.com/2015/10/30/dato-winners-interview-2nd-place-mortehu/,2015-10-30 21:22:10,"In <a href=""https://dato.com/"" target=""_blank"">Dato's</a> <a href=""https://www.kaggle.com/c/dato-native"" target=""_blank"">Truly Native? competition</a> Kagglers were given the HTML of webpages on <a href=""http://www.stumbleupon.com/"" target=""_blank"">StumbleUpon</a> and challenged to identify paid content (in the form of native advertising) from unpaid content. Morten Hustveit finished in second place out of 340 data scientists on 274 teams. His previous work researching and building a text classifier program for HTML documents gave him a unique competitive edge.

[caption id=""attachment_5288"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-30-at-10.55.43-AM.png""><img class=""wp-image-5288 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-30-at-10.55.43-AM-300x142.png"" alt=""Mortehu's profile on Kaggle"" width=""300"" height=""142"" /></a> Kaggle <a href=""https://www.kaggle.com/mortehu"" target=""_blank"">profile</a> for Morten (aka Mortehu)[/caption]
<h2>Background &amp; Tools</h2>
Day-to-day I work for a venture capital firm called <a href=""http://e.ventures/"" target=""_blank"">e.ventures</a>. Part of my job there is to develop tools for helping our analysts discover companies that would make good investments. In the weeks before the “<a href=""https://www.kaggle.com/c/dato-native"" target=""_blank"">Truly Native?</a>” competition began, I had been developing a tool for classifying HTML documents based on their contents. We already use this for a variety of purposes internally. The competition used a very similar type of data, so it was only a matter of writing <a href=""https://github.com/mortehu/text-classifier/blob/master/examples/kaggle-dato-native.py"" target=""_blank"">a small Python script</a> for importing the data to get going.

My text classifier program was inspired by <a href=""http://crm114.sourceforge.net/wiki/doku.php"" target=""_blank"">CRM114</a>, an open source program used primarily to identify e-mail spam. At first I just wanted to use that program as-is, but it turned out to have scaling issues that could not be easily overcome. It could not finish processing even 10% of our data in two hours, and I wanted to build many models on all the data every day. The CRM114 team participated at several TRECs (Text REtrieval Conferences), most recently in 2007, and documented the performance of their various classifiers. Based on this, I decided building a linear SVM model on skip-gram features would be the best way to go. This means pairs of nearby words are converted into a feature by hashing, and each document is represented as a set of these features.

Back in 2006 I was attempting to implement my own web browser from scratch, being dissatisfied with the speed of web browsers at the time. Like some other overly ambitious projects I’ve started, this was put “on hold” after a couple of months. Even so, I was left with a fast HTML parser, which could be reused for this project. In addition to the skip-grams mentioned above, the HTML parser allowed generating features based on simple semantic relationships expressed in the documents. The end result was a setup that could unzip, parse and generate features for all 400,000 HTML documents in less than five minutes. Counting only features occurring in at least two training documents, this resulted in 79 million features.

Not all SVM training programs can build a model with 79 million features in a reasonable timeframe. A paper called “<a href=""https://www.csie.ntu.edu.tw/~cjlin/papers/cddual.pdf"" target=""_blank"">A Dual Coordinate Descent Method for Large-scale Linear SVM</a>”, by Cho-Jui Hsieh et al, describes a training algorithm that works well with this data size. A C++ implementation of that algorithm is available in <a href=""https://cran.r-project.org/web/packages/LiblineaR/index.html"" target=""_blank"">LIBLINEAR</a>, under the name L2R_L2LOSS_SVC_DUAL. However, LIBLINEAR is designed as a user-friendly library, and makes some compromises in how it represents data internally, leading to a high memory overhead. The algorithm requires randomly shuffling the training data, so it’s critical that all of it can fit in main memory. The feature matrix requires more than 25 gigabytes of storage space when using a compact representation, so I decided to reimplement the algorithm to work with minimal memory overhead. The training time for this model was 32 minutes, not counting hyperparameter optimization.

One thing bothering me about many machine learning tools is that they don’t have built-in functionality for hyperparameter optimization. Users are left to build their own system, or to manually edit configuration files. I ensured my text classifier would automatically select the per-class value for C using some basic heuristics and 10-fold cross validation. Integrating hyperparameter search into the training tool itself has the advantage of being able to train all ten folds in parallel using only one in-memory copy of the data, and to use past solutions as the initial state when training with new hyperparameters.
<h2>Competitive Approach</h2>
As is common on Kaggle, competition stiffened in the last couple of weeks before the deadline, and I realized I couldn’t win using just this simple linear model. On the private leaderboard it would have scored only 0.98519, and on September 28th user <a href=""https://www.kaggle.com/gkoundry"" target=""_blank"">Glen</a> had already made a submission scoring 0.98801. I needed a second model.

When I was hired by e.ventures I did not have any machine learning experience, so they provided me with a crash course by Timo Duchrow, who organized the <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge"" target=""_blank"">Otto Group Kaggle competition</a>. He introduced me to the idea of using frequent substrings as features, which has been successfully applied in his old field, bioinformatics. When you first start looking at the problem of counting substring occurrences, you realize that an N character document has N(N+1)/2 unique substrings. Enumerating all the substrings of a gigabyte of text would take longer than the duration of this competition, so what is a programmer to do?

The approach I used involves constructing an alphabetically sorted list of all substrings, called a suffix array, which can be done in O(N) time. From there it’s easy to skip all substrings that only occur once, which usually is the vast majority. The remaining strings can be enumerated and processed in a tiny fraction of the time. This requires a lot of RAM, so I only processed a one gigabyte sample of the training data this way; 512 MB of positive examples, and 512 MB of negative examples. Substrings were counted only once for each document they occurred in, and most redundant substrings were deleted. The running time was about ten minutes.

With the most predictive substrings extracted, I selected the top 30,000 to use in my model. I’m a big fan of automatic feature selection, because this makes the model design process more repeatable and generally applicable. However, I decided to also include one manually crafted feature; 10-digit integers looking like Unix time. Since the competition organizers had largely downloaded the positive and negative training examples on different dates, date related hints inside the documents were very strong features, including references to photographs of Pluto, Donald Trump’s media appearances, and El Chapo’s escape from prison. Had the documents been downloaded in random order, the top score on the leaderboard would probably have been far lower. These numbers were present in about 25% of the training data.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/dato2_epoch.png""><img class=""aligncenter size-full wp-image-5287"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/dato2_epoch.png"" alt=""dato2_epoch"" width=""616"" height=""462"" /></a>

Using the 30,000 substrings and the numeric feature, I trained an <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> model with 2000 trees, doing almost no hyperparameter optimization. This took almost 5 hours. This model alone scores 0.98856 on the private leaderboard. My final submission was a simple linear combination of these two models: the predictions from the SVM model multiplied by 6 added to the predictions of the XGBoost model, resulting in a score of 0.99017. The scaling factor was selected, rather arbitrarily, to make the range of each model roughly the same based on a scatterplot of the predictions from the two models:

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/10/dato2_scatter.png""><img class=""aligncenter size-full wp-image-5286"" src=""http://blog.kaggle.com/wp-content/uploads/2015/10/dato2_scatter.png"" alt=""dato2_scatter"" width=""616"" height=""462"" /></a>

Both the text classifier and substring counter I used are available on <a href=""https://github.com/mortehu/"" target=""_blank"">my GitHub page</a>, under the terms of the GNU General Public License."
"Profiling Top Kagglers: Gilberto Titericz, New #1 in the World",http://blog.kaggle.com/2015/11/09/profiling-top-kagglers-gilberto-titericz-new-1-in-the-world/,2015-11-09 19:28:10,"Kaggle has a new #1 data scientist! <a href=""https://www.kaggle.com/titericz"" target=""_blank"">Gilberto Titericz</a> usurped <a href=""https://www.kaggle.com/owenzhang1"" target=""_blank"">Owen Zhang</a> to take the title of #1 Kaggler after his team finished <a href=""https://www.kaggle.com/c/springleaf-marketing-response/leaderboard/private"" target=""_blank"">2nd</a> in the <a href=""https://www.kaggle.com/c/springleaf-marketing-response"" target=""_blank"">Springleaf Marketing Response</a> competition. As part of our series <a href=""http://blog.kaggle.com/tag/profiling-top-kagglers/"" target=""_blank"">Profiling Top Kagglers</a>, we interviewed Gilberto to learn more about his background and how he made his way to the top of the Kaggle community.

[caption id=""attachment_5298"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/gilberto_profilebadge.png""><img class=""size-medium wp-image-5298"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/gilberto_profilebadge-300x143.png"" alt=""Gilberto's profile on Kaggle"" width=""300"" height=""143"" /></a> Gilberto's <a href=""https://www.kaggle.com/titericz"" target=""_blank"">profile</a> on Kaggle[/caption]
<h2>Gilberto Titericz Q&amp;A</h2>
<h3>How did you start with Kaggle competitions?</h3>
I am an electronic engineer, but I always had interest in machine learning algorithms. In 2012 I attended the Google AI challenge and when it finished I looked for another way of competing online and found Kaggle via a Google search and I loved it. My first competition was <a href=""https://www.kaggle.com/c/GEF2012-wind-forecasting"" target=""_blank"">Global Energy Forecasting Competition 2012 - Wind Forecasting</a> and I luckily placed 3rd using a bag of <a href=""http://www.mathworks.com/products/matlab/"" target=""_blank"">Matlab</a> neural networks. Since then, I've tried to learn more from each competition.
<h3>What is your first plan of action when working on a new competition?</h3>
First of all it is very important understand the problem, the features, and the evaluation metric. After that I look for a good cross validation strategy. Those are the very basic and essential steps. Only after that you can start thinking about feature engineering and training algorithms that match the problem.
<h3>What does your iteration cycle look like?</h3>
I always prepare the dataset and apply feature engineering as much as I can, then I choose a training algorithm and optimize hyperparameters based on a cross validation score. If a model is good and stable I save the trainset and testset predictions. Then I start all over again using another training algorithm or model. When I have a handful of good model predictions, I start ensembling at the second level of training.
<h3>What are your favorite machine learning algorithms?</h3>
<a href=""https://en.wikipedia.org/wiki/Gradient_boosting"" target=""_blank"">Gradient Boosting Machines</a> are the best! It is amazing how GBM can deal with data at a high level of depth. And some details in an algorithm can lead to a very good generalization. GBM is great dealing with linear and non-linear features, also it can handle dense or sparse data. So it's a very good algorithm to understand core parameters and it's always a good choice if you are not sure what algorithm to use. Before I knew of GBM, I was a big fan of neural networks.
<h3>What are your favorite machine learning libraries?</h3>
I have many favourite libraries, but I will try to rank them according the ones I use most:
- <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> - Fast and optimized GBM implementation.
- <a href=""https://github.com/JohnLangford/vowpal_wabbit/wiki"" target=""_blank"">Vowpal Wabbit</a> - Very fast with lots of parameters and linear algorithms.
- <a href=""https://github.com/srendle/libfm"" target=""_blank"">LibFM</a> - Factorization Machines.
- <a href=""http://scikit-learn.org/stable/"" target=""_blank"">scikit-learn</a> - Lots of functions and algorithms.
- <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a> - Very good and fast NN implementation (but unfortunately I don't have a GPU).
- <a href=""http://www.mathworks.com/products/neural-network/"" target=""_blank"">Matlab Neural Network Toolbox</a> - Classic.
- R <a href=""https://cran.r-project.org/web/packages/gbm/index.html"" target=""_blank"">GBM</a>, <a href=""https://cran.r-project.org/web/packages/randomForest/index.html"" target=""_blank"">randomForest</a> and <a href=""https://cran.r-project.org/web/packages/glm2/index.html"" target=""_blank"">glm</a> - Classic.
<h3>What is your approach to hyper-tuning parameters?</h3>
I tried grid search in the past, but I found that it consumes a lot of time. So I often use manual brute force searching based on my previous experience. And I always use cross validation to consistently improve the evaluation scores.
<h3>What is your approach to solid CV/final submission selection and LB fit?</h3>
I always trust my local cross validation score, but in some competitions when the public testset has enough samples and the data is not very noisy I will trust both CV and LB. A good scoring formula I've used before is:
<pre>[(LocalCVscore*number_rows_trainset) + (LBscore*number_of_rows_used_to_calculate_LB)] / (sum_of_number_of_rows_in_CV_and_LB )</pre>
So for my two final selections I use the formula above if the LB is stable. If not, I usually choose my best reliable Local CV model and the best LB submission model.
<h3>In a few words: What wins competitions?</h3>
Some good models and features, consistent cross validation methods, teaming up, ensemble meta models, hard work(~15h/week), and always ""Luck""!!!

[caption id=""attachment_5297"" align=""aligncenter"" width=""413""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/gilberto_top8finishes.png""><img class=""size-full wp-image-5297"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/gilberto_top8finishes.png"" alt=""Gilberto's top 8 finishes"" width=""413"" height=""212"" /></a> Gilberto's top 8 finishes[/caption]
<h3>What is your favourite Kaggle competition and why?</h3>
<a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge"" target=""_blank"">Otto</a> is my favourite. Not only because I won, but in this competition it was possible to apply various methods of machine learning and ensembling techniques. In addition, it was the biggest competition in terms of number of competitors in Kaggle's history.

[caption id=""attachment_5299"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-09-at-11.29.22-AM.png""><img class=""wp-image-5299 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-09-at-11.29.22-AM-e1447097462674.png"" alt=""Final private leaderboard for the Otto Product Classification Challenge"" width=""500"" height=""345"" /></a> Final private leaderboard for the Otto Product Classification Challenge[/caption]
<h3>What was your least favourite Kaggle competition experience?</h3>
Competitions with very small trainset datasets. It makes cross validation and final results unstable. Now I just avoid noisy and small datasets, because it's very hard to apply feature engineering and cross validation efficiently.
<h3>What field in machine learning are you most excited about?</h3>
Nowadays I'm very interested in deep learning. I think it can still surprise us in the future.
<h3>Which machine learning researchers do you study?</h3>
Most of what I learn comes from Kaggle <a href=""https://www.kaggle.com/competitions"" target=""_blank"">competitions</a>, <a href=""https://www.kaggle.com/forums"" target=""_blank"">forums</a>, open source code, and <a href=""https://www.kaggle.com/scripts"" target=""_blank"">scripts</a>.
<h3>Can you tell us something about the last algorithm you hand-coded?</h3>
I prefer to use open source algorithms and I spend most of my time tuning models. Algorithms I usually hand-code are simple, like feature selection or some kind of training automation.
<h3>How important is domain expertise for you when solving data science problems?</h3>
It depends on the problem. For example, for the <a href=""https://www.kaggle.com/c/inria-bci-challenge"" target=""_blank"">BCI Challenge NER 2015</a>, I'm sure you had to be an expert in the subject to be well placed. The same for the last two CERN competitions (<a href=""https://www.kaggle.com/c/flavours-of-physics"" target=""_blank"">Flavour of Physics</a> &amp; <a href=""https://www.kaggle.com/c/higgs-boson"" target=""_blank"">Higgs Boson ML Challenge</a>). If you knew physics, you had a big advantage over others.

But I'm not an expert in the subject matter of all of my winning competitions. Some examples are: Otto Group, Caterpillar, Fast Iron, AMS 2013-2014 Solar Energy Prediction Contest, Springleaf and GEFCom2012. So I believe for most of the problems you can do well even if you aren't an expert.
<h3>What do you consider your most creative trick/find/approach?</h3>
I don't have that creative trick, but if I can say what I use that gives me more spots it is a second training level ensemble technique. Some people also call that: ""stacking models"", but it's all the same. That technique depends a lot of correct cross validation and generation of non over fitted meta features. Also it can increase a lot the performance if meta features are generated using different features and training algorithms, because L2 can extract only the best predictions of each meta feature.

[caption id=""attachment_5301"" align=""aligncenter"" width=""486""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-09-at-11.36.03-AM.png""><img class=""size-full wp-image-5301"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-09-at-11.36.03-AM.png"" alt=""Example of stacking models, taken from ML Kaggle Ensembling Guide"" width=""486"" height=""315"" /></a> Example of stacking taken from the <a href=""http://mlwave.com/kaggle-ensembling-guide/"" target=""_blank"">MLWave Kaggle Ensembling Guide</a>[/caption]
<h3>How are you currently using data science at work and does competing on Kaggle help with this?</h3>
I work as an automation engineer in a oil refinery. So I don't have much chance to use data science for it. But I can say I'm looking foward to change it on next year. And I can say that most of what I know today about data science I owe it to Kaggle.
<h3>What is your opinion on the trade-off between high model complexity and training/test runtime?</h3>
That a good question. I confess that I just care about training runtime. Model complexity must be proportional to the CPU power you have and the perfomance that you want. So I always start using simplest dataset as possible and take notes of its training time and performance. After that a good approach in to increase model complexity and note if it increase performance and times. A good tradeoff between complexity and time depends a lot of how much time you have to spend increasing complexity of model and waiting the training time. Usually models that you can build and train in just a few hours are the best, because you will need time to fine tune parameters and it's normal having to train it a lot of time.
<h3>What is your view on automating Machine Learning?</h3>
Automation is good for preprocess some dataset conditions, cross validation folds, model selection, feature selection and hyperparameters search.
I think all other processes are better if have human interference.
<h3>How did you get better at Kaggle competitions?</h3>
- Joinning a lot of competitions.
- Learning from Kaggle forums and scripts.
- Trying new algorithms and techniques.
- Teaming up with experienced Kagglers.
- Study competitions code and methods from winners.
- After each competition update your personal helping functions library.
- Dedication. Sometimes when coding I share my laptop screen with my two daughters, they love cartoons.
<h3>What did you learn from doing Kaggle competitions?</h3>
Kaggle in an excellent place to learn about Data Science. It offers many kind of datasets covering a vast type of problems. I learned a lot after those three years competing and I can say it is addictive. Specially if you are that type of person that like to see the machine learning magic in action.
<h2>Bio</h2>
<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-09-at-11.52.41-AM.png""><img class="" wp-image-5302 alignright"" style=""margin-top: 5px; margin-left: 10px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-09-at-11.52.41-AM.png"" alt="""" width=""148"" height=""150"" /></a>I have a MS in Electronic Engineering. In the past 16 years I've been working as an Electronic Engineer for big multinationals like <a href=""http://www.siemens.com/entry/cc/en/"" target=""_blank"">Siemens</a>, <a href=""http://www.nokia.com/en_int"" target=""_blank"">Nokia</a>, and later as an Automation Engineer for <a href=""http://www.petrobras.com/en/home.htm"" target=""_blank"">Petrobras Brasil.</a> In 2008 I started learning Data Science by myself, at first to improve personal stock market gains. In 2012 I joined Kaggle.com and started to continually learn more. <em>See Gilberto's <a href=""https://www.linkedin.com/in/gilberto-titericz-jr-6601357"" target=""_blank"">LinkedIn</a>.</em>"
"Flavour of Physics Winner's Interview: 3rd place, Josef Slavicek",http://blog.kaggle.com/2015/11/23/flavour-of-physics-winners-interview-3rd-place-josef-slavicek/,2015-11-24 00:56:43,"The <a href=""https://www.kaggle.com/c/flavours-of-physics"" target=""_blank"">Flavour of Physics competition</a> challenged Kagglers to identify a rare decay phenomenon (<em>τ<sup>-</sup> → μ<sup>+</sup>μ<sup>-</sup>μ<sup>-</sup></em> or τ → 3μ) to help establish proof of ""new physics"". The competition was an exciting opportunity for the community to collaborate with scientists from the <a href=""http://lhcb-public.web.cern.ch/lhcb-public/"" target=""_blank"">LHCb experiment</a> at <a href=""http://home.cern/"" target=""_blank"">CERN</a>. 706 data scientists on 673 teams participated, grappling with the complex subject matter and unusual competition setup. Josef Slavicek finished 3rd with the help of XGBoost (and a lucky typo). Below he explains the competition design, its potential weaknesses, and his wild path to the top of the leaderboard.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
For me, it all started with the great Stanford online course ""<a href=""https://www.udacity.com/course/intro-to-artificial-intelligence--cs271"" target=""_blank"">Introduction to artificial intelligence</a>"" led by Peter Norvig and Sebastian Thrun. (The course is now on <a href=""https://www.udacity.com/"" target=""_blank"">Udacity</a>.) I wish to thank both the professors because they really changed my life. Since then, I've taken several more online courses on <a href=""https://www.coursera.org/"" target=""_blank"">Coursera</a> and Udacity. And in 2013, I started to compete on Kaggle.

[caption id=""attachment_5311"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-23-at-5.21.55-PM.png""><img class=""size-medium wp-image-5311"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-23-at-5.21.55-PM-300x144.png"" alt=""Josef's profile on Kaggle"" width=""300"" height=""144"" /></a> Josef's <a href=""https://www.kaggle.com/josefslavicek"" target=""_blank"">profile</a> on Kaggle[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
The domain of this competition was high energy physics. I studied physics when I was young. Unfortunately, it was not in this century and I've forgotten almost everything I learned then. One of few things I still remember is picture of ant walking on the surface of apple, which is an image I have seen on the cover of one mysterious big black book.

[caption id=""attachment_5313"" align=""aligncenter"" width=""240""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/gravitation.jpg""><img class=""wp-image-5313 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/gravitation-240x300.jpg"" alt=""gravitation"" width=""240"" height=""300"" /></a> Edit: I looked now at the book again after all these years and I found that I remembered it wrong: there is no ant, only an apple and a magnifying glass.[/caption]

So, this is present state of my domain knowledge. Not surprisingly, I didn't attempt to use it in the competition.
<h3>What made you decide to enter this competition?</h3>
The competition gave me chance to participate in something really great - interpreting data from the <a href=""http://lhcb-public.web.cern.ch/lhcb-public/"" target=""_blank"">LHCb experiment</a> at <a href=""http://home.cern/"" target=""_blank"">CERN</a>. To touch the border of human knowledge. For me, there was nothing to decide about. It was clear that I must try it.
<h2> Let's Get Technical</h2>
<h3>What made this competition unusual?</h3>
Let me first briefly describe some unusual properties of this competition. In common classification tasks, we have a dataset containing positive and negative examples and we are asked to train classifiers which can distinguish between them. In the Flavour of Physics competition, the situation was more complex.

The negative examples were actual sensor data from LHCb experiment. The problem is that for positive examples no such real data exists. The goal of the competition was to create a detector of new physics events which hadn't been observed yet. To make this possible, the dataset contained simulated positive examples. An obvious problem with this approach was that if the simulation introduced some easily detectable artifacts into the data, we could easily end up with a classifier distinguishing simulatedEvent X realEvent (instead of the desirable interestingEvent X notInterestingEvent).

To prevent this, the organizers introduced a statistical ""<a href=""https://www.kaggle.com/c/flavours-of-physics/details/agreement-test"" target=""_blank"">Agreement test</a>"" on a separate data set. The agreement dataset contained both simulated and real data for another kind of physical event which was already known to exist and already observed.

The second unusual test in this competition was the so called ""<a href=""https://www.kaggle.com/c/flavours-of-physics/details/correlation-test"" target=""_blank"">Correlation test</a>"". For some reason, in high energy physics, the mass of a particle is considered to be an unreliable feature to detect the actual type of particle. So the Correlation test is designed to prove that the results of our classifiers are not highly correlated with particle mass.

[caption id=""attachment_5307"" align=""aligncenter"" width=""629""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-23-at-5.02.53-PM.png""><img class=""size-full wp-image-5307"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-23-at-5.02.53-PM.png"" alt=""Below is an illustration of this correlation test. On the left shows no correlation between prediction and mass, while on the right there is correlation."" width=""629"" height=""181"" /></a> An illustration of the correlation test. On the left shows no correlation between prediction and mass while on the right there is correlation.[/caption]

The third unusual property of this competition was the evaluation metric. It was ""weighted area under the ROC curve"", as described <a href=""https://www.kaggle.com/c/flavours-of-physics/details/evaluation"" target=""_blank"">here</a>.

[caption id=""attachment_5305"" align=""aligncenter"" width=""461""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-23-at-4.47.09-PM.png""><img class=""size-full wp-image-5305"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-23-at-4.47.09-PM.png"" alt=""The ROC curve is divided into sections based on the True Positive Rate (TPR)."" width=""461"" height=""323"" /></a> The ROC curve is divided into sections based on the True Positive Rate (TPR).[/caption]

Note that a perfect score of 1.0 can be achieved even with a far-from-perfect classifier: because of weighting, it doesn't matter what happens above TPR=0.8. In other words, if your classifier can classify the first 80% of positive examples perfectly without a single false positive, it can fail on the rest without penalty.
<h3>How did you spend your time on this competition?</h3>
Now I can tell the crazy story of the development of my classifier. I started with small and simple neural networks: a single hidden layer of 64 units, all units logistic, learned by backpropagation. Performance of this NN was poor, so I downloaded the introductory <a href=""https://www.kaggle.com/benhamner/flavours-of-physics/rf-xgboost-example"" target=""_blank"">xgboost example</a> from the <a href=""https://www.kaggle.com/c/flavours-of-physics/scripts"" target=""_blank"">competition scripts</a> and tried to average the results of <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">xgboost</a> with the results of my NN. The performance was slightly better than the xgboost alone, scoring around 20th place on LB.

Then I spent weeks trying to improve on it with not much success. As the end of competition approached, someone made available a more sophisticated xgboost script. (I believe, but I'm not sure, that it was <a href=""https://www.kaggle.com/justfor/flavours-of-physics/gridsearchcv-with-feature-in-xgboost"" target=""_blank"">this</a>).

When I combined it with result of my NN, wonders started to happen. The first breakthrough move I discovered was when I tried to preprocess the xgboost input data with PCA (without dimensionality reduction, only to zero-mu, unit-sigma normalize and decorrelate the data). For some reason, xgboost was able to achieve an almost perfect ROC score when applied to preprocessed data. However, it failed desperately on both agreement and correlation tests. But averaging this result with NN with ratios
<pre>0.1 * xgboostResult + 0.9 * NNResult</pre>
produced predictions that passed the tests and still achieved a very good ROC score (at one moment I was at first place on LB). Another small improvement I obtained was when I trained simple mass estimator and added this estimated mass as an additional input (particle mass was known for the training data, but not for the test data).

The second key trick I found really accidentally in the last hours of the competition. By a typo error, I applied strange exponentiation to my averaging formula:
<pre>q * xgboostResult ^ exponent + (1 - q) * NNResult</pre>
The result was surprising for me and until the end of competition I didn't understand how it worked internally. I only observed that:
<ol>
	<li>with high values of exponent, it is possible to use much higher values of q and still pass the the agreement and correlation test.</li>
	<li>with high values of exponent, the ROC score of the averaged result is closer to the (almost perfect) score of xgboostResult even for smaller values of q.</li>
</ol>
With the exponent trick, my solution achieved a perfect score on public LB and placed third on the private LB.

After the end of competition I simplified my solution to the level where it can be run as this Kaggle <a href=""https://www.kaggle.com/josefslavicek/flavours-of-physics/simplified-version-of-my-solution"" target=""_blank"">script</a>.

This simplified version contains no mass estimator and no neural networks. It consists only of two gradient boosted trees and produces slightly worse results than my original solution. However, the results are still sufficient to achieve third place on the private LB. And I finally understood how the exponent trick works. (It is explained in the <a href=""https://www.kaggle.com/josefslavicek/flavours-of-physics/simplified-version-of-my-solution/discussion"" target=""_blank"">discussion</a> below the <a href=""https://www.kaggle.com/josefslavicek/flavours-of-physics/simplified-version-of-my-solution/code"" target=""_blank"">script</a>.)

[caption id=""attachment_5310"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-23-at-5.11.28-PM1-e1448327602364.png""><img class=""wp-image-5310 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-23-at-5.11.28-PM1-e1448327602364.png"" alt="""" width=""614"" height=""481"" /></a> Zoomed histograms of my ""Strong"" and ""Weak"" predictor for test and check_agreement data with peaks cropped[/caption]
<h3>What was the run time for both training and prediction of your winning solution?</h3>
The simplified version finishes in 4min on my local computer (1 x Inter Core i7).
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
Unfortunately, instead of doing something useful, my solution just found ways to bypass the tests introduced by organizers and exploited the weakness of the evaluation metric. (See <a href=""https://www.kaggle.com/josefslavicek/flavours-of-physics/simplified-version-of-my-solution/discussion"" target=""_blank"">discussion</a> below the <a href=""https://www.kaggle.com/josefslavicek/flavours-of-physics/simplified-version-of-my-solution/code"" target=""_blank"">script</a> of my simplified solution). I'm sad about it and I didn't do it intentionally.

It shows that the algorithms we are using are starting to be more clever than we are. When there is a way, they will find it. The Correlation and Agreement tests were carefully designed obstacles to prevent our solutions from using two particular ways of reasoning - and both the obstacles failed.
<h3>Do you have any advice for those just getting started in data science?</h3>
Compete on Kaggle, next time you can be in the top three in a prestigious competition like this one was. Yes, you can do it. If I could, so can anyone :-) .
<h2>Bio</h2>
<b>Josef Slavicek<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/josefportrait.jpg""><img class="" wp-image-5312 alignright"" style=""margin-left: 10px; margin-top: 1px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/josefportrait.jpg"" alt="""" width=""137"" height=""162"" /></a></b><strong>:</strong> I'm not data scientist. I'm not even a professional in a field of data science, machine learning, or something similarly exciting. I'm an ordinary, plain old software developer at an insurance company. To make my life a little bit more interesting, I recreationally compete in cycling races (in the summer). And similarly, in the winter, I recreationally compete on Kaggle.

<hr />

Check out more posts on the Flavour of Physics competition by clicking the tag below!

&nbsp;"
"Flavour of Physics Technical Write-Up: 1st place, Go Polar Bears",http://blog.kaggle.com/2015/11/30/flavour-of-physics-technical-write-up-1st-place-go-polar-bears/,2015-11-30 21:18:51,"<p style=""text-align: left;""><a href=""https://www.kaggle.com/littus"" target=""_blank"">Vlad Mironov</a> and <a href=""https://www.kaggle.com/aguschin"" target=""_blank"">Alexander Guschin</a> of team Go Polar Bears took first place in the <a href=""http://lhcb-public.web.cern.ch/lhcb-public/"" target=""_blank"">CERN LHCb experiment</a> <a href=""https://www.kaggle.com/c/flavours-of-physics"" target=""_blank"">Flavour of Physics competition</a>. Their model was best able to identify a rare decay phenomenon (<em>τ<sup>-</sup> → μ<sup>+</sup>μ<sup>-</sup>μ<sup>-</sup></em> or τ → 3μ) to help establish proof of ""new physics"". Below they share the technical highlights of their approach and solution.</p>

<h2 style=""text-align: left;"">Zero to 1.000000</h2>
We decided to be short and to post the most critical and interesting parts of our solution.
<h3>Feature engineering with mass recreation</h3>
Using scholar physics equations we recreated mass which in our case worked out as a golden feature:

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_mass.png""><img class=""aligncenter wp-image-5316 size-medium"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_mass-243x300.png"" alt="""" width=""243"" height=""300"" /></a>

First of all, we should find the projection of the momentum to the z-axis for each small particle. Then summarize all of them for our big particle and get pz. After that we can find the full momentum p.

From the equation <em>time ∗ Velocity = Distance</em> we can find Velocity (in our case it’s the ‘speed’ feature). Considering that FlightDistance was calculated untruly and since we knew its error, theoretically, we could improve our prediction. But after a few different implementations with no improvement we left ‘FlightDistance’ as it was.

The same thing for ‘new mass’. From the scholar equation <em>p = mV</em> we get the rest.
<h3>Triade of the models</h3>
This competition depended on three main parts: feature engineering (read as improvement of AUC score), <a href=""https://www.kaggle.com/c/flavours-of-physics/details/agreement-test"" target=""_blank"">agreement test</a>, and mass correlation. So there should be no surprise that each of our models solves exactly one problem at a time and, like the famous <a href=""https://en.wikipedia.org/wiki/Borromean_rings"" target=""_blank"">Borromean rings</a>, support each other.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_ring.png""><img class=""aligncenter wp-image-5317"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_ring.png"" alt="""" width=""400"" height=""390"" /></a>
<h3>Small forests</h3>
We found that Mass correlation error could be significantly decreased using <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> with a small number of trees and colsample. For example, here are the parameters for our first XGBoost (XGB1):

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_small_tree.png""><img class=""aligncenter wp-image-5319 size-medium"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_small_tree-300x149.png"" alt="""" width=""300"" height=""149"" /></a>

Later we had some problems with non-deterministic behavior on different machines. We found that this behavior's roots were from <a href=""http://stackoverflow.com/questions/7115459/c-rand-and-srand-gets-different-output-on-different-machines"" target=""_blank"">this</a> unsolvable problem with C++ rand() function. Here’s our <a href=""https://github.com/dmlc/xgboost/pull/614"" target=""_blank"">fix</a>.
<h3>Corrected mass and new features</h3>
The main problem with ‘new mass’ that it poorly correlates with real mass and generates both false-positive and false-negative errors near signal/background border:

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_correction1-e1448917412442.png""><img class=""aligncenter size-full wp-image-5322"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_correction1-e1448917412442.png"" alt="""" width=""614"" height=""326"" /></a>

So we implement model which corrects this behavior. And it’s the heaviest part of the solution. XGBoost with almost three thousand trees and with all features predicts new mass error. Jointly we use multilevel kfold with bagging. In addition at this stage we calculate new mass delta and new mass ratio. This features we’ll use for XBG5 and Neural Networks:

<img class=""aligncenter wp-image-5318"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_new_mass.png"" alt="""" width=""350"" height=""89"" />
<h3>NN</h3>
Neural Network with all new features, bagging. One DenseLayer with 8 neurons. Any other configurations as: more layers, more neurons shows the same or less AUC. Also Dropout leads to poorer results.
All of this could be explained by physical nature of the contest. Every feature has clear and one-way dependence between each other.
<h3>Final combination</h3>
<h2><a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_final-e1448916833155.png""><img class=""aligncenter wp-image-5320 size-full"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/flavour1_final-e1448916833155.png"" alt="""" width=""614"" height=""106"" /></a></h2>
<h2>About us</h2>
<strong>Vlad Mironov</strong>, M.S. in CS, Lomonosov Moscow State University (hello@vladmironov.com)
<strong>Role:</strong> feature engineering, small forests, xgboost fix and team spirit :)

Right now I’m looking for an interesting job in data science including not only ML, but also a heavy load computing and backend. I’m willing to relocate and work with passion.

[caption id=""attachment_5323"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-30-at-1.16.14-PM.png""><img class=""wp-image-5323 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-30-at-1.16.14-PM-300x143.png"" alt=""Kaggle profile for Vlad (aka littus)"" width=""300"" height=""143"" /></a> Kaggle <a href=""https://www.kaggle.com/littus"" target=""_blank"">profile</a> for Vlad (aka littus)[/caption]

<strong>Alexander Guschin</strong>, Moscow Institute of Physics and Technology (1aguschin@gmail.com)
<strong>Role:</strong> NN, mass correction, final mix, Kaggle’s magic.

[caption id=""attachment_5324"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-30-at-1.17.33-PM.png""><img class=""wp-image-5324 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-30-at-1.17.33-PM-300x143.png"" alt=""Kaggle profile for Alexander"" width=""300"" height=""143"" /></a> Kaggle <a href=""https://www.kaggle.com/aguschin"" target=""_blank"">profile</a> for Alexander[/caption]
<h3>Tl;dr</h3>
RTFM, correct your golden features, small forests could be useful.

<hr />

Looking for more in the Flavour of Physics competition? Click the tag below!

&nbsp;"
"Dato Winners' Interview: 1st place, Mad Professors",http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/,2015-12-03 08:01:39,"This contest was organized by <a href=""https://dato.com/"" target=""_blank"">Dato</a> (from <a href=""https://dato.com/products/create/"" target=""_blank"">GraphLab Create</a>).

The task of the <a href=""https://www.kaggle.com/c/dato-native"" target=""_blank"">Truly Native?</a> contest was:

<i>Given the HTML of ~337k websites served to users of StumbleUpon, identify the paid content disguised as real content.</i>

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/dato1.png""><img class=""aligncenter size-full wp-image-5340"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/dato1.png"" alt="""" width=""583"" height=""132"" /></a>
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
<b>Marios Michailidis:</b> I am a PhD. student (on improving recommender systems) and a sr. personalization data scientist at <a href=""https://www.dunnhumby.com/"" target=""_blank"">dunnhumby</a>.

<b>Mathias Müller:</b> I have a Master's in computer science (focus areas cognitive robotics and AI) and work as a machine learning engineer at <a href=""https://www.fsd-web.de/index.php/en/"" target=""_blank"">FSD</a>.

<b>HJ van Veen:</b> I study machine learning and artificial intelligence. I work in R&amp;D for a Dutch healthcare IT startup and write for <a href=""http://mlwave.com"" target=""_blank"">MLWave</a>.
<h3>How did you get started with Kaggle?</h3>
<b>Marios Michailidis</b>: I wanted a new challenge and learn from the <a href=""http://blog.kaggle.com/2015/05/07/profiling-top-kagglers-kazanovacurrently-2-in-the-world/"" target=""_blank"">best</a>.
<b>Mathias Müller</b>: I found Kaggle after doing a <a href=""https://www.topcoder.com/"" target=""_blank"">TopCoder</a> Marathon Match.
<b>HJ van Veen</b>: I was looking for cool challenges and read a post by <a href=""https://github.com/rrenaud"" target=""_blank"">Rob Renaud</a>.
<h2><a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/team.jpg""><img class=""aligncenter size-full wp-image-5328"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/team.jpg"" alt=""team"" width=""600"" height=""305"" /></a></h2>
<h2>Let's Get Technical</h2>
During the competition we tried a wide variety of models and approaches.

Once the fog of war cleared these essentials remained:
<ol>
	<li>A deep XGBoost on text with tokenizer, tfidf-vectorizer, cleaning, stemming and n-grams,</li>
	<li>A weighted rank average of multi-layer meta-model networks (StackNet).</li>
</ol>
The XGBoost model got us to top 10. The meta-modelling then got us to the first position.
<h3>What worked</h3>
<b>Meta-modelling.</b> Stacked generalization in a multi-layered fashion. As described in the <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.1533&amp;rep=rep1&amp;type=pdf"" target=""_blank"">Stacked Generalization</a> paper, the output of a stacker model can serve as the input for yet another stacker model. We did this for 3 levels as it kept increasing our AUC score. The first level predictions are also fed to the highest (3rd) level of stacking (together with a subset of raw features).

<b>Cross-validation.</b> With the large dataset we were able to keep stacking out of fold predictions and improve the AUC score. 5-fold CV for every model was very close (and always a little below) our public leaderboard score. We went up one position on the private leaderboard.

<b>Cleaning</b>. Basic lowercasing, removing double spaces, removing non-alphanumeric characters, repairing documents where the characters are separated by 3 spaces.

<b>Ngrams,</b> chargrams, and skipgrams. 1-3-grams in combination with cleaning captures dates, url's, and natural language indicative of native advertising.

<b>Stemming.</b> Both stemming with the <a href=""http://www.nltk.org/_modules/nltk/stem/snowball.html"" target=""_blank"">Snowball stemmer</a> (<a href=""http://www.nltk.org/"" target=""_blank"">NLTK</a>) and the <a href=""http://blogs.technet.com/b/machinelearning/archive/2014/09/24/online-learning-and-sub-linear-debugging.aspx"" target=""_blank"">Mineiro stemmer</a> worked for detecting the root of tokens or to lessen noise.

<b>Tf-idf.</b> Term frequency can prevent a bias to longer documents. Inverse document frequency can prevent a bias to common tokens. <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" target=""_blank"">tfidf</a> outperformed tf, which outperformed binary frequency.

Small and <b>large memory</b>. We had access to machines ranging from 2-core 4GB laptops to 30-core 256GB servers. Both types of hardware were able to contribute.

<b>Non-alphanumeric</b> features. Building on the <a href=""https://www.kaggle.com/c/dato-native/forums/t/16626/beat-the-benchmark-0-90388-with-simple-model"" target=""_blank"">forum post by David Shinn</a> we created count features for every document (number of dots, spaces, tabs, URL's, script tags, commas etc.). These counts were fed to the highest levels of stackers as raw features.

<b>XGBoost</b>. As the winner of an increasing amount of Kaggle competitions, <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> showed us again to be a great all-round algorithm worth having in your toolbox. With a max_depth of over 30 XGBoost was allowed to build deep trees with the text tokens.

<b>Artificial Neural Nets</b>. We further dipped our toes in the lake of deep learning with single-layer, dual-layer and three-layer neural nets. The shallow nets, like Perceptron were trained on the raw features. The multi-layer nets (<a href=""http://deeplearning.net/software/theano/"" target=""_blank"">Theano</a>, <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a> via <a href=""https://github.com/dnouri/nolearn"" target=""_blank"">NoLearn</a>) were used for stacking.

<b>Extremely Randomized Trees</b>. <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"" target=""_blank"">ExtraTreesClassifier</a> from Scikit-learn was used in all levels of stacking. Especially in the higher levels it proved to be a <a href=""http://link.springer.com/article/10.1007%2Fs10994-006-6226-1"" target=""_blank"">good algorithm</a> with the best variance and bias reduction.

<b>LibLinear</b>. Regularized Logistic Regression with <a href=""https://www.csie.ntu.edu.tw/~cjlin/liblinear/"" target=""_blank"">LibLinear</a> (via Scikit-learn) was used in the first layer of generalization as a ""grunt"" linear algorithm. It did not beat XGBoost in accuracy, but it did so in speed.

<b>Sub-linear debugging</b>. Parameter tuning, feature extraction and model selection was done with <a href=""http://www.machinedlearnings.com/2013/06/productivity-is-about-not-waiting.html"" target=""_blank"">sublinear debugging</a>. For online learning algorithms we looked at progressive validation loss, and for all models we looked at the first round of out-of-fold-guessing.

<b>Patience</b>. The larger deep models took days to train. We resisted early ensembling and kept focus on creating out-of-fold predictions.
<img class=""alignnone"" src=""https://kaggle2.blob.core.windows.net/competitions/kaggle/3333/media/Stacknet.gif"" alt="""" width=""720"" height=""540"" />

<small><i>Animation by Nadja Rößger. Click <a href=""http://i.imgur.com/GfWipUH.gif"" target=""_blank"">here</a> to replay</i></small>
<h3>What more or less worked</h3>
<b>Online learning</b>. For online learning we experimented with the tinrtgu's <a href=""https://www.kaggle.com/c/avazu-ctr-prediction/forums/t/10927/beat-the-benchmark-with-less-than-1mb-of-memory"" target=""_blank"">FTRL</a> and <a href=""https://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10322/beat-the-benchmark-with-less-then-200mb-of-memory/53674"" target=""_blank"">SGD</a> scripts and <a href=""http://hunch.net/~vw/"" target=""_blank"">Vowpal Wabbit</a>. The FTRL script is able to get to ~97 using multiple passes/epochs as shown by <a href=""https://www.kaggle.com/c/dato-native/forums/t/17009/congratulations-to-winners/96189#post96189"" target=""_blank"">Subhajit Mandal</a>.

<b>AdaBoost</b>. In the second generalization layer we added <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"" target=""_blank"">AdaBoosting</a> with decision tree stumps. Random Forests did a little better.

<b>Random Forests</b>. Also from the second generalization layer, and also slightly beaten by Extremely Randomized Trees.

<b>Latent Semantic Analysis</b>. We used TF-IDF followed by <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html"" target=""_blank"">truncatedSVD</a> to generate a new feature set.

<b>Models trained on external data</b>. We trained models on both the <a href=""http://dl.acm.org/citation.cfm?id=2002491"" target=""_blank"">IMDB movie review dataset</a> (from the <a href=""https://www.kaggle.com/c/word2vec-nlp-tutorial"" target=""_blank"">Kaggle Word2Vec tutorial</a>) and on the <a href=""http://qwone.com/~jason/20Newsgroups/"" target=""_blank"">20 newsgroups</a> datasets. These models generated predictions for both the train and test set.
<h3>What did not make it</h3>
<ul>
	<li>Vowpal Wabbit. Models trained before the reset were not updated, as we were further along in the modeling process by then.</li>
	<li>Matlab NN. We had some troubles with instability/overfit using Matlab NN's for stacking.</li>
	<li>Normalized Compression Distance. Though promising for creating informative features this technique was left out. (<a href=""https://github.com/MLWave/normalized-compression-neighbors"" target=""_blank"">code</a>)</li>
	<li>Approximate IDF-weighing. Using a count-min sketch we generated approximate inverse document frequency for online learning.</li>
	<li>Naive Bayes. Using the same count-min sketch we generated approximate Naive Bayes vectors as used in <a href=""https://github.com/mesnilgr/nbsvm"" target=""_blank"">NBSVM</a>. We did not want to add information from the entire train set for out-of-fold predictors.</li>
	<li>We never tried LDA, word vectors, POS-tagging or RNN's.</li>
</ul>
<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/models1.jpg""><img class=""aligncenter size-full wp-image-5337"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/models1.jpg"" alt=""models"" width=""640"" height=""493"" /></a>
<small><i>Different models' performance maps</i></small>
<h3>What did not work</h3>
<b>Fast &amp; simple solution</b>. Our entire solution takes 4 weeks to run on ~55 cores and may take, at peak, around 128GB of memory. <a href=""http://blog.kaggle.com/2015/10/30/dato-winners-interview-2nd-place-mortehu/"" target=""_blank"">Mortehu</a> definitely won, in this regard, with his simpler production-friendly <a href=""https://github.com/mortehu/text-classifier"" target=""_blank"">models</a>.

<b>Headers</b>. There was a brief conflict in the beginning of the competition about the ""correct"" way to store and share out-of-fold predictions. This spawned a meme than ran for the entirety of the contest. Let's just say that it doesn't matter how you store these (.npy, .txt, .csv, with/without headers, with/without ID's, ordered by ID or sample submission), you should agree on one, and only one way to do this.
<pre>  Triskelion: I put the oof's in the svn in .csv format, with 
              headers (file,prediction), just like you asked
 Michailidis: ...
      Mathias: :)
</pre>
<b>Blacklists</b>. Before the restart simple ""blacklists"" seemed to work a bit. After the reset this effect was gone. All URL's on page were parsed and URL's that appeared only within positively labelled documents were added to the blacklist.

<b>Restart.</b> Not all modelling techniques, ideas and vectorizing approaches survived the restart.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/xgbfi-large.png""><img class=""aligncenter wp-image-5342 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/xgbfi-large.png"" alt="""" width=""1203"" height=""441"" /></a>
<small><i>Ranking feature and model interactions with <a href=""https://github.com/far0n/xgbfi"" target=""_blank"">XGBfi</a>. (click to enlarge)</i></small>
<h3>Futuristic</h3>
Team discussion often ventured beyond standard meta-modelling. Interesting questions and ideas popped up:
<ul>
	<li>stacking features: Use the leaf ID's from XGB as features. Also try to use the residual (Y_real-Y_pred). See <a href=""https://www.youtube.com/watch?v=ClAZQI_B4t8"" target=""_blank"">Jeong-Yoon Lee presentation</a>.</li>
	<li>multi-layer stacking: Just how deep can we go for a significant improvement?</li>
	<li>transfer learning: Use XGBoost as a feature-interaction extractor for linear models like those from VW. Train a shallow net on multi-class predictions from ensemble.</li>
	<li>net architecture: We did a form of human backprop. Can this be automated? Likewise, can we implement dropout?</li>
	<li>model averaging: Practical sharing of insights with geometric mean vs. mean vs. rank averaging.</li>
	<li>reusable holdout set: Can we fully eliminate the need for <a href=""http://googleresearch.blogspot.nl/2015/08/the-reusable-holdout-preserving.html"" target=""_blank"">leaderboard feedback</a> and rank models on a <a href=""http://arxiv.org/abs/1502.04585"" target=""_blank"">ladder</a>?</li>
	<li>reusable trained models: Can we store models trained on different data and use their predictions for a new train and test set as features?</li>
	<li>reusable training information: Can we store and reuse the knowledge gained during training on data, models, pipelines, problems and generalization?</li>
	<li>self-guessing:</li>
</ul>
<blockquote>Rather one constructs a generalizer from scratch, requiring it to have zero cross-validation error. Instead of coming up with a set of generalizers and then observing their behavior, one takes the more enlightened approach of specifying the desired behavior first, and then solving the inverse problem of calculating the generalizer with that desired behavior. This approach is called ""self-guessing"". -<cite>Wolpert (1992) Stacked Generalization</cite></blockquote>
<h3>We Learned</h3>
<ul>
	<li>Basic NLP still works very well for this task.</li>
	<li>HTML is a messy ""natural"" language.</li>
	<li>Bigger datasets allow for bigger ensembles.</li>
	<li>Bigger ensembles require bigger hardware.</li>
	<li>Multi-layer stacking can keep squeezing out AUC.</li>
	<li>The higher the stacking level, the shallower the models need to be.</li>
	<li>Teaming up is a good thing to do.</li>
	<li>Diverse independent teams work well with an agile management style.</li>
	<li>Patience. Don't get lost in premature optimization with ensembles.</li>
	<li>Use solid development tools: code/model repositories, issue tracker, communication.</li>
	<li><small>You should always store out of fold predictions with headers and sample ID's.</small></li>
</ul>
<h2>Just For Fun</h2>
<h3>What kind of Kaggle competition would you like to see?</h3>
<b>Marios Michailidis:</b> Predicting (Kaggle) rank from (LinkedIn) profiles.
<b>Mathias Müller:</b> a ML <a href=""http://www.corewars.org/"" target=""_blank"">corewars</a>.
<b>HJ van Veen:</b> Predicting pixel values for partly obscured images.
<h2>Mad Professors</h2>
Our team name is a nod to the Mad Professor -- a creative dub artist twisting hundreds of knobs to mix music -- and pays homage to a real driving force behind (academic) research: all teachers, assistants, and professors.

By name we would like to mention: Abu-Mostafa, Bishop, Caruana, Dean, Graham, Langford, Lecun, Hinton, Huttunen, Ng, Schmidhuber, Vitanyi, Wiering, Wolpert, Zajac.

Followed by a randomly shuffled list, (capped at 99), of people who have, directly or indirectly, contributed brain power to the computation of StackNet.
<pre>random.shuffle(l)
print l[:99]
</pre>
<h2>Thanks</h2>
Thanks to the competitors for the challenge, <a href=""https://www.kaggle.com/"" target=""_blank"">Kaggle</a> for hosting, <a href=""https://dato.com/"" target=""_blank"">Dato</a> for organizing, and <a href=""https://www.stumbleupon.com/"" target=""_blank"">StumbleUpon</a> for providing the data. Thanks to the open source community and the research that makes it all possible.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/stacknet1.png""><img class=""aligncenter size-full wp-image-5338"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/stacknet1.png"" alt=""stacknet"" width=""600"" height=""820"" /></a><a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/stacknet.png"">
</a>
<small><i>You know you like ExtraTreesClassifier, when you feed the output of a logistic regression algorithm A trained on a different dataset to an ExtraTreesClassifier B, its output to another ExtraTreesClassifier C, its output to another ExtraTreesClassifier D, its output to a weighted rank average E. Thanks Gilles Louppe and Geurts et al.!</i></small>
<h2>Bios</h2>
<a href=""https://www.linkedin.com/in/mariosmichailidis"" target=""_blank"">
</a><a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/mariosmichailidis.png""><img class=""wp-image-5333 alignright"" style=""margin-left: 10px; margin-top: 1px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/mariosmichailidis.png"" alt="""" width=""74"" height=""74"" /></a><strong><a href=""https://www.linkedin.com/in/mariosmichailidis"" target=""_blank"">Marios Michailidis</a></strong> (<a href=""https://www.kaggle.com/kazanova"" target=""_blank"">KazAnova</a>) is Senior Data Scientist in dunnhumby and part-time PhD in machine learning at University College London (UCL) with a focus on improving recommender systems. He has worked in both marketing and credit sectors in the UK Market and has led many analytics projects with various themes including: Acquisition, Retention, Uplift, fraud detection, portfolio optimization and more. In his spare time he has created KazAnova, a GUI for credit scoring 100% made in Java.

<a href=""https://de.linkedin.com/in/mathias-m%C3%BCller-69763b103"" target=""_blank"">
</a><a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/mathiasmueller.png""><img class=""wp-image-5334 alignright"" style=""margin-left: 10px; margin-top: 1px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/mathiasmueller.png"" alt=""mathiasmueller"" width=""74"" height=""74"" /></a><strong><a href=""https://de.linkedin.com/in/mathias-m%C3%BCller-69763b103"" target=""_blank"">Mathias Müller</a></strong> (<a href=""https://www.kaggle.com/mmueller"" target=""_blank"">Faron</a>) is a machine learning engineer for FSD Fahrzeugsystemdaten. He has a Master's in Computer Science from the Humboldt University. His thesis was about 'Bio-Inspired Visual Navigation of Flying Robots'.

<a href=""https://www.linkedin.com/in/hjvanveen"" target=""_blank"">
</a><a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/hjvanveen.png""><img class=""wp-image-5335 alignright"" style=""margin-left: 10px; margin-top: 1px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/hjvanveen.png"" alt=""hjvanveen"" width=""74"" height=""74"" /></a><strong><a href=""https://www.linkedin.com/in/hjvanveen"" target=""_blank"">HJ van Veen</a></strong> (<a href=""https://www.kaggle.com/triskelion"" target=""_blank"">Triskelion</a>) is a data engineer and researcher at Zorgon, a Dutch Healthcare &amp; IT startup. He studies machine learning and artificial intelligence, and uses Kaggle to learn more about predictive modelling. He is also the author of MLWave.com."
"How Much Did It Rain? II: 2nd place, Luis Andre Dutra e Silva",http://blog.kaggle.com/2015/12/17/how-much-did-it-rain-ii-2nd-place-luis-andre-dutra-e-silva/,2015-12-17 22:46:05,"<a href=""https://www.kaggle.com/c/how-much-did-it-rain-ii"" target=""_blank"">How Much Did It Rain? II</a> was the second competition (of the same name) that challenged Kagglers to predict hourly rainfall measurements. <a href=""https://www.kaggle.com/mindcool"" target=""_blank"">Luis Andre Dutra e Silva</a> finished in second place, and in doing so, became a Kaggle Master (congrats!). In this blog, Luis shares his approach, and why using an LSTM model ""is like reconstructing a melody with some missed notes.""
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
[caption id=""attachment_5377"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-17-at-2.19.56-PM.png"" rel=""attachment wp-att-5377""><img class=""size-medium wp-image-5377"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-17-at-2.19.56-PM-300x141.png"" alt=""Luis' profile on Kaggle"" width=""300"" height=""141"" /></a> Luis' <a href=""https://www.kaggle.com/mindcool"" target=""_blank"">profile</a> on Kaggle[/caption]

I have been a software developer since 1985 and I developed a predictive analytical software in 2002 for a university foundation. Since then, I have used, not infrequently, time series prediction techniques (which are the basis of this competition) in many other solutions, like in my previous job at the Brazilian National Treasury. I am currently working with predictive models in my current job at the Brazilian Court of Audit.
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
My experience with the meteorological domain was nothing before this competition. Nevertheless, I have been using neural networks since the early 2000's and I begun to study machine learning more seriously after being introduced to the subject at college. Recently, in 2012, I attended a graduate machine learning class at <a href=""https://www.extension.harvard.edu/"" target=""_blank"">Harvard Extension School</a> and, in 2014, I finished with a 100% grade the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Machine Learning Class</a> from Stanford, offered online by <a href=""https://www.coursera.org/"" target=""_blank"">Coursera</a>.
<h3>How did you get started competing on Kaggle?</h3>
I was already involved in spoken language recognition using signal processing techniques and machine learning when my current boss mentioned, in 2015, the existence of a web site dedicated to machine learning competitions. Since then, I am been participating in Kaggle competitions in order to benchmark my knowledge and skills.
<h3>What made you decide to enter this competition?</h3>
My interest in recurrent neural networks, especially <a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" target=""_blank"">LSTM</a>, flourished this year and I have been looking for an opportunity to use them in a concrete problem.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
I used <a href=""http://www.sjsu.edu/faculty/watkins/raindrop.htm"" target=""_blank"">Marshall-Palmer</a> transformation of <a href=""https://en.wikipedia.org/wiki/DBZ_(meteorology)"" target=""_blank"">dBZ</a> values and linearization of DB values as preprocessing and added two new features based on data observations. Each sequence of radar snapshots was used to train a LSTM network that would produce a rainfall estimation as output at the end of each hour.

[caption id=""attachment_5383"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/2ndrossmann_architectureerror-e1450392931582.png"" rel=""attachment wp-att-5383""><img class=""wp-image-5383 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/2ndrossmann_architectureerror-e1450392931582.png"" alt=""The evolution of the model's CV error based on architecture choice."" width=""614"" height=""557"" /></a> The evolution of the model's CV error based on architecture choice.[/caption]
<h3>What was your most important insight into the data?</h3>
The existence of clogged radar measurements were a perfect fit for a LSTM model, since if some observations are not good, this kind of model can fill the gaps and still produce a meaningful rainfall estimate. It is like reconstructing a melody with some missed notes.
<h3>Were you surprised by any of your findings?</h3>
I was surprised in the beginning for the fact that less complex models were better with rainfall predictions than a model with many layers and parameters. The final solution consists of only two layers.
<h3>Which tools did you use?</h3>
I used <a href=""https://github.com/Theano/Theano"" target=""_blank"">Theano</a>/<a href=""https://github.com/fchollet/keras"" target=""_blank"">Keras</a> for neural networks and <a href=""http://scikit-learn.org/stable/"" target=""_blank"">scikit-learn</a> for cross validation and metrics. I developed a particular 50-fold CV algorithm based on RMSE, covariances, and average MAE that was consistently better with lower mini batches size.
<h3>How did you spend your time on this competition?</h3>
Most of the time I spent adjusting model parameters and waiting for each ensemble to be trained.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
The training time took about 10 hours in a Geforce Titan X with mini batches of 256 for all of the 50 different models. The prediction time took around less than 10 minutes.

[caption id=""attachment_5382"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/2ndrossmann_batchsizeerror-e1450392843996.png"" rel=""attachment wp-att-5382""><img class=""wp-image-5382 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/2ndrossmann_batchsizeerror-e1450392843996.png"" alt=""2ndrossmann_batchsizeerror"" width=""614"" height=""521"" /></a> The evolution of the model's CV error based on batch size choice.[/caption]
<h2>Words of wisdom</h2>
<h3>What have you taken away from this competition?</h3>
I have learned that Occam's Razor <a href=""https://en.wikipedia.org/wiki/Occam%27s_razor"" target=""_blank"">principle</a> was not simply a matter that he didn't use to shave his beard.
<h3>Do you have any advice for those just getting started in data science?</h3>
Begin small, progress slowly, target the stars and reach the Moon.
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
I would propose a project similar to another competition about fighting mosquitos, but targeted to <a href=""https://en.wikipedia.org/wiki/Aedes_aegypti"" target=""_blank"">aedes aegypti</a> which are causing an epidemic of many diseases in Latin America.
<h3>What is your dream job?</h3>
My current job is my dream job. I love my colleagues and the institution I work for.
<h2>Bio</h2>
<strong><a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/luisandredutraesilva_headshot.png"" rel=""attachment wp-att-5378""><img class="" wp-image-5378 alignright"" style=""margin-left: 10px; margin-top: 2px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/luisandredutraesilva_headshot.png"" alt="""" width=""116"" height=""117"" /></a>Luis Andre Dutra e Silva</strong> is a Federal Auditor at Brazilian Court of Audit. He earned a BS in computer science, and has 30 years of experience in software development and engineering.

<hr />

Want to read more on the How Much Did It Rain? competitions? Click the tag below!

&nbsp;"
"Rossmann Store Sales, Winner's Interview: 1st place, Gert Jacobusse",http://blog.kaggle.com/2015/12/21/rossmann-store-sales-winners-interview-1st-place-gert/,2015-12-22 00:15:52,"Rossmann operates over 3,000 drug stores in 7 European countries. In their first Kaggle competition, <a href=""https://www.kaggle.com/c/rossmann-store-sales"" target=""_blank"">Rossmann Store Sales</a>, this drug store giant challenged Kagglers to forecast 6 weeks of daily sales for 1,115 stores located across Germany. The competition attracted 3,738 data scientists, making it our second most popular competition by participants ever.

Gert Jacobusse, a professional sales forecast consultant, finished in first place using an ensemble of over 20 XGBoost models. Notably, most of the models individually achieve a very competitive (top 3 leaderboard) score. In this blog, Gert shares some of the tricks he's learned for sales forecasting, as well as wisdom on the why and how of using hold out sets when competing.
<h2>The Basics</h2>
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
My hobby and daily job is to work on data analysis problems, and I participate in a lot of Kaggle competitions. With my own company <a href=""http://www.rogatio.nl/"" target=""_blank"">Rogatio</a> I deliver tailored sales forecasts for several companies - product specific as well as overall. Therefore I knew how to approach the problem.

[caption id=""attachment_5409"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-4.15.02-PM.png"" rel=""attachment wp-att-5409""><img class=""size-medium wp-image-5409"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-4.15.02-PM-300x142.png"" alt=""Gert's profile on Kaggle"" width=""300"" height=""142"" /></a> Gert's <a href=""https://www.kaggle.com/gertjac"" target=""_blank"">profile</a> on Kaggle[/caption]
<h3>How did you get started competing on Kaggle?</h3>
I don’t remember, somehow it has become a part of my life. I enjoy the competitions so much that it is really addictive for me. But in a good way: it is nice exposure for my skills, I learn a lot of new techniques and applications, I get to know other skilled data scientists and if I am lucky I even get paid!
<h3>What made you decide to enter this competition?</h3>
A sales forecast is a tool that can help almost any company I can think of. Many companies rely on human forecasts that are not of a constant quality. Other companies use a standard tool that is not flexible enough to suit their needs. As an individual researcher I can create a solution that really improves business. And that is exactly what this competition is about. I am very eager to further develop and show my skills - therefore I did not hesitate a moment to enter this competition.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
The most important preprocessing was the calculation of averages over different time windows. For each day in the sales history, I calculated averages over the last quarter, last half year, last year and last 2 years. Those averages were split out by important features like day of week and promotions. Second, some time indicators were important: not only month and day of year, but also relative indicators like number of days since the summer holidays started. Like most teams, I used extreme gradient boosting (xgboost) as a learning method.

[caption id=""attachment_5398"" align=""aligncenter"" width=""610""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/rossmann1_fig1.png"" rel=""attachment wp-att-5398""><img class=""size-full wp-image-5398"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/rossmann1_fig1.png"" alt=""-Figure 1 a/b. Illustration of the task: predict sales six weeks ahead, based on historical sales (only last 3 months of train set shown)."" width=""610"" height=""809"" /></a> Figure 1 a/b. Illustration of the task: predict sales six weeks ahead, based on historical sales (only last 3 months of train set shown).[/caption]
<h3>What was your most important insight into the data?</h3>
The most important insight was that I could reliably predict performance improvements based on a hold out set within the trainset. Because of this insight, I did not overfit the public test set, so my model worked very well on the public test set as well as the unseen private test set that was four weeks further ahead.
<h3>Do you always use hold out sets to validate your model in every competition?</h3>
Yes, sometimes using cross-validation (with multiple holdout sets) and sometimes with a single holdout set, like I did in this competition. The advantage of a holdout set is that I can use the public test set as a real test set, not a set that gives me feedback to improve my model. As a consequence, I get reliable feedback about how much I overfitted my own holdout set. Therefore, I do not like competitions where the train/ test split is not-random, while the public/ private split is random: in such competitions, you can build a better model by using feedback from the public leaderboard. I do not like that because I am not aware of any real life problem that would require such an approach. This competition was ideal for me: the train test split was time based, and so was the public/private split!
<h3>Do you have any recommendations for selecting data for a hold out set and using it most effectively?</h3>
For selecting a hold out set, I always try to imitate the way that the train and test set were split. So, if it is a time split, I split my holdout sample time based; if it is a geographical split by city, I split my holdout set by city; and if it is a random split, then my holdout split will be random as well. You can effectively use a holdout set to push the limit towards how much you can learn from the data without overfitting. Don't be afraid to overfit your holdout set, the public leaderboard will tell you if you do so.
<h3>Were you surprised by any of your findings?</h3>
Yes, I was surprised that a model without the most recent month of data (that I used to predict sales further ahead) did almost as well as a model that did include recent data. This finding is very specific for the Rossmann data, and it means that short term changes are less important than they often are in forecasting.

[caption id=""attachment_5407"" align=""aligncenter"" width=""610""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/rossmann1_fig22.png"" rel=""attachment wp-att-5407""><img class=""wp-image-5407 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/rossmann1_fig22.png"" alt=""rossmann1_fig22"" width=""610"" height=""426"" /></a> Figure 2. This picture illustrates the progress we made in this competition. Xgboost predictions without feature engineering (black) were already quite good. The improvements that full feature engineering (red) gave were really about finetuning.[/caption]
<h3>Which tools did you use?</h3>
For preprocessing I loaded the data into an SQL database. For creating features and applying models, I used Python.
<h3>How did you spend your time on this competition?</h3>
I spent 50% on feature engineering, 40% on feature selection plus model ensembling, and less than 10% on model selection and tuning.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
The winning solution consists of over 20 xgboost models that each need about two hours to train when running three models in parallel on my laptop. So I think it could be done within 24 hours. Most of the models individually achieve a very competitive (top 3 leaderboard) score.

[caption id=""attachment_5408"" align=""aligncenter"" width=""610""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/12/rossmann1_fig33.png"" rel=""attachment wp-att-5408""><img class=""size-full wp-image-5408"" src=""http://blog.kaggle.com/wp-content/uploads/2015/12/rossmann1_fig33.png"" alt=""Figure 3. A time indicator for the time until store refurbishment (last four days on the right of the plot) reveals how the sales are expected to change during the weeks before a refurbishment."" width=""610"" height=""427"" /></a> Figure 3. A time indicator for the time until store refurbishment (last four days on the right of the plot) reveals how the sales are expected to change during the weeks before a refurbishment.[/caption]
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
More experience in sales forecasts and a very solid proof of my skills. Plus a nice extra turnover of $15,000 dollars that I had not forecasted.
<h3>Do you have any advice for those just getting started in data science?</h3>
<ol>
	<li>make sure that you understand the principles of cross validation, overfitting and leakage</li>
	<li>spend your time on feature engineering instead of model tuning</li>
	<li>visualize your data every now and then</li>
</ol>
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
You have proven to be very good at creating competitions, I don’t have an idea to improve on that right now ;) But I have the opportunity so let me share one idea for improvement: to create good models and anticipate the kind of error that can be expected, I often miss explicit information on how the train/test and public/private sets are being split. A competition is (even) more fun for me when I don’t have to guess at what types of mechanisms impact model performance.
<h3>What is your dream job?</h3>
Work for a variety of customers - and help them with data challenges that are central to the success of their business. And have enough spare time to participate in Kaggle competitions!"
"How Much Did It Rain? II, Winner's Interview: 1st place, PuPa (aka Aaron Sim)",http://blog.kaggle.com/2016/01/04/how-much-did-it-rain-ii-winners-interview-1st-place-pupa-aka-aaron-sim/,2016-01-04 19:39:17,"Aaron Sim took first place in our recent <a href=""https://www.kaggle.com/c/how-much-did-it-rain-ii"" target=""_blank"">How Much Did It Rain? II</a> competition. The goal of the challenge was to predict a set of hourly rainfall levels from sequences of weather radar measurements. Aaron and his research lab supervisor were in the midst of developing deep learning tools for their own research when the competition was launched. There was sufficient overlap in the statistical tools and datasets to make the competition a great ground for testing their approach on a new dataset. In this blog, Aaron shares his background, competition experience and methodology, and biggest takeaways (<em>hint: Kaggle competitions are anything but covert</em>). To read a more detailed technical analysis, take a look at his personal <a href=""http://simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/"" target=""_blank"">blog post</a> on GitHub.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I am a postdoc researcher in the Theoretical Systems Biology Group at Imperial College London in the UK. My background is in theoretical physics where I worked on the geometry of string theory backgrounds (in other words, no data whatsoever). My current research involves the development of mathematical and statistical models of biological and other complex systems such as protein interaction networks and cities (lots of data!).

[caption id=""attachment_5419"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-04-at-11.23.45-AM.png"" rel=""attachment wp-att-5419""><img class=""size-medium wp-image-5419"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-04-at-11.23.45-AM-300x144.png"" alt=""PuPa (aka Aaron) on Kaggle"" width=""300"" height=""144"" /></a> <a href=""https://www.kaggle.com/aaronsim"" target=""_blank"">Profile</a> for Aaron (aka PuPa) on Kaggle[/caption]
<h3>What made you decide to enter this competition?</h3>
It was clear to me from the start that this prediction task – hourly rainfall values from variable-length, time-labelled, sequences of weather radar observations – is very nearly the classic type of problem that one would not hesitate to throw a recurrent neural network at (more of this below). Since I was in the midst of applying some deep learning methods in my current research project, I saw it as a good opportunity to validate some of my ideas in a different context. That, at least, is my post hoc justification for the time spent on this competition!
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
I used recurrent neural networks (RNN) exclusively with minimal preprocessing. As alluded to above, the prediction of cumulative values (hourly rainfall) from variable-length sequences of vectors with a time component is highly reminiscent of the so-called Adding Problem in machine learning – a toy sequence regression task that is frequently employed to demonstrate the power of RNNs in learning long-term dependencies (see <a href=""http://arxiv.org/abs/1504.00941"" target=""_blank"">Le et al.</a>, Sec 4.1, for a recent example).

[caption id=""attachment_5415"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/Figure_1.png"" rel=""attachment wp-att-5415""><img class=""wp-image-5415"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/Figure_1-1024x268.png"" alt=""Figure 1: The prediction target of 1.7 is obtained by adding up the numbers in the top row where the corresponding number in the bottom row is equal to one (i.e. the green boxes). The regression task is to infer this generative model from a training set of random sequences of arbitrary lengths and their targets."" width=""614"" height=""161"" /></a> Figure 1: The prediction target of 1.7 is obtained by adding up the numbers in the top row where the corresponding number in the bottom row is equal to one (i.e. the green boxes). The regression task is to infer this generative model from a training set of random sequences of arbitrary lengths and their targets.[/caption]

In the rainfall prediction problem, the situation is somewhat less trivial as there is still the additional step of inferring the rainfall numbers (top row) from radar measurements. Furthermore, instead of binary 0/1 values (bottom row) one has continuous time readings between 0 and 60 minutes that have somewhat different roles. Nevertheless, the underlying structural similarities are compelling enough to suggest that RNNs, even simple vanilla RNNs with off-the-shelf architectures (see below), would be well suited to the problem.

[caption id=""attachment_5416"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/Figure_2.png"" rel=""attachment wp-att-5416""><img class=""wp-image-5416"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/Figure_2-1024x403.png"" alt=""Figure 2. A basic RNN setup. The bottom layer represents a single input sequence of radar measurements within a single hour. Each number is the minutes past the top of the hour of the measurement, which is preserved as a component of the feature vector. The output is the cumulative rainfall in the hour as measured by a rain gauge."" width=""614"" height=""242"" /></a> Figure 2. A basic RNN setup. The bottom layer represents a single input sequence of radar measurements within a single hour. Each number is the minutes past the top of the hour of the measurement, which is preserved as a component of the feature vector. The output is the cumulative rainfall in the hour as measured by a rain gauge.[/caption]

If there’s a secret sauce in my approach beyond simply deepening and widening the above RNN architecture, however, it would probably be the implementation of training- and test-time augmentations to the radar sequences. One common way to reduce overfitting is to augment the training set via label-preserving transformations of the data. The canonical examples are found in image classification tasks where images are cropped and perturbed to improve the generalization capabilities of the classifier. Since we have here a regression problem, it is less obvious what such augmentations should be.

My solution was to implement a form of ‘dropin’ augmentation of the datasets where I lengthened the radar measurement sequences to a single fixed length by duplicating the vectors at random time points. This is, loosely speaking, the opposite of performing dropout on the input layer, hence the name. This is illustrated in the figure below:

[caption id=""attachment_5417"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/Figure_3.png"" rel=""attachment wp-att-5417""><img class=""wp-image-5417"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/Figure_3-1024x761.png"" alt=""Figure 3. Lengthening a length-5 sequence to length-8 sequences. Each coloured box represents a vector of radar measurements. Note that the temporal order of the augmented sequence is preserved."" width=""614"" height=""456"" /></a> Figure 3. Lengthening a length-5 sequence to length-8 sequences. Each coloured box represents a vector of radar measurements. Note that the temporal order of the augmented sequence is preserved.[/caption]

The lengths of the sequences in both the training and test sets ranged from one to 19 measurements per hour. Over the competition I experimented with fixed augmented sequence lengths of 19, 21, 24 and 32 timepoints. I found that stretching out the sequence lengths beyond 21 steps was too aggressive as the models began to underfit.

My original intention was to find a way to standardise the sequence lengths to facilitate mini-batch training. However it soon became clear that this simple generalization of a basic padding operation could be a way to train the network to properly factor in the time intervals between observations; specifically, this is achieved by encouraging the network to ignore readings when the intervals are zero, thereby mimicking the input gate in gated variants of RNNs such as LSTM networks. To the best of my knowledge this is a novel, albeit simple, idea.

To predict each rainfall value at test time, I took the mean of 61 separate rain gauge predictions that were generated from different random dropin lengthening of the radar data sequences. Implementing this procedure alone led to a huge improvement (~0.03) in the public leaderboard score, which translates roughly into a jump from 40th position into a top-ten place.

I did not perform any data preprocessing beyond replacing missing components in the radar measurement vectors with zeros.

The best architecture I found over the competition is a 5-layer deep bidirectional RNN with 64 to 256 hidden units, with additional single dense layers after each hidden stack and a single linear layer at the bottom of the network to reduce the dimension of the input vectors. At the top of the network the vector at each time position is fed into a dense layer with a single output and a ReLU non-linearity. The final output is obtained by taking the mean of the predictions from the entire top layer. This is summarised in the figure below:

[caption id=""attachment_5418"" align=""aligncenter"" width=""614""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/Figure_4.png"" rel=""attachment wp-att-5418""><img class=""wp-image-5418"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/Figure_4-808x1024.png"" alt=""Figure 4. The best performing architecture in the final ensemble. The red numbers on the right indicate the size of each layer. The output from this single model had a public leaderboard score of 23.6971, which should be good enough for 5th place in the competition."" width=""614"" height=""778"" /></a> Figure 4. The best performing architecture in the final ensemble. The red numbers on the right indicate the size of each layer. The output from this single model had a public leaderboard score of 23.6971, which should be good enough for 5th place in the competition.[/caption]
<h3>What were some of the challenges thrown up by this particular dataset?</h3>
Since the physical locations of the rain gauges and the calendar date and hour of their measurements were not provided for this competition, this had the somewhat unusual implication that it was difficult, if not impossible, to separate out from the training data a sufficiently independent holdout subset. This was discussed at some length in the forum.

Indeed as the competition progressed I became increasingly suspicious of what my local validation scores were indicating, especially as I was struggling to get my models to overfit (most definitely a data science first-world problem). I began to rely on the public leaderboard submissions to validate my models – yes I was one of those people making the maximum number of submissions every day (*yikes*). This goes completely against the conventional wisdom of building a robust local cross validation setup and holding one’s nerve and trusting it over the public leaderboard scores. I did, however, live in fear of a great leaderboard shakeup, which thankfully for me did not materialise.
<h3>Were you surprised by any of your findings?</h3>
My biggest surprise was that implementing dropout resulted in consistently poorer scores, contrary to what has been reported by many others for RNNs and in other models such as CNNs. I tried many combinations, including varying the dropout percentage and implementing it only at the top or bottom of the network, all without success.

Also, LSTM networks did not appear to work any better than RNNs with standard hidden layers. Perhaps the advantages are only really apparent for much longer sequences with more complex dependencies than the ones here.
<h3>Which tools did you use?</h3>
I used Python with Theano throughout and relied heavily on the Lasagne layer classes to build the RNN architectures. Additionally, I used scikit-learn to implement the cross-validation splits, and pandas and NumPy to process and format the data and submission files. I trained the models on several NVIDIA GPUs in my lab, which include two Tesla K20 and three M2090 cards.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
<ol>
	<li>It is impossible to hide your participation in a Kaggle competition from your partner/spouse. (“You’re doing another Kaggle competition, aren’t you?”)</li>
	<li>Throwing the neural network equivalent of ‘everything but the kitchen sink’ at any and every problem is almost never a bad idea.</li>
	<li>Don’t bother with feature engineering; the machines have won.</li>
</ol>
<h3>Do you have any advice for those just getting started in data science?</h3>
The field is moving very fast – one can’t just rely on standard statistics or machine learning textbooks (or even year-old blog posts). Most research papers are freely available on <a href=""http://arxiv.org/"" target=""_blank"">arXiv</a>, often many months before they are properly published, so that is a good place to hang out.

Also, everything becomes a lot easier to understand once you’ve learnt how to build it. So get stuck in early and don’t worry about not understanding everything from the start.
<h2>Bio</h2>
Aaron Sim has a background in theoretical physics and is currently a postdoc researcher in the Theoretical Systems Biology Group at Imperial College London. Read more by Aaron on his <a href=""http://simaaron.github.io/"" target=""_blank"">github blog</a>."
"Rossmann Store Sales, Winner's Interview: 3rd place, Neokami Inc.",http://blog.kaggle.com/2016/01/22/rossmann-store-sales-winners-interview-3rd-place-cheng-gui/,2016-01-22 18:00:36,"Rossmann operates over 3,000 drug stores in 7 European countries. In their first Kaggle competition, <a href=""https://www.kaggle.com/c/rossmann-store-sales"" target=""_blank"">Rossmann Store Sales</a>, this drug store giant challenged Kagglers to forecast 6 weeks of daily sales for 1,115 stores located across Germany. The competition attracted 3,738 data scientists, making it our second most popular competition by participants ever.

Cheng Guo competed as team Neokami Inc. and took third place using a method, ""entity embedding"", that he developed during the course of the competition. In this blog, he shares more about entity embedding, why he chose to use neural networks (instead of the popular xgboost), and how a simplified version of his model still manages to perform quite well.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I work at <a href=""https://www.neokami.com/"" target=""_blank"">Neokami</a>, a machine learning startup located in Munich. I built the neural network behind some of our computer vision products such as <a href=""https://www.visualcortex.io/"" target=""_blank"">VisualCortex</a>, which lets you create your own image classifier easily. I hold a PhD in theoretical physics and have developed algorithms to simulate quantum systems while studying at <a href=""http://www.en.uni-muenchen.de/index.html"" target=""_blank"">Ludwig Maximilians University</a> in Munich and <a href=""http://english.cas.cn/"" target=""_blank"">Chinese Academy of Sciences</a> in Beijing.

[caption id=""attachment_5466"" align=""aligncenter"" width=""381""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-21-at-1.59.45-PM.png"" rel=""attachment wp-att-5466""><img class=""size-full wp-image-5466"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-21-at-1.59.45-PM.png"" alt=""Profile for Cheng Gui (aka entron) on Kaggle"" width=""381"" height=""181"" /></a> <a href=""https://www.kaggle.com/entron"" target=""_blank"">Profile</a> for Cheng Guo (aka entron) on Kaggle[/caption]
<h3>What made you decide to enter this competition?</h3>
We had a busy year developing our product, but I was not so busy for a few weeks before Christmas. My colleague Felix reminded me that I could join some Kaggle contest. I knew that a very popular German chain store Rossmann had been running a competition on Kaggle for some time, and I thought it would be fun to join it. Our founders Ozel and Andrei liked the idea and supported me, so I joined in the last month of the competition.
<h3>What have you taken away from this competition?</h3>
First, I invented a new method ""entity embedding"" for this competition. It is a general method and can be applied to many other problems.

Second, a special part about this Rossmann competition is that external data is allowed as long as it is shared in the forum. This brought lots of exciting explorations, insights and fun just like in scientific research. There are so many smart and passionate people in Kaggle and there are also many important and difficult questions waiting to be solved. If those problems are carefully divided and well formulated into small and easy to understand subproblems it may be solved collaboratively by the Kaggle community. This is a world changing potential.
<h2>Just For Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
I recently read <a href=""http://www.reuters.com/article/us-china-pollution-idUSKBN0UB1KB20151229"" target=""_blank"">a news article</a> about IBM and Microsoft's effort to forecast China's smog. I think it could have a tremendous environmental, social and economical value to run a competition with similar external data policy like Rossmann Sales to forecast smog. This will help to pinpoint what are the most important contributors (factories/sources etc.) to the air pollution.
<h2>Let's Get Technical</h2>
<h3>What supervised learning methods did you use?</h3>
Deep neural network is very powerful and flexible and it is already the dominant method in many machine learning problems like computer vision and natural language processing. When reading the Rossmann competition forum I was surprised that most top teams used tree based methods like <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">xgboost</a> rather than neural network. As a fan of neural networks I decided to use only neural network and see how it compares with xgboost.

To make a neural network work effectively on this type of problem which has many category features, I proposed a new method <strong>Entity Embedding</strong> to represent category features in a multi-dimensional space. It is inspired by semantic embedding in the natural language processing domain. With entity embedding, I found that neural networks generate better results than xgboost when using the same set of features.

I also used an unusual small dropout 0.02 after the input layer to improve the generalization. The reasoning is that the 0.02 dropout will randomly remove one small feature or a few dimensions of large features and the model should still be able to get similar result based on the remaining features.

I have shared our code in the Kaggle forum <a href=""https://www.kaggle.com/c/rossmann-store-sales/forums/t/17974/code-sharing-3rd-place-category-embedding-with-deep-neural-network"" target=""_blank"">here</a>.
<h3>What was your most important insight into the data?</h3>
My second favorite approach to the Rossmann Sales problem is to use the historical median of the 4 features (<em>store_index, day_of_week, promo, year</em>) as the prediction. Its score on my validation set, which is close to the final score on the leader board, is 0.133. The result is amazingly good considering how simple this approach is. I wanted to include more features to improve it, unfortunately then comes the data sparsity problem. More specifically, after I added another feature ""<em>month</em>"", some combinations of the 5 features don't have any historical record in the dataset leave aside the median. To overcome the data sparsity problem I got the idea to represent the discrete category features in a continuous space in which the distance between different ""category points"" reflects the similarity of the categories. This is the idea behind the entity embedding method. In this way one can interpolate or use nearby data points to approximate missing data points.

To get an intuitive idea about entity embedding, I used t-SNE to map the high dimensional embeddings into 2D figures. First, let's see the German states (Fig. 1).

[caption id=""attachment_5465"" align=""aligncenter"" width=""512""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/state_embedding_crop.png"" rel=""attachment wp-att-5457""><img class=""wp-image-5465 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/state_embedding_crop.png"" alt="""" width=""512"" height=""482"" /></a> Fig 1. t-SNE embedding of German states[/caption]

Though the algorithm does not know anything about German geography and society, the relative positions on the learnt embedding of German states resemble that on the below map (Fig. 2) surprisingly well!

[caption id=""attachment_5458"" align=""aligncenter"" width=""443""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/German_States.png"" rel=""attachment wp-att-5458""><img class=""wp-image-5458 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/German_States.png"" alt=""German_States"" width=""443"" height=""599"" /></a> Fig. 2 Map of German States[/caption]

The reason is that the embedding maps states with similar distribution of features, i.e. similar economical and cultural environments, close to each other, while at the same time two geographically neighboring states are likely sharing similar economy and culture. Especially, the three states on the right cluster, namely ""Sachsen"", ""Thueringen"" and ""SachsenAnhalt"" are all from eastern Germany while states in the left cluster are from western Germany. This shows the effectiveness of entity embedding for abductive reasoning.

Similarly, the following are the learnt embeddings (after converted to 2D) of day of week (Fig. 3), month (Fig. 4) and Rossmann stores (Fig. 5).

[caption id=""attachment_5464"" align=""aligncenter"" width=""505""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/dow_embedding_crop.png"" rel=""attachment wp-att-5459""><img class=""wp-image-5464 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/dow_embedding_crop.png"" alt="""" width=""505"" height=""478"" /></a> Fig. 3 Learnt embedding of days of the week[/caption]

[caption id=""attachment_5463"" align=""aligncenter"" width=""490""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/month_embedding_crop.png"" rel=""attachment wp-att-5460""><img class=""wp-image-5463 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/month_embedding_crop.png"" alt="""" width=""490"" height=""473"" /></a> Fig. 4 Learnt embedding of months[/caption]

[caption id=""attachment_5462"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/store_embedding_crop.png"" rel=""attachment wp-att-5461""><img class=""wp-image-5462"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/store_embedding_crop.png"" alt="""" width=""600"" height=""592"" /></a> Fig. 5 Learnt embedded of Rossmann stores[/caption]

Entity embedding may be applied to many other problems to find the hidden relations between entities based on their interaction with the external environment. For example, based on huge databases about the relations of genes, mutations, proteins, medicines and diseases, one may map those entities into multi-dimensional spaces which can guide the understanding of biological process or drug discovery etc. This an exciting direction for further exploration.
<h3>Which tools did you use?</h3>
I used a new python neural network frame work <a href=""https://github.com/fchollet/keras"" target=""_blank"">Keras</a>. It is simple, flexible and powerful. It can use <a href=""https://github.com/Theano/Theano"" target=""_blank"">Theano</a> or <a href=""https://www.tensorflow.org/"" target=""_blank"">TensorFlow</a> as the backend. I also used many common python packages like <a href=""http://scikit-learn.org/stable/"" target=""_blank"">sklearn</a>, <a href=""http://www.numpy.org/"" target=""_blank"">numpy</a> and <a href=""http://pandas.pydata.org/"" target=""_blank"">pandas</a>. I used Nvidia GTX 980 GPU to run the neural network as it is more than one order of magnitude faster than a CPU.
<h3>What was the run time for both training and prediction of your winning solution</h3>
It takes 20 minutes to train one network on GPU. For our finial submission we averaged the result of 10 networks, so altogether it takes about 3.5 hours. The time spend on the prediction is little."
"NOAA Right Whale Recognition, Winners' Interview: 1st place, deepsense.io",http://blog.kaggle.com/2016/01/29/noaa-right-whale-recognition-winners-interview-1st-place-deepsense-io/,2016-01-29 21:08:44,"With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. In the <a href=""https://www.kaggle.com/c/noaa-right-whale-recognition"" target=""_blank"">NOAA Right Whale Recognition</a> challenge, 470 players on 364 teams competed to build a model that could identify any individual, living North Atlantic right whale from its aerial photographs. The deepsense.io team entered the competition spurred by a recent improvements in their image recognition skills and ended up taking 1st place. In this blog, they share their pipeline, their solution's ""most valuable player"", and what they've taken away from the competition experience.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
Robert began assembling the <a href=""http://deepsense.io/"" target=""_blank"">deepsense.io</a> team about one and a half years ago with the goal of creating a machine learning powerhouse in Warsaw, taking perhaps an unusual approach of seeking out people who did not specialize in data science, but rather algorithmics. Guided by two reasons: first and foremost, he wanted to take a fresh approach to machine learning; second that his alma mater, the <a href=""http://www.mimuw.edu.pl/studies/"" target=""_blank"">Faculty of Mathematics, Informatics and Mechanics</a> at Warsaw University was full of the latter.

[caption id=""attachment_5487"" align=""aligncenter"" width=""885""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/deepsense_noaa_team.png"" rel=""attachment wp-att-5479""><img class=""wp-image-5487 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/deepsense_noaa_team.png"" alt="""" width=""885"" height=""283"" /></a> Top row: <a href=""https://www.kaggle.com/users/212902/iwan-gro-ny"" target=""_blank"">IWAN GROZNY</a>, <a href=""https://www.kaggle.com/maciejk"" target=""_blank"">MACIEJK</a>, <a href=""https://www.kaggle.com/mmucha"" target=""_blank"">MARCIN MUCHA </a>Bottom row: <a href=""https://www.kaggle.com/marekcygan"" target=""_blank"">MAREK CYGAN</a>, <a href=""https://www.kaggle.com/datarat"" target=""_blank"">MILLCHECK</a>, <a href=""https://www.kaggle.com/robibok"" target=""_blank"">ROBIBOK</a>[/caption]

As a result, almost everyone on the deepsense.io team has a strong background in computer science, having competed and won various computer science competitions (including an ACM ICPC win, Google Code Jam victory, and multiple IOI gold medals). He sought out both students and employees, even leading to a situation where one of the members was leading a course another attended. It seems we get along pretty well though.
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
This was actually our second image processing contest, the first being <a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection"" target=""_blank"">Diabetic Retinopathy Detection</a>. Even though we didn’t manage to finish in the “money pool” on that one, we certainly learned a lot (you can read more <a href=""http://deepsense.io/diagnosing-diabetic-retinopathy-with-deep-learning/"" target=""_blank"">here</a>).

We felt unsatisfied, as we believed our image recognition skills have vastly improved since then and wanted to showcase our progress. <a href=""https://www.kaggle.com/c/noaa-right-whale-recognition"" target=""_blank"">Right Whale Recognition</a> was the next such competition, so we jumped straight in. We had no domain knowledge, so we could only go on the information provided by the organizers (well honestly that and Wikipedia). It turned out to be enough though. Robert says it cannot happen again, so we’re currently in the process of hiring a marine biologist ;) Right or wrong, we won’t let any whale catch us off-guard next time.
<h3>How did you get started competing on Kaggle?</h3>
To be frank, it was pretty incidental. Robert and Jan Kanty were coding a “homebrewed” Linear Regression loaded with our tricks (of varying worth) and we wanted to test it on an actual dataset. One of our friends suggested Kaggle, praising it for its well-prepared datasets and before we knew it, we achieved master status and were hungry for more.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
Our preprocessing pipeline is nicely described in the following picture:

[caption id=""attachment_5472"" align=""aligncenter"" width=""975""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/noaa_blog1_1.png"" rel=""attachment wp-att-5472""><img class=""size-full wp-image-5472"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/noaa_blog1_1.png"" alt=""deepsense.io North Atlantic right whale preprocessing pipeline"" width=""975"" height=""236"" /></a> deepsense.io North Atlantic right whale preprocessing pipeline[/caption]

There are two initial phases. In the first, we train a neural net to output coordinates of the bounding box of a whale’s head (as seen in the above figure #2).

During the second, another network is trained to output two special points on whale’s head (bonnet-tip and blowhead in #3). We use these two points to properly rotate and scale the photo (#4), and as a result get a normalized view of the whale’s head.

When looking for the bounding box and the points on the head, we used softmax classification (classes being approximate values of the coordinates) instead of regression - it simply performed better. Seems like determining if a part of an image contains something is easier than determining where it is, who knew?

As you can see in the pictures below, processing the photos this way makes the whales’ heads clearly visible. Good for us that the north atlantic right whales form a counter-culture and decided to get those white tattoos on their faces.

[caption id=""attachment_5473"" align=""aligncenter"" width=""700""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/noaa_blog1_2.png"" rel=""attachment wp-att-5473""><img class="" wp-image-5473"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/noaa_blog1_2.png"" alt=""Processed images"" width=""700"" height=""700"" /></a> Processed images[/caption]

Finally, after preprocessing, the conv-nets are trained on these standardized photos.

What proved to be the MVP of the solution, was when we added additional target attributes to the data - whether the callosity pattern was continuous and whether it was symmetrical. Such “easy” targets made sure that our networks knew where to put their focus even on the earliest epochs - not only vastly reducing the training time, but also preventing overfitting.

Another thing we did was manually “kicking” the learning rate when we found the improvements too stale. Seems like it can, after some commotion, help the network make progress again.

[caption id=""attachment_5474"" align=""aligncenter"" width=""703""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/noaa_blog1_3.png"" rel=""attachment wp-att-5474""><img class="" wp-image-5474"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/noaa_blog1_3.png"" alt=""ncreasing the learning rate mid-training"" width=""703"" height=""635"" /></a> Increasing the learning rate mid-training[/caption]

And here is the final network’s architecture:

[caption id=""attachment_5488"" align=""aligncenter"" width=""162""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/noaa_blog1_5.png"" rel=""attachment wp-att-5488""><img class=""wp-image-5488 size-large"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/noaa_blog1_5-162x1024.png"" alt=""The final network"" width=""162"" height=""1024"" /></a> The final network[/caption]

This is a pretty brief description, if one is interested in the details, we recommend our <a href=""http://deepsense.io/deep-learning-right-whale-recognition-kaggle/"" target=""_blank"">blog post</a>.
<h3>How did you spend your time on this competition?</h3>
We’ve been determined to try out as many different approaches and ideas as possible. As a result, at least half of our time (especially at the start of the competition) was spent on things that have not worked well enough. A non-exhaustive list would be:
<ul>
	<li>Unsupervised cropping</li>
	<li>Other methods of supervised cropping: regression, training a net to distinguish samples of heads and non-heads</li>
	<li>Spatial Transformer Networks</li>
	<li>Deep Residual Networks</li>
	<li>Triplet training</li>
</ul>
We have also spent some time doing manual annotations for the training data. Though, not as much as one would suspect. With the right tools (and perseverance) it takes around 13 hours to recreate them - a mundane, but feasible task. Interestingly, it’s quite hard to pinpoint a single idea that (overall) contributed to such a significant improvement to the score while taking as little time.
<h3>Which tools did you use?</h3>
Aside from the ConvNet trio Python+<a href=""http://www.numpy.org/"" target=""_blank"">NumPy</a>+<a href=""https://github.com/Theano/Theano"" target=""_blank"">Theano</a>, we were using <a href=""http://sloth.readthedocs.org/en/latest/index.html"" target=""_blank"">Sloth</a> for location annotations and a small image-labeling program written in Julia.

We have also implemented an ad-hoc (but useful) tool that helped us in managing the plethora of different experiments that we had been running, and sharing their results with the rest of the team. We hope to release it someday, so stay tuned!
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
First and foremost, the satisfaction that our efforts can actually help preserve an endangered species. When we started fiddling with machine learning, helping to make a difference in the real-world was a distant dream. Now we feel that we can cross “making the world a better place” off our bucket lists.

We have also learned a lot, data-scientifically that is. At multiple points during this competition we had to make judgment calls between repairing the methodological purity and moving forward. There is a lot to learn from every such choice.
<h3>Do you have any advice for those just getting started in data science?</h3>
It’s very important to strike the right balance between theory and practice. Try out things while immersing in theory. Play around with different techniques and ideas, implement them, make them work, and find their limitations.

Also, don’t be afraid to participate in competitions - they’re one of the best training grounds that you’ll find. Especially, if you choose the worthwhale ones ;)
<h2>Teamwork</h2>
<h3>How did your team work together?</h3>
We believe we kept the right proportion of order and chaos. We would assess our progress and assign tasks once every 2-3 weeks, which I believe is often enough to get everyone on track and combine ideas and findings and scarcely enough to allow each teammate’s individuality to thrive. Ad hoc communication was also present throughout the competition, especially when not needed - we thank Marcin for his immense contribution in this area.
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
A lot of those, actually. Like how long am I supposed to microwave food? It’s always either ice cold or steaming hot, no middle ground whatsoever. And don’t even get me started on milk. Maybe if we all join forces we can find the underlying <a href=""https://en.wikipedia.org/wiki/Dirac_measure"" target=""_blank"">Dirac measure</a>.
<h3>A question for fellow Kagglers?</h3>
Over the course of the competition we’ve looked at an extensive number of whale pictures, and we suspect others have too. Which one was your favorite? Don’t tell us why, just show the picture. :)"
"Santa's Stolen Sleigh, Winners' Interview: 2nd place, Woshialex & Weezy",http://blog.kaggle.com/2016/01/28/santas-stolen-sleigh-winners-interview-2nd-place-woshialex-weezy/,2016-01-28 23:02:48,"Kaggle's annual Santa optimization competition wrapped up in early January with a nail-biting finish. When the dust settled, team Woshialex and Weezy had managed to <a href=""https://www.kaggle.com/c/santas-stolen-sleigh/leaderboard"" target=""_blank"">take 2nd place</a> in the competition and also take home the Rudolph prize. (This prize is awarded to the team that held 1st place on the leaderboard for the longest period of time.) In this blog the data scientists on the team, Mirsad and Qi, share the details of their simulated annealing algorithm, what worked / what didn't work, and why they benefited from teaming up.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
<b>Mirsad:</b> I just got my Ph.D. (two weeks before competition start) in Combinatorial Optimization (Operations Research) at <a href=""http://www.mines-ales.fr/"" target=""_blank"">Ecole des mines d'Ales - University of Montpellier</a>. The work conducted during the thesis mainly consisted of developing efficient local search algorithms for several difficult optimization problems.

[caption id=""attachment_5483"" align=""aligncenter"" width=""381""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-28-at-2.59.50-PM.png"" rel=""attachment wp-att-5483""><img class=""size-full wp-image-5483"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-28-at-2.59.50-PM.png"" alt=""Mirsad's profile on Kaggle"" width=""381"" height=""182"" /></a> Mirsad (aka <a href=""https://www.kaggle.com/weeezy/account"" target=""_blank"">weezy</a>) on Kaggle[/caption]

<b>Qi:</b> I got my Ph.D in theoretical physics (about Lattice Quantum chromodynamics &amp; Charge-Parity violation of the Weak interaction) from <a href=""http://www.columbia.edu/"" target=""_blank"">Columbia University</a> 4 years ago and after that I worked as a quantitative trader in a hedge fund.

[caption id=""attachment_5484"" align=""aligncenter"" width=""381""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-28-at-2.59.41-PM.png"" rel=""attachment wp-att-5484""><img class=""size-full wp-image-5484"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/Screen-Shot-2016-01-28-at-2.59.41-PM.png"" alt=""Qi Li (aka woshialex) on Kaggle"" width=""381"" height=""181"" /></a> Qi (aka <a href=""https://www.kaggle.com/woshialex/account"" target=""_blank"">woshialex</a>) on Kaggle[/caption]
<h3>How did you get started competing on Kaggle?</h3>
<b>Mirsad:</b> I got started with last year's Christmas Challenge - <a href=""https://www.kaggle.com/c/helping-santas-helpers"" target=""_blank"">Helping Santa's Helpers</a>. I am generally quite interested in competing in various optimization competitions and after accidentally discovering that one (a bit late though) I decided to give it a try.

<b>Qi:</b> A couple of years ago I found that machine learning stuff were very interesting and started to learn by myself, in order to practice these learned skills I found Kaggle which is a perfect place for me to practice.
<h3>What made you decide to enter this competition?</h3>
<b>Mirsad:</b> Christmas optimization competitions are basically the only ones at Kaggle that I can enter, and after a nice experience with the last year's competition, I was quite happy to enter this one as well. Also, it was just about right time for me since I could devote more time to it than I usually could. I would have probably entered this competition whatever the posted problem was, but the fact that this year's problem was really interesting and simply-defined (while in the same time being quite difficult) was just a plus.

<b>Qi:</b> It is a well defined clean problem so I liked it. Also it requires some hard skills (c++, optimization, etc.) and I think I had a better chance to win this one than other competitions in which people can just play/tune all kinds of existing machine learning algorithms.
<h2>Let's get technical</h2>
<h3>General procedure</h3>
Our two-phase approach to solving the problem consists of:
<ol>
	<li>Greedy procedure for the initial solution construction</li>
	<li>Simulated Annealing (SA) algorithm for improving the solution</li>
</ol>
Even though the quality (and structure) of initial solution highly influences the final results, the second solution step i.e. <a href=""https://en.wikipedia.org/wiki/Simulated_annealing"" target=""_blank"">Simulated Annealing</a> algorithm is a core element of our solution and careful algorithm design choices had to be made in order to have competitive results.

<strong>Initial Solution:</strong> The basic idea is to cluster the gifts based on longitudes and build the trips by sorting the gifts by latitudes in each cluster. Each trip will include the gifts that are close to each other (i.e. have similar longitudes) and will go from north to south. Total weight in each cluster is limited by 980 (slightly smaller than sleigh capacity of 1000) in order to enable easier search space exploration in the SA algorithm. Initial solution constructed this way had an objective of 12.505B
<h3>Simulated Annealing</h3>
In order to improve our initial solution we developed a Simulated Annealing (SA) algorithm with careful choices of parameters (i.e. starting temperature and cooling rate) and exploring several neighborhoods. Classic moves acceptance probability function, <em>e^(-delta/T)</em>, has been used where delta represents the difference in solution objective function (score) and T is the current temperature.

Moves inside a single trip and moves between the trips are implemented. In order to have a reasonable computation time, a limited subset of moves is considered; more precisely, only the moves between near trips (routes) are performed.

Setting <em>initial temperature (T0)</em> and <em>cooling speed/rate (s)</em> values has shown to be the most important in obtaining high quality results.
After performing extensive experiments, these values have been set to 0.25 and 1.00002 respectively. In each iteration of SA algorithm (a single iteration consists of evaluating a fix number of moves) <em>current temperature T</em> (starting from <em>T0</em>) is decreased by <em>s</em>.

[caption id=""attachment_5481"" align=""aligncenter"" width=""750""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/01/santa2016_blog2_1.png"" rel=""attachment wp-att-5481""><img class=""wp-image-5481 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/01/santa2016_blog2_1.png"" alt=""santa2016_blog2_1"" width=""750"" height=""640"" /></a> Illustration of objective function behaviour during the search for different parameter values.[/caption]

Finally, let's define the moves in SA. For the sake of simplicity, those moves that do not significantly influence the final result quality are omitted here (those include all the moves inside a trip).
<ul>
	<li><strong>Shift:</strong> Shift the gift from one route to another - gift to shift and route to shift to are chosen randomly (respecting constraints and range) and the best position to shift to is chosen (in terms of objective function).</li>
	<li><strong>Swap: </strong>Swap two gifts between the routes. Routes and first gift are chosen randomly, while the second gift is chosen to minimize the objective.</li>
	<li><strong>ShiftBlock (or ShiftN): </strong>Shift a block of gifts (gifts between positions i and i + N (N in {2,3}) from one route to another - routes and beginning position i are chosen randomly as before.</li>
	<li><strong>SwapBest: </strong>Choose two gifts g1 and g2 (from routes r1 and r2 respectively), drop them from the current routes and then insert g1 into the best position in r2 and g2 into the best position in r1 (can be seen as generalization of a Swap move).</li>
</ul>
Moves evaluation is done in parallel, using up to 8 threads. Final submission is obtained on 4-core machine (Intel i5) in cca 10 days of computation with s = 1.00002 and additional 2-3 days with s = 1.00005 (1.00002 run would not terminate before the deadline).

Our source code can be found at: <a href=""https://github.com/woshialex/SantaStolenSleigh"" target=""_blank"">https://github.com/woshialex/SantaStolenSleigh</a>
<h3>What worked</h3>
Indeed, Simulated Annealing algorithm seems to fit the problem perfectly given that no running time limit has been imposed and we believe that some sort of SA algorithm has been developed by most of the top ranked teams. Also, quite expected, choosing higher values for initial temperature and smaller cooling rate produces better results. However, running time increases quickly and one has to find a good balance.

Having said that, it was crucial to develop a ""good"" algorithm quickly in order to perform a run with desired parameters that would terminate before the end of the competition.

It was very important to make a multi-threaded version of the algorithm which enabled us to produce high quality solutions much more quickly than before. The first version of multi-threaded program has produced 12.389B solution in around 30h, which indeed enabled us to win the <a href=""https://www.kaggle.com/c/santas-stolen-sleigh/details/prizes"" target=""_blank"">Rudolph prize</a> (prize for the longest period of time at the top of the leaderboard), even though that seemed to be out of reach just the day before.
<h3>What didn't work</h3>
Running the algorithm iteratively, starting from the best solution and using different values of parameters showed to be less good than running the algorithm starting from the initial solution with a high enough initial temperature and smaller cooling rate.

However, we mainly made this type of runs for the first two or three weeks in the competition, which is mostly due to the fact that we wanted to see quick improvements and did not have enough CPU power to run many things in parallel. Several procedures/techniques that were improving intermediate solutions, but could not make any improvements whatsoever when running on the final solution generated by a ""slow"" run.

We also tried to use some other well-known optimization techniques other than SA, such as <a href=""https://en.wikipedia.org/wiki/Tabu_search"" target=""_blank"">Tabu Search</a>, <a href=""https://en.wikipedia.org/wiki/Integer_programming"" target=""_blank"">Mixed Integer Programming</a>, <a href=""https://en.wikipedia.org/wiki/Hill_climbing"" target=""_blank"">Hill Climbing</a>, etc, but none of them showed to be promising or capable of improving results.
<h3>What was your most important insight into the data?</h3>
Obviously, given that gift positions correspond to the points on the earth (and excluding the oceans) and a huge number of gifts is to be delivered (total of 100,000), choosing a given initial solution construction has shown to be important. It was quite easy to figure out quickly that most of the trips should be close to full since we have a sleigh base weight.

Additionally, due to the fact that not many points between Antarctica and other continents exist, the set of gifts with latitude &lt; -60 and the set of gifts with latitude &gt; -60 have been considered separately in initial solution procedure which improves the score from 12.70B to 12.505B. The choice of initial solution then naturally influenced some choices made in SA algorithm.
<h3>Were you surprised by any of your findings?</h3>
Not really. Even if we had to deal with a very large-size problem and complicated objective function  the (non-linear) solution still remained
""simple"" and similar to some common algorithms developed for classical <a href=""https://en.wikipedia.org/wiki/Vehicle_routing_problem"" target=""_blank"">Capacitated Vehicle Routing Problem</a> (CVRP). What surprised us a bit is that items weight shown to be almost irrelevant when studying the data and developing an algorithm and we believe that changing the distribution of weights in the data would not change much, even though in the beginning of the competition we were thinking that something will have to be done about the weights.
<h3>Which tools did you use?</h3>
SA algorithm has been implemented in C++ and without using any external libraries, while, for simplicity reasons, initial solution has been generated in python.
<h3>How did you spend your time on this competition?</h3>
Roughly, we spent 50% of the time on developing an algorithm and tuning, while the other half was just waiting for the runs to finish.
Indeed, no changes were made in our algorithm in the last two weeks of the competition which is a bit odd.

Having more powerful machine(s) to run the algorithm would have enabled us to spend more time on developing the algorithm or
to use even higher initial temperature (T0) or smaller cooling speed (1.00001 for example), possibly producing a solution good enough
to win the final prize.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
<b>Mirsad:</b> First of all, collaborating with Qi was a pleasure and a great experience. Winning the Rudolph prize, especially after we thought it was gone in the middle of the competition, was a great thing. Nevertheless, we feel that we were a bit too ""relaxed"" in the last week or two and that we could have done something to improve the final solution (especially trying to run on more powerful machines) without putting a lot of effort in, but just by running the code differently.

<b>Qi:</b> 1) Collaboration is really extremely helpful when I have a great team mate. 2) Never be over confident, always try all of your best if you want to win the competition.
<h2>Teamwork</h2>
<h3>How did your team form?</h3>
We formed a team quite early in the competition (10 days from the start); both of us had placed in the top five or six during that time and we thought that joining forces could help and was maybe the only option if we wanted to have a chance of winning one of the prizes.
<h3>How did competing on a team help you succeed?</h3>
One thing that helps for sure is having more CPU power than when you compete alone and that was one of the reasons we formed a team.

Other thing that certainly helped is that we had some amount of time spent on the top of the leaderboard before making a team (not much though) and at the end it showed to be important given that the competition was very tight.

We had slightly different methods when made a merge and we could quickly find some improvements by just combining the source.

Afterwards, being able to distribute the work to be done and exchange ideas has certainly helped, at least to gain some time.
<h2>Just for Fun</h2>
<h3>What kind of Kaggle competition would you like to see?</h3>
<b>Mirsad:</b> This kind of competition is perfect for me. Kaggle's team is really doing a great job in defining the problems - both times I competed, I was amazed by the simplicity and elegance of the problem definition and the fact that it was still extremely difficult to solve.

<b>Qi:</b> There are many machine learning competitions on Kaggle, but very few of the competitions require hard coding skills. I'd love to see more like this one. (Like some of the competitions on <a href=""https://www.topcoder.com/"" target=""_blank"">topcoder</a>).
<h2>Bios</h2>
<b>Mirsad Buljubasic</b> holds a Ph.D. in Combinatorial Optimization from Montpellier University, France, and a M.S. in Computer Science, from University of Sarajevo, Bosnia and Herzegovina. He is currently a post-doc researcher in LGI2P laboratory - Ecole des mines d'Ales.

<strong>Qi Liu</strong> earned his Ph.D. in theoretical physics and is currently working at a hedge fund."
"Rossmann Store Sales, Winner's Interview: 2nd place, Nima Shahbazi",http://blog.kaggle.com/2016/02/03/rossmann-store-sales-winners-interview-2nd-place-nima-shahbazi/,2016-02-03 20:59:35,"Rossmann operates over 3,000 drug stores in 7 European countries. In their first Kaggle competition, <a href=""https://www.kaggle.com/c/rossmann-store-sales"" target=""_blank"">Rossmann Store Sales</a>, this drug store giant challenged Kagglers to forecast 6 weeks of daily sales for 1,115 stores located across Germany. The competition attracted 3,738 data scientists, making it our second most popular competition by participants ever.

Nima Shahbazi <a href=""https://www.kaggle.com/c/rossmann-store-sales/leaderboard"" target=""_blank"">took second place</a> in the competition, using his background in data mining to gain an edge. By fully exploring and understanding the dataset, Nima was able to engineer features that many participants overlooked. In fact, one valuable feature was developed from selected data that many Kagglers had removed from their training set entirely.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I am a PhD candidate at the <a href=""http://lassonde.yorku.ca/"" target=""_blank"">Lassonde School of Engineering</a> at York University under the supervision of Prof. <a href=""http://www.cs.yorku.ca/~jarek/"" target=""_blank"">Jarek Gryz</a> and co-supervision of Prof. <a href=""http://www.cse.yorku.ca/~aan/"" target=""_blank"">Aijun An</a>. My main research area is data mining and machine learning. I love problem solving and challenging myself to find the best model for regression and classification/clustering problems. Before entering Kaggle competitions, I used to work on data analysis problems; the most important one was the time series prediction in FOREX market for which I have designed many algorithmic trading strategies.

[caption id=""attachment_5498"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-03-at-2.28.18-PM.png"" rel=""attachment wp-att-5498""><img class=""size-medium wp-image-5498"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-03-at-2.28.18-PM-300x144.png"" alt=""Nima's Kaggle profile"" width=""300"" height=""144"" /></a> Nima's Kaggle <a href=""https://www.Kaggle.com/nimashahbazi"" target=""_blank"">profile</a>[/caption]
<h3>How did you get started competing on Kaggle?</h3>
About 9 months ago, I was informed by one of my friends that the IEEE International Conference on Data Mining series (ICDM) has established an interesting <a href=""https://www.Kaggle.com/c/icdm-2015-drawbridge-cross-device-connections"" target=""_blank"">contest</a> in data mining. In that competition participants are tasked with identifying a set of user connections across different devices without using common user handle information such as name, email, phone number, etc. Moreover, participants were going to be asked to figure out the likelihood that a set of different IDs from different domains belong to the same user and at what performance level. I got very excited and start working on that problem. Finally I ranked 7<sup>th</sup> in that <a href=""https://www.Kaggle.com/c/icdm-2015-drawbridge-cross-device-connections/leaderboard"" target=""_blank"">competition</a> and have learned lots of new approaches for machine learning and data mining problems. I realized that in order to be successful in this field you should challenge yourself with real world problems. Although the theoretical knowledge is a must, without having experience in real world problems you will not able to succeed.
<h3>What made you decide to enter this competition?</h3>
After the ICDM contest I realized that some Kaggle competitions are referenced in <a href=""https://www.Kaggle.com/benhamner/d/benhamner/nips-2015-papers/Kaggle-references-in-nips-2015-papers"" target=""_blank"">NIPS papers</a>. That made me more motivated to join other competitions. I found the Rossmann challenge very interesting since a sales forecast is useful for any company. I really wanted to learn the latest data mining approaches for solving these problems as in the Kaggle community some of the participants share their approaches at the end of the competition. I entered to be involved in the competition and to give myself a chance to win. At the end, I was able to design a model which was consistent both on the public and private leaderboards.
<h2>Let's Get Technical</h2>
<h3>What was your most important insight into the data? What preprocessing and supervised learning methods did you use?</h3>
I spent a great amount of time digging through the data. It was mentioned in the competition evaluation page that any “day” and “store” with 0 sales is ignored in scoring the test or train set. But in the test and train data we do have many stores with sales equal to zero (why bother to include them!). At first it might seem that we can totally remove the rows with sales equal to zero (which are the stores that are closed on that specific day). Also all the <a href=""https://www.kaggle.com/c/rossmann-store-sales/scripts"" target=""_blank"">scripts</a> in the Kaggle community remove the zero sales before starting to create a model. I thought that there must be something related to these zero sale days. So I did not remove them and started extracting knowledge from them. I was surprised when I found out that there was a relation between consequence close (zero sales) and unexpected sales before or after opening the store. For example see Figure 1 for store number 1039. The zero sales (store closed) are highlighted with red and corresponding sales before and after those days are shown with green bars.

[caption id=""attachment_5499"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/rossmann2_1-e1454538607718.png"" rel=""attachment wp-att-5499""><img class=""size-full wp-image-5499"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/rossmann2_1-e1454538607718.png"" alt=""Figure 1. Unusual sales before/after consecutive close"" width=""600"" height=""575"" /></a> Figure 1. Unusual sales before/after consecutive close[/caption]

If I removed the zero sale rows I would not be able to tell my model to learn that specific pattern of unusual sales. So I go went the data and find those zero sales and create a dummy variable called <em>MyRowHoliday</em> to capture the information for these unusual sales. This dummy variable assigned positive integers before and after the consecutive close and “-1” on other days as shown in Figure 2.

[caption id=""attachment_5500"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/rossmann2_2-e1454538642298.png"" rel=""attachment wp-att-5500""><img class=""size-full wp-image-5500"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/rossmann2_2-e1454538642298.png"" alt=""Figure 2. MyRowHoliday codes for more than two consecutive close"" width=""600"" height=""609"" /></a> Figure 2. MyRowHoliday codes for more than two consecutive close[/caption]

I have created 5 more dummy features (<em>refurbishment</em>, <em>MystateHoliday</em>, <em>MySchoolHoliday</em>, <em>MyPromo</em> and <em>Promo2Active</em>) that change these categorical variables to large range of numbers. For example, in the original data the days that have promotions are marked with 1; otherwise 0. I found out that this information is not sufficient for the learners because the beginning and end of the promotion have some effect on sales. Like other teams, I used extreme gradient boosting (<a href=""https://github.com/dmlc/xgboost"" target=""_blank"">xgboost</a>) as a learning method.<strong> </strong>
<h3>Were you surprised by any of your findings?</h3>
Yes, I always got surprised when even one of my finding showed better performance on the leaderboard. The most important one was when I added four more features for time series analysis. Those were simple moving averages (<em>MA_Fast</em>, <em>MA_Slow</em>, <em>MA_Customer_Fast</em> and <em>MA_Customer_Slow</em>) for sales and customers over different time windows. The moving averages were split by important feature like <em>store number</em>, <em>day of week</em> and <em>promotion</em>. The model I built based on moving averages blended well with my previous model and made my rank 3<sup>rd</sup> in the last week of the competition.
<h3>Which tools did you use?</h3>
For preprocessing and exploratory data analysis I used R and I usually write the code both in R and Python.
<h3>How did you spend your time on this competition?</h3>
I spent more than 70% on feature engineering, and 30% on feature selection, model ensembling, and model tuning.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
I did not use a super-fast machine. For this competition I used a machine with 8-core CPU and 16GB of RAM. I ran the models in parallel on my laptop and my own desktop computer. When I woke up I started coding and digging into the data, and while I was sleeping I let the computers run my algorithm for model tuning. The winning solution has 15 models which took more than 25 hours to build and predict.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
First of all, the money. To be honest I was thinking about the prize since I joined the competition. And now I am confident in my abilities that I can win a prize in a world-wide competition with more than 3,300 teams around the world. Plus, I learn a lot anytime I go through the Kaggle <a href=""https://www.kaggle.com/forums"" target=""_blank"">forum</a> or <a href=""https://www.kaggle.com/scripts"" target=""_blank"">scripts</a>.
<h3>Do you have any advice for those just getting started in data science?</h3>
First of all, make sure you understand the math behind regression and classification and the way that a model learns. And the most important thing you should learn is how do learning methods dealing with regularization to avoid overfitting. Second, fully understand the principles of cross validation, and which type of cross validation fit your problem. Finally, do not spend lots of time tuning the model. Instead, spend your time extracting features and understanding the data. The more you play with the data the more you will find interesting insights.
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
I recently saw a competition on a specific cancer with the goal of helping to prevent it by identifying at-risk populations. I would really like to see more cancer related data mining competition and specifically any research related to <a href=""https://en.wikipedia.org/wiki/Cholangiocarcinoma"" target=""_blank"">Cholangiocarcinoma</a> cancer. The rates for this kind of cancer have been rising worldwide over the past several decades [1] and beside that one of my relative is suffering from it.
<h3>What is your dream job?</h3>
A dream job for me is a job in which I can work for myself.
<h2>Bio</h2>
<b>Nima Shahbazi<a href=""http://blog.Kaggle.com/wp-content/uploads/2016/02/nima_headshot_blog.png"" rel=""attachment wp-att-5497""><img class=""alignright wp-image-5497"" style=""margin-left: 10px; margin-top: 0px;"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/02/nima_headshot_blog.png"" alt="""" width=""107"" height=""107"" /></a></b> is a second-year PhD Student in the Data Mining and Database Group at York University. He previously worked in big data analytics, specifically on Forex Market. His current research interests include Mining Data Streams, Big Data Analytics and Deep Learning.

&nbsp;

[1] Patel T. ""<a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC113759/"" target=""_blank"">Worldwide trends in mortality from biliary tract malignancies</a>"". <em>BMC Cancer</em> <strong>2</strong>: 10."
"NOAA Right Whale Recognition, Winner's Interview: 2nd place, Felix Lau",http://blog.kaggle.com/2016/02/04/noaa-right-whale-recognition-winners-interview-2nd-place-felix-lau/,2016-02-04 19:50:28,"With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. In the <a href=""https://www.kaggle.com/c/noaa-right-whale-recognition"" target=""_blank"">NOAA Right Whale Recognition</a> challenge, 470 players on 364 teams competed to build a model that could identify any individual, living North Atlantic right whale from its aerial photographs.

Felix Lau entered the competition with the goal of practicing new techniques in deep learning, and ended up taking second place. This blog shares his background, some of the limitations he ran into, and a high level overview of his approach. For a more technical description of his winning solution, don't miss the <a href=""http://felixlaumon.github.io/2015/01/08/kaggle-right-whale.html"" target=""_blank"">post</a> in his personal blog.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
[caption id=""attachment_5510"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-04-at-12.01.41-PM.png"" rel=""attachment wp-att-5510""><img class=""size-medium wp-image-5510"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-04-at-12.01.41-PM-300x141.png"" alt=""Felix on Kaggle"" width=""300"" height=""141"" /></a> Felix (aka <a href=""https://www.kaggle.com/felixlaumon"" target=""_blank"">felixlaumon</a>) on Kaggle[/caption]

I am currently working at a fashion consulting company as a computer vision research engineer. I have trained a number of deep neural networks at work. I participated some image Kaggle competitions before (<a href=""https://www.kaggle.com/c/diabetic-retinopathy-detection"" target=""_blank"">Diabetic Retinopathy Detection</a> and the <a href=""https://www.kaggle.com/c/datasciencebowl"" target=""_blank"">1st Data Science Bowl</a>). Although I didn’t do well in those competitions, the experiences helped me to get started in this competition quickly.
<h3>What made you decide to enter this competition?</h3>
I was inspired by the new deep learning ideas and techniques proposed in the last few months, and I wanted to apply them to a real problem. At the end, even though some of the ideas didn’t turn out to work well for this problem, it was still a good experience because I now understand the limitations of some of the approaches.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
All my approaches were based on deep convolutional neural network as it is used by most, if not all, winners of the previous image Kaggle competitions. However it turned out for this dataset, the neural network didn't manage to extract the right features from the raw images. In particular, the classifier had troubles focusing on the “whale face” on its own, as I suspect the “whale name” alone was not a strong enough training signal.

[caption id=""attachment_5506"" align=""aligncenter"" width=""306""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/noaa2_1.png"" rel=""attachment wp-att-5506""><img class=""wp-image-5506 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/noaa2_1.png"" alt=""noaa2_1"" width=""306"" height=""308"" /></a> Saliency map of the neural network trained on raw image[/caption]

The approach that I found worked well was to first predict the blowhead and bonnet of the whale and crop the raw images accordingly. The goal is to guide the network to focus on the callosity pattern on the back of the whale. Then I trained a classifier using the cropped images to predict the final “whale name”.

&nbsp;

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/noaa2_2.png"" rel=""attachment wp-att-5507""><img class=""aligncenter size-full wp-image-5507"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/noaa2_2.png"" alt=""noaa2_2"" width=""900"" height=""310"" /></a>

You should check out my <a href=""http://felixlaumon.github.io/2015/01/08/kaggle-right-whale.html"" target=""_blank"">blog post</a> for the details. Below is a summary of my best approach.

<b>Blowhead and Bonnet Localizer</b>

The blowhead and bonnet localizer was based on the VGG-net. The goal is to output the x-y coordinates of the 2 key points (blowhead and bonnet) to create aligned whale face images. I trained the network with mean squared error as its error function. Real-time augmentation (e.g. rotation, translation, contrast augmentation) was applied during training. I found that test-time augmentation improved the accuracy of the coordinates quite significantly.

<b>Classifier</b>

The classifier took the aligned face images to output the probability of the final “whale name”. Because the images fed to this network are always aligned, neither train-time nor test-time augmentation was applied. I tried out different network architectures and found that residual network (ResNet) works quite well.

The final classifier was an ensemble of the following networks:
<ul>
	<li>3 x 19-layer VGGNet</li>
	<li>1 x 31-layer ResNet</li>
	<li>1 x 37-layer ResNet</li>
	<li>1 x 67-layer ResNet</li>
</ul>
<h3>Which tools did you use?</h3>
<strong>Software</strong> – I used Python as a programming language. For frameworks, I used <a href=""https://github.com/Theano/Theano"" target=""_blank"">Theano</a> and <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a> to build up the neural networks, <a href=""https://github.com/dnouri/nolearn"" target=""_blank"">nolearn</a> for the training loop, my nolearn_utils for real-time augmentation, <a href=""http://scikit-learn.org/stable/"" target=""_blank"">scikit-learn</a> for ensembling, and scikit-image for image processing.

<strong>Hardware</strong> – I used GTX 980ti and GTX 670 for training the model locally, and AWS EC2 g2.xlarge for optimizing their hyperparameters (with Docker and Docker Machine).
<h3>How did you spend your time on this competition?</h3>
Most of my work was done in the last 3 weeks. I spent about 20% of my time building up a baseline submission, 30% building up the infrastructure and code refactoring, 50% experimenting with alternative approaches.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
It is still a surprise to me that the model trained from raw images (an end-to-end approach) did not perform well. Providing additional training signals (e.g. bonnet and blowhead coordinates, type of the callosity pattern as in the 1st place approach) proved to be a very important trick for this competition.

When the dataset size is limited, it seems augmenting the training labels is just as important as augmenting the training data (i.e. image perturbation)
<h3>Do you have any advice for those just getting started in data science?</h3>
Building models is only a small part of what a data scientist do. Understanding the problem, collecting the dataset, designing an evaluation metric, and communicating the findings effectively are just as important.
<h2>Bio</h2>
You can find Felix on <a href=""https://twitter.com/phelixlau)"" target=""_blank"">Twitter</a> and on his <a href=""http://felixlaumon.github.io/"" target=""_blank"">blog</a>.

<hr />

For more on the NOAA Right Whale Recognition competition, click the tag below!

&nbsp;"
"Winton Stock Market Challenge, Winner's Interview: 3rd place, Mendrika Ramarlina",http://blog.kaggle.com/2016/02/12/winton-stock-market-challenge-winners-interview-3rd-place-mendrika-ramarlina/,2016-02-12 20:49:15,"The <a href=""https://www.kaggle.com/c/the-winton-stock-market-challenge"" target=""_blank"">Stock Market Challenge</a>, Winton's second recruiting competition on Kaggle, asked participants to predict intra and end of day stock returns. The competition was crafted by research scientist at Winton to mimic the type of problem that they work on everyday. Mendrika Ramarlina <a href=""https://www.kaggle.com/c/the-winton-stock-market-challenge/leaderboard"" target=""_blank"">finished third</a> in the competition with a combination of simple models and intelligently engineered features.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I come from a software engineering background. I have 4 years of experience building data-centric web applications. I have been taking online machine learning classes, reading research papers, and implementing models since 2012.

[caption id=""attachment_5523"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/mendrika_profile.png"" rel=""attachment wp-att-5523""><img class=""size-medium wp-image-5523"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/mendrika_profile-300x143.png"" alt=""Mendrika on Kaggle"" width=""300"" height=""143"" /></a> <a href=""https://www.kaggle.com/ramarlina"" target=""_blank"">Mendrika</a> on Kaggle[/caption]
<h3>How did you get started competing on Kaggle?</h3>
I found out about Kaggle from an article on Bloomberg from back in 2012. At the time, I had just completed <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Andrew Ng's class</a>. I wanted to start working on projects involving machine learning, but wasn't sure what to work on. Kaggle competitions were a great way to get to started.
<h3>What made you decide to enter this competition?</h3>
Forecasting stock returns is a very interesting problem. I have been applying machine learning to the financial market for a few years now. I joined to see how some of my approaches would fare in comparison to others'.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
Feature engineering was a key element in my approach. My final solution was an ensemble made up of 2 SVMs and 1 regularized linear regression all trained on handcrafted features such as peak-to-valley drawdown magnitude and duration or cumulated intraday returns.

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/winton3_1.png"" rel=""attachment wp-att-5519""><img class=""aligncenter size-full wp-image-5519"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/winton3_1.png"" alt=""winton3_1"" width=""800"" height=""600"" /></a>

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/winton3_2.png"" rel=""attachment wp-att-5520""><img class=""aligncenter size-full wp-image-5520"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/winton3_2.png"" alt=""winton3_2"" width=""871"" height=""510"" /></a>
<h3>Which tools did you use?</h3>
For preprocessing and model training, I used a standard pythonic machine learning stack: <a href=""http://pandas.pydata.org/"" target=""_blank"">Pandas</a>, <a href=""http://www.numpy.org/"" target=""_blank"">Numpy</a>, <a href=""http://scikit-learn.org/stable/"" target=""_blank"">Scikit-Learn</a>. I used <a href=""http://matplotlib.org/"" target=""_blank"">matplotlib</a> for visualization, and I did use some Excel for data exploration.
<h3>How did you spend your time on this competition?</h3>
I spent the majority of my time analyzing the data, engineering features. Instead of training and fine- tuning complex models, my approach was to use simple models, then create features that would help improve the performance of these models.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
That it is very difficult to predict minute-wise intraday data using historical data alone.
<h3>Do you have any advice for those just getting started in data science?</h3>
<ul>
	<li>Don't get hung up on one technology stack, tool, or algorithm</li>
	<li>Understand how different statistical models “learn” from the data</li>
	<li>Keep up with research</li>
</ul>
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
I would start a competition to predict what crime will occur when and where, given historical crime data.
<h2>Bio</h2>
<strong>Mendrika Ramarlina<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/headshot_MendrikaRamarlina.png"" rel=""attachment wp-att-5521""><img class=""wp-image-5521 alignright"" style=""margin-left: 15px; margin-top: 0px;"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/headshot_MendrikaRamarlina-150x150.png"" alt="""" width=""137"" height=""137"" /></a></strong> is a founder and machine learning engineer at Madagascar Innovation Lab, a startup that uses machine learning to solve problems with social impacts in Madagascar. His interests include deep learning and reinforcement learning."
"Profiling Top Kagglers: KazAnova, New #1 in the World",http://blog.kaggle.com/2016/02/10/profiling-top-kagglers-kazanova-new-1-in-the-world/,2016-02-10 23:20:53,"<em>This blog was originally published on May 7, 2015 when Marios Michailidis was ranked #2. Marios is now the number 1 data scientist out of 465,000 data scientists! We wanted to re-share the original post, with a few additions and updates from Marios. We've also just published <a href=""http://blog.kaggle.com/2016/02/10/my-kaggle-experience-spot-chasing-retirement/"" target=""_blank"">a post</a> from Marios on his experience chasing the #1 spot on Kaggle, and what he's taken away from the experience. </em>

<hr />

&nbsp;

There are Kagglers, there are Master Kagglers, and then there are top 10 Kagglers. Who are these people who consistently win Kaggle competitions? In this series we try to find out how they got to the top of the leaderboards.

First up is <a href=""https://www.kaggle.com/users/111640/kazanova"" target=""_blank"">KazAnova</a> -- Marios Michailidis -- the current number 2 (<em>now #1</em>) out of nearly 300,000 (<em>now 465,000+) </em>data scientists. Marios is a PhD student in machine learning at <a href=""http://www.cs.ucl.ac.uk/"" target=""_blank"">UCL</a> and a Manager of Data Science at <a href=""http://www.dunnhumby.com"" target=""_blank"">dunnhumby</a> (organizer of the Kaggle competitions '<a href=""https://www.kaggle.com/c/dunnhumbychallenge"" target=""_blank"">Shopper Challenge</a>' and '<a href=""https://www.kaggle.com/c/hack-reduce-dunnhumby-hackathon"" target=""_blank"">Product Launch Challenge</a>').
<h2>Marios Michailidis Q&amp;A</h2>
<h3>How did you start with Kaggle competitions?</h3>
I wanted a new challenge in the field and learn from the Grand Masters.

I was doing software development about machine learning algorithms, which also led to creating a <a href=""http://www.kazanovaforanalytics.com/software.html"" target=""_blank"">GUI for credit scoring/analytics</a> by the name KazAnova- a nick name I frequently use in Kaggle to keep reminding myself the passion I have for the field and how it started, <b>but</b> I could only go so far by myself.

Kaggle seemed the right place to learn from the experts.
<h3>What is your first plan of action when working on a new competition?</h3>
<ol>
	<li>First of all to understand the problem and the <a href=""https://www.kaggle.com/wiki/Metrics"" target=""_blank"">metric</a> we are tested on- this is <b>key</b>.</li>
	<li>To as-soon-as possible create a reliable cross-validation process that best would resemble the leaderboard or the test set in general as this will allow me to explore many different algorithms and approaches, knowing the impact they could yield.</li>
	<li>Understand the importance of different algorithmic families, to see when and where to maximize the intensity (is it a linear or non-linear type of problem?)</li>
	<li>Try many different approaches/techniques on a the given problem and seize it from all possible angles in terms of algorithms 'selection, hyper parameter optimization, feature engineering, missing values' treatment- I treat all these elements as <b>hyper</b> parameters of the final solution.</li>
</ol>
<h3>What does your iteration cycle look like?</h3>
<ol>
	<li>Sacrifice a couple of submissions in the beginning of the contest to understand the importance of the different algorithms -- save energy for last 100 meters.</li>
	<li>Do the following process for multiple models
<ul>
	<li>Select a model and do a recursive loop with the following steps:
<ul>
	<li>Transform data (scaling, log(x+1) values, treat missing values, PCA or none)</li>
	<li>Optimize hyper parameters of the model</li>
	<li>Do feature engineering for that model (as in generate new features)</li>
	<li>Do features' selection for that model (as in reducing them)</li>
	<li>Redo previous steps as optimum parameters are likely to have changed slightly</li>
</ul>
</li>
	<li>Save hold-out predictions to be used later (meta-modelling)</li>
	<li>Check consistency of CV scores with leaderboard. If problematic, re-assess cross-validation process and re-do steps</li>
</ul>
</li>
	<li>Create partnerships. Ideally you look for people that are likely to have taken different approaches than you have. Historically (in contrast) I was looking for friends; people I can learn from and people I can have fun with - not so much winning.</li>
	<li>Find a good way to ensemble</li>
</ol>
[caption id=""attachment_4846"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/kazanova-analytics.png""><img class=""wp-image-4846 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/kazanova-analytics.png"" alt=""kazanova-analytics"" width=""500"" height=""520"" /></a> Screen from the KazAnova Analytics GUI.[/caption]

&nbsp;
<h3>What are your favorite machine learning algorithms?</h3>
I like Gradient Boosting and Tree methods in general:
<ol>
	<li>Scalable</li>
	<li>Non-linear and can capture deep interactions</li>
	<li>Less prone to outliers</li>
</ol>
<h3>What are your favorite machine learning libraries?</h3>
<ol>
	<li><a href=""http://scikit-learn.org/stable/"" target=""_blank"">Scikit</a> for forests.</li>
	<li><a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> for GBM.</li>
	<li><a href=""http://www.csie.ntu.edu.tw/~cjlin/liblinear/"" target=""_blank"">LibLinear</a> for linear models.</li>
	<li><a href=""http://www.cs.waikato.ac.nz/ml/weka/"" target=""_blank"">Weka</a> for all.</li>
	<li><a href=""http://www.heatonresearch.com/encog"" target=""_blank"">Encog</a> for neural nets.</li>
	<li><a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a> for nets, although I learnt it very recently.</li>
	<li><a href=""http://people.cs.umass.edu/~vdang/ranklib.html"" target=""_blank"">RankLib</a> for functions like <a href=""http://en.wikipedia.org/wiki/Discounted_cumulative_gain"" target=""_blank"">NDCG</a>.</li>
</ol>
[caption id=""attachment_4847"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/boostedtree.png""><img class=""wp-image-4847 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/boostedtree.png"" alt=""boostedtree"" width=""500"" height=""210"" /></a> Image from slides <a href=""http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf"" target=""_blank"">'Introduction to Boosted Trees'</a> by <a href=""https://www.kaggle.com/users/32300/tianqi-chen"" target=""_blank"">Tianqi Chen</a> (XGBoost)[/caption]

&nbsp;
<h3>What is your approach to hyper-tuning parameters?</h3>
I do this very manually.

I have only tried once to use something like Gridsearch. I feel I learn more about the algorithms and why they work the way they do by doing this manually. At the same time I feel that ""I do something, it's not only the machine!"".

After 60+ competitions I've found that I can get to the top 90% of the best hyper parameters with the first try, so the manual approach has paid off!
<h3>What is your approach to solid CV/final submission selection and LB fit?</h3>
In regards to CV, I try to best resemble what I am being tested on.

In many situations a random split would not work. For example: In the <a href=""http://www.kaggle.com/c/acquire-valued-shoppers-challenge"" target=""_blank"">Acquire valued shoppers' challenge</a> we were mainly tested on different products (offers) than these available on the train set. I made my CV to always try to predict 1 offer using the rest of the offers as this could resemble the test leaderboard better than a random split.

About final selection, I normally go for best Leaderboard submission and best CV submission. In the case of a happy collision, I select something as different as possible with respectable CV result just in case I am lucky!

<small>(A prayer to the god of overfitting is my secret 3rd submission)</small>
<h3>In a few words: What wins competitions?</h3>
<ol>
	<li>Understand the problem well</li>
	<li>Discipline ; To have a well-thorough and documented approach that you follow religiously and defines all the modelling process/framework from how you cross-validate, select models, avoids over fitting (which requires a lot of ...discipline).</li>
	<li>Allow room to try problem-specific things or new approaches within that framework</li>
	<li>The hours you put in</li>
	<li>Have access to the right tools</li>
	<li>Make key partnerships</li>
	<li>Ensembling</li>
</ol>
<h3>What is your favourite Kaggle competition and why?</h3>
The <a href=""www.kaggle.com/c/acquire-valued-shoppers-challenge"" target=""_blank"">Acquire valued shoppers</a>' challenge, not only because I won and it is relevant to what my team does, but I also had the honour to collaborate with <a href=""http://www.kaggle.com/users/9974/gert"" target=""_blank"">Gert Jacobusse</a>.

[caption id=""attachment_4848"" align=""aligncenter"" width=""500""]<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/leaderboard.png""><img class=""size-full wp-image-4848"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/leaderboard.png"" alt=""The final private leaderboard for the Valued Shopper's Challenge"" width=""500"" height=""470"" /></a> The final private leaderboard for the Valued Shopper's Challenge[/caption]
<h3>What was your least favourite Kaggle competition experience?</h3>
<a href=""https://www.kaggle.com/c/decoding-the-human-brain"" target=""_blank"">DecMeg</a>, <a href=""https://www.kaggle.com/c/inria-bci-challenge"" target=""_blank"">BCI</a> and such channel-wave type of competitions. They have big data and are very domain specific.

I found it hard to even make my CV working properly, plus I did quite bad. Hopefully I will improve.
<h3>What field in machine learning are you most excited about?</h3>
I like recommender systems if it can be considered a separate field.

There is a broad spectrum of techniques you can use (which are field specific) and to be able to understand what the customer likes is very challenging and rewarding.
<h3>Which machine learning researchers do you study?</h3>
I study: <a href=""http://www.kaggle.com/users/25112/steffen-rendle"" target=""_blank"">Steffen Rendle</a>, <a href=""https://www.stat.berkeley.edu/~breiman/"" target=""_blank"">Leo Breiman</a>, <a href=""http://www.ci.tuwien.ac.at/~alexis/About.html"" target=""_blank"">Alexander Karatzoglou</a>, <a href=""https://www.kaggle.com/users/1455/michael-jahrer"" target=""_blank"">Michael Jahrer</a> &amp; <a href=""http://www.researchgate.net/profile/Andreas_Toescher"" target=""_blank"">Andreas Töscher</a>, the <a href=""http://www.csie.ntu.edu.tw/~cjlin/mlgroup/"" target=""_blank"">Machine Learning and Data Mining Group</a> at National Taiwan University, and <a href=""http://web4.cs.ucl.ac.uk/staff/jun.wang/blog/about/"" target=""_blank"">Jun Wang</a> &amp; <a href=""http://www0.cs.ucl.ac.uk/staff/p.treleaven/"" target=""_blank"">Philip Treleaven</a>.
<h3>Can you tell us something about the last algorithm you hand-coded?</h3>
It was <a href=""http://www.libfm.org"" target=""_blank"">LibFM</a> for <a href=""https://www.kaggle.com/c/avazu-ctr-prediction"" target=""_blank"">Avazu competition</a> as I believed it could work well in that particular problem. I could not make it work as well as <a href=""http://www.csie.ntu.edu.tw/~cjlin/libffm/"">LibFFM</a> apparently.
<h3>How important is domain expertise for you when solving data science problems?</h3>
For some competitions it is really important.

The fact that I am employed in the recommendation science field and the kind of work that we do within my team, has helped me win the Acquire valued Shoppers challenge.

However I think you can go a long way by following standard approaches even if you don't know the field well, which is also the beauty of machine learning and the fact that some algorithms do a significant job for you.

<a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/ensembling.png""><img class=""aligncenter wp-image-4849 size-medium"" style=""text-align: center; margin-left: auto; margin-right: auto;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/ensembling-300x272.png"" alt=""ensembling"" width=""300"" height=""272"" /></a>
<h3>What do you consider your most creative trick/find/approach?</h3>
I do multiple ensemble meta-stacking if I can use the term.

During the course of my univariate model tuning I save all the models' outputs. Then I make meta-models with the univariate models selections and most of the times I end up with different ensembles of Meta models.

Sometimes I go to third Meta model with Meta models as inputs (Meta-Meta model).
<h3>How are you currently using data science at work and does competing on Kaggle help with this?</h3>
<b>Classified!</b>

Kaggle does help in optimizing my methods, learning new skills, meet nice people with same passion, be up to date with new tools and generally stay in touch with what's going on in the field.
<h3>What is your opinion on the trade-off between high model complexity and training/test runtime?</h3>
That is a big discussion in principle.

I guess there is a trade-off, but there needs to be an understanding that better models are not necessarily the most interpretable ones and that a more complex model (that is likely to score better) is not necessarily less stable/more dangerous.

I guess the optimum solution should be somewhere in the middle (e.g. not an ensemble of 100 models nor Naive Bayes)

[caption id=""attachment_5532"" align=""aligncenter"" width=""426""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/marios_awards_new.png""><img class=""wp-image-5532 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/marios_awards_new.png"" alt="""" width=""426"" height=""244"" /></a> THE BEST 8 FINISHES FOR KAZANOVA.[/caption]
<h3>How did you get better at Kaggle competitions?</h3>
Did I ?! :D

I guess what has helped a lot is:
<ol>
	<li>Seeing previous solutions and end-of-competition threads</li>
	<li>Participate in Kaggle forums</li>
	<li>Learn the tools</li>
	<li>Read papers, websites, machine learning tutorials</li>
	<li>Optimize processes (use sparse matrices, cut unnecessary steps, write more efficient code)</li>
	<li>Save everything I've done and reuse (and improve). E.g. I keep a separate folder for each competition I've completed.</li>
	<li>Dedicate time (had to reduce video games)</li>
	<li>Collaborate with others</li>
</ol>
I have found the following resources useful:
<ol>
	<li>This <a href=""http://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/4797/starter-code-in-python-with-scikit-learn-auc-885"" target=""_blank"">benchmark</a> by <a href=""https://www.kaggle.com/users/44623/paul-duan"" target=""_blank"">Paul Duan</a> in the <a href=""https://www.kaggle.com/c/amazon-employee-access-challenge"" target=""_blank"">Amazon competition</a> (my first ever attempt with Python) for a general modelling framework.</li>
	<li>From the same competition : <a href=""http://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/4838/python-code-to-achieve-0-90-auc-with-logistic-regression"" target=""_blank"">Python code to achieve 0.90 AUC with logistic regression</a> from <a href=""https://www.kaggle.com/users/66023/miroslaw-horbal"" target=""_blank"">Miroslaw Horbal</a> to create pairwise interactions with cross-validation.</li>
	<li>For text analysis, this benchmark from <a href=""https://www.kaggle.com/users/5309/abhishek"" target=""_blank"">Abhishek</a>: <a href=""http://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878"" target=""_blank"">Beating the benchmark</a> in <a href=""https://www.kaggle.com/c/stumbleupon"" target=""_blank"">StumbleUpon Evergreen Challenge</a></li>
	<li><a href=""http://www.kaggle.com/c/higgs-boson/forums/t/8184/public-starting-guide-to-get-above-3-60-ams-score"" target=""_blank"">XGBoost benchmark</a> in <a href=""https://www.kaggle.com/c/higgs-boson"" target=""_blank"">Higgs Boson competition</a> by <a href=""https://www.kaggle.com/users/43581/bing-xu"" target=""_blank"">Bing Xu</a></li>
	<li><a href=""https://www.kaggle.com/users/185835/tinrtgu"" target=""_blank"">Tinrtgu</a>'s <a href=""http://jmlr.org/proceedings/papers/v15/mcmahan11b/mcmahan11b.pdf"" target=""_blank"">FTRL</a> Logistic model in Avazu: <a href=""http://www.kaggle.com/c/avazu-ctr-prediction/forums/t/10927/beat-the-benchmark-with-less-than-1mb-of-memory"" target=""_blank"">Beat the benchmark with less than 1MB of memory</a></li>
	<li><a href=""https://www.kaggle.com/c/datasciencebowl"" target=""_blank"">Data science Bowl</a> tutorial for image classification: <a href=""http://nbviewer.ipython.org/github/udibr/datasciencebowl/blob/master/141215-tutorial.ipynb"" target=""_blank"">IPython Notebook Tutorial</a>.</li>
	<li><a href=""http://0xdata.com/"" target=""_blank"">H2O</a> (R) <a href=""https://www.kaggle.com/c/afsis-soil-properties/forums/t/10568/ensemble-deep-learning-from-r-with-h2o-starter-kit"" target=""_blank"">deep learning benchmark</a> from <a href=""https://www.kaggle.com/users/234686/arno-candel"" target=""_blank"">Arno Candel</a> in <a href=""https://www.kaggle.com/c/afsis-soil-properties"" target=""_blank"">Africa Soil competition</a></li>
	<li><a href=""http://nbviewer.ipython.org/github/ottogroup/kaggle/blob/master/Otto_Group_Competition.ipynb"" target=""_blank"">Lasagne and nolearn tutorial</a> for <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge"" target=""_blank"">Otto competition</a> (by the admin) :</li>
	<li>Andrew Ng's <a href=""https://www.coursera.org/course/ml"" target=""_blank"">Coursera course in Machine learning</a></li>
	<li>University of Utah <a href=""http://www.cs.utah.edu/~piyush/teaching/cs5350.html"" target=""_blank"">Machine learning slides</a>.</li>
	<li>Wikipedia and Google</li>
</ol>
<h3>Are partnerships important in achieving good results?</h3>
Very.

Sometimes you cannot measure the impact from one competition only as what you learn from the others may be applicable in the future too. I've been very lucky to have made good and fun collaborations so far and I have learnt from all, especially:
<ol>
<ol>
	<li>From <a href=""https://www.kaggle.com/users/9974/gert"" target=""_blank"">Gert</a> I've learnt model ensembling, feature engineering and Fourier transforms.</li>
	<li><a href=""https://www.kaggle.com/users/114978/triskelion"" target=""_blank"">Triskelion</a> and <a href=""https://www.kaggle.com/users/115173/phil-culliton"" target=""_blank"">Phil Culliton</a>: <a href=""http://hunch.net/~vw/"" target=""_blank"">Vowpal Wabbit</a></li>
	<li><a href=""https://www.kaggle.com/users/111641/tvs"" target=""_blank"">TVS</a> to avoid over-fitting</li>
	<li><a href=""https://www.kaggle.com/users/2242/bluefool"" target=""_blank"">Bluefool</a> (or Domcastro) 1st derivatives and <a href=""http://www.stat.rice.edu/~jrojo/4th-Lehmann/slides/mcculloch.pdf"" target=""_blank"">BART</a></li>
	<li><a href=""https://www.kaggle.com/users/38878/mike"" target=""_blank"">Mike</a>, Python!</li>
	<li><a href=""https://www.kaggle.com/users/111776/giulio"" target=""_blank"">Giulio</a>, competition management and <a href=""https://github.com/Lasagne/Lasagne"" target=""_blank"">Lasagne</a>.</li>
</ol>
</ol>
I am SO PROUD that I had the honour/chance/opportunity to play, learn and exchange bits and bytes with Kagglers, like:

<a href=""https://www.kaggle.com/gertjac"" target=""_blank"">Gert</a>, <a href=""https://www.kaggle.com/triskelion/"" target=""_blank"">Triskelion</a>, <a href=""https://www.kaggle.com/yansoftware"" target=""_blank"">Yan Xu</a>, <a href=""https://www.kaggle.com/sudalairajkumar"" target=""_blank"">SRK</a>, <a href=""https://www.kaggle.com/potamitis"" target=""_blank"">Rafael</a>, <a href=""https://www.kaggle.com/mmueller"" target=""_blank"">Faron</a>, <a href=""https://www.kaggle.com/clobber"" target=""_blank"">Clobber</a>, <a href=""https://www.kaggle.com/domcastro"" target=""_blank"">Bluefool</a>, <a href=""https://www.kaggle.com/mlandry"" target=""_blank"">Mark Landry</a>, <a href=""https://www.kaggle.com/adjgiulio"" target=""_blank"">Giulio</a>, and <a href=""https://www.kaggle.com/abhishek"" target=""_blank"">Abhishek</a>

I would keep monitoring their progress :) .
<h2>Bio</h2>
<a href=""https://www.linkedin.com/pub/marios-michailidis/21/665/326/"" target=""_blank"">Marios Michailidis</a> <a href=""http://blog.kaggle.com/wp-content/uploads/2015/04/marios-michailidis.jpg""><img class=""wp-image-4851 alignright"" style=""margin-left: 10px; margin-top: 0px;"" src=""http://blog.kaggle.com/wp-content/uploads/2015/04/marios-michailidis.jpg"" alt=""marios-michailidis"" width=""100"" height=""120"" /></a>is Manager of Data science in dunnhumby and part-time PhD in machine learning at University College London (UCL) with a focus on improving recommender systems. He has worked in both marketing and credit sectors in the UK Market and has led many analytics projects with various themes including: Acquisition, Retention, Uplift, fraud detection, portfolio optimization and more. In his spare time he has created <a href=""http://www.kazanovaforanalytics.com/software.html"" target=""_blank"">KazAnova</a>, a GUI for credit scoring 100% made in Java.

Marios loves competing on Kaggle and learning new machine learning tricks. He told us he will create something good for the ML community soon..."
"Profiling Top Kagglers: Leustagos, Current #7 / Highest #1",http://blog.kaggle.com/2016/02/22/profiling-top-kagglers-leustagos-current-7-highest-1/,2016-02-22 23:59:45,"Next up in our series <a href=""http://blog.kaggle.com/tag/profiling-top-kagglers/"" target=""_blank"">Profiling Top Kagglers</a> is Lucas Eustaquio Gomes da Silva (better known as <a href=""https://www.kaggle.com/leustagos"" target=""_blank"">Leustagos</a> on Kaggle). Leustagos is one of only 13 data scientists to ever hold the #1 spot on Kaggle, and he has been a consistent face at the top of our user rankings since joining the community four years ago. In this blog, Leustagos shares what he's learned in his years competing, his typical approach to a new competition, and also how Kaggle has helped him develop professionally as a data scientist. Leustagos is currently busy <a href=""http://kickingcancerass.blogspot.com.br/"" target=""_blank"">kicking some serious cancer ass</a> so we want to send him extra thanks for taking the time to share this interview with us.
<h3>How did you start with Kaggle competitions?</h3>
I started back in the second half of 2011 just after doing the Andrew Ng Coursera <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">class</a>. I found people talking about ML competitions on the course forum and got curious about it. I tried taking part in one competition and placed pretty badly. After this I really wanted to learn about the magic used by the winners.
<h3>What is your first plan of action when working on a new competition?</h3>
Check the dataset to understand how to build a validation set.

[caption id=""attachment_5552"" align=""aligncenter"" width=""420""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/leustagos_1.png"" rel=""attachment wp-att-5552""><img class=""size-full wp-image-5552"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/leustagos_1.png"" alt=""Leustagos' top 8 finishes"" width=""420"" height=""220"" /></a> Leustagos' top 8 finishes[/caption]
<h3>What does your iteration cycle look like?</h3>
<ol>
	<li>Understand the dataset. At least enough to build a consistent validation set.</li>
	<li>Build a consistent validation set and test its relationship with the leaderboard score.</li>
	<li>Build a very simple model.</li>
	<li>Look for approaches used in similar competitions in the past.</li>
	<li>Start feature engineering, step by step to create a strong model.</li>
	<li>Think about ensembling, be it by creating alternate versions of the feature set or using different modeling techniques (xgb, rf, linear regression, neural nets, factorization machines, etc).</li>
</ol>
<h3>What are your favorite machine learning algorithms?</h3>
Gradient boosted trees by far! I like GBT because it gives pretty good results right off the bat. Look at how many competitions are won using them!
<h3>What are your favorite machine learning libraries?</h3>
<a href=""https://github.com/dmlc/xgboost"" target=""_blank"">xgboost</a>, <a href=""http://scikit-learn.org/stable/"" target=""_blank"">scikit-learn</a> and <a href=""http://pandas.pydata.org/"" target=""_blank"">pandas</a>
<h3>What is your approach to hyper-tuning parameters?</h3>
I look for params used in previous competitions and tweak them one by one until the outcome stops improving.
<h3>What is your approach to solid CV/final submission selection and LB fit?</h3>
We are usually asked to choose 2 submissions. My first submission is always the one that performed more consistently both on cv and the leaderboard. Not necessarily the best leaderboard score if I think its overfit. The second submission is usually one that performs good enough but its somewhat diverse from the first one.
<h3>In a few words: What wins competitions?</h3>
Good feature engineering, strong ensembling skills and, many times, team work.
<h3>What is your favourite Kaggle competition and why?</h3>
<a href=""https://www.kaggle.com/c/GEF2012-wind-forecasting"" target=""_blank"">Global Energy Forecasting Competition 2012 - Wind Forecasting</a>. It was the first competition that I was a prize winner and  1st place at that. It felt so good!
<h3>Have you ever hit a plateau during a competition? What, if anything, helped you move beyond this?</h3>
Many times! I usually try to take another look at the dataset to check for other angles. Seek information in the <a href=""https://www.kaggle.com/forums"" target=""_blank"">forums</a>, <a href=""https://www.kaggle.com/scripts"" target=""_blank"">scripts</a>, and more importantly, when this happens I try and brainstorm with my fellow teammates if I'm on a team. If I'm not, and none of the previous options worked, I look for a team.
<h3>What field in machine learning are you most excited about?</h3>
I like working with time series and classification tasks. Not exactly a field, but I'm not tied to any industry in particular. I just like a good challenge.
<h3>Which machine learning researchers do you study?</h3>
I don't study anyone specifically, but I'm willing to learn anything that works in practice. For that I look into papers. Of course, when dealing with a new problem I look for the state of the art too. Nowadays its very hard to find good and useful papers because researchers are forced to write many of them so the quality goes down.
<h3>Can you tell us something about the last algorithm you hand-coded?</h3>
I don't remember hand coding anything more complex than a linear regression. I'm more empirical than theoretical and given my background in software developing I try to avoid coding from scratch as much as possible. Its very error prone. I prefer to understand algorithms at a high level and to know the task they are best at solving. After that is just putting together pieces of a puzzle.
<h3>How important is domain expertise for you when solving data science problems?</h3>
It helps, but its not mandatory.
<h3>What do you consider your most creative trick/find/approach?</h3>
I like calculating smoothed likelihoods to replace categorical variables with high cardinality. I know some ways of doing so. Applying mathematical transformations to output to more closely match the error function can be very tricky too.
<h3>How are you currently using data science at work and does competing on Kaggle help with this?</h3>
I'm working with threat detection in enterprise networks. We analyse network traffic and use machine learning to identify malicious behaviour, like a command and control attack. Competing on Kaggle, besides being addictively fun, helps to broaden my skills and approaches. There is always something to learn.
<h3>What is your opinion on the trade-off between high model complexity and training/test runtime?</h3>
I really depends on the application. In competitions, better performance outweighs everything. For real applications, the solution I seek for is usually the simplest solution that meets the client's performance criteria. It must be at least good enough in performance, and it must be easy to use and maintain. Many clients cannot afford much trickery for feature engineering or complex algorithms in their workflow.
<h3>What is your view on automating Machine Learning?</h3>
It can work. I don't know how it will shape the data scientist job, but I think there will always be a need for one. A automated system is useless if there is no one that understands how to operate it and interpret its outcome.
<h3>How did you get better at Kaggle competitions?</h3>
Trial and error, reading the forums and copying successful approaches, learning from team mates and always, always, trying to be very creative on top of all that. I like to to add my own personal touch.
<h3>What have you learned from doing Kaggle competitions?</h3>
I've learned how to do competitive machine learning, which is a bit different from practical machine learning. But I've learned how to do the practical too, as those simple concepts of dealing with data and algorithms are needed at every corner in competitive machine learning.
<h2>Bio</h2>
<strong>Lucas Eustaquio Gomes da Silva<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/Leustagos.png"" rel=""attachment wp-att-5555""><img class="" wp-image-5555 alignright"" style=""margin-top: 0px; margin-left: 10px;"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/Leustagos.png"" alt=""Leustagos"" width=""146"" height=""146"" /></a></strong>, also known as Leustagos, is a mostly self taught data scientist who graduated in control and automation engineering. He currently works in the data mining field using machine learning techniques to identify and alert clients about malicious traffic in corporate networks."
"Genentech Cervical Cancer Screening, Winners' Interview: 1st place, Michael & Giulio",http://blog.kaggle.com/2016/02/26/genentech-cervical-cancer-screening-winners-interview-1st-place-michael-giulio/,2016-02-26 23:58:01,"<a href=""https://www.kaggle.com/c/cervical-cancer-screening"" target=""_blank"">Genentech Cervical Cancer Screening</a> was a competition only open to <a href=""https://www.kaggle.com/wiki/UserRankingAndTierSystem"" target=""_blank"">Kaggle Masters</a> that ran from December 2015 through January 2016.  The competition asked top Kagglers to use a dataset of de-identified health records to predict which women would not be screened for cervical cancer on the recommended schedule. Cervical cancer results in approximately 275,000 deaths every year, but it is potentially preventable and curable with regular screenings. Giulio &amp; Michael took first place in this highly competitive challenge, proving their feature engineering skills are some of the best in the world.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
<b>Giulio:</b> I’m a Data Scientist with a HealthCare Insurance plan in Washington State. My academic background is in Statistics and Biostatistics. I have worked a lot with clinical data but I’ve recently transitioned to consumer analytics. I use Machine Learning and Advanced Analytics to mine speech, text, weblogs and improve member experience.

[caption id=""attachment_5564"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-26-at-3.31.30-PM.png"" rel=""attachment wp-att-5564""><img class=""wp-image-5564 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-26-at-3.31.30-PM-300x144.png"" alt=""Giulio's profile badge"" width=""300"" height=""144"" /></a> <a href=""https://www.kaggle.com/adjgiulio"" target=""_blank"">Giulio</a> on Kaggle[/caption]

<b>Michael:</b> I work on applied machine learning for a product company. We integrate and apply algorithms with our platform.

[caption id=""attachment_5563"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-26-at-3.30.40-PM.png"" rel=""attachment wp-att-5563""><img class=""size-medium wp-image-5563"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-26-at-3.30.40-PM-300x143.png"" alt=""Michael on Kaggle"" width=""300"" height=""143"" /></a> <a href=""https://www.kaggle.com/mjahrer"" target=""_blank"">Michael</a> on Kaggle[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
<b>Giulio:</b> I work with claims data a lot and I had thought that my experience was going to help me in this competition. Instead the competition was framed in such a way that I ended up generating most of my insights in a purely data driven way as opposed to using industry knowledge.
<b>Michael:</b> Since I am working usually on low level data processing it was fun to handle 150GB of raw data with custom C++ code.
<h3>How did you get started competing on Kaggle?</h3>
<b>Giulio:</b> I started almost 3 years ago. At that time my main goal was to get exposure to how Machine Learning was utilized in other industries.
<b>Michael:</b> I’m a Kaggle Early Adopter and have participated since day one. I spent much time before that in the Netflix Prize. It’s still fun to process data from new challenges and apply models, ensembles and some secret sauce to.
<h3>What made you decide to enter this competition?</h3>
<b>Giulio:</b> Many Kaggle competitions are about modelling and ensembling. Most of what I do in a business setting is about feature engineering, and modelling takes only 10% of my time. I was eager to see where my feature engineering skills stood among the best data scientists in the world. This was the perfect competition due the the fact that data was provided in raw transactional form.
<b>Michael:</b> I like competitions with huge datasets, the more data the better.

[caption id=""attachment_5560"" align=""aligncenter"" width=""781""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/a.scatter2D.png"" rel=""attachment wp-att-5560""><img class=""wp-image-5560 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/a.scatter2D.png"" alt=""a.scatter2D"" width=""781"" height=""531"" /></a> By using Giulio's dataset (0.96294 single model private score) train and test features are concatenated (2859630 rows) to train an autoencoder neural net. It is an unsupervised learner, that reconstructs itself. Architecture is 98259-4000-4000-4000-2-4000-4000-4000-98259. Middle and output layer is linear, others are ReLU. 20 epochs minibatch SGD on GPU, which took over a day to run. The scatterplot shows the middle layer activations of the train part with targets overlaid (blue=0, red=1). This means that every patient is a dot in this plot.[/caption]
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
<strong>Giulio &amp; Michael:</strong> There wasn’t really much processing, at least in the typical Kaggle fashion. But this was a feature engineering heavy competition and we used a lot of SQL to transform transactional data into features. As for models, we used a lot of XGBoost and Michael’s custom coded neural networks.
<h3>Were you surprised by any of your findings?</h3>
<b>Giulio:</b> I was. Some findings were counterintuitive to my industry experience. For example, prescriptions should be a very powerful predictor of cervical cancer screening, as a large portion of the population will get a screening as a prerequisite for oral contraceptives prescriptions. In this case though, prescription data added very little lift on top of other features based on diagnosis, procedures and providers.
<h3>Which tools did you use?</h3>
<b>Giulio:</b> mostly SQL for feature engineering. Then a simple IPython notebook to test model performance.
<b>Michael:</b> only C++
<h3>How did you spend your time on this competition?</h3>
<b>Giulio:</b> I spent close to 90% of my time on feature engineering. I knew Michael was one of the best modellers on Kaggle and he was willing to undertake most of the modelling. That worked very well for the team. I didn’t have to worry about tuning models and Michael didn’t have to worry about generating features.
<b>Michael:</b> I spend 30% to build the one-hot features, 10% to merge our features and 60% to build and tune the ensemble.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
<b>Giulio:</b> I estimate that only the feature engineering process can take up to a couple of days. A simple model that, alone can place 4th, takes about 2 hours to train, but some of our best models can take much longer.
<b>Michael:</b> Total computational time was a few weeks for a single node. It can be speedup if the machine has many cores since xgboost performance speed is nearly linear with number of cores.

[caption id=""attachment_5561"" align=""aligncenter"" width=""781""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/a.decisionSurfXGB.png"" rel=""attachment wp-att-5561""><img class=""wp-image-5561 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/a.decisionSurfXGB.png"" alt=""a.decisionSurfXGB"" width=""781"" height=""533"" /></a> A xgboost model results in AUC:0.827738 by using this 2D data as input. To visualize the decision surface a X/Y grid in [-10:0.01:+10] interval is created and scored. The plot shows the model output (blue=0, red=1).[/caption]
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
<b>Giulio:</b> Kaggle Master competitions are exponentially more difficult and competitive than regular competitions. You get to face the best of the best and these folks will leave no stone unturned. I was shocked at the amount of progress made, and I was even more shocked at how good all the teams were even without any prior industry knowledge.
<b>Michael:</b> Large datasets and clean data like here has beautiful behavior. Everything is stable and act as good base to optimize into. An improvement at local validation turned into improvement on the leaderboard, this is what I test first when I enter a competition. There are many counterexamples of competitions where this is not the case (<a href=""https://www.kaggle.com/c/how-much-did-it-rain-ii"" target=""_blank"">How Much Did it Rain?</a>, <a href=""https://www.kaggle.com/c/rossmann-store-sales"" target=""_blank"">Rossmann</a>..).
<h3>Do you have any advice for those just getting started in data science?</h3>
<b>Giulio:</b> data science to me is more about enjoying the journey than about reaching a status. Everybody doing analytics can claim to be a data scientist, but it is far easier to succeed long term if you truly enjoy the process of learning something new every day and feeling challenged by new problems.
<b>Michael:</b> drawing conclusions by working on one dataset is often not complete. In kaggle I found a lot of different real world datasets and by competing I know what is state-of-the-art and where I am standing. And finally, finding the truth.

[caption id=""attachment_5562"" align=""aligncenter"" width=""781""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/a.decisionSurfXGBScatter.png"" rel=""attachment wp-att-5562""><img class=""size-full wp-image-5562"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/a.decisionSurfXGBScatter.png"" alt=""Overlay both plots show which patient would be scored as a screener (red=1) or non-screener (blue=0)."" width=""781"" height=""533"" /></a> Overlay both plots show which patient would be scored as a screener (red=1) or non-screener (blue=0).[/caption]
<h2>Teamwork</h2>
<h3>How did your team form?</h3>
<b>Giulio:</b> Michael and I were both doing well on our own early on but the competition was fierce. Since we had different backgrounds and had taken different approaches, we thought we could benefit from a joint effort.
<h3>How did your team work together?</h3>
<strong>Giulio &amp; Michael:</strong> We used Google Drive and Dropbox to share code and data. We used Skype for quick chats and emails for everything else.
<h3>How did competing on a team help you succeed?</h3>
<b>Giulio:</b> For me the key part was that Michael was willing to focus on modelling and ensembling. That allowed me to focus on feature engineering. Without that type of freedom I would have never had enough time to find one of the decisive insights on the data that came 3 days from the end of the competition.
<b>Michael:</b> The key is features + models, like in every competition. Features though had more importance in this competition than in others. Ensembling a few different models (variations of xgb and neural nets) gave us the last boost to end up in first position.
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
<b>Giulio:</b> I really enjoy competitions based on feature engineering more than modelling. I’d like to see competitions where submissions consist of feature datasets, and the scoring mechanism is limited to run those features through a simple pre-determined model. Essentially every team would use the same model and the only difference would be based on features.
<h3>What is your dream job?</h3>
<b>Giulio:</b> A fellow Kaggler and friend of mine, Mark Landry, actually holds that job. He is a Competitive Data Scientist, and gets to solve Kaggle problems for fun and work :-)
<h2>Bio</h2>
<strong>Giulio</strong> is a Data Scientist with Premera Blue Cross. His background is in Statistics and Biostatistics. Most of his work is focused on Machine Learning and Advanced Analytics. He enjoys long distance running and has ran several marathons.

<strong>Michael</strong> works at Opera Solutions. He transfers the knowledge gained from various data mining competitions to the platform solution. His background is Statistics, Software Design and Electronic Engineering."
"Homesite Quote Conversion, Winners' Interview: 3rd place, Team New Model Army | CAD & QuY",http://blog.kaggle.com/2016/02/29/homesite-quote-conversion-winners-interview-3rd-place-team-new-model-army-cad-quy/,2016-02-29 22:45:50,"<a href=""https://www.kaggle.com/c/homesite-quote-conversion"" target=""_blank"">Homesite Quote Conversion</a> challenged Kagglers to predict which customers would purchase an insurance plan after being given a quote. Team New Model Army | CAD &amp; QuY finished in the money in <a href=""https://www.kaggle.com/c/homesite-quote-conversion/leaderboard"" target=""_blank"">3rd place</a> out of 1,924 players on 1,764 teams. In this post, the long-time teammates who formed the New Model Army half of the team share their approach, why feature engineering is important, and why it pays to be paranoid in data science.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I (Konrad Banachewicz) have a PhD in statistics and a borderline obsession with all things data. I started competing on Kaggle at the dawn of time (or dawn of Kaggle anyway) and it has been a very interesting journey so far.

[caption id=""attachment_5570"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-29-at-1.42.17-PM.png"" rel=""attachment wp-att-5570""><img class=""wp-image-5570 size-medium"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-29-at-1.42.17-PM-300x142.png"" alt=""Konrad on Kaggle"" width=""300"" height=""142"" /></a> <a href=""https://www.kaggle.com/konradb"" target=""_blank"">Konrad</a> on Kaggle[/caption]

I (Mike Pearmain) have a degree in Mathematics, and similar to Konrad have an obsession with data, and specifically data science pipelines for dynamic self optimizing systems.

[caption id=""attachment_5571"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-29-at-1.42.27-PM.png"" rel=""attachment wp-att-5571""><img class=""size-medium wp-image-5571"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/Screen-Shot-2016-02-29-at-1.42.27-PM-300x141.png"" alt=""Michael on Kaggle"" width=""300"" height=""141"" /></a> <a href=""https://www.kaggle.com/mpearmain"" target=""_blank"">Mike</a> on Kaggle[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
Domain knowledge: not really – between the two of us, we have quite a bit of experience in financial risk modeling, but since the data was anonymized, it was not very useful here. On the other hand, a number of things (dataset preparation, building pipelines, parameter optimization) that we learned to build along the way – in other Kaggle contests – really worked well together this time around.
<h3>How did you get started competing on Kaggle?</h3>
I heard somewhere about this thing called “data mining contests” and after checking out a few – now mostly defunct – competitors, I ended up on Kaggle. From the start, I really liked the idea of competing against other people in a setting like this: well defined problems, smart competition, clear rules, and a possibility to win money – what’s not to like?

Mike and I met while working as external contractors for the same client in the financial industry – this encounter is one of precious few long term benefits to come out of that particular assignment ☺ We talked a bit about this and that over coffee or lunch, discovered we had a shared interest in data science and decided to give Kaggle teamwork a try. The rest, as they say, is history.
<h3>What made you decide to enter this competition?</h3>
Cursory examination suggested that a lot of the stuff we developed on previous competitions could be put together into a nice pipeline here. In particular, we wanted to try a multi-stage stacking (ensemble of ensembles, really) – and the moderate data size made this one possible.
<h2>Let's get technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
In general, diversity was the name of the game:
<ul>
	<li><strong>We created multiple transformations of the dataset:</strong> treating all categorical variables as integers (for tree-based models), replacing factors by response rates, adding differences of highly correlated pairs of numerical variables; a single dataset yielding best results on individual model level was the one where we combined all those approaches.</li>
	<li><strong>We used qualitatively different models:</strong> <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">xgboost</a> and random forest for tree-based stuff, <a href=""https://github.com/fchollet/keras"" target=""_blank"">keras</a> and <a href=""http://www.h2o.ai/#/"" target=""_blank"">h2o</a> for deep neural networks, svm with radial kernel and some logistic regression (which worked surprisingly well on the non-linear transformations of the dataset).</li>
</ul>
<p style=""text-align: left;"">[table width=""500px"" class=""table table-bordered""]
model,headcount
R::h2o.deeplearning,14
Python::extraTrees,171
Python::keras,3
Python::LogisticRegression,29
R::earth,6
R::ranger,22
Python::svc-rbf,9
Python::xgboost,53
[/table]
<em><strong>Table 1:</strong> summary of types of models used as level 1 metafeatures. The reason there behind the overrepresentation of extraTrees is that – while obviously less powerful than xgboost – they are very cheap computationally, so we could add more “tree stuff” into the ensemble quickly.</em></p>
Some of those had pretty horrible performance (scoring AUC below 0.7), but we kept them anyway – our assumption was that methods like xgboost (especially with optimized parameters) are pretty good at discarding useless features.
<h3><a href=""http://blog.kaggle.com/wp-content/uploads/2016/02/homesite3_1.jpeg"" rel=""attachment wp-att-5569""><img class=""aligncenter size-full wp-image-5569"" src=""http://blog.kaggle.com/wp-content/uploads/2016/02/homesite3_1.jpeg"" alt=""homesite3_1"" width=""294"" height=""350"" /></a></h3>
<h3>What was your most important insight into the data?</h3>
We were quite surprised that almost all variables were relevant, which made us abandon the idea of doing feature selection.
<h3>Were you surprised by any of your findings?</h3>
A kind of “plateau” in the achievable results: it was reasonably easy to reach a score around 0.965, but breaching the 0.97 threshold took quite a bit of effort. This was certainly one of the most saturated leaderboards we have seen. This observation was confirmed by examination of cross-validated results for different models we used to “mix” the level 1 metafeatures: the variation across folds was bigger than the distance between the extreme scores within top 10. This uncertainty kept us on our toes until the very end.

[table width=""500px"" class=""table table-bordered""]
model,mean,std
glmnet,0.968701,0.000937
xgboost,0.968951,0.000962
nnet,0.968056,0.001001
hillclimb,0.968048,0.000940
ranger,0.967308,0.000951
[/table]
<em><strong>Table 2:</strong> summary statistics for different models used for mixing level 1 metafeatures. “Hillclimb” is our implementation of the “libraries of models” approach of Caruana et al, other names correspond to the R packages used.</em>
<h3>Which tools did you use?</h3>
We used R for preparing variants of the dataset and ensemble construction (mostly because we had a working implementation an ensembler from prior contests) and Python for pretty much anything else, like metafeatures generation. Compiling a multithreaded version of xgboost proved especially useful.
<h3>How did you spend your time on this competition?</h3>
Feature engineering took about 40 pct of the time, as did metafeatures generation. The rest was parameter tuning at different stages in the ensemble.
Rather than manual hyper parameter tuning, we used <a href=""https://en.wikipedia.org/wiki/Bayesian_optimization"" target=""_blank"">Bayesian Optimization</a> for a more automated approach. (The same approach we took is now a <a href=""https://www.kaggle.com/mpearmain/bnp-paribas-cardif-claims-management/bayesianoptimization-of-random-forest"" target=""_blank"">script</a> on the <a href=""https://www.kaggle.com/c/bnp-paribas-cardif-claims-management"" target=""_blank"">BNP Paribas</a> competition)
<h3>What was the run time for both training and prediction of your winning solution?</h3>
Putting a number on the whole process is tricky: from the very beginning, we decided to go for the stacked ensemble approach, which means that for any model we tried, we generated stacked predictions across the entire training set and threw them into a shared folder. We sort of accumulated the training set over time.

Building an ensemble predictor using multithreaded xgboost took under an hour, if we also used bagging then the training time scaled almost linearly (so for instance a 10-bag would run overnight on a 16GB/i7 Macbook Pro).
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
In no particular order:
<ul>
	<li>Bragging rights</li>
	<li>We are both Masters now, which means New Model Army can attack private contests</li>
	<li>The prize</li>
	<li>Our approach to competitions is one step closer to being a properly streamlined process</li>
</ul>
<h3>Do you have any advice for those just getting started in data science?</h3>
Apart from the obvious (learn Python, read winners solution descriptions on Kaggle :-), feature engineering is an extremely important skill to acquire: you can’t really follow a course to learn it, so you need to practice and grab/borrow/steal ideas whenever you can.

Smart features combined with a linear model often beat more sophisticated approaches to a variety of problems.
<h2>Teamwork</h2>
<h3>How did your team form?</h3>
We have worked together – on Kaggle and outside of it – before, so joining forces on this one was a natural thing to do.
<h3>How did your team work together?</h3>
Based on past experience, at the start of Homesite we had a pretty good idea of what worked and what did not in our earlier attempts.
<h3>How did competing on a team help you succeed?</h3>
We have enough in common background-wise to communicate efficiently, while the differences allow us to view problems from different angles. Paranoid people live longer, so occasionally double-checking each other’s code helped us ferret out a few nasty bugs which could have led to an epic overfit."
"Santa's Stolen Sleigh, Winners' Interview: 3rd place, Marcin Mucha & Marek Cygan",http://blog.kaggle.com/2016/03/03/santas-stolen-sleigh-winners-interview-3rd-place-marcin-marek/,2016-03-03 21:14:10,"<a href=""https://www.kaggle.com/c/santas-stolen-sleigh"" target=""_blank"">Santa's Stolen Sleigh</a>, Kaggle's annual optimization competition, wrapped up in early January with many familiar faces from previous years on the <a href=""https://www.kaggle.com/c/santas-stolen-sleigh/leaderboard"" target=""_blank"">leaderboard</a>. For the third year in a row, Marcin &amp; Marek competed together and finished in-the-money, continuing to prove their optimization prowess. This year they took third place as team ""master.exploder@deepsense.io"" by carefully designing their local search algorithm. In the end, they discovered its moves were not quite greedy enough to take the top spot. This blog shares the approach of these long-standing teammates and experts in the field.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
We are both active academic researchers in the field of algorithmics and faculty members of the Institute of Informatics, at Warsaw University. (Faculty pages for <a href=""http://www.mimuw.edu.pl/~mucha/wordpress/"" target=""_blank"">Marcin</a> and <a href=""http://www.mimuw.edu.pl/~cygan"" target=""_blank"">Marek</a>). We are particularly interested in ways of dealing with computational hardness: mainly approximation algorithms, and in Marek’s case parameterized complexity.

[caption id=""attachment_5580"" align=""aligncenter"" width=""601""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/03/marcin_marek_profilebadges.png"" rel=""attachment wp-att-5580""><img class=""size-full wp-image-5580"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/marcin_marek_profilebadges.png"" alt=""Marcin (left) &amp; Marek (right) on Kaggle"" width=""601"" height=""140"" /></a> <a href=""https://www.kaggle.com/mmucha"" target=""_blank"">Marcin</a> (left) &amp; <a href=""https://www.kaggle.com/marekcygan"" target=""_blank"">Marek</a> (right) on Kaggle[/caption]

We also both have a long history of competing in all kinds of programming contests. Quick Topcoder style contests, ACM ICPC, marathons, 24-hour challenges - we have done all of these many times.
<h3>What made you decide to enter this competition?</h3>
The Kaggle Santa contest seemed perfectly suited for us. Not only are the problems computationally hard, which is right up our alley, but also the long duration gives these contests a bit of a research flavor. What‘s more, we do not really have that much free time on our hands, less than ever in fact, with both of us becoming fathers quite recently. In particular, the TopCoder style 2-week marathons are really not something we are able to compete in these days. Kaggle’s holiday contest allowed for a much more relaxed attitude (the Rudolf prize havoc in the first half of the competition is a bit stressful though).

Because of all this, we were very happy to take a shot at the <a href=""https://www.kaggle.com/c/packing-santas-sleigh"" target=""_blank"">sleigh packing contest</a> two years ago, had a blast and actually ended up taking the top spot. We then participated <a href=""https://www.kaggle.com/c/helping-santas-helpers"" target=""_blank"">last year</a>, and managed to claim all three prizes. After this, taking up the challenge again was a no-brainer.
<h2>Let's Get Technical</h2>
<h3>What was your most important insight into the data?</h3>
Not really an insight, but what definitely shaped our whole participation was the early realization that this was a local search problem, and most likely no other approach would be able to compete with it. We later attempted to complement it with MIP modelling. Initially, the MIP model helped us evaluate the quality of the trips produced by the local search solver, and identify its weaknesses. However, we failed at adapting it to larger pieces of the data.
<h3>Were you surprised by any of your findings?</h3>
We did run into a couple of rather surprising outcomes. One was our complete failure to get anything out of parallel tempering (which is a parallel simulated annealing minus the annealing, sort of). We knew it was hard to set up and took a long time to arrive at good solutions, but in our case, it just seemed not to work at all. The other was our initial failure when attempting to make a multithreaded solver work. As it turns out, multithreaded programming in C++ is harder than we thought, and we learned a lot this way.
<h3>Which tools did you use?</h3>
We used the same set of tools we usually use for these kind of problems: C++ (with GCC and clang compilers) for the main solution, and python/R/awk/bash for visualizations and log analysis. We also spent quite a lot of time playing with the <a href=""http://www.fico.com/en/products/fico-optimization-modeler"" target=""_blank"">FICO optimizer</a>, trying to design MIP formulations that solve in reasonable time for non-trivial sets of gifts.
<h3>How did you spend your time on this competition?</h3>
Because of the <a href=""https://www.kaggle.com/c/santas-stolen-sleigh/details/prizes"" target=""_blank"">Rudolf Prize</a>, the initial couple weeks are always a bit crazy in these contests. Our first goal was to obtain a fully functional optimizer as fast as possible, in order to not lose points in the Rudolf ranking. Then, we spent a lot of time modifying the optimizer, playing with parameters, and generally trying to be ahead of the pack. This happened to be quite stressful at times. On one of the mornings, after a couple days of relative stability on the leaderboard, during a single hour three different teams claimed the top spot for the very first time. We spent a good amount of time frantically going through contest rules looking for some hidden meaning of this, apparently there was none.

After the initial craze, we cooled off when it became clear that with our current solution, we would not be able to compete with the top team (at that time, i.e. <a href=""http://blog.kaggle.com/2016/01/28/santas-stolen-sleigh-winners-interview-2nd-place-woshialex-weezy/"" target=""_blank"">woshialex &amp; weezy</a>). We spent a lot of time trying to figure out the weaknesses in our solution and find ways to go around them. We arrived at several new ideas this way and spent a lot of time implementing and trying them out.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
It seemed rather clear from the get-go, that this contest would be about implementing the best possible local search algorithm, most likely simulated annealing. This means spending most of your “thinking” time designing the state representation, the moves, the temperature schedule, etc.

In the end, it seems our moves were not greedy enough. In particular, almost all of our moves were attempting to perform a completely random local modification on the current solution. In contrast, the moves used by the <a href=""https://www.kaggle.com/c/santas-stolen-sleigh/leaderboard"" target=""_blank"">top two teams</a> tried to locally perform a modification that was in some sense optimal (or at least the <a href=""http://blog.kaggle.com/2016/01/28/santas-stolen-sleigh-winners-interview-2nd-place-woshialex-weezy/"" target=""_blank"">second place team</a>, we do not really understand the<a href=""https://www.kaggle.com/c/santas-stolen-sleigh/forums/t/18312/winning-entry"" target=""_blank""> first place solution</a> at the moment). This always has pros and cons. The convergence is clearly much faster, but the search space becomes much more disconnected (or rather very loosely connected), and the very best solutions might not be achievable anymore.

Even now, almost a month after the contest has ended, we are having a hard time truly understanding the reasons for greedy moves’ superior performance, so this is definitely something to take away from this competition.
<h2>Teamwork</h2>
<h3>How did your team form?</h3>
Our offices at Warsaw University are opposite each other, and we bump into each other and chat on a regular basis. We also both have a very strong drive to compete in programming contests of all kinds, so it only seemed natural to form a team. And then, after doing very well in the sleigh packing contest, there was really no reason to abandon the winning formula. This year we got some extra support from our <a href=""http://deepsense.io/"" target=""_blank"">deepsense.io</a> R&amp;D team, and it was not just cheering. We got to use one of the multiprocessor Xeon machines for the duration of the contest, which does make a huge difference!
<h3>How did your team work together?</h3>
It does not make much sense to jointly develop a hacky/messy C++ optimizer. Instead, we decided that one of us would focus on developing this program (this would be Marek, who is a faster and less error-prone coder), and the other would perform explore alternative solution paths (i.e. MIP modelling) and perform other tasks not related to the main optimizer. This worked really well in the previous contest, where the problem had much more structure, but not so well in this one.
<h3>How did competing on a team help you succeed?</h3>
This is most likely obvious to anyone who competed with a team at least once, but we will say it anyway: a team is never just a sum of its members. The interactions between its member, inspirations, feeding of each others ideas, etc., brings the joint performance to a whole new level. It also makes it a much more enjoyable experience as well as an opportunity to learn.
<h2>Bios</h2>
<strong>Marek Cygan, PhD</strong>
Marek is the world champion in <a href=""https://icpc.baylor.edu/community/history-icpc-2007"" target=""_blank"">ACM ICPC 2007</a> and the world champion in <a href=""https://community.topcoder.com/pl/?module=static&amp;d1=google05&amp;d2=overview"" target=""_blank"">Google Code Jam 2005</a>. The main themes of Marek’s research interests are different aspects of algorithmics. He focuses on approximation algorithms and fixed parameter tractability. After being a post-doc at the <a href=""http://www.umd.edu/"" target=""_blank"">University of Maryland</a> and <a href=""http://www.idsia.ch/"" target=""_blank"">IDSIA</a>, University of Lugano, Marek returned to the <a href=""http://www.mimuw.edu.pl/"" target=""_blank"">Institute of Informatics</a> at University of Warsaw, where he is currently an Assistant Professor. Moreover, Marek is an active contributor to <a href=""http://deepsense.io/"" target=""_blank"">deepsense.io</a> R&amp;D team, which focuses on Deep Learning.

<strong>Marcin Mucha, PhD</strong>

PhD, DSc. Master’s degree in Computer Science and Mathematics from <a href=""http://en.uw.edu.pl/"" target=""_blank"">University of Warsaw</a>. He specializes in algorithmics. Once Marcin completed his PhD he spent a year at <a href=""https://www.mpi-inf.mpg.de/home/"" target=""_blank"">Max Planck Institute</a> in Saarbruecken, Germany as a post-doc. Then he returned to University of Warsaw, where he is currently employed as an Assistant Professor. His research focuses on approximation algorithms for hard problems. However, he is also interested in other areas, e.g. online algorithms and graph algorithms. He has got a significant experience in practical applications of combinatorial optimization and Machine Learning methods. Finalist of the <a href=""https://icpc.baylor.edu/community/results-1996"" target=""_blank"">ACM ICPC World Finals 1996</a>. Additionally, Marcin is an active contributor to <a href=""http://deepsense.io/"" target=""_blank"">deepsense.io</a> R&amp;D team, which focuses on Deep Learning."
"Airbnb New User Bookings, Winner's Interview: 3rd place: Sandro Vega Pons",http://blog.kaggle.com/2016/03/07/airbnb-new-user-bookings-winners-interview-3rd-place-sandro-vega-pons/,2016-03-08 00:16:08,"<a href=""https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings"" target=""_blank"">AirBnB New User Bookings</a> was a popular recruiting competition that challenged Kagglers to predict the first country where a new user would book travel. This was the first recruiting competition on Kaggle with <a href=""https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings/scripts"" target=""_blank"">scripts</a> enabled. AirBnB encouraged participants to prove their chops through their collaboration and code sharing in addition to their final models. <a href=""https://www.kaggle.com/svpons"" target=""_blank"">Sandro Vega Pons</a> took 3rd place, ahead of 1,462 other competitors, using an ensemble of GradientBoosting, MLP, a RandomForest, and an ExtraTreesClassifier. In this blog, Sandro explains his approach and shares key scripts that illustrate important aspects of his analysis.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I currently work as a postdoctoral researcher at the <a href=""https://nilab.fbk.eu/"" target=""_blank"">NeuroInformatics Laboratory</a>, FBK in Trento, Italy. My current research is mainly focused on the development of machine learning and network theory methods for neuroimaging data analysis. I hold a M.Sc. in Computer Science and a Ph.D. in Applied Mathematics.

[caption id=""attachment_5589"" align=""aligncenter"" width=""300""]<a href=""https://www.kaggle.com/svpons"" rel=""attachment wp-att-5589""><img class=""size-medium wp-image-5589"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-07-at-4.07.25-PM-300x143.png"" alt=""Sandro on Kaggle"" width=""300"" height=""143"" /></a> Sandro's profile on <a href=""https://www.kaggle.com/svpons"" target=""_blank"">Kaggle</a> and <a href=""https://www.linkedin.com/in/sandrovegapons"" target=""_blank"">LinkedIn</a>[/caption]
<h3>How did you get started competing on Kaggle?</h3>
I first heard about Kaggle around three years ago, when a colleague showed me the website. At that time I was starting to work with scientific and analytic Python packages (numpy, scipy, scikit-learn, etc.) and I found Kaggle competitions a perfect way of getting familiar with them. The first competition in which I seriously participated was <a href=""https://www.kaggle.com/c/connectomics"" target=""_blank"">Connectomics</a>, which was somewhat related to my research activities in that time. From that moment on, I have tried to be always active in at least one competition, even though many times I cannot dedicate as much time as I would like to the competitions.

[caption id=""attachment_5590"" align=""aligncenter"" width=""412""]<a href=""https://www.kaggle.com/svpons"" rel=""attachment wp-att-5590""><img class=""size-full wp-image-5590"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/sandro_top8.png"" alt=""Sandro's top 8 finishes"" width=""412"" height=""212"" /></a> Sandro's top 8 finishes[/caption]
<h3>What made you decide to enter this competition?</h3>
After ending up in the top 10% of some competitions, I was still looking for one in which I could finish in the top10, in order to get the master tier. I found this competition very interesting for many reasons, e.g.: there was room for creativity in feature engineering, the problem seemed to be complex enough (12-class classification, highly unbalanced, classes very overlapped, I was not very familiar with the evaluation measure, etc.) and the data was not too big (I could work on my laptop while on Christmas holidays). Then, after trying some initial ideas and seeing that the results were promising, I got completely hooked on this competition.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
My solution was basically based on feature engineering in both <i>users</i> and <i>sessions data</i> and a 3-layers architecture for ensembling classifiers (see Figure 1).

[caption id=""attachment_5586"" align=""aligncenter"" width=""592""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/03/learning_architecture-fig-1-1.png"" rel=""attachment wp-att-5585""><img class=""wp-image-5586 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/learning_architecture-fig-1-1.png"" alt="""" width=""592"" height=""490"" /></a> Fig. 1[/caption]

<b>Feature Engineering</b>:

I extracted features from both <i>users</i> and <i>sessions</i> files. In the case of <i>users</i>, I applied a one-hot-encoding representation for categorical features and computed several features from <i>age</i> and <i>dates</i>, using different techniques for dealing with missing values. The <i>sessions</i> data was aggregated by user's id and different features based on counts, frequency, unique values, etc. were computed. The full feature engineering code can be found in this <a href=""https://www.kaggle.com/svpons/airbnb-recruiting-new-user-bookings/feature-engineering"" target=""_blank"">script</a>.

<b>Learning architecture</b>:
<ul>
	<li><b>1st Layer</b>: I tried many classifiers at this level. Since the beginning, I got the best individual results with <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> GradientBoosting. Later on, I was able to get comparable results with deep MLP implemented on <a href=""https://github.com/fchollet/keras"" target=""_blank"">Keras</a>. I also used other classifiers (with a bit worse performance) implemented in <a href=""http://scikit-learn.org/stable/"" target=""_blank"">scikit-learn</a>, like RandomForest, ExtraTreesClassifier and LogisticRegression. My best submission was computed by using 5 versions of GradientBoosting (trained with different parameters, class weights and subset of features), 5 versions of deep MLP (trained with different parameters, class weights and subset of features), a RandomForest and an ExtraTreesClassifier.
<b></b></li>
	<li><b>2nd Layer</b>: I implemented my own blending strategies based on the <a href=""http://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html"" target=""_blank"">scipy.optimization package</a>. A detailed description, source code of these ensembling methods, and how they can be used inside a 3-layer learning architecture can be found in this <a href=""https://www.kaggle.com/svpons/airbnb-recruiting-new-user-bookings/three-level-classification-architecture"" target=""_blank"">python notebook</a>. I implemented two methods, called <i>EN_optA</i> and <i>EN_optB</i> in the script. I used the two methods and a calibrated version of them (using the CalibratedClassifierCV from scikit-learn) to end up with four ensemble predictions.
<b></b></li>
	<li><b>3rd Layer</b>: This last layer consists of a weighted average of the 4 predictions obtained in the previous layer.</li>
</ul>
<h3>What was your most important insight into the data?</h3>
<ul>
	<li>Although the two problems are different, after some feature engineering on the data of this competition, the resulting classification problem, was somehow similar (number of classes, number of features, number of samples) to that of the <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge"" target=""_blank"">Otto Group Product Classification Challenge</a>. The algorithms I used for that competition (ending up 34th from 3514 teams) were a good starting point.</li>
	<li>The two proposed ensembling techniques (<i>EN_optA</i> and <i>EN_optB</i>) minimize an objective function and the best results were obtained when minimizing the multiclass logloss. This means that in practice, the two methods basically became implementations of logistic regression. However, I always got better results with the proposed techniques rather than with the sklearn implementation of LogisticRegression. They also outperformed non linear classifiers like GradientBoosting (using multiclass logloss as objective function). I made a quick experiment in the last part of this <a href=""https://www.kaggle.com/svpons/airbnb-recruiting-new-user-bookings/three-level-classification-architecture"" target=""_blank"">notebook</a> in order to explore the behavior of the applied techniques in more detail. The idea was to compare the performance of the different ensembling methods on synthetic data for different number of classes. The results of this experiment (for the limited set of parameters values I explored) show that LogisticRegression (as implemented in sklearn) and GradientBoosting produce better results for problems with a few number of classes. Nevertheless, as the number of classes increases, the proposed ensembling techniques become the best option.</li>
</ul>
[caption id=""attachment_5588"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/03/comp_ens-fig-2-1.png"" rel=""attachment wp-att-5587""><img class=""wp-image-5588 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/comp_ens-fig-2-1.png"" alt="""" width=""600"" height=""475"" /></a> Fig. 2[/caption]
<ul>
	<li>The further combination of the two proposed ensembling techniques and their calibrated versions, gave me a significant final boost. This means that they produce comparable results, but different enough to blend well. This was also verified in the experiment with synthetic data (Figure 2 above), where <i>EN_3rd_layer</i> always outperformed <i>EN_optA</i> and <i>EN_optB</i>.</li>
</ul>
<h3>Were you surprised by any of your findings?</h3>
<ul>
	<li>I was happily surprised of being able to create deep MLP models producing results that were comparable to the best XGBoost ones. Moreover, I am always a bit surprised about how well deep learning models (like deep MLP) and tree-based models (like GradientBoosting) complement each other, and therefore how good they perform when ensembled. The combination of these two types of learning algorithms seems to be a recurrent choice in Kaggle competitions. This perhaps requires a more systematic study, beyond intuitive explanations based on the general understanding of the two types of models.</li>
	<li>I always got the best results when using multi-class log-loss as objective function. Specifically, for the the definition of <i>EN_optA</i> and <i>EN_optB</i> (second layer), I tried with different objective functions like <i>mlogloss</i>, <i>mlogloss@5</i> (multi-class log-loss after keeping only the 5 classes with highest probabilities), ndcg@5 (the competition evaluation metric) and <i>mlogloss + 2 - ndcg@5</i> (a combination of <i>mlogloss</i> and <i>ndcg</i> after converting <i>ndcg</i> to a dissimilarity measure). I was expecting the <i>ndcg</i> not to work well, since it is a non-smooth function and therefore not suitable for gradient based optimization. However, I was not sure whether some combination of <i>mlogloss </i>and<i> <i>ndcg</i> </i>could give any improvement.</li>
</ul>
<h3>Which tools did you use?</h3>
Python (<a href=""http://www.numpy.org/"" target=""_blank"">numpy</a>, <a href=""http://pandas.pydata.org/"" target=""_blank"">pandas</a>, <a href=""http://www.scipy.org/"" target=""_blank"">scipy</a>, <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">xgboost</a>, <a href=""https://github.com/fchollet/keras"" target=""_blank"">keras</a>, <a href=""http://scikit-learn.org/stable/"" target=""_blank"">scikit-learn</a>).
<h3>How did you spend your time on this competition?</h3>
At the beginning of the competition I focused on creating a very simple working pipeline that, by only using <i>users</i> data, was able to produce a meaningful submission. I made public this pipeline (it is based on simple feature engineering and XGBoost) on this <a href=""https://www.kaggle.com/svpons/airbnb-recruiting-new-user-bookings/script-0-8655"" target=""_blank"">script</a>. After that, I focused on feature engineering on both <i>users</i> and <i>sessions</i> data and updated my model with the features computed from session data. I was then inactive for more than one month, but when I got back to the competition, I started working on other classifiers. The idea was to tune some classifiers that would potentially blend well with my best XGBoost model. During the final days I focused on ensembling. I first started with a more traditional stacked generalization approach based on LogisticRegression and GradientBoosting, but as I didn’t get very good results, I moved on to building my own ensemble strategies.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
<ul>
	<li>The importance of keeping an eye on the competition forum and contributing to it (I didn’t do much of that before this competition) when possible. Something similar can be said about competition scripts, they can be useful to others, but you can also get valuable feedback.</li>
	<li>Even though my NN models were very simple, I spent quite a bit of time with tools like <a href=""https://github.com/Theano/Theano"" target=""_blank"">Theano</a> and Keras. This gave me the opportunity to understand many concepts of deep learning that were not familiar to me.</li>
	<li>The importance of spending time in understanding the evaluation metric and how to optimize it. This competition is an example of why directly using an evaluation metric as objective function is not always the optimal solution.</li>
</ul>
<h3>Do you have any advice for those just getting started in data science competitions?</h3>
<ul>
	<li>Plan the time that will be spent on the competition. Do not focus on being in the top of the leaderboard from the beginning, make a plan and work accordingly. For example, do not start with complicated ensemble techniques before squeezing out single classifiers.</li>
	<li>Try many ideas, but be careful with the reproducibility of the code (e.g. take care of seeds for random number generators).</li>
	<li>Do not move forward to complex ideas before having a deep understanding of the evaluation measure and a reliable validation strategy. Depending on the competition, this can be trivial or very tricky.</li>
</ul>"
"Prudential Life Insurance Assessment, Winner's Interview: 2nd place, Bogdan Zhurakovskyi",http://blog.kaggle.com/2016/03/14/prudential-life-insurance-assessment-winners-interview-2nd-place-bogdan-zhurakovskyi/,2016-03-15 01:00:28,"<a href=""https://www.kaggle.com/c/prudential-life-insurance-assessment"" target=""_blank"">Prudential Life Insurance Assessment</a> ran on Kaggle from November 2015 to February 2016. It was our most popular recruiting challenge to date, with a total of 2,619 data scientists competing for a career opportunity and the $30,000 prize pool. Bogdan Zhurakovskyi took second place, and learned an important lesson: there is no innate hierarchy to the accuracy of different machine learning algorithms.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I am Ph.d. candidate in statistics. Since I started competing on Kaggle I have gained a lot of practice which has improved my skills significantly.

[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-14-at-5.56.00-PM.png"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-14-at-5.56.00-PM-300x142.png"" alt=""Bogdan on Kaggle"" width=""300"" height=""142"" /></a> <a href=""https://www.kaggle.com/bohdan"" target=""_blank"">Bogdan</a> on Kaggle[/caption]
<h3>What made you decide to enter this competition?</h3>
I like competitions where are a lot of teams and forum discussions. You can get a lot of knowledge from them.
<h2>Let's Get Technical</h2>
<h3>What was your most important insight into the data?</h3>
Till this competition I mistakenly believed that there is some hierarchy among algorithms in terms of accuracy. What I mean is that, for example, gradient boosting gives you the best accuracy, followed by svm and random forest, and in the end linear models. If you want to improve your accuracy further then you make an ensemble of different models. But that is not always true. There are datasets where a linear model can beat a gradient boosting model. This was a new discovery for me.

Below is a boxplot and distribution of kappa score obtained by using train test split function from sklearn 200 times (split koef = 0.3). As one can see, they are almost the same. But I spent a lot of time to find the parameters of xgboost to be so close to linear regression which gave me the best results. So there is no reason to use complex models if one can get the same results with a simple one.

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/03/output_1_0.png"" rel=""attachment wp-att-5595""><img class=""aligncenter size-full wp-image-5595"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/output_1_0.png"" alt=""output_1_0"" width=""609"" height=""439"" /></a>

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/03/output_1_1.png"" rel=""attachment wp-att-5596""><img class=""aligncenter size-full wp-image-5596"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/output_1_1.png"" alt=""output_1_1"" width=""628"" height=""438"" /></a>
<h3>Which tools did you use?</h3>
Python <a href=""http://scikit-learn.org/stable/"" target=""_blank"">scikit-learn</a> and <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> libraries.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
Do not neglect any of your ideas. The gold can hide in the most unexpected places.
<h3>Do you have any advice for those just getting started in data science?</h3>
Try to understand the math behind ML algorithms. Do not use an algorithm without understanding it. Otherwise, at some point, your progress is going to stop.
<h2>Just for Fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
I would love to see some weather prediction competitions. Because we all know how “accurate” modern models are :). Maybe some smart guy could finally improve that thing (joking).
<h3>What is your dream job?</h3>
Artificial Intelligence Developer.
<h2>Bio</h2>
<strong>Bogdan Zhurakovskyi</strong> is a PhD Candidate in Probability Theory and Mathematical Statistics at the <a href=""http://inter.kpi.ua/"" target=""_blank"">Kyiv Polytechnic Institute</a>, supervised by Alexander Ivanov. His research interests include nonlinear regression models, detection of hidden periodicities, and most of statistical machine learning."
"Airbnb New User Bookings, Winner's Interview: 2nd place, Keiichi Kuroyanagi (@Keiku)",http://blog.kaggle.com/2016/03/17/airbnb-new-user-bookings-winners-interview-2nd-place-keiichi-kuroyanagi-keiku/,2016-03-17 17:47:55,"<a href=""https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings"" target=""_blank"">AirBnB New User Bookings</a> challenged Kagglers to predict the first country where a new user would book travel. Participants were given a list of users along with their demographics, web session records, and some summary statistics. Keiichi Kuroyanagi (aka Keiku) took 2nd place, ahead of 1,462 other competitors using 1,312 engineered features and a stacked generalization architecture. In this blog, Keiku provides an in-depth view of his approach, final architecture, and why he didn't get punished by a leaderboard shakeup.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I'm currently working as a consultant for companies of various industries using data science skills. I support mainly the marketing of the companies by means of machine learning techniques, e.g. regression, classification and clustering. Prior to this, I earned an MSc in condensed matter physics at the Department of Physics. I have researched Anderson localization of Bose-Einstein condensation of cold atom in a quasiperiodic optical lattice at university.

[caption id=""attachment_5601"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-14-at-7.31.30-PM.png"" rel=""attachment wp-att-5601""><img class=""size-medium wp-image-5601"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-14-at-7.31.30-PM-300x142.png"" alt=""Keiku on Kaggle"" width=""300"" height=""142"" /></a> <a href=""https://www.kaggle.com/keiku322"" target=""_blank"">Keiku</a> on Kaggle[/caption]
<h3>What made you decide to enter this competition?</h3>
At first sight, I thought that I would not be good at this competition, because the ""date_account_created"" of the test dataset was the last 3 months of the whole dataset. Cross-validation for this kind of dataset becomes very difficult. I'm not good at this kind of cross-validation. That is why I decided to join this competition -- to overcome my weakness in this area. I submitted first submission on Jan. 25, 2016. (This competition was started Nov. 25, 2015 and ended Feb. 11, 2016 (78 total days)). I just started as a way to study in the last 3 weeks of this competition.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
Various datasets were given to us in this competition. It allowed for some creative feature engineering. I created some features as follows:
<ol>
	<li>As for the numerical features, I used the raw of variables of the dataset except ""age"".</li>
	<li>I created age features cleaning up some abnormal values.</li>
	<li>Also, I divided the above age features in a bucket and joined ""age_gender_bkts"" dataset to ""train_users"" and ""test_users"" dataset.</li>
	<li>I encoded the categorical features using one-hot encoding.</li>
	<li>I joined ""countries"" dataset to ""train_users"" and ""test_users"" dataset.</li>
	<li>I calculated the lag of ""date_first_booking"" and ""date_account_created"" and divided this lag feature into four categories (0, [1, 365], [-349,0), NA).</li>
	<li>Similarly, I calculated the lag of ""date_first_booking"" and ""timestamp_first_active"" and divided this lag feature into three categories (0, [1,1369], NA).</li>
	<li>I summarized ""secs_elapsed"" and counted the numbers of rows of ""sessions"" dataset by user_id, action (similarly, action_type, action_detail and device_type).</li>
</ol>
I created a total of 1,312 features from the given dataset.

Then, I calculated out-of-fold CV predictions of 18 models (stacked generalization). The approach of using stacked generalization has won some competitions (See also the <a href=""http://mlwave.com/kaggle-ensembling-guide/"" target=""_blank"">Kaggle Ensembling Guide</a>). In many cases, a target variable is predicted in stacked generalization, but I calculated not only ""country_destination"" (target variable) but also ""age"" and the above categorized lag features (explanatory variable).

I built the <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> model using above base features and out-of-fold CV predictions. My XGBoost model set the custom function of NDCG@5 as ""eval_metric"" parameter. When I made several attempts to build it, I found that some features decreased the NDCG@5 score, so I selected randomly features at the ratio of 90% and built repeatedly a single XGBoost many times. Finally, I selected the best XGBoost model (5 fold-CV: 0.833714) from the built models and I got Public: 0.88209/Private: 0.88682 using the best XGBoost model. (It has published code here: <a href=""https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings/forums/t/18976/2nd-place-solution/108084#post108084"" target=""_blank"">2nd Place Solution</a>).

[caption id=""attachment_5604"" align=""aligncenter"" width=""720""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/03/Learning_Architecture.png"" rel=""attachment wp-att-5604""><img class=""wp-image-5604 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/Learning_Architecture.png"" alt=""Learning_Architecture"" width=""720"" height=""540"" /></a> Learning architecture[/caption]
<h3>Were you surprised by any of your findings?</h3>
This competition was expected to have a Leaderboard Shakeup (See also <a href=""https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings/forums/t/18808/expected-leaderboard-shakeup"" target=""_blank"">Expected Leaderboard Shakeup</a>), but I got comparatively stable results (Public LB: 2nd/Private LB: 2nd). My local 5 fold-CV score was relevant to Public Leaderboard score. As for the final two submissions, one was the best model where both scores were good, and the other was the best model where the score of validation in the last 6 weeks was good. The former (Public: 0.88209/Private: 0.88682) was slightly higher private score than the latter (Public: 0.88195/Private: 0.88678).

<a href=""https://qiita-image-store.s3.amazonaws.com/0/847/50fc2ec1-b2b6-f72b-6e8a-87d36849a38a.png &quot;LB_Score.png"" target=""_blank"" rel=""attachment wp-att-5602""><img class=""aligncenter wp-image-5602 size-full"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/LB_Score-e1458009700492.png"" alt=""LB_Score"" width=""700"" height=""466"" /></a>

I think that the below features slightly prevented a Leaderboard Shakeup for my models. I checked the feature importance of the best XGBoost model. I found that the out-of-fold CV predictions of categorized lag features were very important. As far as I saw in the forum, many of the participants may have not created these features.

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/03/Feature_Importance.png"" rel=""attachment wp-att-5603""><img class=""aligncenter wp-image-5603"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/Feature_Importance.png"" alt=""Feature_Importance"" width=""598"" height=""554"" /></a>
<h3>Which tools did you use?</h3>
I used R in this competition. I used <a href=""https://cran.r-project.org/web/packages/DescTools/index.html"" target=""_blank"">DescTools</a> package for the preprocessing. Desc() function was helpful to check the statistical information about the dataset. As for the modeling, I used <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> and <a href=""https://cran.r-project.org/web/packages/glmnet/index.html"" target=""_blank"">glmnet</a>.
<h2>Words of Wisdom</h2>
<h3>Do you have any advice for those just getting started in data science?</h3>
Try Kaggle competitions at all times. The tasks of Kaggle competitions show one of the issues of the company or the challenges in research. The experiences of Kaggle competitions are able to be utilized for similar issues that we have at work or in research. We can learn not only machine learning techniques, but a lot of other things from Kaggle competitions.
<h2>Bio</h2>
<strong>Keiichi Kuroyanagi</strong> has worked as a consultant at Financial Engineering Group, Inc. He holds an MSc in condensed matter physics at Department of Physics from <a href=""http://www.keio.ac.jp/"" target=""_blank"">Keio University</a>. He is about to start to develop the market of non-financial field besides financial field."
"Telstra Network Disruption, Winner's Interview: 1st place, Mario Filho",http://blog.kaggle.com/2016/03/23/telstra-network-disruption-winners-interview-1st-place-mario-filho/,2016-03-23 17:25:46,"<a href=""https://www.kaggle.com/c/telstra-recruiting-network"" target=""_blank"">Telstra Network Disruptions</a> challenged Kagglers to predict the severity of service disruptions on their network. Using a dataset of features from their service logs, participants were tasked with predicting if a disruption was a momentary glitch or a total interruption of connectivity. 974 data scientists vied for a position at the top of the leaderboard, and an opportunity to join Telstra's Big Data team. Mario Filho, a self-taught data scientist, took first place in his first ""solo win"". In this blog, he shares a high-level view of his approach.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
My background in machine learning is completely “self-taught”. It all began in 2012 when I decided to learn Calculus on my own through the videos from a MIT class. Since then I found a wealth of education materials available online through MOOCs, academic papers and lectures in general.

Since February 2014 I have worked as a machine learning consultant, having worked in projects from small startups and Fortune 500 companies during this period.

[caption id=""attachment_5609"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-14-at-8.16.22-PM.png"" rel=""attachment wp-att-5609""><img class=""size-medium wp-image-5609"" src=""http://blog.kaggle.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-14-at-8.16.22-PM-300x142.png"" alt=""Mario on Kaggle"" width=""300"" height=""142"" /></a> <a href=""https://www.kaggle.com/mariofilho"" target=""_blank"">Mario</a> on Kaggle[/caption]
<h3>What made you decide to enter this competition?</h3>
I was looking for competitions that were ending soon, and this one had only 19 days before the end. And I like competitions with multiple files because it gives you many possibilities to create features.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
The most important preprocessing that I did was manipulating the tables to extract features that looked relevant. It was a very manual process of repeatedly looking at the data, extracting features and testing in my CV.

For neural networks the extra step was standardizing the data.

My best model was a Gradient Boosted Trees ensemble, using <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> (surprise!), trained with all my features.

The winning solution was a three-layer stacked ensemble with 15 models, mostly composed by GBTs, Neural Networks and Random Forests.
<h3>What was your most important insight into the data?</h3>
Definitely finding that the ordering of records in the additional files had high predictive power. I noticed there was some powerful pattern when I saw an unusual gap between the participants in the leaderboard, and knew that I had to find it if I wanted to end in a good position.
<h3>Which tools did you use?</h3>
<a href=""http://pandas.pydata.org/"" target=""_blank"">Pandas</a> for data manipulation. <a href=""http://scikit-learn.org/stable/"" target=""_blank"">Scikit-learn</a>, <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> and <a href=""https://github.com/fchollet/keras"" target=""_blank"">Keras</a> for modeling.
<h3>How did you spend your time on this competition?</h3>
I would say 70% feature engineering and 30% with ensembling and tuning.
<h2>Words of Wisdom</h2>
<h3>What have you taken away from this competition?</h3>
I learned a lot about different ways that you can explore the data and extract features. It’s my first “solo” win, which is quite nice, and I liked the jump to the 12th place in the global ranking.
<h2>Just for fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
I would like to see more machine learning applied to mental health. For example, today people with depression have to try many different medications to find which works for them, so trying to predict which treatment is most likely to work well for a depressed patient would be nice.
<h2>Bio</h2>
<strong>Mario Filho</strong> is a data science consultant focused in helping companies around the world use machine learning to maximize the value they get from data to achieve their business goals. Besides that, he mentors individuals who want to learn how to apply machine learning algorithms to real world data sets."
"Homesite Quote Conversion, Winners' Write-Up, 1st Place: KazAnova | Faron | clobber",http://blog.kaggle.com/2016/04/08/homesite-quote-conversion-winners-write-up-1st-place-kazanova-faron-clobber/,2016-04-08 17:00:20,"The <a href=""https://www.Kaggle.com/c/homesite-quote-conversion"" target=""_blank"">Homesite Quote Conversion</a> competition asked the Kaggle community to predict which customers would purchase a quoted insurance plan in order to help <a href=""https://homesite.com/"" target=""_blank"">Homesite</a> to better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments. The <b>1764</b> competing teams faced an anonymized dataset with around <b>250k</b> training samples in almost <b>300</b> dimensions. They were challenged to predict the probability a customer would purchase an insurance plan given a quote. Team <b>KazAnova | Faron | clobber</b> managed to win a head-to-head race at the end and finished in <b>1st</b> place.
<h2>Our Team</h2>
<strong><a href=""https://www.linkedin.com/in/mariosmichailidis"" target=""_blank"">Marios Michailidis</a></strong> | <a href=""https://www.Kaggle.com/kazanova"" target=""_blank"">KazAnova</a>
<img class=""wp-image-5333 alignright"" style=""margin-left: 10px; margin-top: 1px;"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/marios.png"" alt="""" width=""75"" height=""75"" /> is Manager of Data Science at <a href=""https://www.dunnhumby.com/"">dunnhumby</a> and part-time PhD student in machine learning at University College London (UCL) with a focus on improving recommender systems. He has worked in both marketing and credit sectors in the UK and has led many analytics projects with various themes including acquisition, retention, uplift, fraud detection and portfolio optimization. In his spare time he has created <a href=""http://www.kazanovaforanalytics.com/software.html"">KazAnova GUI</a> for credit scoring. Marios has 70 Kaggle competitions on his turtle shell and started competing on Kaggle to find new challenges and to learn from the best.

<strong><a href=""https://www.linkedin.com/in/muellermat"" target=""_blank"">Mathias Müller</a></strong> | <a href=""https://www.Kaggle.com/mmueller"" target=""_blank"">Faron</a>
<img class=""wp-image-5333 alignright"" style=""margin-left: 10px; margin-top: 1px;"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/mathias.png"" alt="""" width=""75"" height=""75"" /> holds an AI &amp; ML focused Diplom (eq. MSc.) in Computer Science from Humboldt University of Berlin. He tinkered with computer vision in the context of bio-inspired visual navigation of autonomous flying quadrocopters during his studies. Currently, he is working as a ML-Engineer for <a href=""https://www.fsd-web.de"" target=""_blank"">FSD</a> in the automotive sector. Mathias stumbled upon Kaggle while looking for a more ML-focused platform compared to TopCoder, where he entered his first predictive modeling competition. Besides, he likes to contribute to the amazing <a href=""https://github.com/dmlc/xgboost/graphs/contributors"" target=""_blank"">XGBoost</a>.

<strong>Ning Situ</strong> | <a href=""https://www.Kaggle.com/mmueller"" target=""_blank"">clobber</a>
<img class=""wp-image-5333 alignright"" style=""margin-left: 10px; margin-top: 1px;"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/ning.png"" alt="""" width=""75"" height=""75"" /> is a Software-Engineer at <a href=""https://www.microsoft.com/en-gb"">Microsoft</a>. He is currently working on a real-time data processing system hosted on Cassandra. Ning obtained his PhD in Computer Science from University of Houston for doing research in melanoma recognition and considers Kaggle a wonderful place to gain knowledge in Data Science through real projects.

&nbsp;
<div style=""text-align: center;"">
<div class=""wp-caption aligncenter"" style=""width: 290px; display: inline-block;"">

<img class=""wp-image-5570 size-medium"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/badge_kazanova.png"" alt="""" />
<p class=""wp-caption-text""><a href=""https://www.Kaggle.com/kazanova"" target=""_blank"">KazAnova</a> | Marios</p>

</div>
<div class=""wp-caption aligncenter"" style=""width: 290px; display: inline-block;"">

<img class=""wp-image-5570 size-medium"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/badge_faron.png"" alt="""" />
<p class=""wp-caption-text""><a href=""https://www.Kaggle.com/mmueller"" target=""_blank"">Faron</a> | Mathias</p>

</div>
<div class=""wp-caption aligncenter"" style=""width: 290px; display: inline-block;"">

<img src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/badge_clobber.png"" alt="""" />
<p class=""wp-caption-text""><a href=""https://www.Kaggle.com/clobber"" target=""_blank"">clobber</a> | Ning</p>

</div>
</div>
<div style=""clear: both;""></div>
<h2>Our Solution</h2>
From the start we were pursuing an ensembling approach and therefore we tried to get as many diverse models as possible. We were using the following software &amp; tools:
<ul style=""margin-top: -10px; margin-bottom: 10px;"">
	<li>Scientific <a href=""https://www.python.org/"">Python</a> Stack (including <a href=""http://www.numpy.org/"">NumPy</a>, <a href=""http://www.scipy.org/"">SciPy</a>, <a href=""http://pandas.pydata.org/"">Pandas</a>, <a href=""https://github.com/statsmodels/statsmodels/"">StatsModels</a>, <a href=""http://matplotlib.org/"">Matplotlib</a>)</li>
	<li><a href=""https://github.com/dmlc/xgboost"">XGBoost</a></li>
	<li><a href=""http://scikit-learn.org"">Scikit-Learn</a></li>
	<li><a href=""http://keras.io/"">Keras</a></li>
	<li><a href=""https://github.com/Lasagne/Lasagne"">Lasagne</a> &amp; <a href=""https://github.com/dnouri/nolearn"">NoLearn</a></li>
	<li><a href=""http://www.libfm.org/"">LibFM</a></li>
	<li><a href=""http://www.kazanovaforanalytics.com/software.html"">KazAnova GUI</a></li>
	<li><a href=""https://github.com/Far0n/xgbfi"">Xgbfi</a></li>
</ul>
to build a pool of around <b>500</b> base models from which we selected about <b>125</b> for ensembling. Our final ensemble
consists of 3 meta layers. It achieves a public leaderboard AUC of <b>0.97062</b> and a private leaderboard AUC of <b>0.97024</b>:
<div style=""text-align: center;"">
<div class=""wp-caption aligncenter"" style=""width: 720px; display: inline-block;"">

<img class=""aligncenter size-full wp-image-5569"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/ensemble.jpg"" alt="""" />
<div style=""margin-top: -10px;""><em><strong>Fig. 1:</strong></em> Final Ensemble</div>
</div>
</div>
<h3>Data Preprocessing &amp; Feature Engineering</h3>
This dataset required hardly any cleaning and we encoded the provided data in many different ways in order to illuminate the input space as much as possible:
<ul style=""margin-top: -10px; margin-bottom: 10px;"">
	<li>Categorical ⇒ ID per category</li>
	<li>Categorical ⇒ value count for each category</li>
	<li>Categorical ⇒ out-of-fold likelihood for each category with respect to the target attribute</li>
	<li>Categorical ⇒ one-hot encoding</li>
	<li>Numerical ⇒ as is</li>
	<li>Numerical ⇒ percentile transformation</li>
	<li>One-hot encodings and value counts of all features with distinct values below a threshold</li>
</ul>
Regarding feature engineering, we extracted year, month, day &amp; weekday out of <em>Original_Quote_Date</em>, explored summary statistics and 2-, 3- &amp; 4-way feature interactions (sums, differences, products and quotients). The search for the latter was initiated due to the positive effect of the <em>""golden features""</em> (differences of highly correlated features) used in the <a href=""https://www.Kaggle.com/yangnanhai/homesite-quote-conversion/keras-around-0-9633"">""Keras around 0.9633*""</a> public script. We searched interaction candidates either by simple target correlation checks or through logistic regression and XGBoost as wrappers. The space of possible 2-way interactions was searched exhaustively and random searches with adjusted sampling probabilities based on feature rankings were used to find higher-order interactions. Afterwards, we applied the feature selection methods described below to the most promising candidates. Feature interactions helped to get diverse as well as better performing models, which added significant value to our ensemble.
<div style=""text-align: center;"">
<div class=""wp-caption aligncenter"" style=""width: 670px; margin-top: 15px; display: inline-block;"">

<img class=""aligncenter size-full wp-image-5569"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/interactions.png"" alt="""" />
<div style=""margin-top: -10px;""><em><strong>Fig. 2:</strong></em> Search for feature interactions with XGBoost as wrapper</div>
</div>
</div>
<h3>Feature Ranking &amp; Subset Selection</h3>
Feature subset selection turned out to be useful to increase both single model performance and diversity. It was also playing an important role during our meta modeling. We applied and combined a variety of wrapper and embedded methods to rank the features of the base and meta levels:
<ul style=""margin-top: -10px; margin-bottom: 10px;"">
	<li><strong>Forward Selection &amp; Backwards Elimination:</strong> either via brute-force if feasible or greedy in conjunction with feature rankings.</li>
	<li><strong>Single Feature AUC</strong>: by training a model like XGBoost on each feature seperately or calculating gini coefficients on binned versions of the features.
<div style=""text-align: center;"">
<div class=""wp-caption aligncenter"" style=""width: 450px; margin-top: 15px; display: inline-block;"">

<img class=""aligncenter size-full wp-image-5569"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/gini.png"" alt="""" />
<div style=""margin-top: -10px;""><em><strong>Fig. 3:</strong></em> Example of single feature scoring</div>
</div>
</div></li>
	<li><strong>XGBoost:</strong> we used <a href=""https://github.com/Far0n/xgbfi"">Xgbfi</a> to rank the features by different metrics like <em>FScore</em>, <em>wFScore</em>, <em>Gain</em> and <em>ExpectedGain</em> (see Fig. 4).
<div style=""text-align: center;"">
<div class=""wp-caption aligncenter"" style=""width: 700px; margin-top: 15px; display: inline-block;"">

<img class=""aligncenter size-full wp-image-5569"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/xgbfi.png"" alt="""" />
<div style=""margin-top: -10px;""><em><strong>Fig. 4:</strong></em> Feature ranking metrics regarding XGBoost</div>
</div>
</div></li>
	<li><strong>Noise Injection:</strong> we replaced features by noise or added noise to the features after model training and monitored the impact on the validation errors.
<div style=""text-align: center;"">
<div class=""wp-caption aligncenter"" style=""width: 700px; margin-top: 15px; display: inline-block;"">

<img class=""aligncenter size-full wp-image-5569"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/noise.png"" alt="""" />
<div style=""margin-top: -10px;""><em><strong>Fig. 5:</strong></em> Noise injection in order to detect unimportant features</div>
</div>
</div></li>
</ul>
<h3>Modeling</h3>
We started with building some marginally tuned models and constructed our meta modeling early on to get an idea about what adds diversity and to optimize the ensemble performance alongside. Neural networks have been the best performing stackers, followed by XGBoost and logistic regression. Besides, XGBoost and logistic regression showed to be considerably more sensitive to feature selection at the meta levels than neural networks. In general, it turned out to be more useful to train on many different combinations of feature subsets and feature representations than to tune hyperparameters, which is why several (base) models share the same parameter settings.

<a href=""https://github.com/Far0n/Kaggle-homesite"">Our best</a> base model (XGBoost) emerged from feature subset selection and utilization of feature interactions and scores Top15 at the private leaderboard. Regarding public leadboard scores,
we got the following ranking of algorithms at the base level:
<ol style=""margin-top: -10px; margin-bottom: 10px;"">
	<li><a href=""https://github.com/dmlc/xgboost"">XGBoost</a>: ~0.969</li>
	<li><a href=""http://keras.io/"">Keras</a>, <a href=""https://github.com/Lasagne/Lasagne"">Lasagne</a>, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"">GBM</a>: ~0.967</li>
	<li><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"">Random Forest</a>,
<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"">Extra Trees</a>: ~0.966</li>
	<li><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"">Logistic Regression</a>: ~0.965</li>
	<li><a href=""http://www.libfm.org/"">Factorization Machine</a>: ~0.955</li>
</ol>
Around 20% of our selected base models were <em>""sub-models""</em> - trained on different partitions of the data specified by feature values (e.g. one model per weekday). We used <a href=""http://www.kazanovaforanalytics.com/software.html"">KazAnova GUI's</a> optimized binning based on <em>Weights of Evidence (WOE)</em> and <em>Information Value (IV)</em> to ensure sufficiently large data partitions for features with higher cardinality of distinct values like <em>GeographicField6B</em>:
<div style=""text-align: center;"">
<div class=""wp-caption aligncenter"" style=""width: 700px; display: inline-block;"">

<img class=""aligncenter size-full wp-image-5569"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/binning.png"" alt="""" />
<div style=""margin-top: -10px;""><em><strong>Fig. 6:</strong></em> Binning of GeographicField6B</div>
</div>
</div>
<strong style=""font-size: 16px;"">Other Things ... which added diversity or increased single model performance:</strong>
<ul>
	<li>Gradient boosting from random forest predictions (adopted from <a href=""https://sites.google.com/site/rtranking/"">Initialized Gradient Boosted Regression Trees</a> and applied to XGBoost)</li>
	<li>Feature-specific dropout probabilities within XGBoost (non-uniform <em>colsample</em>, special case: adding features at later boosting stages)</li>
	<li>XGBoost feature (interactions) embeddings as inputs for neural networks or linear models</li>
	<li>Boosting of random forests (by setting XGB's <em>num_parallel_tree</em> &gt; 1)</li>
	<li>Model training with different objectives</li>
</ul>
<h3>Cross Validation</h3>
We were using 5-fold stratified cross validation during the whole competition with a small set of seeds for the base models. At first, our meta models were tuned on fixed-seeded folds for the sake of having a common reference, but we were forced to avoid this seed later in the competition once we realized that we had overused it. After that we continued to train our models on different seeds and we sampled different seeded 5-folds out of our oof-predictions in order to compare the scores of the folds the models had been trained on and other folds. Our best public LB submission (0.97065) showed some irregularities with respect to these analyses and therefore we did not select it for final evaluation.

We also took the public leaderboard scores as additional source of information into account and checked for a pair of submissions, whether the observed public LB score difference matched the AUC variations between the local folds of these two submissions. This helped us to identify inconsistencies: In general, an improvement of the CV-mean did not imply AUC improvements on all 5 folds. We saw absolute AUC differences up to 0.000085 for a single fold between two submissions with the same CV-mean. Besides, each fold had a slightly different correlation to the public LB score. We hit a suspicious looking plateau around the LB score of 0.97025, which did not match the <em>normal</em> pattern between AUC changes of our local folds and the public LB score. In fact, it turned out to be caused by a bug in our sources, leading to faulty calculations of 3-way interactions for the test data.
<div style=""text-align: center; margin-top: 1.5ex;"">
<div class=""wp-caption aligncenter"" style=""width: 700px; display: inline-block;"">

<img class=""aligncenter size-full wp-image-5569"" src=""http://blog.Kaggle.com/wp-content/uploads/2016/04/CVvsLB.png"" alt="""" />
<div style=""margin-top: -10px;""><em><strong>Fig. 7:</strong></em> Suspicious plateau around the LB score of 0.97025</div>
</div>
</div>
<h2>Marios' Corner: Some Tips Regarding Cross Validation</h2>
Reliable cross validation, detection of tiny inconsistencies and dealing with serious overfitting issues have probably been the most important ingredients in our solution. So I thought this post would be a good place to share some aspects that I have been considering useful over my past 70+ Kaggle competitions to set up a reliable CV:
<ul>
	<li><strong>Stratified k-fold</strong>. Unless the dataset you are being tested on (e.g the test set ) is in future period, then most of the times use random stratified k-fold - it seems to work well. However, if the test data is in the future - and time seems to be an important factor like in stock marker or store sales - then you need to formulate your CV process to always train on past data and test on future data (e.g. do a time split).</li>
</ul>
<ul>
	<li><strong>Treat your CV like your test data</strong>. That means if there is something that you cannot know or do for your test data, then you have to treat the validation data as if you don't know it too.
<strong>Example:</strong> Let's say you run a neural net (that is easy to overfit / underfit) and you use the validation data to determine at which exact epoch to stop the training and you check the performance for the same validation data. Can you use the test data to know when to stop the training? No, you can't, because you don't know the labels for the test data. Hence it is invalid to use this for validation. Instead, you could use a fixed amount of epochs that works well on average for all your folds. From my perspective, the best way to do that would be to split your training data into 3 parts when you do CV and use them
<ol>
	<li>to train the model,</li>
	<li>to validate when to stop the training,</li>
	<li>to see the performance on a real holdout set.</li>
</ol>
Generally always keep in mind that you need to treat your validation data as much as you can as your <b>test</b> data (e.g. like you don't know the labels), otherwise you may overfit.</li>
</ul>
<ul>
	<li>Always test against the <strong>metric</strong> you are being tested on. If it is AUC, then AUC. If it is RMSE, then RMSE and so on.</li>
</ul>
<ul>
	<li><strong>Model type and size of dataset</strong>. It is more difficult to overfit with 500,000 rows versus with 10,000 rows. If you have less than 400-300, you are kind of doomed! From my experience, it is difficult to form a reliable CV process with so little data. All neural nets are more prone to overfitting than linear models.</li>
</ul>
<ul>
	<li><strong>Test your CV</strong>. Sacrifice a couple of submissions to see whether you got it right. Once you formulated your CV, test it! Bear in mind for anything less than 5,000 rows (empirically), big variations have to be expected (e.g. you might improve in CV, but not on public leaderboard).</li>
</ul>
<ul>
	<li><strong>Don't overtune your models</strong>. For example, when you run logistic regression and you try to find the best value for regularization and your C becomes 1.02412353563, you are definitely overfitting! Do sizable increases/decreases of your hyperparameters. For example, try C=1.2, then C=1.4, then 1.6 and so on.</li>
</ul>
<ul>
	<li><strong>Get intuition from your hyper parameters</strong>. Try to see if changes of the hyperparameters yield meaningful results. Most of the times, the hyperparameters have a peak performance for a specific value. Let’s say the more you increase C in logistic regression the better the performance for your metric becomes. However that is only true until C reaches 2.0 (for example). After that, further increases in C decreases performance. If you don't encounter something like that, or for some reason the best value of your hyperparameter has many peaks, this most of the times indicates a problem regarding your CV.</li>
</ul>
<ul>
	<li>You can always <b>put more size in your validation</b>. If there is too much volatility in your results, increase you validation size. Bagging helps a lot too to stabilize your results (at the cost of more time though).</li>
</ul>
<h2>Trivia Corner</h2>
<div align=""center"">[simpleresponsiveslider]</div>

Read about Marios' <a href=""http://blog.kaggle.com/2016/02/10/my-kaggle-experience-spot-chasing-retirement"" target=""_blank"">experience on Kaggle and spot chasing retirement</a>."
"Diagnosing Heart Diseases in the Data Science Bowl: 2nd place, Team kunsthart",http://blog.kaggle.com/2016/04/13/diagnosing-heart-diseases-with-deep-neural-networks-2nd-place-ira-korshunova/,2016-04-13 17:00:12,"The <a href=""https://www.kaggle.com/c/second-annual-data-science-bowl"">Second National Data Science Bowl</a>, a data science competition where the goal was to automatically determine cardiac volumes from MRI scans, has just ended. We participated with a team of 4 members from the <a href=""http://datasciencelab.ugent.be/MembersList"">Data Science lab</a> at Ghent University in Belgium and finished 2nd!

The team <em>kunsthart</em> (<em>artificial heart</em> in English) consisted of <a href=""https://www.kaggle.com/golondrina"">Ira Korshunova</a>, <a href=""https://www.kaggle.com/jburms"">Jeroen Burms</a>, Jonas Degrave (<a href=""https://twitter.com/317070"">@317070</a>), 3 PhD students, and professor <a href=""https://twitter.com/jonidambre"">Joni Dambre</a>. It's also a follow-up of last year's team <a href=""http://benanne.github.io/2015/03/17/plankton.html"">≋ Deep Sea ≋</a>, which finished in first place for the <a href=""https://www.kaggle.com/c/datasciencebowl"">First National Data Science Bowl</a>.
<h2>Overview</h2>
This blog post is going to be long, here is a clickable overview of different sections.
<ul>
 	<li><a href=""#introductiona-nameintroa"">Introduction</a></li>
 	<li><a href=""#pre-processing-and-data-augmentationa-nameprepa"">Pre-processing and data augmentation</a></li>
 	<li><a href=""#network-architecturesa-namenna"">Network architectures</a></li>
 	<li><a href=""#training-and-ensemblinga-nametraina"">Training and ensembling</a></li>
 	<li><a href=""#software-and-hardwarea-namesofta"">Software and hardware</a></li>
 	<li><a href=""#conclusiona-nameconclusiona"">Conclusion</a></li>
</ul>
<h2>Introduction</h2>
<h3>The problem</h3>
The goal of this year's Data Science Bowl was to estimate minimum (end-systolic) and maximum (end-diastolic) volumes of the left ventricle from a set of MRI-images taken over one heartbeat. These volumes are used by practitioners to compute an ejection fraction: fraction of outbound blood pumped from the heart with each heartbeat. This measurement can predict a wide range of cardiac problems. For a skilled cardiologist analysis of MRI scans can take up to 20 minutes, therefore, making this process automatic is obviously useful.

<iframe id=""fitvid412187"" src=""https://www.youtube.com/embed/dFu_5T0ODrM"" width=""750"" height=""420"" frameborder=""0"" allowfullscreen=""allowfullscreen""></iframe>

Unlike the previous Data Science Bowl, which had very clean and voluminous data set, this year's competition required a lot more focus on dealing with inconsistencies in the way the very limited number of data points were gathered. As a result, most of our efforts went to trying out different ways to preprocess and combine the different data sources.
<h3>The data</h3>
The dataset consisted of over a thousand patients. For each patient, we were given a number of 30-frame MRI videos in the <a href=""https://en.wikipedia.org/wiki/DICOM"">DICOM</a> format, showing the heart during a single cardiac cycle (i.e. a single heartbeat). These videos were taken in different planes including the multiple short-axis views (SAX), a 2-chamber view (2Ch), and a 4-chamber view (4Ch). The SAX views, whose planes are perpendicular to the long axis of the left ventricle, form a series of slices that (ideally) cover the entire heart. The number of SAX slices ranged from 1 to 23. Typically, the region of interest (ROI) is only a small part of the entire image. Below you can find a few of SAX slices and Ch2, Ch4 views from one of the patients. Red circles on the SAX images indicate the ROI's center (later we will explain how to find it), for Ch2 and Ch4 they specify the location of SAX slices projected on the corresponding view.
<table>
<tbody>
<tr>
<td>sax_5<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/213sax_5.gif"" alt="""" /></td>
<td>sax_9<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/213sax_9.gif"" alt="""" /></td>
<td>sax_10<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/213sax_10.gif"" alt="""" /></td>
<td>sax_11<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/213sax_11.gif"" alt="""" /></td>
<td>sax_12<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/213sax_12.gif"" alt="""" /></td>
<td>sax_15<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/213sax_15.gif"" alt="""" /></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>2Ch<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/213_ch2.gif"" alt="""" /></td>
<td>4Ch<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/213_ch4.gif"" alt="""" /></td>
</tr>
</tbody>
</table>
The DICOM files also contained a bunch of metadata. Some of the metadata fields, like <em>PixelSpacing</em> and <em>ImageOrientation</em> were absolutely invaluable to us. The metadata also specified patient’s age and sex.

For each patient in the train set, two labels were provided: the systolic volume and the diastolic volume. From what we gathered (<a href=""https://www.youtube.com/watch?v=wXUt_vjrF0c&amp;feature=youtu.be"">link</a>), these were obtained by cardiologists by manually performing a segmentation on the SAX slices, and feeding these segmentations to a program that computes the minimal and maximal heart chamber volumes. The cardiologists didn’t use the 2Ch or 4Ch images to estimate the volumes, but for us they proved to be very useful.

Combining these multiple data sources can be difficult, however for us dealing with inconsistencies in the data was more challenging. Some examples: the 4Ch slice not being provided for some patients, one patient with less than 30 frames per MRI video, couple of patients with only a handful of SAX slices, patients with SAX slices taken in weird locations and orientations.
<h3 id=""the-evaluation"">The evaluation</h3>
Given a patient’s data, we were asked to output a cumulative distribution function over the volume, ranging from 0 to 599 mL, for both systole and diastole. The models were scored by a Continuous Ranked Probability Score (CRPS) error metric, which computes the average squared distance between the predicted CDF and a Heaviside step function representing the real volume.

An additional interesting novelty of this competition was the two stage process. In the first stage, we were given a training set of 500 patients with a public test set of 200 patients. In the final week we were required to submit our model and afterwards the organizers released the test data of 440 patients and labels for 200 patients from the public test set. We think the goal was to compensate for the small dataset and prevent people from optimizing against the test set through visual inspection of every part of their algorithm. Hand-labeling in the first stage was allowed on the training dataset only, for the second stage it was also allowed for 200 validation patients.
<h3 id=""the-solution-traditional-image-processing-convnets-and-dealing-with-outliers"">The solution: traditional image processing, convnets, and dealing with outliers</h3>
In our solution, we combined traditional image processing approaches, which find the region of interest (ROI) in each slice, with convolutional neural networks, which perform the mapping from the extracted image patches to the predicted volumes. Given the very limited number of training samples, we tried combat overfitting by restricting our models to combine the different data sources in predefined ways, as opposed to having them learn how to do the aggregation. Unlike many other contestants, we performed <strong> no hand-labelling </strong>.
<h2 id=""pre-processing-and-data-augmentationa-nameprepa"">Pre-processing and data augmentation<a name=""prep""></a></h2>
The provided images have varying sizes and resolutions, and do not only show the heart, but the entire torso of the patient. Our preprocessing pipeline made the images ready to be fed to a convolutional network by going through the following steps:
<ul>
 	<li>applying a zoom factor such that all images have the same resolution in millimeters</li>
 	<li>finding the region of interest and extracting a patch centered around it</li>
 	<li>data augmentation</li>
 	<li>contrast normalization</li>
</ul>
To find the correct zooming factor, we made use of the <em>PixelSpacing</em> metadata field, which specifies the image resolution. Further we will explain our approach to ROI detection and data augmentation.
<h3 id=""detecting-the-region-of-interest-through-image-segmentation-techniques"">Detecting the Region Of Interest through image segmentation techniques</h3>
We used classical computer vision techniques to find the left ventricle in the SAX slices. For each patient, the center and width of the ROI were determined by combining the information of all the SAX slices provided. The figure below shows an example of the result.

[caption id="""" align=""alignnone"" width=""1665""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/roi.png"" alt=""ROI extraction steps"" width=""1665"" height=""453"" /> ROI extraction steps[/caption]

First, as was suggested in the <a href=""https://www.kaggle.com/c/second-annual-data-science-bowl/details/fourier-based-tutorial"">Fourier based tutorial</a>, we exploit the fact that each slice sequence captures one heartbeat and use Fourier analyses to extract an image that captures the maximal activity at the corresponding heartbeat frequency (same figure, second image).

From these Fourier images, we then extracted the center of the left ventricle by combining the Hough circle transform with a custom kernel-based majority voting approach across all SAX slices. First, for each fourier image (resulting from a single sax slice), the $$N$$ highest scoring Hough circles for a range of radii were found, and from all of those, the $$M$$ highest scoring ones were retained. $$N$$, $$M$$ and the range of radii are metaparameters that severely affect the robustness of the ROI detected and were optimised manually. The third image in the figure shows an example of the $$M$$ best circles for one slice.

Finally, a ‘likelihood surface’ (rightmost image in figure above) was obtained by combining the centers and scores of the selected circles for all slices. Each circle center was used as the center for a Gaussian kernel, which was scaled with the circle score, and all these kernels were added. The maximum across this surface was selected as the center of the ROI. The width and height of the bounding box of all circles with centers within a maximal distance (another hyperparameter) of the ROI center were used as bounds for the ROI or to create an ellipsoidal mask as shown in the figure.

Given these ROIs in the SAX slices, we were able to find the ROIs in the 2Ch and 4Ch slices by projecting the SAX ROI centers onto the 2Ch and 4Ch planes.
<h3 id=""data-augmentation"">Data augmentation</h3>
As always when using convnets on a problem with few training examples, we used tons of data augmentation. Some special precautions were needed, since we had to preserve the surface area. In terms of affine transformations, this means that only skewing, rotation and translation was allowed. We also added zooming, but we had to correct our volume labels when doing so! This helped to make the distirbution of labels more diverse.

Another augmentation here came in the form of shifting the images over the time axis. While systole was often found in the beginning of a sequence, this was not always the case. Augmenting this, by rolling the image tensor over the time axis, made the resulting model more robust against this noise in the dataset, while providing even more augmentation of our data.

Data augmentation was applied during the training phase to increase the number of training examples. We also applied the augmentations during the testing phase, and averaged predictions across the augmented versions of the same data sample.
<h2 id=""network-architecturesa-namenna"">Network architectures<a name=""nn""></a></h2>
We used convolutional neural networks to learn a mapping from the extracted image patches to systolic and diastolic volumes. During the competition, we played around a lot with both minor and major architectural changes. Our base architecture for most of our models was based on <a href=""http://www.robots.ox.ac.uk/%7Evgg/research/very_deep/"">VGG-16</a>.

As we already mentioned, we trained different models which can deal with different kinds of patients. There are roughly four different kinds of models we trained: single slice models, patient models, 2Ch models and 4Ch models.
<h3 id=""single-slice-models"">Single slice models</h3>
Single slice models are models that take a single SAX slice as an input, and try to predict the systolic and diastolic volumes directly from it. The 30 frames were fed to the network as 30 different input channels. The systolic and diastolic networks shared the convolutional layers, but the dense layers were separated. The output of the network could be either a 600-way softmax (followed by a cumulative sum), or the mean and standard deviation of a Gaussian (followed by a layer computing the cdf of the Gaussian).

Although these models obviously have too little information to make a decent volume estimation, they benefitted hugely from test-time augmentation (TTA). During TTA, the model gets slices with different augmentations, and the outputs are averaged across augmenations and slices for each patient. Although this way of aggregating over SAX slices is suboptimal, it proved to be very robust to the relative positioning of the SAX slices, and is as such applicable to all patients.

Our single best single slice model achieved a local validation score of 0.0157 (after TTA), which was a reliable estimate for the public leaderboard score for these models. The approximate architecture of the slice models is shown on the following figure.
<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/model_ss.jpg"" target=""_blank""><img style=""max-width: 100%;"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/model_ss.jpg"" alt="""" /></a>
<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/model_ss_annot.jpg"" target=""_blank""><img style=""max-width: 100%;"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/model_ss_annot.jpg"" alt="""" /></a>
<h3 id=""ch-and-4ch-models"">2Ch and 4Ch models</h3>
These models have a much more global view on the left ventricle of the heart than single SAX slice models. The 2Ch models also have the advantage of being applicable to every patient. Not every patient had a 4Ch slice.
We used the same VGG-inspired architecture for these models. Individually, they achieved a similar validation score (0.0156) as was achieved by averaging over multiple sax slices. By ensembling only single slice, 2Ch and 4Ch models, we were able to achieve a score of 0.0131 on the public leaderboard.
<h3 id=""patient-models"">Patient models</h3>
As opposed to single slice models, patient models try to make predictions based on the entire stack of (up to 25) SAX slices. In our first approaches to these models, we tried to process each slice separately using a VGG-like single slice network, followed by feeding the results to an overarching RNN in an ordered fashion. However, these models tended to overfit badly. Our solution to this problem consists of a clever way to merge predictions from multiple slices. Instead of having the network learn how to compute the volume based on the results of the individual slices, we designed a layer which combines the areas of consecutive cross-sections of the heart using a <a href=""https://en.wikipedia.org/wiki/Frustum"">truncated cone approximation</a>.

Basically, the slice models have to estimate the area $$A_i$$ (and standard deviation thereof) of the cross-section of the heart in a given slice $$i$$. For each pair of consecutive slices $$i$$ and $$i+1$$, we estimate the volume of the heart between them as $$V_i = \frac{1}{3} * h_{i, i+1} * (A_i + \sqrt{A_i A_{i+1}} + A_{i+1})$$, where $$h_{i, i+1}$$ is the distance between the slices. The total volume is then given by $$V = \sum_i V_i$$.

Ordering the SAX slices and finding the distance between them was achieved through looking at the <em>SliceLocation</em> metadata fields, but this field was not very reliable in finding the distance between slices, neither was the <em>SliceThickness</em>. We looked for the two slices that were furthest apart, drew a line between them, and projected every other slice onto this line. This way, we estimated the distance between two slices ourselves.

Our best single model achieved a local validation score of 0.0105 using this approach. This was no longer a good leaderboard estimation, since our local validation set contained relatively few outliers compared to the public leaderboard in the first round. The model had the following architecture:

<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/model.jpg"" alt="""" />
<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/model_annot.jpg"" alt="""" />
<table>
<thead>
<tr>
<th style=""text-align: left;"">Layer Type</th>
<th>Size</th>
<th>Output shape</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input layer</td>
<td></td>
<td>(8, 25, 30, 64, 64)*</td>
</tr>
<tr>
<td>Convolution</td>
<td>128 filters of 3x3</td>
<td>(8, 25, 128, 64, 64)</td>
</tr>
<tr>
<td>Convolution</td>
<td>128 filters of 3x3</td>
<td>(8, 25, 128, 64, 64)</td>
</tr>
<tr>
<td>Max pooling</td>
<td></td>
<td>(8, 25, 128, 32, 32)</td>
</tr>
<tr>
<td>Convolution</td>
<td>128 filters of 3x3</td>
<td>(8, 25, 128, 32, 32)</td>
</tr>
<tr>
<td>Convolution</td>
<td>128 filters of 3x3</td>
<td>(8, 25, 128, 32, 32)</td>
</tr>
<tr>
<td>Max pooling</td>
<td></td>
<td>(8, 25, 128, 16, 16)</td>
</tr>
<tr>
<td>Convolution</td>
<td>256 filters of 3x3</td>
<td>(8, 25, 256, 16, 16)</td>
</tr>
<tr>
<td>Convolution</td>
<td>256 filters of 3x3</td>
<td>(8, 25, 256, 16, 16)</td>
</tr>
<tr>
<td>Convolution</td>
<td>256 filters of 3x3</td>
<td>(8, 25, 256, 16, 16)</td>
</tr>
<tr>
<td>Max pooling</td>
<td></td>
<td>(8, 25, 256, 8, 8)</td>
</tr>
<tr>
<td>Convolution</td>
<td>512 filters of 3x3</td>
<td>(8, 25, 512, 8, 8)</td>
</tr>
<tr>
<td>Convolution</td>
<td>512 filters of 3x3</td>
<td>(8, 25, 512, 8, 8)</td>
</tr>
<tr>
<td>Convolution</td>
<td>512 filters of 3x3</td>
<td>(8, 25, 512, 8, 8)</td>
</tr>
<tr>
<td>Max pooling</td>
<td></td>
<td>(8, 25, 512, 4, 4)</td>
</tr>
<tr>
<td>Convolution</td>
<td>512 filters of 3x3</td>
<td>(8, 25, 512, 4, 4)</td>
</tr>
<tr>
<td>Convolution</td>
<td>512 filters of 3x3</td>
<td>(8, 25, 512, 4, 4)</td>
</tr>
<tr>
<td>Convolution</td>
<td>512 filters of 3x3</td>
<td>(8, 25, 512, 4, 4)</td>
</tr>
<tr>
<td>Max pooling</td>
<td></td>
<td>(8, 25, 512, 2, 2)</td>
</tr>
<tr>
<td>Fully connected (S/D)</td>
<td>1024 units</td>
<td>(8, 25, 1024)</td>
</tr>
<tr>
<td>Fully connected (S/D)</td>
<td>1024 units</td>
<td>(8, 25, 1024)</td>
</tr>
<tr>
<td>Fully connected (S/D)</td>
<td>2 units (mu and sigma)</td>
<td>(8, 25, 2)</td>
</tr>
<tr>
<td>Volume estimation (S/D)</td>
<td></td>
<td>(8, 2)</td>
</tr>
<tr>
<td>Gaussian CDF (S/D)</td>
<td></td>
<td>(8, 600)</td>
</tr>
</tbody>
</table>
* The first dimension is the batch size, i.e. the number of patients, the second dimension is the number of slices. If a patient had fewer slices, we padded the input and omitted the extra slices in the volume estimation.

Oftentimes, we did not train patient models from scratch. We found that initializing patient models with single slice models helps against overfitting, and severely reduces training time of the patient model.

The architecture we described above was one of the best for us. To diversify our models, some of the good things we tried include:
<ul>
 	<li>processing each frame separately, and taking the minimum and maximum at some point in the network to compute systole and diastole</li>
 	<li>sharing some of the dense layers between the systole and diastole networks as well</li>
 	<li>using discs to approximate the volume, instead of truncated cones</li>
 	<li><a href=""http://benanne.github.io/2015/03/17/plankton.html#rolling-feature-maps"">cyclic rolling layers</a></li>
 	<li>leaky RELUs</li>
 	<li><a href=""http://arxiv.org/pdf/1302.4389.pdf"">maxout units</a></li>
</ul>
One downside of the patient model approach was that these models assume that SAX slices nicely range from one end of the heart to the other. This was trivially not true for patients with very few (&lt; 5) slices, but it was harder to detect automatically for some other outlier cases as in figure below, where something is wrong with the images or the ROI algorithm fails.
<table>
<tbody>
<tr>
<td>sax_12<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/561sax_12.gif"" alt="""" /></td>
<td>sax_15<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/561sax_12.gif"" alt="""" /></td>
<td>sax_17<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/561sax_17.gif"" alt="""" /></td>
<td>sax_36<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/561sax_36.gif"" alt="""" /></td>
<td>sax_37<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/561sax_37.gif"" alt="""" /></td>
<td>sax_41<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/561sax_41.gif"" alt="""" /></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>2Ch<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/561_ch2.gif"" alt="""" /></td>
<td>4Ch<img src=""http://blog.kaggle.com/wp-content/uploads/2016/04/561_ch4.gif"" alt="""" /></td>
</tr>
</tbody>
</table>
<h2 id=""training-and-ensemblinga-nametraina"">Training and ensembling<a name=""train""></a></h2>
<strong>Error function</strong>. At the start of the competition, we experimented with various error functions, but we found optimising CRPS directly to work best.

<strong>Training algorithm</strong>. To train the parameters of our models, we used the Adam update rule (<a href=""http://arxiv.org/abs/1412.6980"">Kingma and Ba</a>).

<strong>Initialization</strong>. We initialised all filters and dense layers orthogonally (<a href=""http://arxiv.org/abs/1312.6120"">Saxe et al.</a>). Biases were initialized to small positive values to have more gradients at the lower layer in the beginning of the optimization. At the Gaussian output layers, we initialized the biases for mu and sigma such that initial predictions of the untrained network would fall in a sensible range.

<strong>Regularization</strong>. Since we had a low number of patients, we needed considerable regularization to prevent our models from overfitting. Our main approach was to augment the data and to add a considerable amount of dropout.
<h3 id=""validation"">Validation</h3>
Since the trainset was already quite small, we kept the validation set small as well (83 patients). Despite this, our validation score remained pretty close to the leaderboard score. Also, in cases where it didn’t, it helped us identify issues in our models, namely problematic cases in the test set which were not represented in our validation set. We noticed for instance that quite some of our patient models had problems with patients with too few SAX slices (&lt; 5).
<h3 id=""selectively-train-and-predict"">Selectively train and predict</h3>
By looking more closely at the validation scores, we observed that most of the accumulated error was obtained by wrongly predicting only a couple of such outlier cases. At some point, being able to handle only a handful of these meant the difference between a leaderboard score of 0.0148 and 0.0132!

To mitigate such issues, we set up our framework such that each individual model could choose not to train on or predict a certain patient. For instance, models on patients’ SAX slices could choose not to predict patients with too few SAX slices, models which use the 4Ch slice would not predict for patients who don’t have this slice. We extended this idea further by developing expert models, which only trained and predicted for patients with either a small or a big heart (as determined by the ROI detection step). Further down the pipeline, our ensembling scripts would then take these non-predictions into account.
<h3 id=""ensembling-and-dealing-with-outliers"">Ensembling and dealing with outliers</h3>
We ended up creating about 250 models throughout the competition. However, we knew that some of these models were not very robust to certain outliers or patients whose ROI we could not accurately detect. We came up with two different ensembling strategies that would deal with these kind of issues.

Our first ensembling technique followed the following steps:
<ol>
 	<li>For each patient, we select the best way to average over the test time augmentations. Slice models often preferred a geometric averaging of distributions, whereas in general arithmetic averaging worked better for patient models.</li>
 	<li>We average over the models by calculating each prediction’s KL-divergence from the average distribution, and the cross entropy of each single sample of the distribution. This means that models which are further away from the average distribution get more weight (since they are more certain). It also means samples of the distribution closer to the median-value of 0.5 get more weight. Each model also receives a model-specific weight, which is determined by optimizing these weights over the validation set.</li>
 	<li>Since not all models predict all patients, it is possible for a model in the ensemble to not predict a certain patient. In this case, a new ensemble without these models is optimized, especially for this single patient. The method to do this is described in step 2.</li>
 	<li>This ensemble is then used on every patient on the test-set. However, when a certain model’s average prediction disagrees too much with the average prediction of all models, the model is thrown out of the ensemble, and a new ensemble is optimized for this patient, as described in step 2. This meant that about ~75% of all patients received a new, ‘personalized’ ensemble.</li>
</ol>
Our second way of ensembling involves comparing an ensemble that is suboptimal, but robust to outliers, to an ensemble that is not robust to them. This approach is especially interesting, since it does not need a validation set to predict the test patients. It follows the following steps:
<ol>
 	<li>Again, for each patient, we select the best way to average over the test time augmentations again.</li>
 	<li>We combine the models by using a weighted average on the predictions, with the weights summing to one. These weights are determined by optimising them on the validation set. In case not all models provide a prediction for a certain patient, it is dropped for that patient and the weights of the other models are rescaled such that they again sum to one. This ensemble is not robust to outliers, since it contains patient models.</li>
 	<li>We combine all 2Ch, 4Ch and slice models in a similar fashion. This ensemble is robust to outliers, but only contains less accurate models.</li>
 	<li>We detect outliers by finding the patients where the two ensembles disagree the most. We measure disagreement using CRPS. If the CRPS exceeds a certain threshold for a patient, we assume it to be an outlier. We chose this threshold to be 0.02.</li>
 	<li>We retrain the weights for the first ensemble, but omit the outliers from the validation set. We choose this ensemble to generate predictions for most of the patients, but choose the robust ensemble for the outliers.</li>
</ol>
Following this approach, we detected three outliers in the test set during phase one of the competition. Closer inspection revealed that for all of them either our ROI detection failed, or the SAX slices were not nicely distributed across the heart. Both ways of ensembling achieved similar scores on the public leaderboard. (0.0110)
<h3 id=""second-round-submissions"">Second round submissions</h3>
For the second round of the competition, we were allowed to retrain our models on the new labels (+ 200 patients). We were also allowed to plan two submissions. Of course, it was impossible to retrain all of our models during this single week. For this reason, we chose to only train our 44 best models, according to our ensembling scripts.

For our first submission, we splitted of a new validation set. The resulting models were combined using our first ensembling strategy.

For our second submission, we trained our models on the entire training set (i.e. there was no validation split). We assembled them using the second ensembling method. Since we had no validation set to optimise the weights of the ensemble, we computed the weights by training an ensemble on the models we trained with a validation split, and transferred them over.
<h2 id=""software-and-hardwarea-namesofta"">Software and hardware<a name=""soft""></a></h2>
We used <a href=""http://lasagne.readthedocs.org/en/latest/index.html"">Lasagne</a>, <a href=""https://www.python.org/"">Python</a>, <a href=""http://www.numpy.org/"">Numpy</a> and <a href=""http://deeplearning.net/software/theano/"">Theano</a> to implement our solution, in combination with the <a href=""https://developer.nvidia.com/cudnn"">cuDNN</a> library. We also used <a href=""https://mathema.tician.de/software/pycuda/"">PyCUDA</a> for a few custom kernels. We made use of <a href=""http://scikit-image.org/"">scikit-image</a> for pre-processing and augmentation.

We trained our models on the NVIDIA GPUs that we have in the lab, which include GTX TITAN X, GTX 980, GTX 680 and Tesla K40 cards. We would like to thank <a href=""http://www.fredericgodin.com/"">Frederick Godin</a> and <a href=""https://twitter.com/sailenav"">Elias Vansteenkiste</a> for lending us a few extra GPUs in the last week of the competition.
<h2 id=""conclusiona-nameconclusiona"">Conclusion<a name=""conclusion""></a></h2>
In this competition, we tried out different ways to preprocess data and combine information from different data sources, and thus, we learned a lot in this aspect. However, we feel that there is still a room for improvement. For example, we observed that most of our error still hails from a select group of patients. These include the ones for which our ROI extraction fails. In hindsight, hand-labeling the training data and training a network to do the ROI extraction would be a better approach, but we wanted to sidestep doing a lot of this kind of manual effort as much as possible. In the end, labeling the data would probably have been less time intensive.

The code is now available on GitHub: <a href=""https://github.com/317070/kaggle-heart"">https://github.com/317070/kaggle-heart</a>

<hr />

This article was originally published on Ira Korshunova's <a href=""http://irakorshunova.github.io/2016/03/15/heart.html"" target=""_blank"">blog</a>.

&nbsp;"
"The Allen AI Science Challenge, Winner's Interview: 3rd place, Alejandro Mosquera",http://blog.kaggle.com/2016/04/09/the-allen-ai-science-challenge-winners-interview-3rd-place-alejandro-mosquera/,2016-04-09 19:48:39,"The <a href=""https://www.kaggle.com/c/the-allen-ai-science-challenge"" target=""_blank"">Allen Institute for Artificial Intelligence (AI2) competition</a> ran on Kaggle from October 2015 to February 2016. 170 teams with 302 players competed to pass 8th grade science exams with flying colors. Alejandro Mosquera took third place in the competition using a Logistic Regression  3-class classification model over Information Retrieval, neural network embeddings, and heuristic/statistical corpus features. In this blog, Alejandro describes his approach and the surprising conclusion that sometimes simpler models outperform ensemble methods.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
My PhD topic is related to social media text analysis but for the last 4 years I have been working in cyber-security analytics.

[caption id=""attachment_5700"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/AM_Kaggle.png"" rel=""attachment wp-att-5700""><img class=""size-full wp-image-5700"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/AM_Kaggle.png"" alt=""Alejandro on Kaggle"" width=""300"" height=""143"" /></a> <a href=""https://www.kaggle.com/x75a40890"" target=""_blank"">Alejandro</a> on Kaggle[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
Even though I have an NLP background I haven't had much exposure to QA tasks before. I think that being familiar with the right tools is always an advantage, especially when you deal with difficult problems like this one.
<h3>How did you get started competing on Kaggle?</h3>
Everything started with the Microsoft Malware Classification Challenge. I had quite a lot of domain knowledge so I've managed to stay in the top-20 for most of the competition with a simple Random Forest and around 25 features. However, at that time I wasn’t very familiar with Kaggle mechanics nor some of the usual methods that make you improve your score when you run out of ideas for new features, (e.g. stacking, blending…etc) and I ended up dropping quite a lot of positions in the private LB. Good learning experience though.
<h3>What made you decide to enter this competition?</h3>
I've missed most of the previous NLP-related competitions in Kaggle so I thought that was a good idea to participate in this one.
<h2>Let's Get Technical</h2>
<h3>Overview</h3>
The solution is based on a Logistic Regression (LR) 3-class classification model over Information Retrieval (IR), neural network embeddings (W2V) and heuristic/statistical corpus features. The rationale behind using 3 classes (Answer A, Answer B, Other) instead of 4 (one per answer) was to augment the training dataset and deal better with low confidence predictions.
<h2>Feature Selection / Extraction</h2>
<h3>What were the most important features?</h3>
I used 22 features in total (11 for each Question/Answer pair and then pairwise combinations between these for each question):
<ul>
 	<li><strong>ES_raw_lemma</strong>: IR scores by using ES and raw/lemmatized KB.</li>
 	<li><strong>ES_lemma_regex</strong>: Regex scoring (number of characters matched) after IR results.</li>
 	<li><strong>W2V_COSQA</strong>: Cosine similarity between question and answer embeddings.</li>
 	<li><strong>CAT_A1</strong>: Is multiphrase question + short response?</li>
 	<li><strong>CAT_A2</strong>: Is fill the _______ + no direct question + long response?</li>
 	<li><strong>CAT_A3</strong>: Is multiphrase question + long response?</li>
 	<li><strong>ANS_all_above</strong>: Is ""all of the above"" answer?</li>
 	<li><strong>ANS_none_above</strong>: Is ""none of the above"" answer?</li>
 	<li><strong>ANS_both</strong>: Is ""both X and Y"" answer?</li>
 	<li><strong>ES_raw_lemmaLMJM</strong>: IR scores by using ES and raw/lemmatized KB with LMJM scoring.</li>
 	<li><strong>ES_lemma_regexLMJM</strong>: Regex scoring (number of characters matched) after IR results using LMJM.</li>
</ul>
The scaled LR coefficients show that both corpus heuristics and embedding-based features have the highest weight, followed by combined IR scores.

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/features.png"" rel=""attachment wp-att-5701""><img class=""aligncenter size-full wp-image-5701"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/features.png"" alt=""features"" width=""387"" height=""400"" /></a>
<h3>How did you select features?</h3>
The best features were selected by using cross-validation and also manual analysis of the training data.
<h3>Did you make any important feature transformations?</h3>
My data pipeline was the following:
<ul>
 	<li>Noise reduction (question boilerplate removal)</li>
 	<li>Stopword removal (standard English NLTK stopwords)</li>
 	<li>Lemmatization (NLTK Wordnet Lemmatizer)</li>
</ul>
<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/feature_processing-higherres.png"" rel=""attachment wp-att-5716""><img class=""aligncenter size-full wp-image-5716"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/feature_processing-higherres.png"" alt=""ai3_codesnippet"" width=""625"" height=""173"" /></a>
<h3>Did you find any interesting interactions between features?</h3>
I've found a huge performance increase when using a 3-class classification approach, rather than using a 2-class or 4-class one. This third class was used to distribute the probabilities over the remaining two questions.
<h3>Did you use external data?</h3>
The KB was indexed in ES and was automatically generated by querying the <a href=""https://quizlet.com/api/2.0/docs"" target=""_blank"">Quizlet API</a> for a fixed set of science topic keywords. I've also added the CK-12 workbooks published in the forum. This was a one-off process so no information from the training or testing set was used in order to build the KB. The machine learning model is fully standalone and does not require any additional external data in order to generate predictions.

I've also used the following multi-state question/answer datasets in order to augment the original training data:
<ul>
 	<li>Regents 4 grade QA dataset (from Aristo website)</li>
 	<li>Multi-State QA dataset (from Aristo website)</li>
</ul>
<h2>Training Methods</h2>
<h3>What training methods did you use?</h3>
<strong>Unsupervised</strong>: Gensim's Word2Vec in order to extract embeddings from the KB.

<strong>Supervised</strong>: 3-class  Logistic Regression over IR, W2V and heuristic/statistical corpus features. Local CV scores (5-fold) were around 0.59-0.61 depending on the feature set.
<h3>Did you ensemble the models?</h3>
I've tried ensembling XGBoost and RNN models but nothing could beat the simpler linear model.
<h2>Interesting findings</h2>
<h3>What was the most important trick you used?</h3>
I jumped to the first position after treating the QA challenge as a question-answer pair ranking problem. I've also expanded the Word2Vec model vocabulary by using fuzzy string matching. It helped me to improve the score during the first stage of the competition.
<h3>What do you think set you apart from others in the competition?</h3>
I think my knowledge base was quite good in terms of coverage. I've found Wikipedia IR models to be broader but also noisier.
<h3>Was there anything you tried that didn't improve your score?</h3>
I couldn't get anything useful out of state-of-the-art RNNs or CNNs. I've also tried to use resources such as ConceptNet or WordNet in order to augment my KB or improve the quality of the embeddings. However, none of these experiments gave me any substantial improvement.
<h3>What was your most important insight into the data?</h3>
I've found that questions with short answers (less than 4 words) were positively sensitive to n-gram matching (more than 0.7 accuracy by using IR only) while the longer ones required combined knowledge (inference) to be solved correctly.

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/qa_categorization.png"" rel=""attachment wp-att-5703""><img class=""aligncenter wp-image-5703"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/qa_categorization-300x112.png"" alt=""qa_categorization"" width=""500"" height=""187"" /></a>

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/classification_accuracy.png"" rel=""attachment wp-att-5705""><img class=""aligncenter wp-image-5705"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/classification_accuracy.png"" alt=""classification_accuracy"" width=""550"" height=""171"" /></a>

[caption id=""attachment_5704"" align=""aligncenter"" width=""650""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/category_distribution.png"" rel=""attachment wp-att-5704""><img class=""wp-image-5704"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/category_distribution.png"" alt=""Figure 1. Distribution of Q&amp;A categories in train and validation sets"" width=""650"" height=""333"" /></a> Distribution of Q&amp;A categories in train and validation sets[/caption]
<h3>Were you surprised by any of your findings?</h3>
I was expecting RNNs to perform better, especially taking into account the state-of-the-art in QA. During my experiments I couldn't make them outperform the simpler linear model.
<h3>Which tools did you use?</h3>
My tools of choice were Elasticsearch 2.0, Gensim and the Sklearn/Numpy stack.
<h3>How did you spend your time on this competition?</h3>
I spent the first weeks of the competition trying to build a relevant knowledge base and understanding how the features from the questions and the answers could be combined in the most optimal way. The last two weeks were mostly dedicated to model tuning and feature selection.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
The slowest part is the knowledge base generation and indexing in ElasticSearch. That takes around a couple of hours in a normal computer. Feature generation with the initial data (including the Word2Vec embeddings) needs another two hours for each dataset.
<h2>Words of wisdom</h2>
<h3>What have you taken away from this competition?</h3>
There was the possibility to submit only one model in this competition, I had two and I chose the one with higher CV score rather than the one with higher LB score (which turned out not being the best performing with the second-stage dataset). Cross-validation is helpful but not always conclusive.
<h2>Just for fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
I think that anything related to reinforcement learning would be interesting, like for example building a model able to play a video game or solve a computer-related task.
<h2>Bio</h2>
<strong>Alejandro Mosquera</strong> is a Sr. Principal Research Engineer at Symantec. When he is not taking down phishing and malware campaigns he works towards finishing his PhD in Human Language Technologies at the University of Alicante (Spain)."
"Yelp Restaurant Photo Classification, Winner's Interview: 1st Place, Dmitrii Tsybulevskii",http://blog.kaggle.com/2016/04/28/yelp-restaurant-photo-classification-winners-interview-1st-place-dmitrii-tsybulevskii/,2016-04-28 16:00:03,"The <a href=""https://www.kaggle.com/c/yelp-restaurant-photo-classification"" target=""_blank"">Yelp Restaurant Photo Classification</a> recruitment competition ran on Kaggle from December 2015 to April 2016. 355 Kagglers accepted Yelp's challenge to predict multiple attribute labels for restaurants based on user-submitted photos. Dmitrii Tsybulevskii took the cake by finishing in 1st place with his winning solution. In this blog, Dmitrii dishes the details of his approach including how he tackled the multi-label and multi-instance aspects of this problem which made this competition a unique challenge.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I hold a degree in Applied Mathematics, and I'm currently working as a software engineer on computer vision, information retrieval and machine learning projects.

[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/u1234x1234.png"" target=""_blank"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/u1234x1234.png"" alt=""Dmitrii on Kaggle"" width=""300"" height=""142"" /></a> <a href=""https://www.kaggle.com/u1234x1234"" target=""_blank"">Dmitrii</a> on Kaggle[/caption]
<h3 id=""do-you-have-any-prior-experience-or-domain-knowledge-that-helped-you-succeed-in-this-competition"">Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
Yes, since I work as a computer vision engineer, I have image classification experience, deep learning knowledge, and so on.
<h3 id=""how-did-you-get-started-competing-on-kaggle"">How did you get started competing on Kaggle?</h3>
At first I came to Kaggle through the MNIST competition, because I've had interest in image classification and then I was attracted to other kinds of ML problems and data science just blew up my mind.
<h3 id=""what-made-you-decide-to-enter-this-competition"">What made you decide to enter this competition?</h3>
There are several reasons behind it:
<ul>
	<li>I like competitions with raw data, without any anonymized features, and where you can apply a lot of feature engineering.</li>
	<li>Quite large dataset with a rare type of problem (multi-label, multi-instance). It was a good reason to get new knowledge.</li>
</ul>
<h2 id=""lets-get-technical"">Let's get technical:</h2>
<h3 id=""what-preprocessing-and-supervised-learning-methods-did-you-use"">What preprocessing and supervised learning methods did you use?</h3>
Outline of my approach depicted below:

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/pipeline.png"" target=""_blank"" rel=""attachment wp-att-5722""><img class=""aligncenter size-full wp-image-5722"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/pipeline.png"" alt=""pipeline"" width=""729"" height=""825"" /></a>
<h3 id=""photo-level-feature-extraction"">Photo-level feature extraction</h3>
One of the most important things you need for training deep neural networks is a clean dataset. So, after viewing the data, I decided not to train a neural network from scratch and not to do fine-tuning. I've tried several state-of-the-art neural networks and several layers from which features were obtained. Best performing (in decreasing order) nets were:
<ul>
	<li><a href=""https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-21k-inception.md"" target=""_blank"">Full ImageNet trained Inception-BN</a></li>
	<li><a href=""https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-v3.md"" target=""_blank"">Inception-V3</a></li>
	<li><a href=""https://github.com/facebook/fb.resnet.torch"" target=""_blank"">ResNet</a></li>
</ul>
The best features were obtained from the antepenultimate layer, because the last layer of pretrained nets are too ""overfitted"" to the ImageNet classes, and more low-level features can give you a better result. But in this case, dimensions of the features are much higher (50176 for the antepenultimate layer of ""Full ImageNet trained Inception-BN""), so I used <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html"" target=""_blank"">PCA compression</a> with ARPACK solver, in order to find only few principal components. In most cases feature normalization was used.

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/nn.png"" target=""_blank"" rel=""attachment wp-att-5721""><img class=""aligncenter size-full wp-image-5721"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/nn.png"" alt=""nn"" width=""404"" height=""578"" /></a>
<h3 id=""how-did-you-deal-with-the-multi-instance-aspect-of-this-problem"">How did you deal with the multi-instance aspect of this problem?</h3>
In this problem we only needed in the bag-level predictions, which makes it much simpler compared to the instance-level multi-instance learning. I used a paradigm which is called ""Embedded Space"", according to the paper: <a href=""http://158.109.8.37/files/Amo2013.pdf"" target=""_blank"">Multiple Instance Classification: review, taxonomy and comparative study</a>. In the Embedded Space paradigm, each bag X is mapped to a single feature vector which summarizes the relevant information about the whole bag X. After this transform you can use ordinary supervised classification methods.

For the business-level (bag-level) feature extraction I used:
<ul>
	<li>Averaging of photo-level features
Simple, but very efficient in the case of outputs of neural networks.</li>
	<li><a href=""http://www.vlfeat.org/api/fisher-fundamentals.html"" target=""_blank"">Fisher Vectors</a>
Fisher Vector was the best performing image classification method before ""Advent"" of deep learning in 2012. Usually FV was used as a global image descriptor obtained from a set of local image features (e.g. SIFT), but in this competition I used them as an aggregation of the set of photo-level features into the business-level feature. With Fisher Vectors you can take into account multi-instance nature of the problem.</li>
	<li><a href=""http://www.vlfeat.org/api/vlad-fundamentals.html"" target=""_blank"">VLAD descriptor</a>
Very similar to Fisher Vectors.</li>
</ul>
After some experimentation, I ended up with a set of the following business-level features:
<ol>
	<li>Averaging of L2 normalized features obtained from the penultimate layer of [Full ImageNet Inception-BN]</li>
	<li>Averaging of L2 normalized features obtained from the penultimate layer of [Inception-V3]</li>
	<li>Averaging of PCA projected features (from 50716 to 2048) obtained from the antepenultimate layer of [Full ImageNet Inception-BN]</li>
	<li>L2 normalized concatenation of 2., 3.</li>
	<li>PCA projected 4. to 128 components.</li>
	<li>Fisher Vectors over PCA projected 3. to 64 components.</li>
	<li>VLAD over PCA projected 3. to 64 components.</li>
</ol>
<h3 id=""how-did-you-deal-with-the-multi-label-aspect-of-this-problem"">How did you deal with the multi-label aspect of this problem?</h3>
I used <a href=""https://en.wikipedia.org/wiki/Multi-label_classification#Problem_transformation_methods"" target=""_blank"">Binary Relevance (BR)</a> and <a href=""https://www.google.ru/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwippqSiuJ7MAhVlKpoKHYPWAlcQFggiMAE&amp;url=http%3A%2F%2Fwww.cs.waikato.ac.nz%2Fml%2Fpublications%2F2009%2Fchains.pdf&amp;usg=AFQjCNFGk9RJTFDJ24kWKPf7sYdJ826wGA&amp;sig2=jevjCZBw8MUAiCk8MyhNzA&amp;bvm=bv.119745492,d.bGs"" target=""_blank"">Ensemble of Classifier Chains (ECC)</a> with binary classification methods in order to handle the multi-label aspect of the problem. But my best performing single model was the multi-output neural network with the following simple structure:

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/multioutput-nn.png"" target=""_blank"" rel=""attachment wp-att-5724""><img class=""aligncenter size-full wp-image-5724"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/multioutput-nn.png"" alt=""multioutput-nn"" width=""725"" height=""370"" /></a>

This network shares weights for the different label learning tasks, and performs better than several BR or ECC neural networks with binary outputs, because it takes into account the multi-label aspect of the problem.
<h3 id=""classification"">Classification</h3>
<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/classification.png"" target=""_blank"" rel=""attachment wp-att-5720""><img class=""aligncenter size-full wp-image-5720"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/classification.png"" alt=""classification"" width=""798"" height=""508"" /></a>

Neural network has much higher weight(6) compared to the LR(1) and XGB(1) at the weighing stage. After all, 0, 1 labels were obtained with a simple thresholding, and for all labels a threshold value was the same.
<h3 id=""what-didnt-work-for-you"">What didn't work for you?</h3>
<ul>
	<li>Label powerset for multi-label classification</li>
	<li><a href=""https://www.google.ru/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi-1M2QvZ7MAhVGOJoKHTtmDEYQFggiMAE&amp;url=http%3A%2F%2Flpis.csd.auth.gr%2Fpublications%2Ftsoumakas-tkde10.pdf&amp;usg=AFQjCNEsKsmG7bS0qC8fuKYjO240Dvdbrg&amp;sig2=ELl5blzYY6Oh34UuQi3Ikw&amp;bvm=bv.119745492,d.bGs"" target=""_blank"">RAkEL</a> for multi-label classification</li>
	<li>Variants of <a href=""http://www.sciencedirect.com/science/article/pii/S0031320307000027"" target=""_blank"">ML-KNN</a> for multi-label classification</li>
	<li>Removing duplicate photos</li>
	<li>XGBoost. I added some XGBoost models to the ensemble just out of respect to this great tool, although local CV score was lower.</li>
	<li><a href=""https://github.com/garydoranjr/misvm"" target=""_blank"">MISVM</a> for multi-instance classification</li>
	<li>More image crops in the feature extractor</li>
	<li>Stacking. It's pretty easy to overfit with a such small dataset, which has only 2000 samples.</li>
</ul>
<h3 id=""were-you-surprised-by-any-of-your-findings"">Were you surprised by any of your findings?</h3>
<ul>
	<li>Features extracted from the Inception-V3 had a better performance compared to the ResNet features. Not always better error rates on ImageNet led to the better performance in other tasks.</li>
	<li>Simple Logistic Regression outperforms almost all of the widely used models such as Random Forest, GBDT, SVM.</li>
	<li>Binary Relevance is a very good basiline for the multi-label classification.</li>
</ul>
<h3 id=""which-tools-did-you-use"">Which tools did you use?</h3>
<a href=""https://github.com/dmlc/mxnet/"" target=""_blank"">MXNet</a>, <a href=""https://github.com/scikit-learn/scikit-learn/"" target=""_blank"">scikit-learn</a>, <a href=""https://github.com/torch/torch7"" target=""_blank"">Torch</a>, <a href=""https://github.com/vlfeat/vlfeat"" target=""_blank"">VLFeat</a>, <a href=""https://github.com/Itseez/opencv"" target=""_blank"">OpenCV</a>, <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a>, <a href=""https://github.com/BVLC/caffe"" target=""_blank"">Caffe</a>
<h3 id=""how-did-you-spend-your-time-on-this-competition"">How did you spend your time on this competition?</h3>
50% feature engineering, 50% machine learning
<h3 id=""what-was-the-run-time-for-both-training-and-prediction-of-your-winning-solution"">What was the run time for both training and prediction of your winning solution?</h3>
Feature extraction: 10 hours
Model training: 2-3 hours
<h2 id=""words-of-wisdom"">Words of wisdom:</h2>
<h3 id=""what-have-you-taken-away-from-this-competition"">What have you taken away from this competition?</h3>
A ""Prize Winner"" badge and a lot of Kaggle points.
<h3 id=""do-you-have-any-advice-for-those-just-getting-started-in-data-science"">Do you have any advice for those just getting started in data science?</h3>
Kaggle is a great platform for getting new knowledge.
<h2 id=""just-for-fun"">Just for fun:</h2>
<h3 id=""if-you-could-run-a-kaggle-competition-what-problem-would-you-want-to-pose-to-other-kagglers"">If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
I'd like to see reinforcement learning or some kind of unsupervised learning problems on Kaggle.
<h3>Bio</h3>
<strong><a href=""https://www.linkedin.com/in/dmitrii-tsybulevskii-8a911a11b"" target=""_blank"">Dmitrii Tsybulevskii</a></strong> is a Software Engineer at a photo stock agency. He holds a degree in Applied Mathematics, and mainly focuses on machine learning, information retrieval and computer vision."
"Homesite Quote Conversion, Winners' Interview: 2nd Place, Team Frenchies | Nicolas, Florian, & Pierre",http://blog.kaggle.com/2016/05/02/homesite-quote-conversion-winners-interview-2nd-place-team-frenchies-nicolas-florian-pierre/,2016-05-02 16:05:55,"The <a href=""https://www.kaggle.com/c/homesite-quote-conversion"" target=""_blank"">Homesite Quote Conversion</a> competition challenged Kagglers to predict the customers most likely to purchase a quote for home insurance based on an anonymized database of information on customer and sales activity. 1925 players on 1764 teams competed for a spot at the top and team Frenchies found themselves in the money with their special blend of 600 base models. Nicolas, Florian, and Pierre describe how the already highly separable classes challenged them to work collaboratively to eke out improvements in performance through feature engineering, effective cross validation, and ensembling.
<h2>Frenchies</h2>
The team started off with Pierre and Florian as they are longtime friends. Nicolas asked to join later in the competition and it was one of the best decisions of this challenge! All of us were finalists in the “Cdiscount.com” competition hosted on <a href=""datascience.net"" target=""_blank"">datascience.net</a>, the “French Kaggle”. It was a real pleasure for all of us to work as French guys and to demonstrate our skill on an international contest.
<h3>Nicolas Gaude</h3>
[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/nicolas.png"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/nicolas.png"" alt=""Nicolas on Kaggle"" width=""300"" height=""142"" /></a> <a href=""https://www.kaggle.com/ngaude"" target=""_blank"">Nicolas</a> on Kaggle[/caption]

Working for Bouygues Telecom, a French telecom operator with 15M subscribers, I’m heading its data-science team with a focus on production efficiency and scalability. With a 10 years background in embedded software development, I moved to the big data domain 3 years ago and fell in love with machine learning. Kaggle is for me a unique opportunity to sharpen my skills and to compete with other data scientists around the world. And honestly Kaggle is the only place where a 0.0001% improvement matters so much that you can go for 100’s models ensemble to get to the top, and that’s a lot of fun.
<h3>Florian Laroumagne</h3>
[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/florian.png"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/florian.png"" alt=""Florian on Kaggle"" width=""300"" height=""142"" /></a> <a href=""https://www.kaggle.com/laroumagne"" target=""_blank"">Florian</a> on Kaggle[/caption]

Currently working as a BI Analyst at EDF (the major French and worldwide electricity provider) I graduated from the ENSIIE, a top French maths &amp; IT engineering school. I had some statistical and machine learning courses however I had no opportunity in my professional life to apply it. To improve my skills, I followed some MOOC (on “france-universite-numerique” and on “Coursera”) about statistics with R, big data and machine learning. After having acquired theoretical lessons, I wanted to put them into practice. This is how I ended up on Kaggle.
<h3>Pierre Nowak</h3>
[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/pierre.jpg"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/pierre.jpg"" alt=""Pierre on Kaggle"" width=""300"" height=""143"" /></a> <a href=""https://www.kaggle.com/pierrenowak"" target=""_blank"">Pierre</a> on Kaggle[/caption]

I graduated from ENSIIE &amp; Université d’Evry Val d’Essonne with a double degree in Financial Mathematics. My interest in machine learning came with my participation in a text mining challenge hosted by datascience.net. I have been working for 7 months at EDF R&amp;D first on text mining problems and recently changed to forecasting daily electricity load curves. Despite the fact many people say Kaggle is brute force only, I find it to be the place to learn brand new algorithms and techniques. I especially had the opportunity to learn Deep Learning with Keras and next level blending thanks to Nicolas and some public posts from Gilberto and the Mad Professors.
<h3>Background</h3>
None of us had prior background on the business of Homesite since we do not work in the same field. However, we weren’t hurt by this. We think the fact the data was anonymized brought most of the competitors approximately to the same level.

About the technologies used, there are two schools inside our team. Nicolas was pro-efficient in python while Florian was more R focused. Pierre was quite polyvalent and was the glue between the 2 worlds.
<h2>The solution</h2>
<h3>Feature engineering</h3>
We have to admit that feature engineering wasn’t very easy for us. Sure, we tried some differences between features which can then be selected (or not) via a feature selection process but at the end, we had only the basic dataset with a few engineered features. The kept ones were:
<ul>
 	<li>Count of 0, 1 and N/A row-wise</li>
 	<li>PCA top component features</li>
 	<li>TSNE 2D</li>
 	<li>Cluster ID generated with k-means</li>
 	<li>Some differences among features (especially the “golden” features found in a public script)</li>
</ul>
This challenge was really fun because even at the beginning of the competition, the AUC was really high (around 97% already). As we can see, the two classes are in fact quite easily separable:

[caption id=""attachment_5744"" align=""aligncenter"" width=""1024""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/pca.png""><img class=""size-large wp-image-5744"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/pca-1024x564.png"" alt=""Dataset plotted against the top 2 components of PCA (negative in green, positive in red)"" width=""1024"" height=""564"" /></a> Dataset plotted against the top 2 components of PCA (negative in green, positive in red)[/caption]

[caption id=""attachment_5747"" align=""aligncenter"" width=""1024""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/tsne.png""><img class=""size-large wp-image-5747"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/tsne-1024x564.png"" alt=""Dataset plotted against the top 2 dimensions of TSNE (negative in green, positive in red)"" width=""1024"" height=""564"" /></a> Dataset plotted against the top 2 dimensions of TSNE (negative in green, positive in red)[/caption]
<h3>Dataset modeling</h3>
As other teams, we encoded categorical features. Most of the time, it was done using a very common “label encoder”: all features where replaced with an ID. Despite the simplicity of this method, it works quite well for tree-based classifiers. However for linear ones it’s not recommended, that’s why we also generated “one hot encoded” features. Finally we also tried target encoding in order to find a ratio of the categorical features related to the target. It didn’t improve our score a lot but was worth having in our blend.

Now that we have different versions of the dataset, we also split it. We used a full version (all features, all rows) for the majority of our classifiers but we also trained weaker models based on a subset of columns. For example we trained a model on the “personal” columns only, another one on the “geographical” columns only and so on.
<h3>Training &amp; Ensembling</h3>
With all the different versions of the dataset, we were able to train them using well known and well performing machine learning models, such as:
<ul>
 	<li>Logistic Regression</li>
 	<li>Regularized Greedy Forest</li>
 	<li>Neural Networks</li>
 	<li>Extra Trees</li>
 	<li>XGBoost</li>
 	<li>H2O Random Forest (just 1 or 2 models into our first stage: not really important)</li>
</ul>
Our base level consists of around 600 models. 100 were built by “hand” with different features and hyperparameters of all of the above technologies. Then, to add some diversity we built a robot creating the 500 remaining models. This robot automatically trained models with XGBoost, Logistic Regression and Neural Networks, all based on randomly chosen features.

All our models were built on a 5 fold stratified CV. It allowed us to have a local way to check our improvement and to avoid overfitting the leaderboard. Furthermore, with the CV we were able to use an ensemble method.

Example of the diversity between two models, despite the fact they are highly correlated:

[caption id=""attachment_5746"" align=""aligncenter"" width=""484""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/RGFvXGB.png""><img class=""size-full wp-image-5746"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/RGFvXGB.png"" alt=""Predictions of an RGF model plotted against predictions of a XGB model"" width=""484"" height=""266"" /></a> Predictions of an RGF model plotted against predictions of a XGB model[/caption]

To blend our 600 models, we tried different ways. After some failures, we retained 3 well performing blenders: a classical Logistic Regression, an XGBoost and (a bag) of Neural Networks. Those three blends naturally outperformed our best single model and were able to capture the information in different manners. Then, we transformed our predictions into ranks and we simply averaged the ranks of the 3 blends to have our final submission.

Here is a sketch that sums up this multi-level stacking:

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/ensemble.png""><img class=""aligncenter size-full wp-image-5740"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/ensemble.png"" alt=""Model ensemble"" width=""480"" height=""360"" /></a>
<h2>Words of wisdom</h2>
Here is a short list of what we learnt and / or what worked for us:
<ul>
 	<li>Read forums, there are lots of useful insights</li>
 	<li>Use the best script as a benchmark</li>
 	<li>Don’t be afraid to generate lot of models and keep all the data created this way. You could still select them later in order to blend them… Or let the blender take the decision for you :)</li>
 	<li>Validate the behavior of your CV. If you have a huge jump in local score that doesn’t reflect on the leaderboard there is something wrong</li>
 	<li>Grid search in order to find hyperparameters works fine</li>
 	<li>Blending is a powerful tool. Please read the following post if you haven't have already: <a href=""http://mlwave.com/kaggle-ensembling-guide/"" target=""_blank"">http://mlwave.com/kaggle-ensembling-guide/</a></li>
 	<li>Be aware of the standard deviation when blending. Increasing the metric is good but not that much if the SD increases</li>
 	<li>Neural nets are capricious :( bag them if necessary</li>
 	<li>Merging is totally fine and helps each teammates to learn from others</li>
 	<li>If you are a team, use collaborative tools: Skype, Slack, svn, git...</li>
</ul>
<h3>Recommendations to those just starting out</h3>
As written just above, read the forums. There are nice tips, starter codes or even links to great readings. Don’t hesitate to download a dataset and test lot of things on it. Even if most of the tested methods fail or give poor results, you will acquire some knowledge about what is working and what is not.

Merge! Seriously, learning from others is what makes you stronger. We all have different insights, backgrounds or techniques which can be beneficial for your teammates. Last but not least, do not hesitate to discuss your ideas. This way you can find some golden thoughts that can push you to the top!"
"Yelp Restaurant Photo Classification, Winner's Interview: 2nd Place, Thuyen Ngo",http://blog.kaggle.com/2016/05/04/yelp-restaurant-photo-classification-winners-interview-2rd-place-thuyen-ngo/,2016-05-04 16:00:50,"The <a href=""https://www.kaggle.com/c/yelp-restaurant-photo-classification"" target=""_blank"">Yelp Restaurant Photo Classification</a> competition challenged Kagglers to assign attribute labels to restaurants based on a collection of user-submitted photos. In this recruitment competition, 355 players tackled the unique multi-instance and multi-label problem and in this blog the 2nd place winner describes his strategy. His advice to aspiring data scientists is clear: just do it and you will improve. Read on to find out how Thuyen Ngo dodged overfitting with his solution and why it doesn't take an expert in computer vision to work with image data.
<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I am a PhD student in Electrical and Computer Engineering at UC Santa Barbara. I am doing research in human vision and computer vision. In a nutshell I try to understand how humans explore the scene and apply that knowledge to computer vision systems.

[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/plankton.png"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/plankton.png"" alt=""Thuyen (AKA Plankton) on Kaggle"" width=""300"" height=""143"" /></a> <a href=""https://www.kaggle.com/thuyenvn"" target=""_blank"">Thuyen</a> (AKA Plankton) on Kaggle[/caption]
<h3>How did you get started competing on Kaggle?</h3>
My labmate introduced Kaggle to me about a year ago and I participated in several competitions since then.
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
All of my projects involved images. So It's fair to say that I have some ""domain knowledge"". However, for this competition I think knowledge from image processing is not as important as machine learning. Since everyone uses similar image features (from pre-trained convolutional neural networks, which have been shown to contain good global descriptions of images and therefore are very suitable to our problem), the difficult part is to choose a learning framework that can combine information from different instances in an effective way.
<h3>What made you decide to enter this competition?</h3>
I am interested in image data in general (even though in the end there was not much image analysis involved). The problem is very interesting itself since there's no black-box solution that can be applied directly.
<h2>Let's get technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
Like most participants, I used pre-trained convolutional networks to extract image features, I didn't do any other preprocessing or network fine-tuning. I started with the inception-v3 network from google but ended up using the pre-trained resnet-152 provided by Facebook.

My approach is super simple. It's just a neural network.
<h3>How did you deal with the multi-instance and multi-label aspect of this problem?</h3>
I used multilayer perceptron since it gave me the flexibility to handle both the multiple label and multiple instance at the same time. For multiple label, I simply used 9 sigmoid units and for multiple instance, I employed something like the attention mechanism in the neural network literature. The idea is to let the network learn by itself how to combine information from many instances (which instance to look at).

Each business is represented by a matrix of size N x 2048, where N is the number of images for that business. The network is composed of four fully connected (FC) layers. Attention model is a normal FC layer but the activation is a softmax over images, weighting the importance of each image for a particular feature. I experimented with many different architectures, in the end, the typical architecture for the final submission is as follows:

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/04/yelp-444x.png""><img class=""aligncenter size-full wp-image-5738"" src=""http://blog.kaggle.com/wp-content/uploads/2016/04/yelp-444x.png"" alt=""yelp-444x"" width=""444"" height=""541"" /></a>

The model is trained using the business-level labels (each business is a training example) as opposed to image-level labels (like many others). I used the standard cross entropy as the loss function. The training is done with Nesterov's accelerated SGD.
<h3>What was your most important insight into the data?</h3>
Finding the reliable local validation is quite challenging to me. It's a multiple label problem, and thus there's no standard stratified split. I tried some greedy methods to do stratified 5-fold split but it didn't perform very well. At the end I resorted to a random 5-fold split. My submission is normally the average of 5 models from 5-fold validation.

Another problem is that we only have 2000 businesses for training and another 2000 test cases. Even though it sounds a lot of data, training signals (the labels) are not that many. In combination with the instability of F measure, it makes the validation even more difficult.

Since the evaluation metric is F1 score, it is reasonable to use F-measure as the loss function, but somehow I couldn't make it work as well as the cross entropy loss.

With limited labeled data, my approach would have badly overfitted the data (it has more than 2M parameters). I used dropout for almost all layers, applied L2 regularization and early stopping to mitigate overfitting.
<h3>How did you spend your time on this competition?</h3>
Most of the time for machine learning (training), 1% for preprocessing I guess.
<h3>Which tools did you use?</h3>
I used Tensorflow/torch for feature extraction (with the provided code from Google/Facebook) and Lasagne (Theano) for my training.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
It takes about two and a half hours to train one model and about a minute to make the predictions for all test images.
<h2>Words of wisdom</h2>
<h3>What have you taken away from this competition?</h3>
Neural networks can do any (weird) thing :)
<h3>Do you have any advice for those just getting started in data science?</h3>
Just do it, join Kaggle, participate and you will improve.
<h2>Bio</h2>
<strong>Thuyen Ngo</strong> is a PhD student in Electrical and Computer Engineering at the University of California, Santa Barbara."
"March Machine Learning Mania 2016, Winner's Interview: 1st Place, Miguel Alomar",http://blog.kaggle.com/2016/05/10/march-machine-learning-mania-2016-winners-interview-1st-place-miguel-alomar/,2016-05-10 16:04:59,"The annual <a href=""https://www.kaggle.com/c/march-machine-learning-mania-2016"" target=""_blank"">March Machine Learning Mania</a> competition sponsored by <a href=""https://www.kaggle.com/c/march-machine-learning-mania-2016/details/about-the-sponsor"" target=""_blank"">SAP</a> challenged Kagglers to predict the outcomes of every possible match-up in the 2016 men's NCAA basketball tournament. Nearly 600 teams competed, but only the first place forecasts were robust enough against upsets to top this year's bracket. In this blog post, Miguel Alomar describes how calculating the offensive and defensive efficiency played into his winning strategy.

<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
I earned a Master’s Degree in Computer Science from UIB in Mallorca, Spain. For nearly 20 years, I have been involved in software development, business intelligence and data warehousing. Recently, I have developed an interest in analytics and forecasting.

[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/mallorqui.png"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/mallorqui.png"" alt=""Miguel (AKA Mallorqui) on Kaggle"" width=""300"" height=""142"" /></a> <a href=""https://www.kaggle.com/usmallorqui"" target=""_blank"">Miguel</a> (AKA Mallorqui) on Kaggle[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
In Spain, I played amateur basketball for 10 years. I like to think that is the reason I won.

The truth is I missed most of the basketball games this season and did not have a good feel for the any of the team’s quality. That most likely helped me because if I had seen more games, my judgment may have changed some of the forecasts. Normally, I am pretty bad at picking winners.
<h3>How did you get started competing on Kaggle?</h3>
I found Kaggle through some data science lessons I was taking on <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera</a>.
<h3>What made you decide to enter this competition?</h3>
I really like analytics and sports so I thought it was a perfect competition for me.

But the key factor is that moderators and other members make it easy to enter, they provide lots of help, data, advice and feedback. Data is already formatted and prepared so the data gathering and manipulating task is made very easy. Some members of the community seemed more interested in sharing and discovering new methods and insights than in winning the competition.
<h2>Let's get technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
I used logarithmic regression and random forests. I did try ADA Boost but didn’t get very good results so I didn’t use it in my final model.
<h3>What was your most important insight into the data?</h3>
The data behind this competition is very simple, the box stats from basketball games are very simple to understand. The key factor for me was the offensive and defensive efficiency, how to calculate those? What weight to give to strength of schedule? Can you ""penalize"" a team because they haven’t played against the best teams in the nation? Can you lower their rating for something that didn’t happen?

Those are the kind of questions I was trying to answer, I developed several models with different degrees of adjusted efficiency ratings and checked their scores against past seasons.

Since my scores in Stage1 of the competition were not very good, I kept changing my model after Stage1 was closed.

My goal for next year is to formally test those different models to find out if there is any validity to my ideas.

<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/first_round_upsets.jpg"" target=""_blank""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/05/first_round_upsets-1024x626.jpg"" alt=""first_round_upsets"" width=""1024"" height=""626"" class=""aligncenter size-large wp-image-5766"" /></a>

<h3>Were you surprised by any of your findings?</h3>
After building the submission files, I put them into brackets using a script provided by one of the Kaggle members. My first model had a more conservative look to it and my second model (the final winner) just didn’t look right to me. Teams like SF Austin, Indiana and Gonzaga were predicted to go very far in the bracket. I almost scrapped it but since it was my 2nd model I decided to go with it. This model got most of the first round upsets right, that surprised me.

[caption id=""attachment_5765"" align=""aligncenter"" width=""1024""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/Bracket_manual2.jpg"" target=""_blank""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/05/Bracket_manual2-1024x791.jpg"" alt=""NCAA bracket"" width=""1024"" height=""791"" class=""size-large wp-image-5765"" /></a> Click to expand.[/caption]

<h3>Which tools did you use?</h3>
I used R, R studio and SQL.
<h3>How did you spend your time on this competition?</h3>
I would say my time allocation was 35% reading forums and blogs, 15% manipulating data, 25% building models and 25% evaluating results.
<h3>What was the run time for both training and prediction of your winning solution?</h3>
Five minutes. I trained my model using only 2016 data, so the amount of data to process is very small.
<h2>Bio</h2>
<strong>Miguel Alomar</strong> has a Master’s Degree in Computer Science from UIB in Mallorca, Spain. For nearly 20 years, he has been involved in software development, business intelligence and data warehousing."
"BNP Paribas Cardif Claims Management, Winners' Interview: 1st Place, Team Dexter's Lab | Darius, Davut, & Song",http://blog.kaggle.com/2016/05/13/bnp-paribas-cardif-claims-management-winners-interview-1st-place-team-dexters-lab-darius-davut-song/,2016-05-13 17:30:00,"The <a href=""https://www.kaggle.com/c/bnp-paribas-cardif-claims-management"" target=""_blank"">BNP Paribas Claims Management</a> competition ran on Kaggle from February to April 2016. Just under 3000 teams made up of over 3000 Kagglers competed to predict insurance claims categories based on data collected during the claim filing process. The anonymized dataset challenged competitors to dig deeply into data understanding and feature engineering and the keen approach taken by Team Dexter's Lab claimed first place.

<h2>The basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
<strong>Darius:</strong> BSc and MSc in Econometrics at Vilnius University (Lithuania). Currently work as an analyst at a local credit bureau, Creditinfo. My work mainly involves analyzing and making predictive models with business and consumer credit data for financial sector companies.

[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/raddar.png"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/raddar.png"" alt=""Darius (AKA raddar) on Kaggle"" width=""300"" height=""141"" /></a> <a href=""https://www.kaggle.com/raddar"" target=""_blank"">Darius</a> (AKA raddar) on Kaggle[/caption]

<strong>Davut:</strong> BSc and MSc in Computer Engineering and Electronics Engineering (Double Major), and currently PhD student in Computer Engineering at Istanbul Technical University (Turkey). I work as a back-end service software developer at company which provides stock exchange data to users. My work is not related to any data science subjects but I work on Kaggle when I get any spare time. I live in Istanbul - traffic is a headache here - I spend almost 4-5 hours in traffic and during my commute I code for Kaggle :)

[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/davut.png"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/davut.png"" alt=""Davut on Kaggle"" width=""300"" height=""143"" /></a> <a href=""https://www.kaggle.com/davutpolat"" target=""_blank"">Davut</a> on Kaggle[/caption]

<strong>Song:</strong> Two masters (Geological Engineering and Applied Statistics). Currently I am working in an insurance company. My work is mainly building models - pricing models for insurance products, fraud detection, etc.

[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/song.png"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/song.png"" alt=""Song (AKA onthehilluu) on Kaggle"" width=""300"" height=""143"" /></a> <a href=""https://www.kaggle.com/newtohere"" target=""_blank"">Song</a> (AKA onthehilluu) on Kaggle[/caption]
<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
<strong>Davut:</strong> Song has lots of experience in the insurance field and Darius in the finance field. At first, we were stuck with mediocre results for 2-3 weeks until Darius came up with a great idea which we then had nice discussions about. Both perspectives helped us improve our score and lead us to the victory.
<h3>How did you get started competing on Kaggle?</h3>
<strong>Darius:</strong> I've heard of Kaggle few years ago, but just recently started Kaggling. I was looking for challenges and working with different types of data. Surprisingly, my data insights and feature engineering was good enough to claim prize money in my very first serious competition. Kaggle has become my favorite hobby since.

<strong>Davut:</strong> Three years ago, I took a course during my Master's degree when a professor gave us a term project from Kaggle (Adzuna Job Salary Prediction). I did not participate then but 6 months passed and I started to Kaggle. The Higgs Boson Machine Learning Challenge was my first serious competition and since then I've participated in more competitions and met great data scientists and friends.

<strong>Song:</strong> I have been studying machine learning by myself. Kaggle is an excellent site to learn by doing and learn from each other.
<h3>What made you decide to enter this competition?</h3>
<strong>Darius:</strong> I like working with anonymous data and I thought that I had an edge over the competition as I had discovered interesting insights in previous competitions as well. And Davut wanted to team up in a previous competition, so we joined forces early in the competition.

<strong>Davut:</strong> Kaggle became kind of an addiction to me like many others :) After Prudential, I wanted to participate in one more.

<strong>Song:</strong> Nothing special.
<h2>Let's get technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
<strong>Darius:</strong> The most important part was setting a stratified 10-fold CV scheme early on. For most of the competition, a single XGBoost was my benchmark model (in the end, the single model would have scored 4th place). In the last 2 weeks, I made a few diverse models such as rgf, lasso, elastic net, and SVM.

<strong>Davut:</strong> We got different feature sets, and trained various diverse models on those such as knn, extra tree classifiers, random forest, and neural networks. We also tried different objectives in XGBoost.

<strong>Song:</strong> In our final model, we had XGBoost as an ensemble model, which included 20 XGBoost models, 5 random forests, 6 randomized decision tree models, 3 regularized greedy forests, 3 logistic regression models, 5 ANN models, 3 elastic net models and 1 SVM model.
<h3>What was your most important insight into the data?</h3>
<strong>Darius:</strong> The most important insight was understanding what kind of data we were given. It is hard to make assumptions about anonymous data, but I dedicated 3 weeks of competition time for data exploration, which paid its dividends.

First, as every feature in the given dataset was scaled and had some random noise introduced, I figured that identifying how to deal with noise and un-scaling the data could be important. I thought of a simple but fast method to detect the scaling factor for integer type features. It took some time, but in the end it was a crucial part of our winning solution.

Second, given our assumptions about variable meanings, we built efficient feature interactions. We devised, among other ideas, a lag and lead feature based on our impression that we were dealing with panel data. In the end, our assumptions about panel data and variable meaning were not realistic (indeed it would mean that the same client could face hundreds or thousands of claims). However, our lag and lead features did bring significant value to our solution, which is certainly because it was an efficient way to encode interactions. This is consistent with the other top two teams' solutions, which also benefited from encoding some interactions between v22 and other variables with different methods aside from lag and lead. In our opinion, there is certainly very interesting business insight for the host in these features.

<h3>Were you surprised by any of your findings?</h3>
<strong>Darius:</strong> To my surprise, our approach was not overfitting at all. Other than that, I believed in our assumptions (be they correct or not) and we figured that other teams were just doing approximations of our findings - which other top teams admitted.

<h3>How did you spend your time on this competition?</h3>
Some Kagglers start to train models in first place and keep doing that till the end and only focus on ensembling. But we focused on how to improve a single model with new features. We spent 45 days on feature engineering, then rest of the time for model training and stacking.

<h3>What was the run time for both training and prediction of your winning solution?</h3>
Our single best model only takes less than an hour to train on an 8-12 core machine. However, the ensemble itself takes several days to finish.
<h2>Words of wisdom</h2>
<h3>What have you taken away from this competition?</h3>
<strong>Darius:</strong> I tried new XGBoost parameters, which I have not tried before which also proved to be helpful in this competition. Also created my own <a href=""http://stat.rutgers.edu/home/tzhang/software/rgf/"" target=""_blank"">R wrapper</a> for rgf. Also got noticed by top Kagglers, which I did not think would happen so soon.

<strong>Davut:</strong> The team play was amazing, and we had so much fun during the competition, tried so many crazy ideas which failed mostly but still it was really fun :)

<strong>Song:</strong> Keep learning endlessly. Taking a competition in a team is really a happy journey.
<h3>Do you have any advice for those just getting started in data science?</h3>
<strong>Darius:</strong> Be prepared to work hard as good results don't come easy. Make a list of what you want to get good at first and prioritize. Don't let XGBoost be the only tool in your toolbox.

<strong>Davut:</strong> Spend sufficient time on feature engineering, study the previous competitions' solutions, no matter how much time has passed. For example, our winning approach is so similar to Josef Feigl's winner solution in <a href=""https://www.kaggle.com/c/loan-default-prediction"" target=""_blank"">Loan Default Prediction</a>.

<strong>Song:</strong> Keep learning.
<h2>Just for fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
<strong>Darius:</strong> As an econometrician, I love competitions which involve predicting future trends. I'd love to put ARIMA and other time series methods into action more often.

<strong>Davut:</strong> In the Higgs Boson Challenge, high-energy physicists and data scientists competed together. I liked the spirit then and remember Lubos Motl and his posts brought new aspects to the approaches. I would like to pose a multidisciplinary problem.

<strong>Song:</strong> Any problem balancing exploration and exploitation.
<h3>What is your dream job?</h3>
<strong>Darius:</strong> Making global impact to people's lives with data science projects.

<strong>Davut:</strong> Using data science for early diagnosis for severe diseases like cancer, heart attack, etc.

<strong>Song:</strong> Data Scientist.
<h2>Acknowledgments</h2>
We want to thank <a href=""http://blog.kaggle.com/2016/03/17/airbnb-new-user-bookings-winners-interview-2nd-place-keiichi-kuroyanagi-keiku/"" target=""_blank"">this Kaggle blog post</a>, which helped us greatly with shaking some of our prior beliefs about the data and helping with brainstorming new ideas."
"Home Depot Product Search Relevance, Winners' Interview: 1st Place | Alex, Andreas, & Nurlan",http://blog.kaggle.com/2016/05/18/home-depot-product-search-relevance-winners-interview-1st-place-alex-andreas-nurlan/,2016-05-18 16:30:11,"A total of 2,552 players on over 2,000 teams participated in the <a href=""https://www.kaggle.com/c/home-depot-product-search-relevance"" target=""_blank"">Home Depot Product Search Relevance</a> competition which ran on Kaggle from January to April 2016. Kagglers were challenged to predict the relevance between pairs of real customer queries and products. In this interview, the first place team describes their winning approach and how computing query centroids helped their solution overcome misspelled and ambiguous search terms.

<h2>The Basics</h2>
<h3>What was your background prior to entering this challenge?</h3>

<strong>Andreas:</strong> I have a PhD in Wireless Network Optimization using statistical and machine learning techniques. I worked for 3.5 years as Senior Data Scientist at AGT International applying machine learning in different types of problems (remote sensing, data fusion, anomaly detection) and I hold an IEEE Certificate of Appreciation for winning first place in a prestigious IEEE contest. I am currently Senior Data Scientist at Zalando SE.

[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/andreas.png"" target=""_blank"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/andreas.png"" alt=""Andreas on Kaggle"" width=""300"" height=""141"" /></a> <a href=""https://www.kaggle.com/andreasmerentitis"" target=""_blank"">Andreas on Kaggle</a>[/caption]

<strong>Alex:</strong> I have a PhD in computer science and work as data science consultant for companies in various industries. I have built models for e-commerce, smart home, smart city and manufacturing applications, but never worked on a search relevance problem.

[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/alexander.png"" target=""_blank"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/alexander.png"" alt=""Alex on Kaggle"" width=""300"" height=""141"" /></a> <a href=""https://www.kaggle.com/allexius"" target=""_blank"">Alex on Kaggle</a>[/caption]

<strong>Nurlan:</strong> I recently completed my PhD in biological sciences where I worked mainly with image data for drug screening and performed statistical analysis for gene function characterization. I have also experience in application of recommender system approaches for novel gene function predictions.

[caption id=""attachment_5597"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/nurlan.png"" target=""_blank"" rel=""attachment wp-att-5597""><img class=""size-medium wp-image-5597"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/nurlan.png"" alt=""Nurlan on Kaggle"" width=""300"" height=""144"" /></a> <a href=""https://www.kaggle.com/nurlan"" target=""_blank"">Nurlan on Kaggle</a>[/caption]

<h3>How did you get started competing on Kaggle?</h3>

<strong>Nurlan:</strong> The wide variety of competitions hosted on Kaggle motivated me to learn more about applications of machine learning across various industries.

<strong>Andreas:</strong> The opportunity to work with real-world datasets from various domains and also interact with a community of passionate and very smart people was a key driving factor. In terms of learning while having fun, it is hard to beat the Kaggle experience. Also, exactly because the problems are coming from the real world, there are always opportunities to apply what you learned in a different context, be it another dataset or a completely different application domain.

<strong>Alex:</strong> I was attracted by the variety of real world datasets hosted on Kaggle and the opportunity to learn new skills and meet other practitioners. I was a bit hesitant to join competitions in the beginning as I was not sure if I would be able to dedicate the time for it, but then never regretted to get started. The leaderboard, the knowledge exchange in forums and working in teams creates a very exciting and enjoyable experience, and I was often able to transfer knowledge gained on Kaggle to customer problems in my day job.

<h3>What made you decide to enter this competition?</h3>

<strong>Alex:</strong> Before Home Depot, I participated in several competitions with anonymized datasets where feature engineering was very difficult or didn’t work at all. I like the creative aspect of feature engineering and I expected a lot of potential for feature engineering in this competition. Also I saw a chance to improve my text mining skills on a very tangible dataset.

<strong>Nurlan:</strong> I had two goals in this competition: mastering state of the art methods in natural language processing and model ensembling techniques. Teaming up with experienced kagglers and kaggle community through forums provided opportunities to achieve my goals.

<strong>Andreas:</strong> Learning more about both feature engineering and ML models that are doing well in NLP was a first driver. The decent but not overwhelming amount of data gave also good opportunities for ensembling and trying to squeeze the most out of the models, something that I enjoy doing when there are no inherent time or other business constraints (as is often the case in commercial data science applications).

<h2>Let’s get technical</h2>

<h3>What preprocessing and supervised learning methods did you use?</h3>

[caption id=""attachment_5785"" align=""aligncenter"" width=""1024""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/model.png"" target=""_blank""><img class=""size-large wp-image-5785"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/model-1024x768.png"" target=""_blank"" alt=""Overview of our prediction pipeline"" width=""1024"" height=""768"" /></a> Figure 1: Overview of our prediction pipeline - most important features and models highlighted in orange.[/caption]

<strong>Preprocessing and Feature Engineering</strong>

Our preprocessing and feature engineering approach can be grouped into five categories: keyword match, semantic match, entity recognition, vocabulary expansion and aggregate features.

<strong>Keyword Match</strong>

In keyword match we counted the number of matching terms between search term and different sections of product information and also stored the matching term position. To overcome the misspellings we used fuzzy match where we counted the character n-grams matches instead of complete term. We also computed tf-idf normalized scores of the matching terms to normalize for the non-specific term matches.

<strong>Semantic Match</strong>

[caption id=""attachment_5787"" align=""aligncenter"" width=""1024""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/word-embeddings.png"" target=""_blank""><img class=""size-large wp-image-5787"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/word-embeddings-1024x608.png"" alt=""Visualization of word embedding vectors trained on product descriptions and titles"" width=""1024"" height=""608"" /></a> Figure 2: Visualization of word embedding vectors trained on product descriptions and titles - related words cluster in word embedding space (2D projection using multi-dimensional scaling on cosine distance matrix, k-means clustering).[/caption]

To capture the semantic similarity (e.g. shower vs bathroom) we performed matrix decomposition using latent semantic analysis (LSA) and non-negative matrix factorization (NMF). To further catch the similarities that were not captured with LSA or NMF, which were trained on Home Depot corpus, we used pre-trained word2vec and GloVe word embeddings that are trained on various external corpora. Among LSA, NMF, GloVe and word2vec, GloVe word embeddings gave the best performance. See in figure 2 how it captures similar entities.

<strong>Main Entity Extraction</strong>

The main motivation was to extract main entities being searched and being described in the queries and product titles respectively. Our primary approach was to include positional information of the matched terms but oob error analysis revealed that it was not enough. We also experimented with POS tagging but we noticed that many of the terms that represent entity attributes and specifications were also captured as nouns and there was no obvious pattern to distinguish them from the the main entity terms. Instead, we decided to extract last N terms as potential main entities after reversing the order of the terms whenever we see prepositions such as ""for"", ""with"", ""in"", etc., which were usually followed by entity attributes/specifications.

<strong>Vocabulary Expansion</strong>

To catch 'pet' vs 'dog' type of relationships we performed vocabulary expansion for main entities extracted from the search terms and product titles. Vocabulary expansion included synonym, hyponym and hypernym extraction from WordNet.

<strong>Aggregate Features</strong>

See “What was your most important insight into the data?” section for details.

<strong>Feature Interactions</strong>

We also performed basis expansions by including polynomial interaction terms between important features. These features also contributed further to the performance of our final model.

<strong>Supervised Learning Methods</strong>

Apart from the usual suspects like xgboost, random forest, extra trees and neural nets, we worked quite a lot with combinations of unsupervised feature transformations and generalized linear models, especially sparse random and Gaussian projections as well as Random Tree Embeddings (which did really good). On the supervised part, we tried a large number of Generalized Linear Models using the different feature transformations and different loss functions. Bayesian Ridge and Lasso with some of the transformed features did really well, the first also getting almost no hyperparameter tuning (and thus saving time). Another thing that worked really good was the regression through classification approach based on Extra Tree Classifiers. Selecting the optimal number of classes and tweaking the model to get reliable posterior probability estimates was important and took computational effort but it contributed some of the best models (just next to the very best xgboost models).

The idea was always to get models that are individually good on their own but have as little correlation as possible so that they can contribute meaningfully in the ensemble. The feature transformations, different loss functions, regression through classification, etc. all played well in this general goal.

[caption id=""attachment_5788"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/image03.png"" target=""_blank""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/05/image03.png"" alt=""Comparison of unsupervised random tree embedding and supervised classification in separating the relevant and non-relevant points (2D projections)."" width=""802"" height=""967"" class=""size-full wp-image-5788"" /></a> Figure 3. Comparison of unsupervised random tree embedding and supervised classification in separating the relevant and non-relevant points (2D projections).[/caption]

The two figures above are showing the effectiveness of the unsupervised Random Tree Embedding transform (upper of the two pictures). The separation visualized here is between two classes only (highly relevant points tend to be high on the left and not relevant low and towards the right) and it is mingled. But we need to consider that this is a 2D projection done in a completely unsupervised way (the classes are actually visualized on top of the data and the labels were not used for anything other than visualization). For comparison, the other image (bottom picture) visualizes the posterior estimates for the two classes derived from a supervised Extra Tree classification algorithm (again the highly relevant area is up and to the left, while the non-relevant bottom right).

<h3>How did you settle on a strong cross-validation strategy?</h3>

<strong>Alex:</strong> I think everyone joining the competition realized very early that a simple cross-validation does not properly reflect the generalization error on the test set. The amount of search terms and products only present in the test set biased the cross-validation error and lead to overfitted models. To avoid that, I tried first to generate cross-validation folds that account for both unseen search terms and products simultaneously, but I was not able to come up with a sampling strategy that meets these requirements. I finally got the idea to “ensemble” multiple sampling schemes and it turned out to work very well. We created two runs of 3-fold cross-validation with disjoint search terms among the folds, and one 3-fold cross-validation with disjoint product id sets. Taking the average error of the three runs turned out to be a very good predictor for the public and private leaderboard score.

<h3>What was your most important insight into the data?</h3>

[caption id=""attachment_5789"" align=""aligncenter"" width=""697""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/image01.png"" target=""_blank""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/05/image01.png"" alt=""Information extraction about relevance of products to the query and quantification of query ambiguity by aggregating the products retrieved for each query."" width=""697"" height=""322"" class=""size-full wp-image-5789"" /></a> Figure 4. Information extraction about relevance of products to the query and quantification of query ambiguity by aggregating the products retrieved for each query.[/caption]

In the beginning we were measuring search term to product similarity by different means, but search terms were quite noisy (i.e. misspellings). Since most of the products retrieved are relevant, we clustered products for each query, then computed cluster centroid and used this centroid as a reference. Calculating similarity of the products to the query centroid provided powerful information (See figure above, left panel).

On top of this, some queries are ambiguous (e.g. ‘manual’ as opposed to ‘window lock’) and these ambiguous terms would be unclear for the human raters too and might lead to less relevant score. We decided to include this information as well by computing the mean similarity of the products to the query centroid for each query. Figure above (right panel) shows this relationship.

<h3>Were you surprised by any of your findings?</h3>

<strong>Andreas:</strong> One surprising finding was that the residual errors of our predictions were exhibiting a strange pattern (different behavior in the last few tens of thousands of records), that hinted towards a bias somewhere in the process. After discussing it, we thought that a plausible explanation was a change of annotators or change in the annotations policy. We decided to model this by adding a binary variable (instead of including the id directly) and it proved a good bet.

<strong>Alex:</strong> I was surprised by the excellent performance of word embedding features compared to classical TF-IDF approach, even though the word embeddings were trained on a rather small corpus.

<h3>Which tools did you use?</h3>

<strong>Andreas:</strong> We used a Python tool chain, with all of the standard tools of the trade (scikit-learn, nltk, pandas, numpy, scipy, xgboost, keras, hyperopt, matplotlib). Sometimes R was also used for visualization (ggplot).

<h3>How did you spend your time on this competition?</h3>

<strong>Alex:</strong> We spent most of the time on preprocessing and feature engineering. To tune the models and the ensemble, we reused code from previous competitions to automate hyperparameter optimization, cross-validation and stacking, so we could run them overnight and while we were at work.

<h3>What was the run time for both training and prediction of your winning solution?</h3>
<strong>Alex:</strong> To be honest, recalculating the full feature extraction and model training pipeline takes several days, although our best features and models would finish after a few hours. We often tried to remove models and features to reduce the complexity of our solution, but it almost always increased the prediction error. So we kept adding new models and features incrementally over several weeks, leading to more than 20 independent feature sets and about 300 models in the first ensemble layer.

<h2>Words of wisdom</h2>
<h3>What have you taken away from this competition?</h3>

<strong>Alex:</strong> Never give up and keep working out new ideas, even if you are falling behind on the public leaderboard. Never throw away weak features or models, they could still contribute to your final ensemble.

<strong>Nurlan:</strong> Building a cross-validation scheme that's consistent with leaderboard score and power of ensembling.

<strong>Andreas:</strong> Persistency and application of best practices on all aspects (cross-validation, feature engineering, model ensembling, etc.) is what makes it work. You cannot afford to skip any part if you want to compete seriously in Kaggle these days.

<h3>Do you have any advice for those just getting started in data science?</h3>

<strong>Alex:</strong> Data science is a huge field - focus on a small area first and approach it through hands-on experimentation and curiosity. For machine learning, pick a simple toy dataset and an algorithm, automate the cross validation, visualize decision boundaries and try get a feeling for the hyperparameters. Have fun! Once you feel comfortable, study the underlying mechanisms and theories and expand your experiments to more techniques.

<strong>Nurlan:</strong> This was my first competition and in the beginning I was competing alone. The problem of unstable local CV score demotivated me a bit as I couldn't tell how much my new approach helped until I made a submission. Once I joined the team, I learnt great deal from Alexander and Andreas. So get into a team with experienced Kagglers.

<strong>Andreas:</strong> I really recommend participating in Kaggle contests even for experienced data scientists. There is a ton of things to learn and doing it while playing is fun! Even if in the real world you will not get to use an ensemble of hundreds of models (well most of the time at least), learning a neat trick on feature transformations, getting to play with different models in various datasets and interacting with the community is always worth it. Then you can pick a paper or ML book and understand better why that algorithm worked or did not work so well for a given dataset and perhaps how to tweak it in a situation you are facing.

<h2>Teamwork</h2>

<h3>How did your team form?</h3>

<strong>Alex:</strong> Andreas and me are former colleagues and after we left the company we always planned to team up once for a competition. I met Nurlan at a Predictive Analytics meet-up in Frankfurt and invited him to join the team.

<h3>How did your team work together?</h3>

<strong>Alex:</strong> We settled on a common framework for the machine learning part at the very beginning and synchronized changes in the machine learning code and in hyper parameter configuration using a git repository. Nurlan and me had independent feature extraction pipelines, both producing serialized pandas dataframes. We shared those and the oob predictions using cloud storage services. Nurlan produced several new feature sets per week and kept Andreas and me very busy tuning and training models for them. We communicated mostly via group chat in Skype, only had two voice calls during the whole competition.

<h3>How did competing on a team help you succeed?</h3>

<strong>Andreas:</strong> We combined our different backgrounds and thus were able to cover a lot of alternatives fast. Additionally, in this contest having a lot of alternate ways of doing things like pre-processing, feature engineering, feature transformations, etc. was quite important in increasing the richness of the models that we could add in our stacking ensemble.

<h2>Just for fun</h2>

<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>

<strong>Alex:</strong> If I had access to a suitable dataset, I would run a competition on predictive maintenance to predict remaining useful lifetime of physical components. Also I would love to work on a competition where reinforcement learning can be applied.

<h2>The Team</h2>

<strong>Dr. Andreas Merentitis</strong> received B.Sc., M.Sc., and Ph.D. degrees from the Department of Informatics and Telecommunications, National Kapodistrian University of Athens (NKUA) in 2003, 2005, and 2010 respectively. Between 2011-2015 he was Senior Data Scientist at AGT International. Since 2015 he works as Senior Data Scientist at Zalando SE. He has more than 30 publications in machine learning, distributed systems, and remote sensing, including publications in flagship conferences and journals. He was awarded an IEEE Certificate of Appreciation as a core member of the team that won the first place in the “Best Classification Challenge” of the 2013 IEEE GRSS Data Fusion Contest. He has a master ranking in Kaggle.

<strong>Alexander Bauer</strong> is a data science consultant with 10 years of experience in statistical analysis and machine learning. He holds a degree in electrical engineering and a PhD in computer science.

<strong>Nurlanbek Duishoev</strong> received his BSc and PhD degrees in biological sciences from Middle East Technical and from Heidelberg University respectively. His research focused on drug screenings and biological image data analysis. He later moved on to apply recommender system approaches for gene function prediction. The wealth of data being generated in the biomedical field, like in many other industries, motivated him to master state-of-the-art data science techniques via various MOOCs and participate in Kaggle contests."
"Home Depot Product Search Relevance, Winners' Interview: 3rd Place, Team Turing Test | Igor, Kostia, & Chenglong",http://blog.kaggle.com/2016/06/01/home-depot-product-search-relevance-winners-interview-3rd-place-team-turing-test-igor-kostia-chenglong/,2016-06-01 17:15:17,"The <a href=""https://www.kaggle.com/c/home-depot-product-search-relevance"" target=""_blank"">Home Depot Product Search Relevance</a> competition which ran on Kaggle from January to April 2016 challenged Kagglers to use real customer search queries to predict the relevance of product results. Over 2,000 teams made up of 2,553 players grappled with misspelled search terms and relied on natural language processing techniques to creatively engineer new features. With their simple yet effective features, Team Turing Test found that a carefully crafted minimal model is powerful enough to achieve a high ranking solution. In this interview, Team Turing Test team walks us through their full approach which landed them in third place.

<h2>The basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
<strong>Chenglong Chen</strong>: I have received my Ph.D. degree in Communication Engineering from Sun Yat-sen University, Guangzhou, China, in 2015. As a Ph.D. student, I mainly focused on passive digital image forensics, and applied various machine learning methods, e.g., SVM and deep learning, to detect whether a digital image has been edited/doctored. I have participated in a few Kaggle competitions before this one.

[caption id=""attachment_5809"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/chenglong.png""><img class=""size-medium wp-image-5809"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/chenglong-300x143.png"" alt=""Kaggler Chenglong"" width=""300"" height=""143"" /></a> <a href=""https://www.kaggle.com/chenglongchen"" target=""_blank"">Chenglong on Kaggle</a>[/caption]

<strong>Igor Buinyi</strong>: I was a Ph.D. student at the Institute of Physics, Academy of Sciences of Ukraine. Later I received my MA degree in Economics from Kyiv School of Economics, then applied my skills in the financial sector and computer games industry. I have solid skills in statistics, data analysis and data processing, but I started seriously working on machine learning only recently after having graduated with the Udacity Data Analyst Nanodegree. Now I analyze customer behavior at Elyland.

[caption id=""attachment_5810"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/igor.png""><img class=""size-medium wp-image-5810"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/igor-300x142.png"" alt=""Kaggler Igor"" width=""300"" height=""142"" /></a> <a href=""https://www.kaggle.com/buinyi"" target=""_blank"">Igor on Kaggle</a>[/caption]

<strong>Kostiantyn Omelianchuk</strong>: I have an MS degree in System Analysis and Management from Kyiv Polytechnic Institute and 3 years of data analysis and data processing experience in the financial sector and game development industry. Now I analyze customer behavior at Elyland.

[caption id=""attachment_5811"" align=""aligncenter"" width=""300""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/kostia.png""><img class=""size-medium wp-image-5811"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/kostia-300x143.png"" alt=""Kaggler Kostia"" width=""300"" height=""143"" /></a> <a href=""https://www.kaggle.com/komelianchuk"" target=""_blank"">Kostia on Kaggle</a>[/caption]

<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>
<strong>Chenglong</strong>: I have limited knowledge of NLP tasks. In addition to the <a href=""https://www.kaggle.com/c/crowdflower-search-relevance"" target=""_blank"">CrowdFlower Search Results Relevance competition</a> at Kaggle (where I ended up in 1st place), reading forum posts, previous Kaggle winning solutions, related papers, and lots of Google searches have given me many inspirations.

<strong>Igor</strong>: In a student project, I used NLP and machine learning to analyze the Enron email dataset.
<h3>How did you get started competing on Kaggle?</h3>
<strong>Kostia</strong>: Almost a year ago I read <a href=""https://habrahabr.ru/post/264653/"" target=""_blank"">an article</a> about a solution to a Kaggle competition. I was very impressed by that text, so I registered at Kaggle and invited Igor to do the same. We then participated in Springleaf Marketing Response, however, we only finished in the top 15%.

<strong>Chenglong</strong>: It dates back 2012. At that time, I was taking Prof. Hsuan-Tien Lin's Machine Learning Foundations <a href=""https://www.coursera.org/course/ntumlone"" target=""_blank"">course on Coursera</a>. He encouraged us to compete on Kaggle to apply what we have learnt to real world problems. From then on, I have occasionally participated in competitions I find interesting.
<h3>What made you decide to enter this competition?</h3>
<strong>Igor</strong>: I just graduated from the Udacity Nanodegree program and was eager to apply my new skills in a competition, no matter what. At that time this competition had started, so I joined and invited Kostia to the team.

<strong>Chenglong</strong>: I have prior successful experience in the CrowdFlower Search Relevance competition on Kaggle which is quite similar to this one. I also had some spare time and wanted to strengthen my skills.
<h2>Let's get technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
The documentation and code for our approach are available <a href=""https://github.com/ChenglongChen/Kaggle_HomeDepot"" target=""_blank"">here</a>. Below is a high level overview of the method.

[caption id=""attachment_5799"" align=""aligncenter"" width=""1024""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/1_FlowChart.jpg"" target=""_blank""><img class=""size-large wp-image-5799"" src=""http://blog.kaggle.com/wp-content/uploads/2016/05/1_FlowChart-1024x373.jpg"" alt=""method flowchart"" width=""1024"" height=""373"" /></a> Fig. 1 Overall flowchart of our method.[/caption]

The text preprocessing step included text cleaning (removing special characters, unifying the punctuation, spell correction and thesaurus replacement, removing stopwords, stemming) and finding different meaningful parts in text (such as concatenating digits with measure units, extracting brands and materials, finding part-of-speech tags for each word, using patterns within the text to identify product names and other important words).

Our features can be grouped as follows:
<ul>
<ul>
 	<li>Basic features like word count, character count, percentage of digits, etc.</li>
 	<li>Intersection and distance features (various intersections between search term and other text information, Jaccard and Dice coefficients calculated using different algorithms).</li>
 	<li>Brand/material match features. Brands and materials were among the key determinants of the search results relevance.</li>
 	<li>First and last ngram features. They are designed to put more weight on the first and last ngram. The general idea would be to incorporate position weighting into these features.</li>
 	<li>LSA features. Most of our models are not efficient in dealing with sparse TFIDF vector space features. So we used the dimension reduced version via SVD.</li>
 	<li>TFIDF features in different combinations. They allow accounting for word frequency within the whole text corpus or particular query-product pair.</li>
 	<li>Word2vec features from pretrained models or models trained on the HomeDepot data.</li>
 	<li>WordNet similarity features. It allowed us to assess the closeness of the words as defined by NLTK WordNet. We calculated synset similarity for pairs of important words (ideally, we wanted important words to be product names and their main characteristics) or even whole strings.</li>
 	<li>Query expansion. The idea was to group similar queries together and then estimate how common a particular product description was. The relevance distribution was significantly skewed to 3, so in each small subset the majority of relevances would be closer to 3 than to 1. Thus, a higher intersection of a particular product description with the list of the most common words would indicate higher relevance.</li>
 	<li>Dummies: brands, materials, important words.</li>
</ul>
</ul>
<h3>How did you settle on a strong cross-validation strategy?</h3>
<strong>Chenglong</strong>: After seeing quite a few LB shakeups in Kaggle competitions, the first thing I do after entering a Kaggle competition is performing some data analysis and setting up an appropriate cross-validation strategy. This is quite important as the CV results will act as our guide for optimizing in the remaining procedure (and we don’t want to be misled). It also will help to accelerate the circle of change-validate-check. After some analysis, we have found that there are search terms only in the training set, only in the testing set and those in both training and testing set. This also applies for product uid. Taking these into consideration, we have designed our splitter for the data. Using this splitter, the gap between local CV and public LB is consistent around 0.001~0.002 which is within the 2-std range of CV for both single models and ensembled models. In the following figure, we compare different splits of product uid and search term via Venn-Diagram.

[gallery size=""full"" ids=""5800,5801,5802""]
[gallery size=""full"" ids=""5803,5804,5805""]
Fig. 2 Comparison of different splits on search term (top row) and product uid (bottom row). From left to right, shown are actual train/test split, naive 0.69 : 0.31 random shuffle split, and the proposed split.

<h3>What was your most important insight into the data?</h3>
<strong>Chenglong</strong>: Including this competition, I have participated in two relevance prediction competitions. From my point of view, the most important features are those measuring the distance between searching query and the corresponding results. Such distance can be measured via various distance measurements (Jaccard, Dice, cosine similarity, KL, etc.), and from different level (char ngram, word ngram, sentence, document), and with different weighting strategy (position weighting, IDF weighting, BM25 weighting, entropy weighting).

<strong>Kostia</strong>: We found the data too noisy to be captured by a single unified text processing approach. Therefore, we needed to apply as many preprocessing/feature extraction algorithms as we could in order to get a better performance.

<strong>Igor</strong>: We also found it extremely important to account for the number of characters in a word while generating various features. For example, the two strings ‘first second’ and ‘first third’ have Jaccard coefficient of 1/(2+2-1)=1/3 in terms of words and 5/(11+10-5)=5/16 in terms of characters. Such incorporation of the number of characters information into the intersection features, distance features and TFIDF features was very beneficial.

<h3>Were you surprised by any of your findings?</h3>
<strong>Kostia</strong>: I discovered that routine work gives 90% of the result. Only the minor part comes from luck and knowledge of ‘magic’ data science, at least in this competition. One could also easily compensate the lack of particular knowledge with some invested time and effort. Perseverance and desire to learn are the key factors for good performance.

<strong>Chenglong</strong>: Some data in the testing set are poisoned data that are not used in both public LB and private LB.

<strong>Igor</strong>: During the competition we had to add more and more features to move forward. In total we ended up with about 500 features used in our single models. After the competition had ended, we tried to produce a simple model as required by the Kaggle solution template. We were surprised that our 10-feature model would yield private RMSE of 0.44949 without any ensembling, enough to be on the 31st leaderboard place. It shows that a solution to such a problem could be simple and effective at the same time. (To be clear, we did not use the released information about the relevance from the test set to produce our simple model).

<h3>Which tools did you use?</h3>
<strong>Chenglong</strong>: We mostly used Python with packages such as numpy, pandas, regex, mathplotlib, gensim, hyperopt, keras, NLTK, sklearn, xgboost. I also used R with Rtsne package for computing the TNSE features. Igor and Kostia have used Excel to perform some descriptive analysis and generate some charts.

<h3>How did you spend your time on this competition?</h3>
<strong>Igor</strong>: Kostia and I spent almost equal amounts of time on feature engineering and machine learning. For some tasks we employed specialization: I focused on text preprocessing and using NLTK WordNet, Kostia got the most from word2vec. At some point we realized that we had chances to win, so we had to properly document our work and adhere to the requirements of the winning solutions. So, in four final weeks of the competition we spent much time on rewriting and clearing our code and recalculating features according to a unified text processing algorithm (which did not lead to an improvement of the results; we even lose some performance due to a reduced variance of our features).

<strong>Chenglong</strong>: In the very beginning of the competition, I focused on figuring out the appropriate CV strategy. After that I have started to reproduce the solution I used in the CrowdFlower competition. Meanwhile, I spent quite some time refactoring the whole framework for the purpose of adding data processor/feature generator/model learner easily. With a scalable codebase, I spent about 70% of the time figuring out and generating various and effective features.

<strong>Kostia</strong>: During the final week the whole team spent a few days on discussing patterns in the dataset (Fig. 3) and trying to figure out how to deal with them. Since our model predictions (one coming from Chenglong with some our features and another from me and Igor) for the test set were quite different, we expected that one of our approaches would be more robust to the leaderboard shakeup. In fact, we did not observe a major difference between the models’ performance in the public and private sets.

[caption id=""attachment_5814"" align=""aligncenter"" width=""764""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/05/patterns_in_the_dataset.png""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/05/patterns_in_the_dataset.png"" alt=""charts demonstrating patterns in the dataset"" width=""764"" height=""1341"" class=""size-full wp-image-5814"" /></a> Fig. 3 Some charts that demonstrate the patterns in the dataset.[/caption]

<h3>What was the run time for both training and prediction of your winning solution?</h3>
<strong>Igor</strong>: The text preprocessing and feature generation part takes a few days. Though a single xgboost model can be trained in about 5 minutes, training and predicting all models for the winning solution takes about 1 full day of computation.
<h2>Words of wisdom</h2>
<h3>What have you taken away from this competition?</h3>
<ul>
<ul>
 	<li>Spelling correction/synonyms replacement/text cleaning are very useful for searching query relevance prediction as also revealed in CrowdFlower.</li>
 	<li>In this competition, it is quite hard to improve the score via ensemble and stacking. To get the most out of ensemble and stacking, one should really focus on introducing diversity. To that goal, try various text processing, various feature subsets, and various learners. Team merging also contributes a lot of diversity!</li>
 	<li>That said, know how to separate the effect of an improved algorithm from the effect of increased diversity. Careful experiments are necessary. If some clear and effective solution does not work as well as your old models, do not discard it until you compare both approaches in the same conditions.</li>
 	<li>Keep a clean and scalable codebase. Keep track/log of the change you have made, especially how you create those massive crazy ensemble submissions.</li>
 	<li>Set up an appropriate and reliable CV framework. This allows trying various local optimizations on the features and models and getting feedback without the need to submit them. This is important for accelerating the change-validate-check cycle.</li>
</ul>
</ul>
<h2>Teamwork</h2>
<h3>How did your team form?</h3>
<strong>Igor</strong>: Kostia and I worked together since the start of the competition. Before the merger deadline, the competitors started to merge very intensely. We also realized that we had to merge in order to remain on the top. If we remember correctly, at the moment of our final merger Chenglong was the only sole competitor in the top 10 and there were few other teams of two in the top 10. So, our merger was natural.
<h3>How did your team work together?</h3>
<strong>Chenglong</strong>: For most of our discussion, we used Skype. For data and file sharing, we used Google Drive and Gmail.
<h3>How did competing on a team help you succeed?</h3>

<strong>Chenglong</strong>: About one or two weeks before the team merging deadline, I was the only one in the top 10 that competed solely. If I hadn't merged with Igor and Kostia, I might not have been able to enter the top 10 in the private LB. Competing on a team helps to introduce variants for obtaining better results. Also, we have learned a lot from each other.

<strong>Igor</strong>: All top teams in this competition merged during the later stages, and this fact signifies the importance of merging for achieving top performance. In our team we were able to quickly test a few ideas due to information sharing and cooperation.

<h2>Just for fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>
<strong>Kostia</strong>: Developing an AI or recommendation tool for poker.
<h3>What is your dream job?</h3>
<strong>Chenglong</strong>: Chef."
"Home Depot Product Search Relevance, Winners' Interview: 2nd Place | Thomas, Sean, Qingchen, & Nima",http://blog.kaggle.com/2016/06/15/home-depot-product-search-relevance-winners-interview-2nd-place-thomas-sean-qingchen-nima/,2016-06-15 17:28:00,"The <a href=""https://www.kaggle.com/c/home-depot-product-search-relevance"" target=""_blank"">Home Depot Product Search Relevance</a> competition challenged Kagglers to predict the relevance of product search results.  Over 2000 teams with 2553 players flexed their natural language processing skills in attempts to feature engineer a path to the top of the leaderboard. In this interview, the second place winners, Thomas (Justfor), Sean (sjv), Qingchen, and Nima, describe their approach and how diversity in features brought incremental improvements to their solution.

<h2>The basics</h2>

<h3>What was your background prior to entering this challenge?</h3>

<strong>Thomas</strong> is a pharmacist, with his PhD in Informatics and Pharmaceutical Analytics and works in Quality in the pharmaceutical industry. At Kaggle he joined earlier competitions and got the Script of the Week award.

<strong>Sean</strong> is an undergraduate student in computer science and mathematics at the Massachusetts Institute of Technology (MIT).

[gallery columns=""2"" size=""large"" ids=""5841,5844""]

<strong>Qingchen</strong> is a data scientist at ORTEC Consulting and a PhD researcher at the Amsterdam Business School. He has experience competing on Kaggle but this was the first time with a competition related to natural language processing.

<strong>Nima</strong> is a PhD candidate at the Lassonde School of Engineering at York University focusing on research in data mining and machine learning. He has also experience competing on Kaggle but up to now focused on other types of competitions.

[gallery columns=""2"" size=""large"" ids=""5843,5842""]

Between the four of us, we have quite a bit of experience with Kaggle competitions and machine learning, but minor experience in natural language processing.

<h3>What made you decide to enter this competition?</h3>

For all of us, the primary reason was that we wanted to learn more about natural language processing (NLP) and information retrieval (IR). This competition turned out to be great for that, especially in providing practical experience.

<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>

All of us have strong theoretical experience with machine learning in general, and it naturally helps with the understanding and implementation of NLP and IR methods. However, none of us have had any real experience in this domain.

<h2>Let's get technical</h2>

<h3>What preprocessing and supervised learning methods did you use?</h3>

The key to this competition was mostly preprocessing and feature engineering as the primary data is text. Our processed text features can broadly be grouped into a few categories: categorical features, counting features, co-occurrence features, semantic features, and statistical features.

<ul>
	<li>Categorical features:  Put words in categories such as colors, units, brands, core. Count the number of those words in the query/title and count number of intersection between query and title for each category.</li>
	<li>Counting features: Length of query, number of common grams between query and title, Jacquard similarity, etc.</li>
	<li>Co-occurrence features:  Measures of how frequently words appear together. e.g., Latent Semantic Analysis (LSA).</li>
	<li>Semantic features:  Measure how similar the meaning of two words is.</li>
	<li>Statistical features: Compare queries with unknown score to queries with known relevance score.</li>
</ul>

It seems that a lot of the top teams had similar types of features, but the implementation details are probably different. For our ensemble we used different variations of xgboost along with a ridge regression model.

[caption id=""attachment_5850"" align=""aligncenter"" width=""600""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/06/wordcloud.png""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/06/wordcloud.png"" alt=""Word cloud of Home Depot product search terms."" width=""600"" height=""500"" class=""size-full wp-image-5850"" /></a> Word cloud of Home Depot product search terms.[/caption]

For models and ensemble we started with random forest, extra trees and gbm-models. Furthermore xgboost and ridge were in our focus. Shortly prior to the end of the competition we found out, that first random forest and then extra trees did not help our ensembles anymore. So we focused on xgboost, gbm and Ridge. 
Our best single model was a xgboost-model and scored 0.43347 on the public LB. The final ensemble consists of 19 models based on xgboost, gbm and Ridge. The xgboost-models were made with different parameters including binarizing the target, objective reg:linear, and objective count:poisson. We found, that the Ridge Regression helped in nearly every case, so we included it in the final ensemble.

[caption id=""attachment_5880"" align=""aligncenter"" width=""800""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/06/our_pipeline-800x.png""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/06/our_pipeline-800x.png"" alt=""Our data processing pipeline."" width=""800"" height=""426"" class=""size-full wp-image-5880"" /></a> Our data processing pipeline.[/caption]

<h3>Were you surprised by any of your findings?</h3>

A surprising finding was the large number of features which had predictive ability. In particular, when we teamed up, it was better to combine our features than to ensemble our results. This is quite unique as most of the time new features are more likely to cause overfit but not in this case. As a result, adding more members to the team was highly likely to improve score which is why the top-10 were all teams of at least 3 people.

<h3>Which tools did you use?</h3>

We used mainly Python 3 and Python 2. The decision for Python 2 is interesting as some of the used libraries are still not available for Python 3. In our processing chain we used the Python standard tools for machine learning (scikit-learn, nltk, pandas, numpy, scipy, xgboost, gensim). Nima used R for feature generation.

<h3>How did you spend your time on this competition?</h3>

After teaming up, Sean and Nima spent most of their time on feature engineering and Thomas and Qingchen spent most of their time on model tuning.

<h3>What was the run time for both training and prediction of your winning solution?</h3>

In general, training/prediction time is very fast (minutes), but we used some xgboost parameters that took much longer to train (hours) for small performance gains. Text processing and feature engineering took a very long time (easily over 8 hours for a single feature set).

<h2>Words of wisdom</h2>

<h3>What have you taken away from this competition?</h3>

First of all quite a lot of Kaggle ranking points and Thomas got his Master badge! Overall this was a very difficult competition and we learned a lot about natural language processing  and information retrieval in practice. It now makes sense why Google is able to use such a large number of features in their search algorithm as many seemingly insignificant features in this competition were still able to provide a tangible performance boost.

<h2>Teamwork</h2>

<h3>How did your team form?</h3>

Initially Thomas and Sean teamed up as Sean had strong features and Thomas experience in models and Kaggle. The models were complementing well and ensembling brought the team into the top-10. A further boost was made when Qingchen joined with his features and models. At this point we (and other teams) realized that it's a necessity to form larger teams in order to be competitive as combining features really helps improve performance. We decided to ask Nima to join us as he had an excellent track record and was also doing quite well on his own.

Working together was quite interesting as we are from Germany, US, Netherlands and Canada. The different time zones made direct communication difficult; we opted therefore for mail communication. For getting results and continue working on ideas the different time zones were helpful. 

<h2>Team bios</h2>
<strong><a href=""https://www.linkedin.com/in/thomasheiling"" target=""_blank"">Dr. Thomas Heiling</a></strong> is a pharmacist, with his PhD in Informatics and Pharmaceutical Analytics and works in Quality in the pharmaceutical industry. 

<strong>Sean J. Vasquez</strong> is a second year undergraduate student at the Massachusetts Institute of Technology (MIT), studying computer science and mathematics. 

<strong><a href=""https://www.linkedin.com/in/qingchenwang"" target=""_blank"">Qingchen Wang</a></strong> is a Data Scientist at ORTEC Consulting and a PhD researcher in Data Science and Marketing Analytics at the Amsterdam Business School.

<strong><a href=""https://www.linkedin.com/in/NimaShahbazi"" target=""_blank"">Nima Shahbazi</a></strong> is a second-year PhD student in the Data Mining and Database Group at York University. He previously worked in big data analytics, specifically on Forex Market. His current research interests include Mining Data Streams, Big Data Analytics and Deep Learning."
"Avito Duplicate Ads Detection, Winners' Interview: 3rd Place, Team ADAD | Mario, Gerard, Kele, Praveen, & Gilberto",http://blog.kaggle.com/2016/07/27/avito-duplicate-ads-detection-winners-interview-3rd-place-mario-gerard-kele-praveen-gilberto/,2016-07-27 16:43:31,"The <a href=""https://www.kaggle.com/c/avito-duplicate-ads-detection/"" target=""_blank"">Avito Duplicate Ads Detection competition</a> ran on Kaggle from May to July 2016 and attracted 548 teams with 626 players. In this challenge, Kagglers sifted through classified ads to identify which pairs of ads were duplicates intended to vex hopeful buyers. This competition, which saw over 8,000 submissions, invited unique strategies given its mix of Russian language textual data paired with 10 million images. In this interview, team ADAD describes their winning approach which relied on feature engineering including an assortment of similarity metrics applied to both images and text.
<h2>The basics</h2>
<h3>What was your background prior to entering this challenge?</h3>
<a href=""https://www.kaggle.com/mariofilho"" target=""_blank""><strong>Mario Filho</strong></a>: My background in machine learning is completely “self-taught”. I found a wealth of education materials available online through MOOCs, academic papers and lectures in general. Since February 2014 I have worked as a machine learning consultant in projects from small startups and Fortune 500 companies.

[caption id=""attachment_6009"" align=""aligncenter"" width=""948""]<img class=""size-full wp-image-6009"" src=""http://blog.kaggle.com/wp-content/uploads/2016/07/mario.png"" alt=""Mario on Kaggle"" width=""948"" height=""450"" /> <a href=""https://www.kaggle.com/mariofilho"" target=""_blank"">Mario on Kaggle</a>[/caption]

<a href=""https://www.kaggle.com/remap1"" target=""_blank""><strong>Gerard Toonstra</strong></a>: I worked as a scientific developer at Thales for 3 years, which introduced me into more scientific development methods and algorithms. Most of the specific ML knowledge was acquired through courses on Coursera and just getting your hands dirty in Kaggle competitions and the forum interactions.

[caption id=""attachment_6010"" align=""aligncenter"" width=""950""]<img class=""size-full wp-image-6010"" src=""http://blog.kaggle.com/wp-content/uploads/2016/07/gerard.png"" alt=""Gerard on Kaggle"" width=""950"" height=""452"" /> <a href=""https://www.kaggle.com/remap1/competitions"" target=""_blank"">Gerard on Kaggle</a>[/caption]

<a href=""https://www.kaggle.com/kelexu"" target=""_blank""><strong>Kele Xu</strong></a>: I am a PhD student with the topic on ""silent speech interface"".

[caption id=""attachment_6011"" align=""aligncenter"" width=""950""]<img class=""size-full wp-image-6011"" src=""http://blog.kaggle.com/wp-content/uploads/2016/07/kele.png"" alt=""Kele on Kaggle"" width=""950"" height=""451"" /> <a href=""https://www.kaggle.com/kelexu/"" target=""_blank"">Kele on Kaggle</a>[/caption]

<a href=""https://www.kaggle.com/praveenadepu"" target=""_blank""><strong>Praveen Adepu</strong></a>: Academically, I have Bachelor of Technology and working as full stack BI Technical Architect/Consultant.

[caption id=""attachment_6013"" align=""aligncenter"" width=""949""]<img class=""size-full wp-image-6013"" src=""http://blog.kaggle.com/wp-content/uploads/2016/07/praveen.png"" alt=""Praveen on Kaggle"" width=""949"" height=""450"" /> <a href=""https://www.kaggle.com/praveenadepu"" target=""_blank"">Praveen on Kaggle</a>[/caption]

<a href=""https://www.kaggle.com/titericz"" target=""_blank""><strong>Gilberto Titericz</strong></a>: I'm graduated in electronic engineering and M.S. in wireless communication area. In 2011 I started to learn data science by myself and after joining Kaggle I started to learn even more.

[caption id=""attachment_6012"" align=""aligncenter"" width=""949""]<img class=""size-full wp-image-6012"" src=""http://blog.kaggle.com/wp-content/uploads/2016/07/gilberto.png"" alt=""Gilberto on Kaggle"" width=""949"" height=""451"" /> <a href=""https://www.kaggle.com/titericz"" target=""_blank"">Gilberto on Kaggle</a>[/caption]
<h3>How did you get started competing on Kaggle?</h3>
<strong>Mario Filho</strong>: I heard about Kaggle when I was taking my first courses about data science, and after I learned more about it I decided to try some competitions.

<strong>Gerard Toonstra</strong>: I was active in the Netflix grand prize quite some while ago and at the end, it pointed to the Kaggle site as another potential source of getting your hands dirty. I ignored that up to July last year when I decided to start on the <a href=""https://www.kaggle.com/c/avito-context-ad-clicks"" target=""_blank"">Avito click challenge</a>. It's pretty cool that exactly one year later, after some avid kaggling, I'm part of the 3rd place submission.

<strong>Kele Xu</strong>: I participated in KDD Cup 2015, and ended as 40th there. That was my first competition. After KDD Cup 2015, I became a Kaggler, which helped me to learn a lot during the last year.

<strong>Praveen Adepu</strong>: I am new to machine learning, R and Python. I like learning by doing and I realised Kaggle is the best fit for my learning by participating in competitions. Initially I experimented with few competitions to find learning patterns and started working seriously from last 6 months and I learnt a lot in the past 6 months and looking forward to learn more from Kaggle.

<strong>Gilberto Titericz</strong>: After the Google AI challenge 2011 I was searching on the internet for other online competition platform and found Kaggle.
<h3>What made you decide to enter this competition?</h3>
<strong>Mario Filho</strong>: At the time it was the only competition that had a reasonable dataset size and was not too focused on images. So I thought it would be possible to get a good result with feature engineering and the usual tools.

<strong>Gerard Toonstra</strong>: I feel that my software engineering background can give me an edge in certain competitions. The huge amount of data requires a bit of planning and modularizing the code. I started doing that in the <a href=""https://www.kaggle.com/c/dato-native"" target=""_blank"">Dato competition</a>, continued doing it a bit better for <a href=""https://www.kaggle.com/c/home-depot-product-search-relevance"" target=""_blank"">Home Depot</a> and in Avito I started some more serious pipelining and feature engineering. It's not that the pipelines are sophisticated, the only purpose is to reduce feature building time. Instead of waiting for one script to finish in 8 hours, I just build features in parts and glue/combine them together.

<strong>Kele Xu</strong>: When I decide to go to a new competition, I would like to select some topic which I have no experience before. On that case, I will take more from the competition. In fact, before this competition, I have few experiments on NLP topics. That’s the main reason I entered this competition.

<strong>Praveen Adepu</strong>:
<ol>
 	<li>I like feature engineering, this completion requires lot of hand craft feature engineering</li>
 	<li>Very high LB bench mark score attracted me a lot to test my learning skills from previous competitions.
I left with lot of feature engineering ideas even after passing the bench mark in couple of weeks so planned to work bit more time on this competition.</li>
</ol>
<strong>Gilberto Titericz</strong>: Lately I have interest in competitions involving image processing and deep learning.
<h2>Let's Get Technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>
<strong>Mario Filho</strong>: I stemmed and cleaned the text fields, then used tf-idf to compute similarities between them. Used the hash similarity script available in the forums to compute the similarity between images. After creating and testing lots of features, I used XGBoost to train a model.

<strong>Gerard Toonstra</strong>: I had one feature array for textual features, which is essentially pretty much what others did in the forum. Then I built another feature set with additional text features, a minhash similarity feature, tf-idf+svd over description and then I did work on image hashes: ahash,phash,dhash with min, max, avg features and the count of zero divided by max number of images. I kept focusing on making these normalized distance metrics, so divide by 'length' where appropriate or the number of images. As things advanced, I also did image histograms on RGB, histograms on HSV and Earth Mover Distance between images.

I ended up only using four model types: logistic regression over tf-idf features over cleaned text, XGB, Random Forests and FTRL.

<strong>Kele Xu</strong>: Here, I am only using XGBoost. XGBoost is really competitive here. My best single XGBoost model can get to top 14 in both of public and private Leaderboard.

<strong>Praveen Adepu</strong>: Not any special pre-processing methods but just followed all best practices of feature engineering and taken care of processing times when creating new features.
Used - XGBoost, h2o Random Forest, h2o deep learning, Extra Trees and Regularised Greedy Forest.

<strong>Gilberto Titericz</strong>: Most preprocessing was done in text features, like stop words removal and stemming. Also I build a very interesting feature using deep learning. For this I used the MXNet Inception 21k pre-trained model to predict one class for each one of the 10M images in dataset. Measuring some statistics between those classes helped improve our models.

Supervised learning methods used are based in gradient boosting (XGBoost), RandomForest, ExtraTrees, FTRL, Linear Regression, Keras and MXNet. We also used libffm and RGF algorithms, but we dropped it in the end.
<h3>What have you taken away from this competition?</h3>
<strong>Mario Filho</strong>: I learned new techniques for calculating image similarity based on hashes, and 2 or 3 Russian words.

<strong>Gerard Toonstra</strong>: The benefit of working in teams and learning from that experience. I really wanted to learn about stacking. There is a good description on mlwave.com, but when you get around to actually doing that in practice, there are still questions that pop up. The other observation is that as a competition draws to a close, it's all hands on deck and you have to put the last effort in, especially the last week. I did not have enough hardware resources and used in the most extreme case three 16-cpu machines spread across three geographical zones on Google Cloud. Despite the hardware challenges and exploration we had to do, I noticed some solo competitors who were able to gain ground quickly and rocket up. I really respect that. It tells me that those guys have acquired so much experience that they consistently make right decisions on what to spend time on and I still have a long way to go.

<strong>Kele Xu</strong>: I have learned a lot from our teammates, especially on the hyper-parameter optimization for XGBooost model and the ensemble techniques. Also, I got to know how to do some NLP tasks.

<strong>Praveen Adepu</strong>: I think joining the team at right time and with right team members is best decision I have made in this competition.

I started with two goals while joining the team:
<ol>
 	<li>Learn from the team while contributing to the best of my knowledge</li>
 	<li>Learn ensembling/stacking</li>
</ol>
I would like to take this opportunity to say thank you my entire team Gilberto, Mario, Gerard and Kele.

Gilberto - thank you for teaching many many concepts including stacking
Mario - thank you for guiding me while experimenting many models
Gerard - never forget our initial discussion on feature engineering
Kele - thank you for the invite to join the team and making this happen

I learnt a lot from this team in one month than alone in the past 6 months and looking forward to work with them in near future.

<strong>Gilberto Titericz</strong>: I learned a lot about text and image distance metrics.
<h3>Do you have any advice for those just getting started in data science?</h3>
<strong>Mario Filho</strong>: if you plan to apply machine learning, try to understand the fundamentals of the models, concepts like validation, bias and variance. Really understand these concepts and try to apply them to data.

<strong>Gerard Toonstra</strong>: The first thing is to understand cross validation score and its specific relation to the leaderboard in that competition and how to establish a sound folding strategy in general. After that, I recommend establishing a very simple model based on features that do not overfit and evaluate the behavior. Then add a couple of features one by one that are unlikely to overfit and keep evaluating. Record the difference between CV and LB when you make submissions. As you establish confidence in your local CV, start working out more features and do a couple of in-between checks to ensure you're not overfitting locally; especially depending on how some aggregation features are built, they can have overfitting effects.

When you get started, don't allow yourself to get bogged down by endless hours of parametrizing and hyperparameterizing the models. You may get the extra +0.00050 to jump up 3 positions, but it's not what most on the LB are doing. Figure out how to create new informative features, which features should be avoided and when you grow tired of that, only <em>then</em> spend some time optimizing what you have.

<strong>Kele Xu</strong>: In fact, I think Kaggle is quite a good platform for both the data scientists and those who are just getting started, as Kaggle can provide a platform for the new guys to test how each algorithm performs.

As to the technique suggestions, I would say a solid local CV is the key point to win a competition. Usually, I will spend 2-4 weeks to test my local CV.

Feature engineering is more important than hyper-parameter optimization.

Although XGBoost is enough to get a relative good rank in some competitions, we should also test mode models to add the diversity of the models, not just select different seed or set different max depth for tree-based methods.

The last thing is: just test your ideas, and you can get some feedback from the LB. When you get to higher rank in one competition, you will have the motivation to make it better.

<strong>Praveen Adepu</strong>: I think, I do call these my experience rather than advice:
<ol>
 	<li>Start with clear objectives, learn slowly and progressively with basics and learn quickly with advanced concepts and end with mastering</li>
 	<li>Never be afraid to start and fail</li>
 	<li>Clear understanding of local CV, underfit and overfit</li>
 	<li>Start learning from feature engineering and end with stacking</li>
</ol>
<strong>Gilberto Titericz</strong>: Read a lot of related material. Search for problems to solve. Read about problems' solutions. And make your fingers work, program and learn from your mistakes.
<h2>Bios</h2>
<strong>Mario Filho</strong> is a machine learning consultant focused in helping companies around the world use machine learning to maximize the value they get from data to achieve their business goals. Besides that, he mentors individuals who want to learn how to apply machine learning algorithms to real world data sets.

<strong>Gerard Toonstra</strong> graduated as a nautical officer+engineer, but mostly worked as software engineer and started his own company in Brazil working with drones for surveying. He now works as scrum master for the BI department at Coolblue in The Netherlands.

<strong>Kele Xu</strong> is a PhD student and writing his PhD thesis at Langevin Institute (University of Pierre and Marie Curie). His main interests are include: silent speech recognition, machine learning and computer vision.

<strong>Praveen Adepu</strong> is currently working as BI Technical Architect/Consultant at Fred IT Melbourne based IT product company and main interests in machine learning and data architecture.

<strong>Gilberto Titericz</strong> is an electronics engineer with a M.S. in telecommunications. For the past 16 years he's been working as an engineer for big multinationals like Siemens and Nokia and later as an automation engineer for Petrobras Brazil. His main interests are in machine learning and electronics areas.

<hr />

<h2>Kernel Corner</h2>
Getting hashes from images was an important strategy in detecting similarities across the 10 million images in this competition as <strong>Gerard Toonstra</strong> explains:
<blockquote>
<p style=""text-align: center;"">phash calculates a hash in a special way such that it reduces the dimensionality of the source image into a 64-bit number. It captures the structure of the image. When you then subtract two hashes, you get a number that resembles the 'structural distance' between the two images. The higher the number, the more the distance between the two.</p>
</blockquote>
Kaggler <a href=""https://www.kaggle.com/rightfit"" target=""_blank"">Run2</a> shared <a href=""https://www.kaggle.com/rightfit/avito-duplicate-ads-detection/get-hash-from-images/code"" target=""_blank"">code</a> on Kaggle Kernels which allowed Kagglers to incorporate distance metrics from images without performing heavy duty image processing or deep learning.

[caption id=""attachment_6014"" align=""aligncenter"" width=""776""]<img class=""size-full wp-image-6014"" src=""http://blog.kaggle.com/wp-content/uploads/2016/07/get_hash_script.png"" alt=""Get hash from image script on Kaggle Kernels"" width=""776"" height=""470"" /> Part of Kaggler <a href=""https://www.kaggle.com/rightfit"" target=""_blank"">Run2's</a> script <a href=""https://www.kaggle.com/rightfit/avito-duplicate-ads-detection/get-hash-from-images/code"" target=""_blank"">Get Hash From Images</a>.[/caption]"
"Facebook V: Predicting Check Ins, Winner's Interview: 2nd Place, Markus Kliegl",http://blog.kaggle.com/2016/08/02/facebook-v-predicting-check-ins-winners-interview-2nd-place-markus-kliegl/,2016-08-02 16:49:55,"Facebook ran its fifth recruitment competition on Kaggle, <a href=""https://www.kaggle.com/c/facebook-v-predicting-check-ins"" target=""_blank"">Predicting Check Ins</a>, from May to July 2016. This uniquely designed competition invited Kagglers to enter an artificial world made up of over 100,000 places located in a 10km by 10km square. For the coordinates of each fabricated mobile check-in, competitors were required to predict a ranked list of most probable locations. In this interview, the second place winner <a href=""https://www.kaggle.com/mkliegl"" target=""_blank"">Markus Kliegl</a> discusses his approach to the problem and how he relied on semi-supervised methods to learn check-in locations' variable popularity over time.

[latexpage]

<h2>The basics</h2>
<h3>What was your background prior to entering this challenge?</h3>

I recently completed a PhD in mathematical fluid dynamics. Through various courses, internships, and contract work, I had some background in scientific computing, inverse problems, and machine learning.

[caption id=""attachment_6020"" align=""aligncenter"" width=""950""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/07/markus_profile.png"" alt=""Markus Kliegl on Kaggle"" width=""950"" height=""452"" class=""size-full wp-image-6020"" /> <a href=""https://www.kaggle.com/mkliegl/competitions"" target=""_blank"">Markus on Kaggle</a>[/caption]

<h2>Let's get technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>

The overall approach was to use Bayes' theorem: Given a particular data point (x, y, accuracy, time), I would try to compute for a suitably narrowed set of candidate places the probability

$$ P(place | x, y, accuracy, time) \propto P(x, y, accuracy, time | place) P(place) \,, $$

and rank the places accordingly. A la Naive Bayes, I further approximated

P(x, y, accuracy, time | place) as

$$  P(x, y, accuracy, time | place) \approx P(x, y | place) \cdot $$
$$  P(accuracy | place) \cdot P(time\, of\, day | place) \cdot $$
$$ P(day\, of\, week | place) \,. $$

I decided on this decomposition after a mixture of exploratory analysis and simply trying out different assumptions on the independence of variables on a validation set.

One challenge given the data size was to efficiently learn the various conditional distributions on the right-hand side. Inspired by the effectiveness of <a href=""https://www.kaggle.com/zfturbo"" target=""_blank"">ZFTurbo's</a> <a href=""https://www.kaggle.com/zfturbo/facebook-v-predicting-check-ins/mad-scripts-battle"" target=""_blank"">""Mad Scripts Battle"" kernel</a> early in the competition, I decided to start by just learning these distributions using histograms.

To make the histograms more accurate, I made them periodic for time of day and day of week and added smoothing using various filters (triangular, Gaussian, exponential). I also switched to C++ to further speed things up. (Early in the competition this got me to the top of the leaderboard with a total runtime of around 40 minutes single-threaded, while others were already at 15-50 hours. Unfortunately, I could not keep things this fast for very long.)

For later submissions, I averaged the P(x, y | place) histograms with Gaussian Mixture Models.

<h3>What was your most important insight into the data?</h3>

The relative popularity of places, P(place), varied substantially over time (really it should be written as P(place, time)), and it seemed hard tome to forecast it from the training data (though others like <a href=""https://www.kaggle.com/rsakata"" target=""_blank"">Jack (Japan)</a> in third place had some success doing this). Since the quality of the predictions even with a rough guess for P(place) was already fairly high, however, I realized a semi-supervised approach might stand a good chance of being able to learn P(place, time). My final solution performed 20 semi-supervised iterations on the test data.

[caption id=""attachment_6025"" align=""aligncenter"" width=""735""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/07/checkins_by_month.png"" alt=""Number of check-ins over time varied irregularly for many places."" width=""735"" height=""345"" class=""size-full wp-image-6025"" /> The number of checkins over time varied quite irregularly for many places. A semi-supervised approach helped me overcome this irregularity.[/caption]

Getting this to actually work well took some effort. There is more discussion in <a href=""https://www.kaggle.com/c/facebook-v-predicting-check-ins/forums/t/22078/solution-sharing/126235#post126235"" target=""_blank"">this thread</a>.

<h3>Were you surprised by any of your findings?</h3>

Accuracy was quite mysterious at first. I initially focused on analyzing the relationship between accuracy and the uncertainty in the x coordinate and tried to incorporate that into my model. However, this helped only a tiny bit. I eventually came to the conclusion that accuracy is most gainfully employed directly by adding a factor P(accuracy | place): different places attract different mixes of accuracies. As suggested in the forums, this makes sense if one thinks of accuracy as a proxy for device type.

Another surprise was this: On the last day, I tried ensembling different initial guesses for P(place), but this improved the score only by 0.00001 over the best initial guess, which in turn was only 0.00015 better than the worst initial guess. Though I was disappointed to not be able to improve my score in this way (rushed experiments on a small validation set had looked a little more promising), this insensitivity to the initial guess is actually a good property of the solution. It speaks to the stability of convergence of the algorithm.

<h3>Which tools did you use?</h3>

[caption id=""attachment_6026"" align=""aligncenter"" width=""725""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/07/kdeplot_x_y.png"" alt=""Apparent multinomial distributions of check-ins."" width=""725"" height=""225"" class=""size-full wp-image-6026"" /> The (x, y) distributions of checkins looked multimodal. KDE or Gaussian Mixture Models were thus natural to try for learning P(x, y | place).[/caption]

I used Python with the usual stack (pandas, matplotlib, seaborn, numpy, scipy, scikit-learn) for data exploration and for learning Gaussian Mixture Models for the P(x, y | place) distributions. The main model is written in C++. Finally, I used some bash scripts and the <a href=""http://www.gnu.org/software/parallel/"" target=""_blank"">GNU parallel</a> utility to automate parallel runs on slices of the data.

<h3>How did you spend your time on this competition?</h3>

I spent a little time early on exploring the data, in particular doing case studies of individual places. After that, I spent almost all my time on implementing, optimizing, and tuning my custom algorithm.

<h3>What was the run time for both training and prediction of your winning solution?</h3>

Aside from one-time learning of Gaussian Mixture Models (which probably took around 40 hours), the run time was around 60 CPU hours. Since the problem parallelizes well, the non-GMM run time was about 15 hours on my laptop. For the last few days of the competition, I borrowed compute time on an 8-core workstation, where the run time ended up at around around 4-5 hours.

In this <a href=""https://github.com/mkliegl/kaggle-Facebook-V"" target=""_blank"">Github repository</a>, I also posted a simplified single-pass version that would have gotten me to 6th place and that runs in around 90 minutes single-threaded on my laptop (excluding the one-time GMM training time). Compared to my full solution, this semi-supervised online learning version also has the nicer property of never using any information from the future.


<h2>Bio</h2>
<img src=""http://blog.kaggle.com/wp-content/uploads/2016/07/Markus-Kliegl-profile-photo-150x150.jpeg"" alt=""Markus Kliegl profile photo"" width=""150"" height=""150"" class=""alignright size-thumbnail wp-image-6021"" />
Markus Kliegl recently completed a PhD in Applied and Computational Mathematics at Princeton University. His current interests lie in machine learning research and applications."
"Facebook V: Predicting Check Ins, Winner's Interview: 1st Place, Tom Van de Wiele",http://blog.kaggle.com/2016/08/16/facebook-v-predicting-check-ins-winners-interview-1st-place-tom-van-de-wiele/,2016-08-16 16:18:16,"From May to July 2016, over one thousand Kagglers competed in <a href=""https://www.kaggle.com/c/facebook-v-predicting-check-ins"" target=""_blank"">Facebook's fifth recruitment competition: Predicting Check-Ins</a>. In this challenge, Kagglers were required to predict the most probable check-in locations occurring in artificial time and space. As the first place winner, <a href=""https://www.kaggle.com/tvdwiele"" target=""_blank"">Tom Van de Wiele</a>, notes in this winner's interview, the uniquely designed test dataset contained about one trillion place-observation combinations, posing a huge difficulty to competitors. Tom describes how he quickly rocketed from his first <a href=""https://www.kaggle.com/c/kobe-bryant-shot-selection"" target=""_blank"">getting started competition</a> on Kaggle to first place in Facebook V through his remarkable insight into data consisting only of x,y coordinates, time, and accuracy using k-nearest neighbors and XGBoost. 

<h2>The basics</h2>
<h3>What was your background prior to entering this challenge?</h3>

I have completed two Master programs at two different Belgian universities (Leuven and Ghent), one in Computer Science (2010) and one in Statistics (2016). I graduated from the Statistics program during the Kaggle competition and was combining it with a full time job at a manufacturing plant at Eastman in Ghent during the past couple of years. Initially I started as an automation engineer and in a next phase I was mostly working on process improvements using the <a href=""https://en.wikipedia.org/wiki/Six_Sigma"" target=""_blank"">six sigma methodology</a>. In the beginning of 2015 I got to the really good stuff when I started working with the data science group at Eastman where I am currently employed as an analytics consultant. We solve various complex analysis problems with an amazing team and mostly rely on R which we often combine with Shiny to develop interactive web applications.

[caption id=""attachment_6073"" align=""aligncenter"" width=""950""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/tom_van_de_wiele.png"" alt=""Tom Van de Wiele"" width=""950"" height=""450"" class=""size-full wp-image-6073"" /> <a href=""https://www.kaggle.com/tvdwiele"" target=""_blank"">Tom Van de Wiele</a> on Kaggle.[/caption]

<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>

I have always had a passion for modeling complex problems and think that this mindset helped me to do well more than anything else. The problem setting is very tangible and all four predictors can be interpreted by anyone so that made it a very accessible contest where mobile data domain knowledge doesn’t really help. The problem can be translated to a classification setting with the only major complication that there are a large number of classes (>100K). I did however read a lot about other winning solutions prior to the contest. The <a href=""http://blog.kaggle.com/2014/08/01/learning-from-the-best/"" target=""_blank"">learning from the best</a> post on this blog has been especially useful.

<h3>How did you get started competing on Kaggle?</h3>

Through a Kaggle ‘Getting Started’ competition :) I was a passive user for a long time before entering my first competition. Like many others I wanted to compete one day but never really took the step to my first submission. Things changed when a colleague with a chemical engineering background wanted to get into machine learning and participated in the <a href=""https://www.kaggle.com/c/kobe-bryant-shot-selection"" target=""_blank"">Kobe Bryant shot selection competition</a>. He asked some great questions and I tried to point him into the right direction but his questions got me excited enough to download the data and implement my suggestions. Two evenings later I got close to the top 10 on the leaderboard at the time with about 500 participants and I started to dream about future competitions. That second evening was the launch date of the <a href=""https://www.kaggle.com/c/facebook-v-predicting-check-ins"" target=""_blank"">Facebook V competition</a> and I wouldn’t have to dream for long!

<h3>What made you decide to enter this competition?</h3>

The promise of a possible interview at Facebook was a strong motivation to participate although I considered it to be highly unlikely given that it was my first featured Kaggle competition and I already had a fully booked agenda. My second main motivation was the promise of learning new techniques and insights from other contestants. 

In hindsight I am very happy to be interviewing at one of the best companies in the world for machine learning professionals but I am even more grateful for everything I learned from my own struggles and the other participants. A competition setting makes you think outside of the box and continuously challenge your approach. The tremendous code sharing on the forums was a great catalyst in this process.

<h2>Let's get technical</h2>
Extended details of the technical approach can be found on <a href=""https://ttvand.github.io/Winning-approach-of-the-Facebook-V-Kaggle-competition/"" target=""_blank"">my blog</a>. The R code is available on <a href=""https://github.com/ttvand/Facebook-V/"" target=""_blank"">my GitHub</a> account along with high level instructions to construct the final submission.

<h3>What was your general strategy?</h3>

The main difficulty of this problem is the extended number of classes (places). With 8.6 million test records there are about a trillion (10^12) place-observation combinations. Luckily, most of the classes have a very low conditional probability given the data (x, y, time and accuracy). The major strategy on the forum to reduce the complexity consisted of calculating a separate classifier for many x-y rectangular grids. It makes much sense to make use of the spatial information since this shows the most obvious and strong pattern for the different places. This approach makes the complexity manageable but is likely to lose a significant amount of information since the data is so variable. I decided to model the problem with a single stacked two level binary classification model in order to avoid to end up with many high variance models. The lack of any major spatial patterns in the exploratory analysis supports this approach.

Generating a single classifier for all place-observation combinations would be impractical even with a powerful cluster. My approach consists of a stepwise strategy in which the place probability (the target class) conditional on the data is only modeled for a set of place candidates. A simplification of the overall strategy is shown below:

[caption id=""attachment_6056"" align=""aligncenter"" width=""1024""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/strategy-1024x593.png"" alt=""strategy"" width=""1024"" height=""593"" class=""size-large wp-image-6056"" /> The overall strategy used[/caption]

The given raw train data is split in two chronological parts, with a similar ratio as the ratio between the train and test data. The summary period contains all given train observations of the first 408 days (minutes 0-587158). The second part of the given train data contains the next 138 days and will be referred to as the train/validation data from now on. The test data spans 153 days as mentioned before.

The summary period is used to generate train and validation features and the given train data is used to generate the same features for the test data.

The three raw data groups (train, validation and test) are first sampled down into batches that are as large as possible but can still be modeled with the available memory. I ended up using batches of approximately 30,000 observations on a 48GB workstation. The sampling process is fully random and results in train/validation batches that span the entire 138 days’ train range.

Next, a set of models using 430 numeric features is built to reduce the number of candidates to 20 using 15 XGBoost models in the second candidate selection step. The conditional probability P(place_match|features) is modeled for all ~30,000*100 place-observation combinations and the mean predicted probability of the 15 models is used to select the top 20 candidates for each observation. These models use features that combine place and observation measures of the summary period.

The same features are used to generate the first level learners. Each of the 100 first level learners are again XGBoost models that are built using ~30,000*20 feature-place_match pairs. The predicted probabilities P(place_match|features) are used as features of the second level learners along with 21 manually selected features. The candidates are ordered using the mean predicted probabilities of the 30 second level XGBoost learners.

All models are built using different train batches. Local validation is used to tune the model hyperparameters.

<h3>What was your most important insight into the data?</h3>

I think I had a good insight into several of the accuracy related patterns. The accuracy distribution seems to be a mixed distribution with three peaks which changes over time. It is likely to be related to three different mobile connection types (GPS, Wi-Fi or cellular). The places show different accuracy patterns and features were added to indicate the relative accuracy group densities. The middle accuracy group was set to the 45-84 range. I added relative place densities for 3 and 32 approximately equally sized accuracy bins.

[caption id=""attachment_6053"" align=""aligncenter"" width=""799""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/accuracy_group.png"" alt=""accuracy groups"" width=""799"" height=""428"" class=""size-full wp-image-6053"" /> Mean variation from the median in x versus 6 time and 32 accuracy groups.[/caption]

It was also discovered that the location is related to the three accuracy groups for many places. This pattern was captured by the addition of additional features for the different accuracy groups. 

[caption id=""attachment_6055"" align=""aligncenter"" width=""1024""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/place_counts-1024x558.png"" alt=""Location and accuracy groups"" width=""1024"" height=""558"" class=""size-large wp-image-6055"" />The x-coordinates seem to be related to the accuracy group for places like 8170103882.[/caption]

Studying the places with the highest daily counts also pointed me towards obvious yearly patterns which were translated to valuable features. The green line in the image below goes back 52 weeks since the highest daily count.

[caption id=""attachment_6054"" align=""aligncenter"" width=""824""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/day_counts.png"" alt=""places with the highest daily counts"" width=""824"" height=""449"" class=""size-full wp-image-6054"" />Clear yearly pattern for place 5872322184. The green line goes back 52 weeks since the highest daily count.[/caption]

<h3>Were you surprised by any of your findings?</h3>

The strength of K nearest neighbors was remarkable in this problem. Nearest neighbor features make up a large share of my solution and the leading public script relied on the K nearest neighbor classifier. I was also surprised that I couldn’t find clear spatial patterns in the data (e.g. a party district). 

<h3>Which tools did you use?</h3>

All code was implemented in R and I created an <a href=""http://www.rcpp.org/"" target=""_blank"">Rcpp package</a> to address the major bottleneck using C++. The most important package I used was by far the <a href=""https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.pdf"" target=""_blank"">data.table package</a>. I was not familiar with the syntax heading into the competition but going through the trouble of learning it enabled me to handle the dimensions of the problem. Other critical tools are the <a href=""http://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html"" target=""_blank"">xgboost package</a> and the <a href=""https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf"" target=""_blank"">doParallel package</a>. The exploratory data analysis lead to a <a href=""https://tvdwiele.shinyapps.io/Facebook-V/"" target=""_blank"">Shiny application</a> which was shared with the other participants.

<h3>How did you spend your time on this competition?</h3>

I was forced to spend the first 10 days of the competition thinking about possible high level approaches due to other priorities and ended up with an approach that strongly resembled my final framework. The next 10 days were used to generate about 50 features and build the framework except for the second level learners. This intermediate result got me to the first spot on the public leaderboard and encouraged me to expand the feature set. I spent most of the remaining time on detailed feature engineering and started building the second tier of the binary classifier three weeks before the end of the contest. The last two weeks were mostly dedicated to hyperparameter optimization.

<h3>What was the run time for both training and prediction of your winning solution?</h3>

Running all steps to train the model and generate the final submission would take about a month on my 48GB workstation. That seems like a ridiculously long time but it is explained by the extended computation time of the nearest neighbor features. While calculating the NN features I was continuously working on other parts of the workflow so speeding the NN logic up would not have resulted in a better final score. 

<h2>Words of wisdom</h2>
<h3>What have you taken away from this competition?</h3>

I learned a lot from the technical issues I ran into but have learned most from the discussions on the forum. It is great to learn from brilliant people like <a href=""http://blog.kaggle.com/2016/08/02/facebook-v-predicting-check-ins-winners-interview-2nd-place-markus-kliegl/"" target=""_blank"">Markus</a>. The way he used semi-parametric learning to learn from the future was an eye-opener. Many others made significant contributions but it was especially useful to learn from <a href=""https://www.kaggle.com/c/facebook-v-predicting-check-ins/forums/t/22078/solution-sharing/128862#post128862"" target=""_blank"">Larry Freeman</a> and <a href=""https://www.kaggle.com/c/facebook-v-predicting-check-ins/forums/t/22663/facebook-results-dataset"" target=""_blank"">Ben Hamner</a> that we are better when we work together. An ensemble of top solutions can do much better than my winning submission!

[caption id=""attachment_6060"" align=""aligncenter"" width=""871""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/PrivateLB.png"" alt=""private leaderboard scores"" width=""871"" height=""428"" class=""size-full wp-image-6060"" /> Private leaderboard score (MAP@3) - two teams stand out from the pack.[/caption]

<h3>Do you have any advice for those just getting started in data science?</h3>

I would suggest to start with a study of various data science topics. <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Andrew Ng’s course</a> is an excellent place to start. Getting your hands dirty with appropriate feedback is the next step if you want to get better. Kaggle is of course an excellent platform to do so. I am very impressed with the quality and general atmosphere on the forum and would suggest everyone to start competing!

<h2>Bio</h2>
<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/tom_van_de_wiele-282x300.jpg"" alt=""tom_van_de_wiele"" width=""282"" height=""300"" class=""alignright size-medium wp-image-6057"" />
<a href=""https://www.kaggle.com/tvdwiele"" target=""_blank"">Tom Van de Wiele</a> recently completed his master of statistical data analysis at the University of Ghent. Tom has a background in computer science engineering and works in the data science group of Eastman as an Analytics Consultant where he works on various complex data challenges. His current interests lie in applied machine learning and statistics."
"Facebook V: Predicting Check Ins, Winner's Interview: 3rd Place, Ryuji Sakata",http://blog.kaggle.com/2016/08/18/facebook-v-predicting-check-ins-winners-interview-3rd-place-ryuji-sakata/,2016-08-18 17:12:00,"The <a href=""https://www.kaggle.com/c/facebook-v-predicting-check-ins"" target=""_blank"">Facebook recruitment challenge, Predicting Check Ins</a>, ran from May to July 2016 attracting over 1,000 competitors who made more than 15,000 submissions. Kagglers competed to predict a ranked list of most likely check-in places given a set of coordinates. Using just four variables, the real challenge was making sense of the enormous number of possible categories in this artificial 10km by 10km world. The third place winner, <a href=""https://www.kaggle.com/rsakata"" target=""_blank"">Ryuji Sakata, AKA Jack (Japan)</a>, describes in this interview how he tackled the problem using just a laptop with 8GB of RAM and two hours of run time.

<h2>The basics</h2>

<h3>What was your background prior to entering this challenge?</h3>

I'm working as a data scientist at an electronics company in Japan. My major at university (Aeronautics and Astronautics) is actually far from my current job, and I didn't have knowledge about data science at all. I got to know this field after starting work, and keep learning until now.

[caption id=""attachment_6068"" align=""aligncenter"" width=""950""]<a href=""https://www.kaggle.com/rsakata""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/ryuji.png"" alt=""Ryuji Sakata"" width=""950"" height=""452"" class=""size-full wp-image-6068"" /></a> <a href=""https://www.kaggle.com/rsakata"" target=""_blank"">Ryuji Sakata, AKA Jack (Japan)</a> on Kaggle.[/caption]

<h3>How did you get started competing on Kaggle?</h3>

I joined Kaggle about two and a half years ago in order to learn data mining in practice. The first competition I attended was <a href=""https://www.kaggle.com/c/allstate-purchase-prediction-challenge"" target=""_blank"">Allstate Purchase Prediction Challenge</a> and then I realized the fun of data mining. Since then, I spent much of my spare time on Kaggle. Competing with other Kagglers is always very exciting and it has been a good learning experience!

<h3>What made you decide to enter this competition?</h3>

This competition seemed to be something special compared with other challenges. The objective variable we had to predict has so many categories, and the number of variables we can use to predict is only 4! I was attracted to the concept that this competition was testing Kaggler's ingenious ideas.

<h2>Let's get technical</h2>

<h3>What preprocessing and supervised learning methods did you use?</h3>

My basic idea is very similar to that of <a href=""https://www.kaggle.com/mkliegl"" target=""_blank"">Markus Kliegl</a> as written in his <a href=""http://blog.kaggle.com/2016/08/02/facebook-v-predicting-check-ins-winners-interview-2nd-place-markus-kliegl/"" target=""_blank"">Winner's Interview</a>, which is Naive Bayes approach.

<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/naive_bayes-1024x131.png"" alt=""naive_bayes"" width=""1024"" height=""131"" class=""aligncenter size-large wp-image-6066"" />

What I had to do is to find place which maximizes above probability. Brief steps of my approach are as follows:

<ol>
<li>To narrow down candidate places for each record, I counted check-ins of each place using 100 x 200 grid, and places which have at least 2 check-ins are treated as candidates of each cell.</li>
<li>Estimate p(place_id) from past trend of check-in frequency by xgboost regression (refer to Figure A).</li>
<li>Estimate each distribution by using histogram (refer to Figure B).</li>
<li>Calculate the product of probabilities of all candidates for each record.</li>
<li>Select top 3 places which have high probability and create submission.</li>
</ol>

Regarding the second step, it seemed that time variation of check-in frequency is quite different between places, but I believe that some patterns of trend exists. So I decided to predict the number of future check-ins of each place from history of all places. The concept is as shown in the figure below.

[caption id=""attachment_6064"" align=""aligncenter"" width=""1024""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/concept-1024x782.png"" alt=""Concept"" width=""1024"" height=""782"" class=""size-large wp-image-6064"" /> Figure A[/caption]

Regarding the third step, I also illustrated the concept of estimation in the figure below.

[caption id=""attachment_6065"" align=""aligncenter"" width=""1024""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/estimation-1024x906.png"" alt=""Estimation"" width=""1024"" height=""906"" class=""size-large wp-image-6065"" /> Figure B[/caption]

There are some additional notes regarding the above figure:

<ul>
<li>After counting check-ins for each bin, counted values are smoothed by using neighbor bins.</li>
<li>About ""x"", ""y"", and ""accuracy"", the concept of time decay was introduced by multiplying weight when counting check-ins in the following manner.</li>
<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/time_decay-1024x131.png"" alt=""time_decay"" width=""1024"" height=""131"" class=""aligncenter size-large wp-image-6067"" />
<li>About ""y"", it seemed that distribution follows a normal distribution for many places, so I also estimated the center and standard deviation of places and calculated the probability from that.</li>
<li>About ""time of day"", ""day of week"", and ""accuracy"", small constant (pseudocount) is added after counting check-ins in order to avoid probability becoming zero.</li>
</ul>

Simple version of my solution is uploaded on my kernel <a href=""https://www.kaggle.com/rsakata/facebook-v-predicting-check-ins/3rd-place-solution-simple-version"" target=""_blank"">here</a>.

<h3>How did you spend your time on this competition?</h3>

The approach was almost fixed in the early stage of competition, and I conducted trials and errors many and many times to maximize the local validation score. I spent much time to optimize hyperparameters to estimate distributions, such as:

<ul>
	<li>The number of bins for each variable</li>
	<li>How to smooth the distribution (number of neighbors and weight)</li>
	<li>How to decay the counting weight according to time elapse</li>
	<li>What pseudocount should be added</li>
</ul>

At the end of competition, however, I couldn’t improve the score by changing the above parameters, so I shifted to improving the precision of p(place_id).

<h3>Which tools did you use?</h3>

I always use R for Kaggle competitions just because I am familiar with it. However, I would like to master python too in future.

<h3>What was the run time for both training and prediction of your winning solution?</h3>

My solution takes only about 2 hours by 8GB RAM laptop, and maybe this is the most prominent feature of my approach. Some other competitors seemed to apply xgboost for many small cells, and took very long time, but it was quite unbearable to me! I decided to focus on achieving both accuracy and speed. Thanks to that, I can conduct the trial and error method so many times.

<h2>Words of wisdom</h2>

<h3>What have you taken away from this competition?</h3>

Through this great competition, I gained a little more confidence in this field, but at the same time, I was surprised by other Kagglers' ideas which I would have never came up with. From this experience, I realized there are still many things I have to learn. So I would like to keep learning.

<h3>Do you have any advice for those just getting started in data science?</h3>

I am still a beginner in data science, but if there is one thing to say, “What one likes, one will do well.” For me, it is Kaggle. I always enjoy competing and communicating with other Kagglers!

<h2>Bio</h2>

<a href=""https://www.kaggle.com/rsakata"" target=""_blank"">Ryuji Sakata</a> works at Panasonic group as a data scientist. He has been involved in data science for about 3 years. He holds a master's degree in Aeronautics and Astronautics at Kyoto University.
"
Draper Satellite Image Chronology: Pure ML Solution | Damien Soukhavong,http://blog.kaggle.com/2016/09/08/draper-satellite-image-chronology-damien-soukhavong/,2016-09-08 16:08:23,"The <a href=""https://www.kaggle.com/c/draper-satellite-image-chronology"" target=""_blank"">Draper Satellite Image Chronology Competition (Chronos)</a> ran on Kaggle from April to June 2016. This competition, which was novel in a number of ways, challenged Kagglers to put order to time and space. That is, given a dataset of satellite images taken over the span of five days, the 424 brave competitors were required to determine their correct order. The challenge, which <a href=""http://www.draper.com/"" target=""_blank"">Draper hosted in order to <a href=""http://www.draper.com/news/solving-problems-satellite-data"" target=""_blank"">contribute to a deeper understanding of how to process and analyze images</a>, was a first for Kaggle--it allowed hand annotation as long as processes used were replicable.

While the winners of the competition used a mixture of machine learning, human intuition, and brute force, <a href=""https://www.kaggle.com/laurae2"" target=""_blank"">Damien Soukhavong (Laurae)</a>, a Competitions and Discussion Expert on Kaggle, explains in this interview how factors like the limited number of training samples which deterred others from using pure machine learning methods appealed to him. Read how he ingeniously minimized overfit by testing how his XGBoost solution generalized on new image sets he created himself. Ultimately his efforts led to an impressive leap in 242 positions from the public to private leaderboard.

<a href=""https://www.kaggle.com/c/draper-satellite-image-chronology""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/draper_lg.png"" alt=""Draper satellite images"" width=""650"" height=""200"" class=""aligncenter size-full wp-image-6148"" /></a>

<h2>The basics</h2>

<h3>What was your background prior to entering this challenge?</h3>

I hold an MSc in Auditing, Management Accounting, and Information Systems. I am self-taught in Data Science, Statistics, and Machine Learning since 2010. This autodidactism also helped me earn my MSc with distinctions with a thesis on data visualization. I worked with data for about 6 years from now. Although data science can be technical, I feel more with a creative and designing mind than a technical mind!

[caption id=""attachment_6147"" align=""aligncenter"" width=""948""]<a href=""https://www.kaggle.com/laurae2""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/laurae.png"" alt=""Damien Soukhavong (Laurae)"" width=""948"" height=""451"" class=""size-full wp-image-6147"" /></a> <a href=""https://www.kaggle.com/laurae2"" target=""_blank"">Damien Soukhavong (Laurae)</a> on Kaggle.[/caption]

<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>

I love manipulating images and generating code to process them, along with helping researchers making reproducible research in the image manipulation field. It helped me to look at convenient features that may at least generalize on unknown images unrelated to this competition.

<h3>How did you get started competing on Kaggle?</h3>

I found Kaggle randomly when a laboratory researcher asked me to look up for online competitions involving data. Kaggle delighted me quickly, as they provide different competition datasets, along with a convenient interface, straightforward to use even for newcomers. There are even tutorial competitions, which are like fast introductions to data science and machine learning techniques to get you productive immediately.

<h3>What made you decide to enter this competition?</h3>

Three reasons that may look as disadvantages to enter this competition:

<ul>
<li>It is an image-based competition, requiring preprocessing of images, along the selection of features which are not apparent at first sight.</li>
<li>It is about ordering images in time with a tiny dataset. Hence, one would not throw a Deep Learning model out of the box and expect it to work.</li>
<li>Overfitting is an issue ""thanks to"" the 70 training sets provided: I personally like leakage and overfitting issues, as fighting them is like avoiding the mine in a minefield (think: Minesweeper).</li>
</ul>

<h2>Let's get technical</h2>

<h3>What preprocessing and supervised learning methods did you use?</h3>

A visual overview of my methods is on the following picture:

[caption id=""attachment_6135"" align=""aligncenter"" width=""1754""]<a href=""http://blog.kaggle.com/wp-content/uploads/2016/09/image00.png""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image00.png"" alt=""A visual overview of methods"" width=""1754"" height=""1240"" class=""size-full wp-image-6135"" /></a> A visual overview of methods used. Click to open up a larger view.[/caption]

For the preprocessing method, I started by registering and masking each set of images, so it aligns them and so they contain the information that are mutual on all the image of a same set. Then, I used a feature extraction technique to harvest general describing features from each image. I generated all the permutations of each set of five images along with their theoretical performance metric, raising the training sample size to 8400 (120 permutations per set, 72 sets), and reducing the overfitting issue to a very residual issue. This also turned the (5-class) ranking problem into a regression problem.

<div class='tableauPlaceholder' id='viz1472412374932' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ka&#47;KaggleDraperDemoTableau10&#47;StoryExamplesofBivariateInteractions&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='site_root' value='' /><param name='name' value='KaggleDraperDemoTableau10&#47;StoryExamplesofBivariateInteractions' /><param name='tabs' value='yes' /><param name='toolbar' value='no' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ka&#47;KaggleDraperDemoTableau10&#47;StoryExamplesofBivariateInteractions&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='no' /><param name=""display_share"" value=""no""/></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1472412374932');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>
For the supervised learning method, I used Extreme Gradient Boosting (XGBoost) with a custom objective and custom evaluation function.

<h3>What was your most important insight into the data?</h3>

Hand labeling images was easy once you trained yourself to recognize the related features (think: the neural network in your brain), and I could go for a (near) perfect score if I wanted to. However, my interest was to use pure machine learning techniques, which are generalizing on unknown samples. Thus, I did not explore the manual way for too long.

A simple example for hand-labeling using objects: 

[caption id=""attachment_6144"" align=""aligncenter"" width=""899""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/stack_aligned_selection-900x676.gif"" alt=""Example of hand-labeling objects"" width=""899"" height=""676"" class=""size-full wp-image-6144"" /> Simple example for hand-labeling.[/caption]

I made a tutorial for recognizing the quantifying the apparition and removal of objects: 

[caption id=""attachment_6141"" align=""aligncenter"" width=""1024""]<a href=""https://www.kaggle.com/c/draper-satellite-image-chronology/forums/t/21209/tutorial-visualizing-difference-between-three-pictures""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/RGB_stack-1024x770.jpg"" alt=""Tutorial on competition forums."" width=""1024"" height=""770"" class=""size-large wp-image-6141"" /></a> A <a href=""https://www.kaggle.com/c/draper-satellite-image-chronology/forums/t/21209/tutorial-visualizing-difference-between-three-pictures"" target=""_blank"">tutorial</a> shared on the competition forums.[/caption]

<h3>Were you surprised by any of your findings?</h3>

I have found several interesting findings that surprised me:

<ul>
<li>Leave-one-out cross-validation (LOOCV) might be a good method to handle the tiny training set to validate a supervised machine learning model, however some image sets are leaking into others which make the cross-validation invalid right at the beginning!</li>

<li>Using the file size of pictures was leaking information out of the box… I believe one can get 0.30 (or more) using only the image file sizes with a simple predictive model.</li>

<li>Working with JPEG (1GB) pictures instead of TIFF (32GB) worked better than expected (I was expecting around 0.20 only).</li>

<li>Using a scoring model from the three highest scoring predictions gave a light boost on the performance metric. However, using predicted scores from all the 120 permuted sets per set to create the scoring model was giving out worse results than random predictions (overfitting).</li>
</ul>

This shows the model used can generalize to Draper's test sets. However, one must test the model in a real situation. Does it generalize? To test this hypothesis, I took two different image sets:

<ul>
<li>20 random locations in Google Earth where the satellite images were crystal clear, and day-to-day (100 pictures, 20 sets in total)</li>
<li>250 pictures from different areas I took myself when commuting during workdays (250 pictures, 50 sets)</li>
</ul>

On Google Earth, computing the performance metric from my predictions gave a score of 0.112, which is better than pure randomness. There is a slight bias towards a good prediction, though. Having access to the predictions and to the right order of the pictures, I assumed a uniform distribution of the predictions and ran a Monte-Carlo simulation over it. After 100,000 trials, here are the results:

[caption id=""attachment_6137"" align=""aligncenter"" width=""486""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image02.png"" alt=""Histogram showing results of Monte-Carlo simulation"" width=""486"" height=""262"" class=""size-full wp-image-6137"" /> Results of Monte-Carlo simulation.[/caption]

I will not say it is bad, but it is not that good either. The mean is 0.09, while the standard deviation is 0.07. Assuming a normal distribution, we have about 90% chance to predict better than random numbers. Reaching 0.442 (the maximum after 100,000 random tries) is clearly not straightforward.

Now coming for my personal pictures, I got... 0.048 only, which is disappointing. To ensure this is not a mistake, I ran the same Monte-Carlo simulation I used previously but on the new predictions:

[caption id=""attachment_6136"" align=""aligncenter"" width=""486""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image01.png"" alt=""Histogram showing results of second Monte-Carlo simulation"" width=""486"" height=""262"" class=""size-full wp-image-6136"" /> Results of second Monte-Carlo simulation[/caption]

Clearly, having 64% chance to predict better than random is on the low-end. The mean is 0.02, and the standard deviation is 0.07. With more samples to train the model on, overfitting should be less an issue than it is on non-aerial imagery.

<h3>Which tools did you use?</h3>

I used macros in <a href=""https://imagej.nih.gov/ij/"" target=""_blank"">ImageJ</a> for the feature extraction (more specifically: Fiji version, a supercharged ImageJ version), and R + XGBoost for the final preprocessing and supervised learning. When using XGBoost, I used my custom objective and evaluation functions to not only get the global performance metric, but also account for the ranking of predictions in the set (for voting which picture order is the most probable).

XGBoost was in version 0.47. The current XGBoost version 0.60 refuses to run my objective and evaluation functions properly.

<h3>How did you spend your time on this competition?</h3>

I spent about 20% of the time reading the threads on the forum, as they are a wealth of information you may not find yourself. 60% of the time was spent on preprocessing and feature engineering, and the 20% left on predictive modeling, validating, and submitting.

I also tested different models and features after my first idea (ironically, it consumed over 1 day):

<ul>
<li>Deep Learning gives no result, as it predicts random numbers and never seems converge even after 500 epochs (I tried data augmentation, image manipulation, and many architectures such as CaffeNet, GoogLeNet, VGG-16, ResNet-19, ResNet-50…).</li>
<li>Neural networks on the file sizes… are incredible as they abuse leakage. They will not generalize on a real scenario.</li>
<li>Random Forests are… overfitting severely and hard to control. They do overfit with enough noise, and this dataset is a good one for such issue (in pictures there can be… clouds!).</li>
<li>Data Visualization using Tableau, to lookup for the best interactions between features. It is clearly hard to notice the interactions, although XGBoost managed a great score for such hard task.</li>
</ul>

<h3>What was the run time for both training and prediction of your solution?</h3>

Early on entering the competition, I set a macro in ImageJ very quickly to extract features and to save them in a CSV file. It took about 1 hour, which allowed me to setup the final preprocessing and XGBoost code properly in R. Afterwards, it took only 30 minutes to make the code work properly, preprocess the data, train the model, and make my first submission. I spent 10 more minutes to cross-validate using five targeted folds by categorizing pictures by theme, and make a new submission (that was slightly better than the former).

In total, this took me about 1 hour and a half. Knowing my cross-validation method was correct, I did not care about seeing such low performing score on the public leaderboard with only 17% of testing samples (thanks to the forum threads). This ensured a push by over 242 places from the public to the private leaderboard (ending 32nd), the former being a sample feedback on unknown samples, and the latter being the one where we must maximize our performance (but we cannot see this private leaderboard until ending of the competition).

<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image04.jpg"" alt=""Rules acceptance"" width=""598"" height=""262"" class=""aligncenter size-full wp-image-6139"" />

<h2>Words of wisdom</h2>

<h3>The future of satellite imagery and technology will allow us to revisit locations with unprecedented frequency. Now that you've participated in this competition, what broader applications do you think there could be for images such as these?</h3>

There are many applications for satellite imagery and its associated technology. As a spatial reconstruction on a time dimension, one may:

<ul>
<li>Analyze the land usage over time</li>
<li>Plan the development of urban and rural areas</li>
<li>Manage resources and disasters in a better way (like analyzing the damage of a fire)</li>
<li>Work on a very large database usable for virtual reality (so you can feel how San Francisco looks from your own town for instance)</li>
<li>Initiate statistical studies</li>
<li>Regulate the environment (in a legal way)</li>
<li>Pinpoint tactical and safe areas for humans (moving hospitals, etc.)</li>
</ul>

Currently, there are satellites capable of recording things we cannot see as humans. One of the most known is <a href=""http://landsat.usgs.gov/"" target=""_blank"">Landsat</a>, whose pictures are usable for finding minerals or many other diverse elements (gas, oil...). Using data science and machine learning should give a hefty boom in predictive modeling from satellite imagery for businesses.

<h3>What have you taken away from this competition?</h3>

The benefits I have taken from this competition were: 

<ul>
<li>Working with a tiny training set, as there were only 70 training sets.</li>
<li>Fighting overfitting, as learning observations is easier than learning to generalize with such tiny training set!</li>
<li>Rationally transforming the ranking problem into a regression problem, a skill that requires regular practice and smart ideas.</li>
</ul>

A minor benefit was using ImageJ not for pure research, but for a data science competition. I was not expecting to use it at all. I provided an <a href=""https://www.kaggle.com/laurae2/draper-satellite-image-chronology/imagej-pre-processing-for-deep-learning"" target=""_blank"">example starter script</a> for using ImageJ in this competition:

[caption id=""attachment_6140"" align=""aligncenter"" width=""866""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/imageJ.png"" alt=""ImageJ starter script"" width=""866"" height=""411"" class=""size-full wp-image-6140"" /> Code from starter script for using ImageJ shared as a <a href=""https://www.kaggle.com/laurae2/draper-satellite-image-chronology/imagej-pre-processing-for-deep-learning"" target=""_blank"">competition kernel</a>.[/caption]

<h3>Do you have any advice for those just getting started in data science?</h3>

Several key pieces of advice for newcomers in data science:

<ul>
<li>I cannot say it enough times: efficiency is key.</li>
<li>Another important piece of advice: learn to tell stories (storytelling). Non-technical managers will not care about ""but my XGBoost model had 92% accuracy!"" when they will ask you the value to get from such model in their business environment. A good story is worth thousands of words in less than ten seconds!</li>
</ul>

More specific advice about predictive modeling for newcomers:

<ul>
<li>Knowing your features and understanding how to validate your models, allows you to fight overfitting and underfitting appropriately.</li>
<li>Understanding the ""business"" behind what you are doing in data science when working with a dataset has high value: domain knowledge is a starting key to success.</li>
<li>Creating (only) (good) predictive models in a business environment does not mean you are a good data scientist: statistical and business knowledge must be learnt a way or another.</li>
<li>Thinking you can go the XGBoost way in any industry is naïve. Try to explain all the interactions caught between [insert 100+ feature names] in a 100+ tree XGBoost model.</li>
<li>Your Manager will not like seeing you lurk for improving your model accuracy from 55.01% to 55.02% during one week of work. Where is the value?</li>
</ul>

N.B: Many ideas you may have can fail miserably. It is valuable to be brave and scrape what you did to start from scratch. Otherwise, you might stick forever the wrong way in a specific project, trying to push something you cannot push any further. Learning from mistakes is a key factor for self-improvement.

For pure supervised machine learning advice, three elements to always keep an eye on:

<ul>
<li>Choosing the appropriate way to validate a model depending on the dataset, and this comes with experience.</li>
<li>Looking up for potential leakage, as it may be what invalidates your model at the end when dealing with unknown samples.</li>
<li>Engineering features pushes your score higher than tuning hyperparameters in 99.99% of cases, unless you are using a horrible combination of hyperparameters right at the beginning.</li>
</ul>

<h2>Just for fun</h2>

<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>

I would run a data compression problem. It would open the way for finding novel methods to optimize the loss of information to a minimum, while decreasing the feature count to a bare minimum, using supervised methods.

<h3>What is your dream job?</h3>

Being a data science evangelist and managing talents!

<h2>Bio</h2>

<a href=""https://www.kaggle.com/laurae2""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image03.jpg"" alt=""Damien Soukhavong"" width=""203"" height=""206"" class=""alignleft size-full wp-image-6138"" /></a>
<a href=""https://www.kaggle.com/laurae2"" target=""_blank"">Damien Soukhavong</a> is a data science and an artificial intelligence trainer. Graduated from an MSc in Auditing, Accounting Management, and Information Systems, he seeks the maximization of value of data in companies using data science and machine learning, while looking for efficiency in performance. He mentors individually creative professionals and designers who are investing time to push data science for their daily creative/design job."
"Avito Duplicate Ads Detection, Winners' Interview: 1st Place Team, Devil Team | Stanislav Semenov & Dmitrii Tsybulevskii",http://blog.kaggle.com/2016/08/24/avito-duplicate-ads-detection-winners-interview-1st-place-team-devil-team-stanislav-dmitrii/,2016-08-24 16:01:31,"The <a href=""https://www.kaggle.com/c/avito-duplicate-ads-detection"" target=""_blank"">Avito Duplicate Ads Detection</a> competition ran from May to July 2016. This competition, a feature engineer's dream, challenged Kagglers to accurately detect duplicitous duplicate ads which included 10 million images and Russian language text. In this winners' interview, <a href=""https://www.kaggle.com/stasg7"" target=""_blank"">Stanislav Semenov</a> and <a href=""https://www.kaggle.com/u1234x1234"" target=""_blank"">Dmitrii Tsybulevskii</a> describe how their single XGBoost model scores among the top three and their ensemble snagged them first place. Stanislav's third Avito competition was a special one, too; his first place win as part of Devil Team boosted him to #1 Kaggler status!

<h2 id=""the-basics"">The basics:</h2>

<h3 id=""what-was-your-background-prior-to-entering-this-challenge"">What was your background prior to entering this challenge?</h3>

<p><strong>Dmitrii Tsybulevskii</strong>: I hold a degree in Applied Mathematics, and I’ve worked as a software engineer on computer vision and machine learning projects.

[caption id=""attachment_6109"" align=""alignright"" width=""950""]<a href=""https://www.kaggle.com/u1234x1234"" target=""_blank""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/dmitrii-1.png"" alt=""Dmitrii Tsybulevskii"" width=""950"" height=""452"" class=""size-full wp-image-6109"" /></a> <a href=""https://www.kaggle.com/u1234x1234"" target=""_blank"">Dmitrii Tsybulevskii</a> on Kaggle.[/caption]

<strong>Stanislav Semenov:</strong> I hold a Master's degree in Computer Science. I've worked as a data science consultant, teacher of machine learning classes, and quantitative researcher.</p>

[caption id=""attachment_6108"" align=""alignright"" width=""949""]<a href=""https://www.kaggle.com/stasg7"" target=""_blank""><img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/stanislav.png"" alt=""Stanislav Semenov"" width=""949"" height=""451"" class=""size-full wp-image-6108"" /></a> <a href=""https://www.kaggle.com/stasg7"" target=""_blank"">Stanislav Semenov</a> on Kaggle.[/caption]

<h3 id=""do-you-have-any-prior-experience-or-domain-knowledge-that-helped-you-succeed-in-this-competition"">Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>

<p><strong>Dmitrii Tsybulevskii</strong>: Yes, I’ve worked on image duplicate detection and text classification problems before, and I know the Russian language.

<strong>Stanislav Semenov:</strong> This is my 3rd Avito competition on Kaggle! And yes, I know the Russian language, too.</p>

<h3 id=""what-made-you-decide-to-enter-this-competition"">What made you decide to enter this competition?</h3>

<p><strong>Dmitrii Tsybulevskii</strong>: A lot of raw data, both text and images - large field for feature engineering, and I like feature engineering.

<strong>Stanislav Semenov:</strong> A large area for feature engineering.</p>

<h2 id=""lets-get-technical"">Let’s get technical:</h2>

<h3 id=""what-preprocessing-and-supervised-learning-methods-did-you-use"">What preprocessing and supervised learning methods did you use?</h3>

<p>It was all about feature engineering. So we tried to generate as many strong features as we could. XGBoost was the only learning method used. Our single XGBoost model can get to the top three! Our final model just averaged XGBoost models with different random seeds.</p>

We used following text preprocessing:
<ul>
 <li>stemming</li>

 <li>lemmatization</li>

 <li>transliteration</li>
</ul>

Our features:

<ul>
 <li>different similarity features between title-title, title-description, title-json like Cosine distance, Levenshtein, Jaccard, NCD, etc</li>

 <li>different features of exact match of words in title, description</li>

 <li>general features such as prices, places, number of images, exact match of title, description, etc</li>

 <li>different similarity features of trained w2v models</li>

 <li>LSI features of ads union, ads XOR</li>

 <li>one-hot-encoding of categoryID</li>

 <li>ratios of the title, description, json lengths</li>

 <li>distances between BRIEF image descriptors</li>

 <li>distances between color histogram in LAB space, HOG histograms</li>

 <li>distances between features, extracted with pretrained neural network <a href=""https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-21k-inception.md"" target=""_blank"">MXNet BN-Inception-21k</a>, first averaged PCA components of this features</li>

 <li>number of matches computed with AKAZE local visual feature detector &amp; descriptor</li>
</ul>

<p>The most important trick was to submit our best result before 2 hours of competition ending. That was EXTREMELY fun! =)</p>

<h3 id=""did-knowing-russian-help-you-in-this-competition-if-so-how"">Did knowing Russian help you in this competition? If so, how?</h3>

<p><strong>Stanislav Semenov:</strong> Not so much. Of course, you can see where your model is wrong and a close look at the ads. But it did not give any new information.

<strong>Dmitrii Tsybulevskii</strong>: On the one hand it was comfortable to work with Russian texts, because you know what the ads were about. On the other hand we had no killer features based on it.</p>

<h3 id=""which-tools-did-you-use"">Which tools did you use?</h3>

<p><a href=""http://jupyter.org/"" target=""_blank"">Jupyter Notebook</a>, <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a>, <a href=""http://pandas.pydata.org/"" target=""_blank"">Pandas</a>, <a href=""https://github.com/scikit-learn/scikit-learn/"" target=""_blank"">scikit-learn</a>, <a href=""https://github.com/vlfeat/vlfeat"">VLFeat</a>, <a href=""https://github.com/Itseez/opencv"" target=""_blank"">OpenCV</a>, <a href=""https://github.com/dmlc/mxnet/"" target=""_blank"">MXNet</a></p>

<h3 id=""what-was-the-run-time-for-your-winning-solution"">What was the run time for your winning solution?</h3>

<p>Feature extraction: 3-4 days
Model training: 1-2 weeks</p>

<h2 id=""words-of-wisdom"">Words of wisdom:</h2>

<h3 id=""what-have-you-taken-away-from-this-competition"">What have you taken away from this competition?</h3>

<p><strong>Dmitrii Tsybulevskii</strong>: I have learned about NCD distance and some convenient things about team cooperation.

<strong>Stanislav Semenov:</strong> A lot of fun and much needed ranking points. ;)</p>

<h3 id=""do-you-have-any-advice-for-those-just-getting-started-in-data-science"">Do you have any advice for those just getting started in data science?</h3>

<p><strong>Stanislav Semenov:</strong> Solving practical problems is your best friend.

<strong>Dmitrii Tsybulevskii</strong>: Kaggle is a great platform for getting new knowledge.</p>

<h2 id=""bio"">Bio</h2>

<a href=""https://www.linkedin.com/in/stanislav-semenov-83398072
"" target=""_blank"">Stanislav Semenov</a> is a Data Scientist and Quantitative Researcher.

<a href=""https://www.linkedin.com/in/dmitrii-tsybulevskii-8a911a11b"">Dmitrii Tsybulevskii</a> is a software engineer. He holds a degree in Applied Mathematics. His main interests are computer vision and machine learning.</p>"
"Avito Duplicate Ads Detection, Winners' Interview: 2nd Place, Team TheQuants | Mikel, Peter, Marios, & Sonny",http://blog.kaggle.com/2016/08/31/avito-duplicate-ads-detection-winners-interview-2nd-place-team-the-quants-mikel-peter-marios-sonny/,2016-08-31 15:16:47,"The <a href=""https://www.kaggle.com/c/avito-duplicate-ads-detection"" target=""_blank"">Avito Duplicate Ads competition</a> ran on Kaggle from May to July 2016. Over 600 competitors worked to feature engineer their way to the top of the leaderboard by identifying duplicate ads based on their contents: Russian language text and images. TheQuants, made up of Kagglers <a href=""https://www.kaggle.com/anokas"" target=""_blank"">Mikel</a>, <a href=""https://www.kaggle.com/thequants"" target=""_blank"">Peter</a>, <a href=""https://www.kaggle.com/kazanova"" target=""_blank"">Marios</a>, & <a href=""https://www.kaggle.com/sonnylaskar"" target=""_blank"">Sonny</a>, came in second place by generating features independently and combining their work into a powerful solution. 

In this interview, they describe the many features they used (including text and images, location, price, JSON attributes, and clustered rows) as well as those that ended up in the ""feature graveyard."" In the end, a total of 587 features were inputs to 14 models which were ensembled through the weighted rank average of random forest and XGBoost models. Read on to learn how they cleverly explored and defined their feature space to carefully avoid overfitting in this challenge.

<h2>The basics</h2>

<h3>What was your background prior to entering this challenge?</h3>
<p><b><a href=""https://www.kaggle.com/anokas"" target=""_blank"">Mikel Bober-Irizar</a>:</b> Past Predictive modelling competitions, financial predictions and medical diagnosis.</p>
<p><b><a href=""https://www.kaggle.com/thequants"" target=""_blank"">Peter Borrmann</a>:</b> Ph.D. in theoretical physics, research assistant professor as well as previous Kaggle experiences.</p>
<p><b><a href=""https://www.kaggle.com/kazanova"" target=""_blank"">Marios Michailidis</a>:</b> I am a Part-Time PhD student at<a href=""https://www.ucl.ac.uk/"" target=""_blank""> UCL </a>, data science manager at <a href=""https://www.dunnhumby.com/"" target=""_blank"">dunnhumby</a> and fervent <a href=""https://www.kaggle.com"" target=""_blank"">Kaggler</a>.  </p>
<p><b><a href=""https://www.kaggle.com/sonnylaskar"" target=""_blank"">Sonny Laskar</a>:</b> I am an Analytics Consulting Manager with <a href=""http://www.microland.com/"" target=""_blank"">Microland</a> working on implementing Big Data Solutions; mostly dealing with IT Operations data.</p>

<h3>How did you get started with Kaggle?</h3>

<p><b>Mikel Bober-Irizar</b>: I wanted to learn about machine learning and use that knowledge to compete in competitions.<br></p>
<p><b>Peter Borrmann</b>: I wanted to improve my skillset in the field.<br></p>
<p><b>Marios Michailidis</b>: I wanted a new challenge and learn from the best.<br></p>
<p><b>Sonny Laskar</b>: I got to know about Kaggle few years back when I was pursuing my MBA.<br></p>

[caption id=""attachment_6119"" align=""aligncenter"" width=""889""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/team.jpg"" alt=""The Quants"" width=""889"" height=""212"" class=""size-full wp-image-6119"" /> The Quants Team[/caption]

<h2>Summary</h2>

<p>Our approach to this competition was divided into several parts :</p>
<p>
	<ol>
		<li> Merging early based on standing of the leaderboard.</li>
		<li> Generate features independently (on cleaned or raw data) that would potentially capture the similarity between the contents of 2 ads and could be further divided to more categories (like text similarities or image similarities).</li>
		<li> Build a number of different classifiers and regressors independently with a hold out sample. </li>
		<li> Combine all members' work</li>		
		<li> Ensemble the results through weighted rank average of a 2-layer meta-model network ( <a href=""http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/"" target=""_blank"">StackNet</a>).</li>
	</ol>
</p>

[caption id=""attachment_6118"" align=""aligncenter"" width=""587""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/summary.jpg"" alt=""summary"" width=""587"" height=""381"" class=""size-full wp-image-6118"" />Approach summary.[/caption]

<h3>Data Cleaning and Feature Engineering</h3>

<strong><u>Data Cleaning</u></strong>

<p>In order to clean the text, we applied stemming using the NLTK Snowball Stemmer, and removed stopwords/punctuation as well as transforming to lowercase. In some situations we also removed non-alphanumeric characters too. </p>

<strong><u>Feature Engineering vol 1</u>: Features we actually used  </strong>

<p>In order to pre-emptively find over-fitting features, we built a script that looks at the changes in the properties (histograms and split purity) of a feature over time, which allowed us to quickly (200ms/feature) identify overfitting features without having to run overnight <a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a> jobs.</p>

<p>After removing overfitting features, our final feature space had 587 features derived from different themes:</p>

<p><u>General: </u></p>
	<ul>
		<li>CategoryID, parentCategoryID raw CategoryID, parentCategoryID one-hot (except overfitting ones).</li>
		<li>Price difference / mean.</li>
		<li>Generation3probability (output from model trained to detect generationmethod=3).</li>
	</ul>

<p><u>Location: </u></p>
	<ul>
		<li>LocationID & RegionID raw.</li>
		<li>Total latitude/longtitude.</li>
		<li>SameMetro, samelocation, same region etc.</li>
		<li>Distance from city centres (Kalingrad, Moscow, Petersburg, Krasnodar, Makhachkala, Murmansk, Perm, Omsk, Khabarovsk, Kluichi, Norilsk)
		Gaussian noise was added to the location features to prevent overfitting to specific locations, whilst allowing XGBoost to create its own regions.</li>
	</ul>

<p><u>All Text: </u></p>
	<ul>
		<li>Length / difference in length.</li>
		<li>nGrams Features (n = 1,2,3) for title and description (Both Words and Characters).</li>
		<ul>
			<li>Count of Ngrams (#, Sum, Diff, Max, Min).</li>
			<li>Length / difference in length.</li>
			<li>Count of Unique Ngrams.</li>
			<li>Ratio of Intersect Ngrams.</li>		
			<li>Ratio of Unique Intersect Ngrams.</li>
		</ul>
		<li>Distance Features between the titles and descriptions:</li>
		<ul>
			<li><a href=""https://en.wikipedia.org/wiki/Jaccard_index"" target=""_blank""> Jaccard </a></li>		
			<li><a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" target=""_blank""> Cosine </a></li>
			<li><a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" target=""_blank""> Levenshtein </a> </li>
			<li><a href=""https://en.wikipedia.org/wiki/Hamming_distance"" target=""_blank""> Hamming </a></li>		
		</ul>		 
		<li>Special Character Counting & Ratio Features:</li>		
		<ul>
			<li>Counting & Ratio features of Capital Letters in title and description.</li>
			<li>Counting & Ratio features of Special Letters (digits, punctuations, etc.) in title and description.</li>		
		</ul>	
		<li>Similarity between sets of words/characters.</li>
		<li><a href=""https://pypi.python.org/pypi/fuzzywuzzy"" target=""_blank"">Fuzzywuzzy</a> distances.</li>
		<li><a href=""https://pypi.python.org/pypi/jellyfish"" target=""_blank""> jellyfish </a> distances.</li>		
		<li>Number of overlapping sets of n words (n=1,2,3).</li>		
		<li>Matching moving windows of strings.</li>	
		<li>Cross-matching columns (eg. title1 with description2).</li>			
	</ul>

<p><u>Bag of words: </u></p>

<p>For each of the text columns, we created a bag of words for both the intersection of words and the difference in words and encoded these in a sparse format resulting in ~80,000 columns each. We then used this to build Naive Bayes, SGD and similar models to be used as features.</p>

<p><u>Price Features: </u></p>
	<ul>
		<li>Price Ratio.</li>
		<li>Is both/one price NaN.</li>
		<li>Total Price.</li>
	</ul>
	
<p><u>JSON Features: </u></p>
	<ul>
		<li>Attribute Counting Features.</li>
		<ul>
			<li>Sum, diff, max, min.</li>		
		</ul>		
		<li>Count of Common Attributes Names.</li>
		<li>Count of Common Attributes Values.</li>
		<li><a href=""http://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html"" target=""_blank""> Weights of Evidence </a> on keys/values, XGBoost model on sparse encoded attributes.</li>		
	</ul>	


<p><u>Image Features: </u></p>

	<ul>
		<li># of Images in each Set.</li>
		<li>Difference Hashing of images.</li>
		<li>Hamming distance between each pair of images.</li>
		<li>Pairwise comparison of file size of each image.</li>
		<li>Pairwise comparison of dimension of each image.</li>
		<li>BRISK keypoint/descriptor matching.</li>	
		
		<li>Image histogram comparisons.</li>
		<li>Dominant colour analysis.</li>
		<li>Uniqueness of images (how many other items have the same images).</li>		
		<li>Difference in number of images.</li>		
	</ul>

<p><u>Clusters: </u></p>
<p>
We found clusters of rows by grouping rows which contain the same items (eg. if row1 has items 123, 456 and row2 has items 456, 789 they are in the same cluster). We discovered that the size of these clusters was a very good feature (larger clusters were more likely to be non-duplicates), as well as the fact that clusters always the same generationMethod. Adding cluster-size features gave us a 0.003 to 0.004 improvement.
</p>

<strong><u>Feature Engineering vol 2</u> : The ones that did not make it</strong>

<p> <em>Overfitting</em> was probably the biggest problem throughout the competition, and lots of features which (over)performed in validation didn't do so well on the leaderboard. This is likely because the very powerful features learn to recognise specific products or sellers that do not appear in the test set. Hence, a feature graveyard was a necessary evil.</p>

<p><u>TF-IDF: </u></p>
<p>This was something we tried very early into the competition, adapting our code from the <a href=""https://www.kaggle.com/c/home-depot-product-search-relevance"" target=""_blank"">Home Depot competition</a>. Unfortunately, it overfitted very strongly, netting us 0.98 val-auc and only 0.89 on LB. We tried adding noise, reducing complexity, but in the end we gave up. </p>

<p><u>Word2vec: </u></p>
<p>We tried both training a model on our cleaned data and using the pretrained model posted in the forums. We tried using word-mover distance from our model as features, but they were rather weak (0.70AUC) so in the end we decided to drop these for simplicity. Using the pre-trained model did not help, as the authors used MyStem for stemming (which is not open-source) so we could not replicate their data cleaning. After doing some transformations on the pre-trained model to try and make it work with our stemming (we got it down to about 20% missing words), it scored the same as our custom word2vec model.</p>

<p><u>Advanced cluster features: </u></p>
<p>We tried to expand the gain from our cluster features in several ways. We found that taking the mean prediction for the cluster as well as cluster_size * (1-cluster_mean) provided excellent features in validation (50% of gain in xgb importance), however these overfitted. We also tried taking features such as the standard deviation of locations of items in a cluster, but these overfitted too.</p>

<p><u>Grammar features: </u></p>
<p>We tried building features to <em>fingerprint</em> different types of sellers, such as usage of capital letters, special characters, newlines, punctuation etc. However while these helped a lot in CV, they overfitted on the leaderboard.</p>

<p><u>Brand violations: </u></p>
<p>We built some features based around words that could never appear together in duplicate listings. (For example, if one item wrote 'iPhone 4s' but the other one wrote 'iPhone 5s', they could not be duplicates). While they worked well at finding non-duplicates, there were just too few cases where these violations occurred to make a difference to the score.</p>

<h3>Validation Scheme</h3>
<p>Initially, we were using a random validation set before switching to a set of non-overlapping items, where none of the items in the valset appeared in the train set. This performed somewhat better, however we had failed to notice that the training set was ordered based on time! We later noticed this (inspired by <a href=""https://www.kaggle.com/c/avito-duplicate-ads-detection/forums/t/20765/cv-and-lb-scores/119711#post119711"" target=""_blank"">this post</a>)  and switched to using last 33% as a valset.</p>

[caption id=""attachment_6121"" align=""aligncenter"" width=""308""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/validation.jpg"" alt=""validation scheme"" width=""308"" height=""241"" class=""size-full wp-image-6121"" /> Validation scheme.[/caption]


<p> This set correlated relatively well with the leaderboard until the last week, when we were doing meta-modelling and it fell apart - at a point where it would be too much work to switch to a better set. This hurt us a lot towards the end of the competition. </p>

<h3>Modelling</h3>

<strong><u>Modelling vol 1</u> : The ones that made it</strong>

<p> In this section we built various models (classifiers and regressors) on different input data each time (since the modelling process was overlapping with the feature engineering process. All models were training with the first 67% of the training data and validated on the remaining 33%. All predictions were saved (so that they can be used later for meta modelling. The most dominant models were: </p>

<p><u><a href=""https://github.com/dmlc/xgboost"" target=""_blank"">XGBoost</a>: </u></p>
<p> Trained with all 587 of our final features with 1000 estimators, maximum depth equal to 20 and minimum child of 10, and particularly high Eta (0.1) - bagged 5 times. We also replaced <em>nan</em> values with -1 and <em>Infinity</em> values with 99999.99. It scored <b>0.95143</b> on private leaderboard. Bagging added 0.00030 approximately.</p>

<p><u><a href=""http://keras.io/"" target=""_blank"">Keras</a>: Neural Network</u></p>

<p> Trained with all our final features, transformed with standard scaler as well as with logarithm plus 1, where all negative features have been replaced with zero. The main architecture involved 3 hidden layers with 800 hidden units plus 60% dropout. The main activation function was Softmax and all intermediate ones were standard rectifiers (Relu). We bagged it 10 times. It scored <b>0.94912</b> on private leaderboard. It gave +0.00080-90 when rank-averaged with the XGBoost model</p>

<strong><u>Modelling vol 2</u>: The ones that didn't </strong>

<p> We build a couple of deeper Xgboost models with higher Eta (0.2) that although performed well in cv, they overfitted the leaderboard.</p>

<p> We used a couple of models to predict generation method in order to use that as feature for meta-modelling but it did not add anything so we removed it.</p>

<h3>Meta-Modelling</h3>

<p>The previous modelling process generated 14 different models including linear models as well as XGBoosts and NNs, that were later used for meta-modelling </p>

<p> For validation purposes we splitted the remaining (33%) data again into 67-33 in order to tune the hyper parameters of our meta-models that used as input the aforementioned 14 models. <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" target=""_blank""> Sklearn's Random Forest</a> which performed slightly better than XGBoost (0.95290 vs 0.95286).  Their rank average yielded our best Leaderboard score of <b> 0.95294</b></p>

<p>The Modelling and Meta-Modelling process is also illustrated below : </p>

[caption id=""attachment_6120"" align=""aligncenter"" width=""699""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/the_quants_ensemble.png"" alt=""The Quants ensemble"" width=""699"" height=""497"" class=""size-full wp-image-6120"" />The Quants ensemble.[/caption]


<h2>Thanks</h2>
<p>Thanks to the competitors for the challenge, <a href=""https://www.kaggle.com/"">Kaggle</a> for hosting, <a href=""https://www.avito.ru/"">Avito </a> for organizing. Thanks to the open source community and the research that makes it all possible.</p>


<h2>Teamwork</h2>

<h3>How did your team form?</h3>
<p>Early on into the competition Peter & Sonny & Mikel formed a team as they held the top 3 spots at the time, and  decided to join forces to see how far they could go. Later on, Marios was spotted lurking at the bottom of the leaderboard, and was asked to join because of his extensive Kaggle experience.

<h3>How did your team work together?</h3>

<p>We were all quite independent, branching out and each working on our own features as there was lots of ground to cover, while also brainstorming and discussing ideas together. At the end we came together to consolidate everything into one featurespace and to build models for it. </p>

<h2>Bios</h2>

<p style=""clear:both;"">
<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/mikel.jpg"" alt=""Mikel Bober-Irizar"" align=""left"", style=""width:70px;height:70px;"" hspace=""10"">
<a href=""https://www.linkedin.com/in/mikel-bober-irizar-a46b7230"" target=""_blank"">Mikel Bober-Irizar </a> (<a href=""https://www.kaggle.com/anokas"">anokas</a>) is a young and ambitious Data Scientist and Machine Learning Enthusiast. He has been participating in various predictive modelling competitions, and has also developed algorithms for various problems, including financial prediction and medical diagnosis. Mikel is currently finishing his studies at Royal Grammar School, Guildford, UK, and plans to go on to study Math or Computer Science.

</p>
<br>

<p style=""clear:both;"">
<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/peter.jpg"" alt=""Peter Borrmann"" align=""left"", style=""width:70px;height:70px;"" hspace=""10"">
<a href="""">Priv.-Doz. Dr. Peter Borrmann</a> (<a href=""https://www.kaggle.com/thequants"" target=""_blank"">NoName</a>) is head of <a href=""http://the-quants.com/"" target=""_blank"">The Quants Consulting</a> focusing on quantitative modelling and strategy. Peter studied in Göttingen, Oldenburg and Bremen and has a Ph.D. in theoretical physics. He habilitated at the University of Oldenburg  where he worked six years as a research assistant professor. Before starting his own company Peter worked at IBM Business Consulting Services in different roles.
</p>

<p style=""clear:both;"">
<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/mariosmichailidis.png"" alt=""Marios Michailidis"" align=""left"", style=""width:70px;height:70px;"" hspace=""10""> 
<a href=""https://www.linkedin.com/in/mariosmichailidis"" target=""_blank"">Marios Michailidis</a> (<a href=""https://www.kaggle.com/kazanova"">KazAnova</a>) is Manager of Data Science at Dunnhumby and part-time PhD in machine learning at University College London (UCL) with a focus on improving recommender systems. He has worked in both marketing and credit sectors in the UK Market and has led many analytics projects with various themes including: Acquisition, Retention, Uplift, fraud detection, portfolio optimization and more. In his spare time he has created KazAnova, a GUI for credit scoring 100% made in Java. He is former <a href=""http://blog.kaggle.com/2016/02/10/profiling-top-kagglers-kazanova-new-1-in-the-world/"" target=""_blank""> Kaggle #1</a>.
</p>

<p style=""clear:both;"">
<img src=""http://blog.kaggle.com/wp-content/uploads/2016/08/sonny_pic.jpg"" alt=""Sonny Laskar"" align=""left"", style=""width:70px;height:70px;"" hspace=""10"">
<a href=""https://www.linkedin.com/in/sonnylaskar"" target=""_blank"">Sonny Laskar</a> (<a href=""https://www.kaggle.com/sonnylaskar"" target=""_blank"">Sonny Laskar</a>) is an Analytics Consulting Manager at <a href=""http://www.microland.com/"" target=""_blank"">Microland (India)</a> where he is building IT Operations Analytics platform. He has over eight years of experience spread across IT Infrastructure, Cloud and Machine learning. He holds an MBA from India's premiere B School <a href=""http://www.iimidr.ac.in/"" target=""_blank"">IIM, Indore</a>. He is an avid break dancer and loves solving logic puzzles.
</p>"
Draper Satellite Image Chronology: Pure ML Solution | Vicens Gaitan,http://blog.kaggle.com/2016/09/15/draper-satellite-image-chronology-machine-learning-solution-vicens-gaitan/,2016-09-15 16:16:15,"Can you put order to space and time? This was the challenge posed to competitors of the <a href=""https://www.kaggle.com/c/draper-satellite-image-chronology"" target=""_blank"">Draper Satellite Image Chronology Competition (Chronos)</a> which ran on Kaggle from April to June 2016. Over four-hundred Kagglers chose a path somewhere between man and machine to accurately determine the chronological order of satellite images taken over five day spans.

In collaboration with Kaggle, <a href=""http://www.draper.com/news/solving-problems-satellite-data"" target=""_blank"">Draper designed the competition</a> to stimulate the development of novel approaches to analyzing satellite imagery and other image-based datasets. In this spirit, competitors’ solutions were permitted to rely on hand annotations as long as their methodologies were replicable. And indeed the top-three winners used non-ML approaches.

In this interview, <a href=""https://www.kaggle.com/vicensgaitan"" targt=""_blank"">Vicens Gaitan</a>, a Competitions Master, describes how re-assembling the arrow of time was an irresistible challenge given his background in high energy physics. Having chosen the machine learning only path, Vicens spent about half of his time on image processing, specifically registration, and his remaining efforts went into building his XGBoost model which landed him well within the top 10%.

<h2>The basics</h2>
<h3>What was your background prior to entering this challenge?</h3>

I’d like to define myself as a high-energy physicist, but this was in the 90’s ... ooh, I’m talking about the past century! At that time we used neural networks to classify events coming from the detectors, and NN were cool. Currently I’m managing a team of data scientists in a software engineering firm, and working actively on machine learning projects. Now NN are again cool ☺

<h3>Do you have any prior experience or domain knowledge that helped you succeed in this competition?</h3>

In fact, no. It was a very good opportunity to learn about image processing.

<h3>How did you get started competing on Kaggle?</h3>

I discovered Kaggle through the <a href=""https://www.kaggle.com/c/higgs-boson"" target=""_blank"">Higgs Boson challenge</a>. Immediately I realized that competing on Kaggle is the most efficient way to follow the state-of-the art in machine learning, and compare our own methodologies against the best practitioners in the world.

<h3>What made you decide to enter this competition?</h3>

The “time arrow” is still an open problem in science.  It was interesting to see if a machine learning approach could say something interesting about it.  Moreover, I have always been fascinated by satellite imagery. I could not resist the temptation to enter the challenge!

<h2>Let's get technical</h2>
<h3>What preprocessing and supervised learning methods did you use?</h3>

From the start, I realize that the first thing to be done was to “register” the images: scale and rotate the images in order to match them, in the same way we do when building photo panoramas.

Because there was no available image registration package for R, I decide to build the process from scratch. And this was a lucky decision, because during this process I generated some features that were very useful for the learning task.
The registration process is described in detail in <a href=""https://www.kaggle.com/vicensgaitan/draper-satellite-image-chronology/image-registration-the-r-way/notebook"" target=""_blank"">this notebook</a>:


The main idea in the “registration” process is to find “interesting points” that can be identified across images. Those will be the so called keypoints.  A simple way to define keypoints is to look for “corners”, because a corner will continue to be a corner (approximately) if you rotate and scale the images.  

[caption id=""attachment_6162"" align=""aligncenter"" width=""857""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image04.png"" alt=""Left: Image gradient square blurred. Right: Keypoints (local maxima from left image)."" width=""857"" height=""495"" class=""size-full wp-image-6162"" /> Left: Image gradient square blurred. Right: Keypoints (local maxima from left image).[/caption]

Once you have the key-points, it is necessary to describe them in an economical way using few numbers, to allow finding the matches efficiently.  30x30 rotated patches around the key-points, subsampled to 9x9 seem to work well for the granularity of these images.

[caption id=""attachment_6161"" align=""aligncenter"" width=""671""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image03.png"" alt=""Example of keypoint descriptors"" width=""671"" height=""759"" class=""size-full wp-image-6161"" /> Example of keypoint descriptors.[/caption]

To do the match, k-nearest neighbors of descriptors comes into play and the magic of RANSAC to select the true matches over the noise.

[caption id=""attachment_6164"" align=""aligncenter"" width=""929""]<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image06.png"" alt=""Image key-points"" width=""929"" height=""549"" class=""size-full wp-image-6164"" /> In red, all the key-points. In green, the ones matching between images.[/caption]

Once we have the matching points, it is easy to fit a homomorphic transformation between the images, and then compare them and look for temporal differences.

<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image05.png"" alt=""image05"" width=""929"" height=""549"" class=""aligncenter size-full wp-image-6163"" />

Are you able to do that by hand? I’m not, so I take the machine learning path:  

The number of available classified images is low, so it seems more promising to build a model at keypoint level (we have one hundred per image) using the descriptor information, and then average them for each image.  Also it seems to be a good idea to predict temporal differences between keypoints, instead to predict directly de-position in the ordered sequence.

Let’s describe the procedure:

<strong>Feature engineering:</strong>  (this was the easiest one, because it was already done after the registration process)

<strong>a) Key point detection For each image:</strong> Detect keypoints. Generate descriptors: 30x30 (downsampled to 9x9) oriented patches for each keypoint

<strong>b) Inter-set registering:</strong> For all sets, for every couple of image: Identify common points using RANSAC and fit a homomorphic transformation. Keep values of the transformation and patches for the identified inliers.  The parameters of the homomorphic transformation will result in informative feature when comparing couples of images.

<strong>c) Extra set registering:</strong> Interestingly, the same idea can be used between images coming from different sets.  Sampling from the full database of descriptors, using knn, find candidates to neighbor sets, and then register every possible image from one set to every image to the second set. Look for “almost perfect” matching taking into account a combination of number of identified common keypoints and the rmse in the overlapping region as a matching value. This procedure automatically detects “neighbor sets”

<strong>d) Set Clustering:</strong> using the previous matching information build a graph with sets as nodes and edges between neighbor sets with weight proportional to the matching value. It is relatively easy to select couple of images in neighbor sets with “perfect match” (High number of inliers, and very low rmse in the intersecting area) The connected component of this graph gives the set clusters without need to explicitly georeference the images by hand. This saves a lot of manual work.

<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image01-1.png"" alt=""image01"" width=""787"" height=""715"" class=""aligncenter size-full wp-image-6159"" />

<strong>PATCH LEVEL MODEL (Siamese gradient boosting)</strong>

The model is trained over pairs of common keypoints in images of the same set, trying to predict the number of days between one image and the other (with sign). The features used are:

<em>Patch im1 (9x9), Patch im2 (9x9), coefficients from the homomorphic transformation (h1..h9) , number of inliers between both images and rmse in the overlapping region</em>


The model is trained with XGBoost to a reg:linear objective using 4-fold cross validation (assuring that images in the same cluster belongs to the same fold). Surprisingly, this model is able to discriminate the time arrow quite well.


<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image00-1.png"" alt=""image00"" width=""777"" height=""568"" class=""aligncenter size-full wp-image-6158"" />


<strong>IMAGE LEVEL MODEL</strong>

Average the contribution of all patches in an image. Patches with absolute value of  DeltaT below a certain threshold level (alpha) are discarded. If the Patch Level Model were “perfect” this would result in the average of difference in days from one image to the rest of images in the set, so the expected values for an ordered set will be -2.5,-1.5,0,1.5,2.5 respectively. In practice, the ordering is calculated by sorting this average contribution.

Additionally, the average is refined by <strong>adding the average of images from overlapping sets</strong> (using the graph previously defined) taking into account the number of inliers between both images and the rmse in the intersection of them, weighted with a parameter beta, and iterating until convergence.

The optimal alpha and beta are adjusted using public leader board feedback (the training set is not representative enough of the test set) and as expected, there was some overfitting and a dropping of five positions in the private leaderboard.

The local CV obtained is 0.83 +/-0.05.

The model scored 0.76786 in the public leaderboard and 0.69209 in the private. I’m convinced that with more training data, this methodology can get a score of.8x or even .9x.

<h3>What was your most important insight into the data?</h3>
Probably the fact that by comparing local keypoint descriptors it is possible to obtain some information about the deltaT between them. Not only the number of days, but also the sign. Of course, this is a very weak model, but ensembling for all keypoints in an image, and taking into account the deltaT obtained in the neighbor sets (selecting the image with “perfect match”).

<h3>Were you surprised by any of your findings?</h3>

Initially, I only used keypoints matched by RANSAC between couples of images. This implies that the descriptors are very similar. When I try to add all keypoints, the result doesn’t improve. This is telling us that the temporal information is coded in subtle variations of pixel intensities, and not big changes of macroscopic objects, like other presence or not of a car, for example.

In fact, the bigger surprise is that these descriptors are able to code some information about the time arrow

<h3>Which tools did you use?</h3>

I tried to develop the full pipeline in R. I used mainly the <a href=""https://cran.r-project.org/web/packages/imager/vignettes/gettingstarted.html"" target=""_blank"">“imager”</a> package for image processing and <a href=""http://xgboost.readthedocs.io/en/latest/"" target=""_blank"">“xgboost”</a> for the model. The “FNN” for fast k-nearest neighbors library was also very helpful because of its speed.

<h3>How did you spend your time on this competition? (For example: What proportion on feature engineering vs. machine learning?)</h3>

Half of the competition was devoted to developing the registration process.  After that, building the model was relatively fast.

<h3>What was the run time for both training and prediction of your winning solution? </h3>

The keypoint extraction and descriptor calculation takes about 2 hours on a 12 core machine for the full dataset. Registration (especially the extra-set one) is more CPU consuming and lasts for more than 8 hours for the full dataset.

Then, model training can be done in few hours (I’m doing 4-fold cross validation for selecting the hyperparameters).

<h2>Words of wisdom</h2>

<h3>The future of satellite imagery and technology will allow us to revisit locations with unprecedented frequency. Now that you've participated in this competition, what broader applications do you think there could be for images such as these?</h3>

After the surprising fact that it is possible to correlate time arrow with image information, why not to try to predict changes in other indicators (with changes in a time interval of several days) that can be weakly correlated with the images: 

Predict risk of failure of infrastructures, demographics, social behavior, level of wealth, happiness ... I’m just dreaming.

<h3>What have you taken away from this competition?</h3>
Knowledge in a previously unknown area. The fact that XGBoost can be useful for image challenges (you doubt it).  And the most important, a lot of fun.

<h3>Do you have any advice for those just getting started in data science?</h3>
The only way to learn is to try by yourself.  Don’t be afraid of problems in which you don’t have background.  There is a lot of very interesting resources out there (forums, code... ), but the only way is hands in.

<h2>Just for fun</h2>
<h3>If you could run a Kaggle competition, what problem would you want to pose to other Kagglers?</h3>

I think that the most interesting open problem is, given a dataset and a learner, try to predict the optimal set of hyperparameters for that combination of dataset-model.  I know, the problem is how to build a labeled set for this supervised problem...  Maybe reinforcement learning can be a way to do that, but wait a moment... what should be the optimal hyperparameters for the policy learner?...  It seems I’m entering in a kind of Goedelian loop...
Better forget it ;)  Why not to try to predict TV audiences based on time-stamp and EPG information?

<h3>What is your dream job?</h3>

I already have it ;)

<h2>Bio</h2>

<img src=""http://blog.kaggle.com/wp-content/uploads/2016/09/image02.jpg"" alt=""Vicens Gaitan"" width=""229"" height=""229"" class=""alignleft size-full wp-image-6160"" /><strong>Vicens Gaitan</strong> is R&D director in the Grupo AIA innovation area. He studied physics and received a PhD in Machine Learning applied to experimental High Energy Physics in 1993 with the ALEPH collaboration at CERN. Since 1992 he has worked at AIA in complex problem solving and algorithmic development applied to model estimation, simulation, forecasting and optimization, mainly in the energy, banking, telecommunication and retail sectors for big companies. He has experienced knowledge in advanced methodologies including machine learning, numerical calculations, game theory and graph analysis, using lab environments like Python or R, or in production code in C and Java."
